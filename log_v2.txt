==========
iteration 0
==========
weights [-0.08410756 -0.31708325 -0.40439986 -0.52150567 -0.58915789  0.3313151 ]
expeced value MDP LP -1.640679819826811
demonstration
[(32, 1), (33, 1), (34, 1), (35, 3), (43, 1), (44, 1), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.35857207 -0.39495472  0.31526552  0.63504148  0.1054895  -0.44904206]
w_map [-0.21384021 -0.58779255 -0.32074174 -0.52755414 -0.47538852  0.03986737] loglik -1.3862943707885478
accepted/total = 1670/3000 = 0.5566666666666666
-------
true weights [-0.08410756 -0.31708325 -0.40439986 -0.52150567 -0.58915789  0.3313151 ]
features
0 	1 	0 	4 	3 	4 	3 	2 	
4 	3 	3 	0 	4 	1 	3 	3 	
3 	2 	1 	4 	0 	3 	3 	2 	
4 	3 	1 	1 	4 	0 	3 	2 	
0 	4 	0 	0 	4 	2 	4 	3 	
2 	0 	1 	4 	0 	0 	1 	3 	
1 	1 	0 	2 	4 	1 	2 	0 	
2 	0 	4 	2 	0 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	<	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.88	-2.82	-2.53	-2.80	-2.68	-2.41	-2.85	-2.48	
-3.41	-2.85	-2.47	-2.23	-2.18	-1.84	-2.35	-2.09	
-2.85	-2.35	-1.97	-2.17	-1.61	-1.54	-2.05	-1.59	
-2.57	-2.17	-1.67	-1.60	-1.61	-1.03	-1.54	-1.20	
-2.00	-1.94	-1.36	-1.29	-1.22	-0.96	-1.06	-0.80	
-1.98	-1.59	-1.52	-1.22	-0.64	-0.56	-0.48	-0.28	
-2.12	-1.82	-1.52	-1.45	-1.06	-0.48	-0.16	0.24	
-2.09	-1.70	-1.63	-1.05	-0.66	-0.58	0.01	0.33	
map_weights [-0.21384021 -0.58779255 -0.32074174 -0.52755414 -0.47538852  0.03986737]
MAP reward
-0.21	-0.59	-0.21	-0.48	-0.53	-0.48	-0.53	-0.32	
-0.48	-0.53	-0.53	-0.21	-0.48	-0.59	-0.53	-0.53	
-0.53	-0.32	-0.59	-0.48	-0.21	-0.53	-0.53	-0.32	
-0.48	-0.53	-0.59	-0.59	-0.48	-0.21	-0.53	-0.32	
-0.21	-0.48	-0.21	-0.21	-0.48	-0.32	-0.48	-0.53	
-0.32	-0.21	-0.59	-0.48	-0.21	-0.21	-0.59	-0.53	
-0.59	-0.59	-0.21	-0.32	-0.48	-0.59	-0.32	-0.21	
-0.32	-0.21	-0.48	-0.32	-0.21	-0.48	-0.59	0.04	
Map policy
>	>	>	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7416052213224786
mean w [-0.10005659 -0.46528844 -0.26454004 -0.57898037 -0.33942331  0.03744625]
Mean policy from posterior
>	>	>	v	v	v	>	v	
v	>	>	v	v	v	<	v	
v	v	v	>	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.10	-0.47	-0.10	-0.34	-0.58	-0.34	-0.58	-0.26	
-0.34	-0.58	-0.58	-0.10	-0.34	-0.47	-0.58	-0.58	
-0.58	-0.26	-0.47	-0.34	-0.10	-0.58	-0.58	-0.26	
-0.34	-0.58	-0.47	-0.47	-0.34	-0.10	-0.58	-0.26	
-0.10	-0.34	-0.10	-0.10	-0.34	-0.26	-0.34	-0.58	
-0.26	-0.10	-0.47	-0.34	-0.10	-0.10	-0.47	-0.58	
-0.47	-0.47	-0.10	-0.26	-0.34	-0.47	-0.26	-0.10	
-0.26	-0.10	-0.34	-0.26	-0.10	-0.34	-0.47	0.04	
mean = 0.06367785306973106, map = 0.0788428120839113
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.06979831383442758, 0.06775029816820655, 0.06775029819602496, 0.0673203912849758, 0.06732039165363113
==========
iteration 1
==========
weights [-0.33233128 -0.19253504 -0.18998862 -0.66592242 -0.41271132  0.4501191 ]
expeced value MDP LP -1.6399212618541847
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[ 0.13587123  0.47922037 -0.51670966  0.31720838 -0.04563755  0.61821844]
w_map [-0.53988169 -0.26537349 -0.21297893 -0.6166493  -0.46096383  0.00079012] loglik -0.693147180532403
accepted/total = 1966/3000 = 0.6553333333333333
-------
true weights [-0.33233128 -0.19253504 -0.18998862 -0.66592242 -0.41271132  0.4501191 ]
features
1 	3 	4 	1 	4 	2 	0 	2 	
2 	0 	0 	4 	0 	4 	2 	0 	
2 	4 	2 	1 	0 	4 	4 	3 	
1 	0 	1 	3 	3 	4 	2 	0 	
1 	2 	2 	2 	1 	3 	2 	4 	
1 	3 	1 	3 	1 	2 	2 	3 	
2 	3 	0 	3 	1 	4 	3 	1 	
2 	3 	2 	3 	0 	3 	0 	5 	
optimal policy
v	v	v	v	>	>	v	v	
v	>	v	v	>	>	v	<	
v	>	v	<	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
optimal values
-2.54	-3.12	-2.56	-2.59	-2.43	-2.03	-1.86	-2.03	
-2.37	-2.48	-2.17	-2.42	-2.26	-1.94	-1.55	-1.86	
-2.20	-2.25	-1.86	-2.03	-2.08	-1.77	-1.37	-1.80	
-2.03	-2.00	-1.69	-1.98	-1.81	-1.37	-0.97	-1.15	
-1.86	-1.68	-1.51	-1.33	-1.15	-1.44	-0.79	-0.82	
-2.03	-2.33	-1.69	-1.63	-0.97	-0.79	-0.60	-0.42	
-2.20	-2.63	-1.98	-1.66	-1.01	-0.82	-0.42	0.25	
-2.37	-2.36	-1.71	-1.54	-0.88	-0.55	0.11	0.45	
map_weights [-0.53988169 -0.26537349 -0.21297893 -0.6166493  -0.46096383  0.00079012]
MAP reward
-0.27	-0.62	-0.46	-0.27	-0.46	-0.21	-0.54	-0.21	
-0.21	-0.54	-0.54	-0.46	-0.54	-0.46	-0.21	-0.54	
-0.21	-0.46	-0.21	-0.27	-0.54	-0.46	-0.46	-0.62	
-0.27	-0.54	-0.27	-0.62	-0.62	-0.46	-0.21	-0.54	
-0.27	-0.21	-0.21	-0.21	-0.27	-0.62	-0.21	-0.46	
-0.27	-0.62	-0.27	-0.62	-0.27	-0.21	-0.21	-0.62	
-0.21	-0.62	-0.54	-0.62	-0.27	-0.46	-0.62	-0.27	
-0.21	-0.62	-0.21	-0.62	-0.54	-0.62	-0.54	0.00	
Map policy
v	<	v	>	>	v	v	v	
v	v	v	v	>	>	v	<	
v	>	v	v	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	^	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.6172481635543938
mean w [-0.53122545 -0.19445709 -0.11985764 -0.49792798 -0.37073332  0.10444133]
Mean policy from posterior
v	<	v	>	>	v	v	v	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	>	v	
^	<	^	>	>	>	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.19	-0.50	-0.37	-0.19	-0.37	-0.12	-0.53	-0.12	
-0.12	-0.53	-0.53	-0.37	-0.53	-0.37	-0.12	-0.53	
-0.12	-0.37	-0.12	-0.19	-0.53	-0.37	-0.37	-0.50	
-0.19	-0.53	-0.19	-0.50	-0.50	-0.37	-0.12	-0.53	
-0.19	-0.12	-0.12	-0.12	-0.19	-0.50	-0.12	-0.37	
-0.19	-0.50	-0.19	-0.50	-0.19	-0.12	-0.12	-0.50	
-0.12	-0.50	-0.53	-0.50	-0.19	-0.37	-0.50	-0.19	
-0.12	-0.50	-0.12	-0.50	-0.53	-0.50	-0.53	0.10	
mean = 0.015266636610738127, map = 0.011529385506224177
CVaR policy
v	>	v	>	>	v	v	v	
v	v	v	v	>	>	v	<	
v	>	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
^	>	>	>	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	v	>	v	v	<	
v	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
^	^	>	>	>	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	<	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	v	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	<	
v	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	>	v	
^	^	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	v	v	
^	<	^	>	>	>	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.02724004964527138, 0.01742845972738416, 0.009281132404989734, 0.014727534469431092, 0.015266636608362916
==========
iteration 2
==========
weights [-0.17656382 -0.09591669 -0.40970874 -0.69359356 -0.55654522  0.03081159]
expeced value MDP LP -2.1813044872200917
demonstration
[(32, 1), (33, 1), (34, 1), (35, 3), (43, 1), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.09232946 -0.10218565  0.1584452  -0.38815849 -0.8670476  -0.2312789 ]
w_map [-0.21642923 -0.21621852 -0.6815777  -0.52853149 -0.38246617  0.12741217] loglik -0.6931471777995313
accepted/total = 2033/3000 = 0.6776666666666666
-------
true weights [-0.17656382 -0.09591669 -0.40970874 -0.69359356 -0.55654522  0.03081159]
features
3 	4 	4 	2 	3 	2 	2 	2 	
4 	4 	3 	3 	2 	1 	4 	0 	
3 	0 	1 	4 	3 	4 	3 	3 	
3 	0 	4 	4 	2 	0 	0 	4 	
1 	4 	1 	1 	0 	3 	4 	3 	
2 	1 	3 	0 	4 	0 	1 	0 	
1 	4 	2 	3 	2 	2 	2 	4 	
3 	0 	4 	4 	0 	1 	4 	5 	
optimal policy
v	v	v	>	>	v	<	v	
>	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.30	-3.64	-3.63	-3.70	-3.33	-2.66	-3.04	-3.13	
-3.64	-3.11	-3.10	-3.33	-2.66	-2.27	-2.71	-2.75	
-3.25	-2.58	-2.43	-2.82	-2.73	-2.20	-2.18	-2.60	
-3.10	-2.51	-2.36	-2.28	-2.05	-1.66	-1.50	-1.93	
-2.43	-2.36	-1.82	-1.74	-1.66	-1.64	-1.33	-1.38	
-2.80	-2.41	-2.34	-1.66	-1.50	-0.95	-0.79	-0.70	
-2.64	-2.57	-2.26	-1.87	-1.19	-1.02	-0.93	-0.53	
-2.71	-2.04	-1.88	-1.34	-0.79	-0.62	-0.53	0.03	
map_weights [-0.21642923 -0.21621852 -0.6815777  -0.52853149 -0.38246617  0.12741217]
MAP reward
-0.53	-0.38	-0.38	-0.68	-0.53	-0.68	-0.68	-0.68	
-0.38	-0.38	-0.53	-0.53	-0.68	-0.22	-0.38	-0.22	
-0.53	-0.22	-0.22	-0.38	-0.53	-0.38	-0.53	-0.53	
-0.53	-0.22	-0.38	-0.38	-0.68	-0.22	-0.22	-0.38	
-0.22	-0.38	-0.22	-0.22	-0.22	-0.53	-0.38	-0.53	
-0.68	-0.22	-0.53	-0.22	-0.38	-0.22	-0.22	-0.22	
-0.22	-0.38	-0.68	-0.53	-0.68	-0.68	-0.68	-0.38	
-0.53	-0.22	-0.38	-0.38	-0.22	-0.22	-0.38	0.13	
Map policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.955000171426227
mean w [-0.14738582 -0.18056709 -0.52326234 -0.55852062 -0.3247142  -0.27187921]
Mean policy from posterior
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.56	-0.32	-0.32	-0.52	-0.56	-0.52	-0.52	-0.52	
-0.32	-0.32	-0.56	-0.56	-0.52	-0.18	-0.32	-0.15	
-0.56	-0.15	-0.18	-0.32	-0.56	-0.32	-0.56	-0.56	
-0.56	-0.15	-0.32	-0.32	-0.52	-0.15	-0.15	-0.32	
-0.18	-0.32	-0.18	-0.18	-0.15	-0.56	-0.32	-0.56	
-0.52	-0.18	-0.56	-0.15	-0.32	-0.15	-0.18	-0.15	
-0.18	-0.32	-0.52	-0.56	-0.52	-0.52	-0.52	-0.32	
-0.56	-0.15	-0.32	-0.32	-0.15	-0.18	-0.32	-0.27	
mean = 0.02600814801067397, map = 0.010232757614738741
CVaR policy
>	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.004782135442686464, 0.018853546879299188, 0.026008159273617437, 0.026008147337476917, 0.026008147356867628
==========
iteration 3
==========
weights [-0.19171245 -0.31367861 -0.54062804 -0.33488767 -0.23603901  0.63616761]
expeced value MDP LP -1.1948997534909562
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[ 0.78819635 -0.05612973  0.48654819 -0.00899663 -0.3644358   0.07728154]
w_map [-0.17717941 -0.44838908 -0.67286682 -0.44713681 -0.33270473 -0.06466195] loglik -2.0794415404886735
accepted/total = 1538/3000 = 0.5126666666666667
-------
true weights [-0.19171245 -0.31367861 -0.54062804 -0.33488767 -0.23603901  0.63616761]
features
3 	2 	4 	2 	0 	4 	3 	4 	
0 	0 	1 	0 	0 	0 	3 	0 	
4 	3 	4 	2 	0 	4 	0 	0 	
4 	3 	4 	4 	1 	1 	0 	1 	
3 	3 	1 	1 	3 	1 	2 	4 	
3 	3 	1 	2 	1 	4 	0 	0 	
1 	3 	0 	1 	3 	2 	0 	3 	
4 	1 	3 	2 	1 	4 	3 	5 	
optimal policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.54	-2.58	-2.10	-2.11	-1.59	-1.45	-1.37	-1.05	
-2.23	-2.06	-1.89	-1.59	-1.41	-1.23	-1.15	-0.82	
-2.44	-2.23	-1.91	-1.76	-1.23	-1.05	-0.82	-0.64	
-2.23	-2.01	-1.69	-1.47	-1.25	-0.94	-0.64	-0.45	
-2.21	-1.89	-1.57	-1.27	-0.97	-0.64	-0.63	-0.14	
-2.02	-1.71	-1.38	-1.17	-0.64	-0.33	-0.09	0.10	
-1.70	-1.41	-1.08	-0.90	-0.59	-0.44	0.10	0.29	
-1.65	-1.43	-1.12	-0.80	-0.26	0.06	0.29	0.64	
map_weights [-0.17717941 -0.44838908 -0.67286682 -0.44713681 -0.33270473 -0.06466195]
MAP reward
-0.45	-0.67	-0.33	-0.67	-0.18	-0.33	-0.45	-0.33	
-0.18	-0.18	-0.45	-0.18	-0.18	-0.18	-0.45	-0.18	
-0.33	-0.45	-0.33	-0.67	-0.18	-0.33	-0.18	-0.18	
-0.33	-0.45	-0.33	-0.33	-0.45	-0.45	-0.18	-0.45	
-0.45	-0.45	-0.45	-0.45	-0.45	-0.45	-0.67	-0.33	
-0.45	-0.45	-0.45	-0.67	-0.45	-0.33	-0.18	-0.18	
-0.45	-0.45	-0.18	-0.45	-0.45	-0.67	-0.18	-0.45	
-0.33	-0.45	-0.45	-0.67	-0.45	-0.33	-0.45	-0.06	
Map policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4091139185094275
mean w [-0.11988209 -0.22880543 -0.69108923 -0.31200925 -0.32301712  0.1974304 ]
Mean policy from posterior
v	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	v	v	
>	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.31	-0.69	-0.32	-0.69	-0.12	-0.32	-0.31	-0.32	
-0.12	-0.12	-0.23	-0.12	-0.12	-0.12	-0.31	-0.12	
-0.32	-0.31	-0.32	-0.69	-0.12	-0.32	-0.12	-0.12	
-0.32	-0.31	-0.32	-0.32	-0.23	-0.23	-0.12	-0.23	
-0.31	-0.31	-0.23	-0.23	-0.31	-0.23	-0.69	-0.32	
-0.31	-0.31	-0.23	-0.69	-0.23	-0.32	-0.12	-0.12	
-0.23	-0.31	-0.12	-0.23	-0.31	-0.69	-0.12	-0.31	
-0.32	-0.23	-0.31	-0.69	-0.23	-0.32	-0.31	0.20	
mean = 0.049712454146175755, map = 0.007524863419744232
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	>	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	v	v	>	v	
^	^	^	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	^	^	>	>	>	>	.	
cvar = , 0.006273431872730217, 0.005292380034839006, 0.0052923803093649635, 0.04971245289027104, 0.04971245295274107
==========
iteration 4
==========
weights [-0.28507528 -0.44883582 -0.47654705 -0.19042536 -0.65668186  0.15062708]
expeced value MDP LP -2.4888440735753576
demonstration
[(32, 1), (33, 1), (34, 3), (42, 3), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.09616497  0.18821462  0.01255049  0.89314614  0.32911867 -0.22167751]
w_map [-0.21943649 -0.49305117 -0.44129808 -0.33263139 -0.63510364  0.00197353] loglik -0.6931471943407104
accepted/total = 1870/3000 = 0.6233333333333333
-------
true weights [-0.28507528 -0.44883582 -0.47654705 -0.19042536 -0.65668186  0.15062708]
features
1 	1 	4 	0 	2 	3 	4 	2 	
2 	4 	2 	4 	3 	4 	1 	4 	
1 	1 	2 	0 	1 	2 	1 	4 	
2 	4 	1 	1 	3 	3 	2 	0 	
0 	2 	0 	4 	1 	4 	2 	2 	
1 	2 	0 	4 	2 	1 	0 	1 	
2 	0 	2 	3 	2 	1 	1 	4 	
3 	2 	3 	0 	0 	2 	1 	5 	
optimal policy
v	>	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.90	-4.61	-4.20	-3.58	-3.33	-3.37	-3.43	-3.41	
-4.49	-4.40	-3.81	-3.51	-2.88	-3.21	-2.81	-2.96	
-4.06	-3.78	-3.36	-2.98	-2.72	-2.58	-2.38	-2.33	
-3.64	-3.54	-2.92	-2.72	-2.29	-2.12	-1.95	-1.69	
-3.20	-2.94	-2.49	-2.78	-2.35	-2.10	-1.49	-1.42	
-3.01	-2.68	-2.23	-2.14	-1.92	-1.46	-1.02	-0.95	
-2.59	-2.23	-1.96	-1.50	-1.52	-1.19	-0.75	-0.51	
-2.13	-1.96	-1.50	-1.33	-1.05	-0.77	-0.30	0.15	
map_weights [-0.21943649 -0.49305117 -0.44129808 -0.33263139 -0.63510364  0.00197353]
MAP reward
-0.49	-0.49	-0.64	-0.22	-0.44	-0.33	-0.64	-0.44	
-0.44	-0.64	-0.44	-0.64	-0.33	-0.64	-0.49	-0.64	
-0.49	-0.49	-0.44	-0.22	-0.49	-0.44	-0.49	-0.64	
-0.44	-0.64	-0.49	-0.49	-0.33	-0.33	-0.44	-0.22	
-0.22	-0.44	-0.22	-0.64	-0.49	-0.64	-0.44	-0.44	
-0.49	-0.44	-0.22	-0.64	-0.44	-0.49	-0.22	-0.49	
-0.44	-0.22	-0.44	-0.33	-0.44	-0.49	-0.49	-0.64	
-0.33	-0.44	-0.33	-0.22	-0.22	-0.44	-0.49	0.00	
Map policy
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0688133466069107
mean w [-0.13437226 -0.46883209 -0.28346902 -0.26047743 -0.5169496  -0.0694556 ]
Mean policy from posterior
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	<	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.47	-0.47	-0.52	-0.13	-0.28	-0.26	-0.52	-0.28	
-0.28	-0.52	-0.28	-0.52	-0.26	-0.52	-0.47	-0.52	
-0.47	-0.47	-0.28	-0.13	-0.47	-0.28	-0.47	-0.52	
-0.28	-0.52	-0.47	-0.47	-0.26	-0.26	-0.28	-0.13	
-0.13	-0.28	-0.13	-0.52	-0.47	-0.52	-0.28	-0.28	
-0.47	-0.28	-0.13	-0.52	-0.28	-0.47	-0.13	-0.47	
-0.28	-0.13	-0.28	-0.26	-0.28	-0.47	-0.47	-0.52	
-0.26	-0.28	-0.26	-0.13	-0.13	-0.28	-0.47	-0.07	
mean = 0.03464084037364845, map = 0.014748629402913327
CVaR policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	<	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	<	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.11663683864322794, 0.08809987181004253, 0.014748631527474743, 0.0346408526129145, 0.03464083914955163
==========
iteration 5
==========
weights [-0.29346291 -0.17950379 -0.77916458 -0.47179135 -0.1487552   0.17275788]
expeced value MDP LP -1.6481470730449301
demonstration
[(32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.03204997 -0.02574625 -0.57804975  0.50055726 -0.51890099  0.37993764]
w_map [-0.64730944 -0.14155558 -0.60770781 -0.38617871 -0.20389384  0.030611  ] loglik -1.38629430898213
accepted/total = 1966/3000 = 0.6553333333333333
-------
true weights [-0.29346291 -0.17950379 -0.77916458 -0.47179135 -0.1487552   0.17275788]
features
1 	1 	0 	0 	0 	3 	3 	4 	
2 	3 	4 	2 	2 	4 	3 	2 	
4 	0 	0 	2 	4 	1 	0 	3 	
0 	2 	1 	1 	0 	1 	2 	0 	
0 	4 	4 	4 	2 	3 	2 	1 	
0 	4 	2 	3 	3 	2 	3 	0 	
4 	4 	1 	4 	0 	1 	1 	0 	
1 	2 	0 	2 	2 	1 	0 	5 	
optimal policy
>	>	v	<	>	v	v	v	
v	>	v	<	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	<	>	>	v	
>	v	<	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
optimal values
-2.77	-2.61	-2.46	-2.73	-2.64	-2.37	-2.53	-2.23	
-2.93	-2.64	-2.19	-2.95	-2.67	-1.91	-2.08	-2.11	
-2.17	-2.33	-2.06	-2.43	-1.91	-1.78	-1.62	-1.34	
-2.04	-2.25	-1.78	-1.67	-1.95	-1.81	-1.65	-0.88	
-1.77	-1.49	-1.62	-1.50	-2.00	-1.71	-1.36	-0.59	
-1.63	-1.35	-1.85	-1.37	-1.23	-1.25	-0.77	-0.41	
-1.35	-1.22	-1.08	-0.91	-0.77	-0.48	-0.30	-0.12	
-1.52	-1.98	-1.36	-1.68	-1.08	-0.30	-0.12	0.17	
map_weights [-0.64730944 -0.14155558 -0.60770781 -0.38617871 -0.20389384  0.030611  ]
MAP reward
-0.14	-0.14	-0.65	-0.65	-0.65	-0.39	-0.39	-0.20	
-0.61	-0.39	-0.20	-0.61	-0.61	-0.20	-0.39	-0.61	
-0.20	-0.65	-0.65	-0.61	-0.20	-0.14	-0.65	-0.39	
-0.65	-0.61	-0.14	-0.14	-0.65	-0.14	-0.61	-0.65	
-0.65	-0.20	-0.20	-0.20	-0.61	-0.39	-0.61	-0.14	
-0.65	-0.20	-0.61	-0.39	-0.39	-0.61	-0.39	-0.65	
-0.20	-0.20	-0.14	-0.20	-0.65	-0.14	-0.14	-0.65	
-0.14	-0.61	-0.65	-0.61	-0.61	-0.14	-0.65	0.03	
Map policy
>	v	v	>	>	v	v	v	
>	>	v	>	v	v	<	v	
v	v	v	>	>	v	<	v	
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
expeced value MDP LP -1.779100490357181
mean w [-0.35430952 -0.18514189 -0.56316804 -0.45252303 -0.17920965  0.16583964]
Mean policy from posterior
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.19	-0.19	-0.35	-0.35	-0.35	-0.45	-0.45	-0.18	
-0.56	-0.45	-0.18	-0.56	-0.56	-0.18	-0.45	-0.56	
-0.18	-0.35	-0.35	-0.56	-0.18	-0.19	-0.35	-0.45	
-0.35	-0.56	-0.19	-0.19	-0.35	-0.19	-0.56	-0.35	
-0.35	-0.18	-0.18	-0.18	-0.56	-0.45	-0.56	-0.19	
-0.35	-0.18	-0.56	-0.45	-0.45	-0.56	-0.45	-0.35	
-0.18	-0.18	-0.19	-0.18	-0.35	-0.19	-0.19	-0.35	
-0.19	-0.56	-0.35	-0.56	-0.56	-0.19	-0.35	0.17	
mean = 0.04762471003794788, map = 0.14740178750517452
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
cvar = , 0.07450032933366146, 0.06096054853611377, 0.047624709750824445, 0.047624711484034465, 0.047624711861889324
==========
iteration 6
==========
weights [-0.41968772 -0.19150613 -0.43021903 -0.38564015 -0.19455769  0.64461476]
expeced value MDP LP -1.1838108390563458
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.38989768  0.35839661  0.20512755 -0.21606387 -0.26851249  0.74744351]
w_map [-0.61647338 -0.26608768 -0.53330149 -0.35357586 -0.35936709 -0.1028924 ] loglik -7.335643203987274e-11
accepted/total = 1836/3000 = 0.612
-------
true weights [-0.41968772 -0.19150613 -0.43021903 -0.38564015 -0.19455769  0.64461476]
features
2 	3 	0 	0 	4 	4 	3 	4 	
0 	2 	4 	2 	2 	0 	4 	2 	
2 	3 	2 	4 	1 	1 	0 	0 	
2 	2 	1 	0 	4 	0 	0 	1 	
4 	2 	2 	2 	0 	1 	4 	4 	
1 	4 	1 	2 	1 	4 	2 	1 	
0 	4 	4 	3 	2 	0 	0 	1 	
2 	4 	4 	0 	2 	0 	2 	5 	
optimal policy
>	>	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.06	-2.66	-2.29	-2.11	-1.70	-1.52	-1.53	-1.17	
-2.70	-2.30	-1.89	-1.71	-1.53	-1.34	-1.16	-0.98	
-2.49	-2.08	-1.71	-1.30	-1.11	-0.93	-0.97	-0.56	
-2.12	-1.94	-1.52	-1.35	-0.94	-0.75	-0.56	-0.14	
-1.71	-1.77	-1.59	-1.17	-0.75	-0.33	-0.14	0.05	
-1.53	-1.35	-1.17	-0.99	-0.56	-0.37	-0.18	0.25	
-1.93	-1.53	-1.35	-1.20	-0.82	-0.40	0.02	0.45	
-1.84	-1.42	-1.24	-1.06	-0.64	-0.21	0.21	0.64	
map_weights [-0.61647338 -0.26608768 -0.53330149 -0.35357586 -0.35936709 -0.1028924 ]
MAP reward
-0.53	-0.35	-0.62	-0.62	-0.36	-0.36	-0.35	-0.36	
-0.62	-0.53	-0.36	-0.53	-0.53	-0.62	-0.36	-0.53	
-0.53	-0.35	-0.53	-0.36	-0.27	-0.27	-0.62	-0.62	
-0.53	-0.53	-0.27	-0.62	-0.36	-0.62	-0.62	-0.27	
-0.36	-0.53	-0.53	-0.53	-0.62	-0.27	-0.36	-0.36	
-0.27	-0.36	-0.27	-0.53	-0.27	-0.36	-0.53	-0.27	
-0.62	-0.36	-0.36	-0.35	-0.53	-0.62	-0.62	-0.27	
-0.53	-0.36	-0.36	-0.62	-0.53	-0.62	-0.53	-0.10	
Map policy
>	v	v	>	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6953065409149957
mean w [-0.52575149 -0.12512965 -0.33617018 -0.41355694 -0.2955219   0.031175  ]
Mean policy from posterior
>	v	v	>	v	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.34	-0.41	-0.53	-0.53	-0.30	-0.30	-0.41	-0.30	
-0.53	-0.34	-0.30	-0.34	-0.34	-0.53	-0.30	-0.34	
-0.34	-0.41	-0.34	-0.30	-0.13	-0.13	-0.53	-0.53	
-0.34	-0.34	-0.13	-0.53	-0.30	-0.53	-0.53	-0.13	
-0.30	-0.34	-0.34	-0.34	-0.53	-0.13	-0.30	-0.30	
-0.13	-0.30	-0.13	-0.34	-0.13	-0.30	-0.34	-0.13	
-0.53	-0.30	-0.30	-0.41	-0.34	-0.53	-0.53	-0.13	
-0.34	-0.30	-0.30	-0.53	-0.34	-0.53	-0.34	0.03	
mean = 0.07806823096479998, map = 0.02415513551165671
CVaR policy
>	v	v	>	v	>	>	v	
v	v	v	v	v	v	>	v	
>	>	v	>	>	v	v	v	
v	>	v	>	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
v	v	v	v	v	v	>	v	
>	v	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
v	>	v	v	v	v	>	v	
v	>	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
v	>	v	v	v	v	>	v	
v	>	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.08478847465510797, 0.09560662765770123, 0.07806823173834831, 0.07806823100174576, 0.07806823582776357
==========
iteration 7
==========
weights [-0.25808208 -0.27387159 -0.54798065 -0.11659288 -0.73758758  0.02181429]
expeced value MDP LP -1.7243150469732487
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.05090367 -0.18055281  0.55875142 -0.80352496  0.02234431  0.08034131]
w_map [-0.22224423 -0.4394945  -0.64023655 -0.22422265 -0.5452242  -0.00200247] loglik -6.693525733680872e-09
accepted/total = 1684/3000 = 0.5613333333333334
-------
true weights [-0.25808208 -0.27387159 -0.54798065 -0.11659288 -0.73758758  0.02181429]
features
0 	3 	1 	1 	4 	0 	0 	4 	
3 	2 	1 	4 	1 	2 	1 	2 	
2 	2 	3 	1 	1 	4 	0 	1 	
3 	4 	0 	4 	4 	3 	1 	2 	
1 	1 	4 	4 	4 	3 	3 	1 	
0 	3 	3 	1 	3 	1 	4 	1 	
4 	0 	4 	1 	2 	1 	1 	3 	
2 	2 	2 	1 	4 	0 	3 	5 	
optimal policy
v	>	v	>	>	>	v	<	
v	>	v	>	v	>	v	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	>	v	
>	^	>	>	>	>	>	.	
optimal values
-2.91	-2.89	-2.80	-2.97	-2.72	-2.01	-1.77	-2.49	
-2.68	-3.07	-2.55	-2.92	-2.20	-2.06	-1.52	-1.98	
-2.59	-2.82	-2.30	-2.20	-1.95	-1.69	-1.26	-1.44	
-2.07	-2.45	-2.33	-2.41	-1.69	-0.97	-1.01	-1.18	
-1.97	-1.73	-2.09	-1.99	-1.59	-0.86	-0.75	-0.64	
-1.71	-1.47	-1.37	-1.26	-1.00	-0.89	-1.10	-0.37	
-2.43	-1.71	-2.09	-1.43	-1.16	-0.62	-0.37	-0.09	
-2.77	-2.24	-1.88	-1.35	-1.09	-0.35	-0.09	0.02	
map_weights [-0.22224423 -0.4394945  -0.64023655 -0.22422265 -0.5452242  -0.00200247]
MAP reward
-0.22	-0.22	-0.44	-0.44	-0.55	-0.22	-0.22	-0.55	
-0.22	-0.64	-0.44	-0.55	-0.44	-0.64	-0.44	-0.64	
-0.64	-0.64	-0.22	-0.44	-0.44	-0.55	-0.22	-0.44	
-0.22	-0.55	-0.22	-0.55	-0.55	-0.22	-0.44	-0.64	
-0.44	-0.44	-0.55	-0.55	-0.55	-0.22	-0.22	-0.44	
-0.22	-0.22	-0.22	-0.44	-0.22	-0.44	-0.55	-0.44	
-0.55	-0.22	-0.55	-0.44	-0.64	-0.44	-0.44	-0.22	
-0.64	-0.64	-0.64	-0.44	-0.55	-0.22	-0.22	-0.00	
Map policy
v	>	v	>	>	>	v	<	
v	>	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8196734156691157
mean w [-0.13294779 -0.26889431 -0.54849821 -0.17186882 -0.54860868 -0.16208422]
Mean policy from posterior
v	>	v	>	>	>	v	<	
v	>	v	>	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.13	-0.17	-0.27	-0.27	-0.55	-0.13	-0.13	-0.55	
-0.17	-0.55	-0.27	-0.55	-0.27	-0.55	-0.27	-0.55	
-0.55	-0.55	-0.17	-0.27	-0.27	-0.55	-0.13	-0.27	
-0.17	-0.55	-0.13	-0.55	-0.55	-0.17	-0.27	-0.55	
-0.27	-0.27	-0.55	-0.55	-0.55	-0.17	-0.17	-0.27	
-0.13	-0.17	-0.17	-0.27	-0.17	-0.27	-0.55	-0.27	
-0.55	-0.13	-0.55	-0.27	-0.55	-0.27	-0.27	-0.17	
-0.55	-0.55	-0.55	-0.27	-0.55	-0.13	-0.17	-0.16	
mean = 0.044476633006934385, map = 0.02570510417479177
CVaR policy
>	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	>	v	<	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	v	<	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	v	<	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	v	<	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.10233107621305404, 0.08030612475476939, 0.04447663298246929, 0.044476633109534536, 0.044476633047855874
==========
iteration 8
==========
weights [-0.26192152 -0.1579791  -0.15035918 -0.61724254 -0.70747556  0.04818527]
expeced value MDP LP -1.9647020575994665
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.1976689  -0.25130565  0.26171836 -0.66040295  0.61939271  0.09745045]
w_map [-0.29449914 -0.19702155 -0.29077782 -0.58130308 -0.58064368  0.3388815 ] loglik -8.01581023779363e-12
accepted/total = 1918/3000 = 0.6393333333333333
-------
true weights [-0.26192152 -0.1579791  -0.15035918 -0.61724254 -0.70747556  0.04818527]
features
3 	4 	4 	4 	1 	3 	2 	4 	
1 	2 	0 	4 	0 	1 	0 	3 	
0 	0 	2 	0 	4 	3 	3 	3 	
1 	0 	3 	1 	4 	0 	3 	3 	
0 	3 	4 	0 	0 	0 	4 	4 	
0 	2 	2 	1 	3 	4 	0 	3 	
2 	1 	3 	0 	3 	1 	2 	2 	
0 	3 	3 	1 	2 	3 	4 	5 	
optimal policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	<	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
optimal values
-3.27	-3.23	-3.10	-3.40	-2.72	-2.94	-2.71	-3.39	
-2.68	-2.55	-2.42	-2.74	-2.59	-2.35	-2.59	-3.18	
-2.55	-2.42	-2.18	-2.05	-2.74	-2.21	-2.42	-2.62	
-2.31	-2.53	-2.41	-1.81	-2.30	-1.61	-1.82	-2.02	
-2.18	-2.29	-2.25	-1.67	-1.61	-1.36	-1.21	-1.42	
-1.93	-1.69	-1.55	-1.42	-1.63	-1.11	-0.51	-0.72	
-1.96	-1.83	-1.88	-1.27	-1.02	-0.41	-0.25	-0.10	
-2.20	-2.43	-1.91	-1.31	-1.16	-1.02	-0.66	0.05	
map_weights [-0.29449914 -0.19702155 -0.29077782 -0.58130308 -0.58064368  0.3388815 ]
MAP reward
-0.58	-0.58	-0.58	-0.58	-0.20	-0.58	-0.29	-0.58	
-0.20	-0.29	-0.29	-0.58	-0.29	-0.20	-0.29	-0.58	
-0.29	-0.29	-0.29	-0.29	-0.58	-0.58	-0.58	-0.58	
-0.20	-0.29	-0.58	-0.20	-0.58	-0.29	-0.58	-0.58	
-0.29	-0.58	-0.58	-0.29	-0.29	-0.29	-0.58	-0.58	
-0.29	-0.29	-0.29	-0.20	-0.58	-0.58	-0.29	-0.58	
-0.29	-0.20	-0.58	-0.29	-0.58	-0.20	-0.29	-0.29	
-0.29	-0.58	-0.58	-0.20	-0.29	-0.58	-0.58	0.34	
Map policy
v	v	v	>	v	v	v	<	
v	v	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -2.017161117471118
mean w [-0.21060863 -0.19680619 -0.18767582 -0.44887596 -0.559881   -0.20345005]
Mean policy from posterior
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.45	-0.56	-0.56	-0.56	-0.20	-0.45	-0.19	-0.56	
-0.20	-0.19	-0.21	-0.56	-0.21	-0.20	-0.21	-0.45	
-0.21	-0.21	-0.19	-0.21	-0.56	-0.45	-0.45	-0.45	
-0.20	-0.21	-0.45	-0.20	-0.56	-0.21	-0.45	-0.45	
-0.21	-0.45	-0.56	-0.21	-0.21	-0.21	-0.56	-0.56	
-0.21	-0.19	-0.19	-0.20	-0.45	-0.56	-0.21	-0.45	
-0.19	-0.20	-0.45	-0.21	-0.45	-0.20	-0.19	-0.19	
-0.21	-0.45	-0.45	-0.20	-0.19	-0.45	-0.56	-0.20	
mean = 0.03530727108429499, map = 0.04004814362836662
CVaR policy
v	v	v	>	v	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
v	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.06273248224152361, 0.059940897330106324, 0.03725655959624996, 0.03530728009496009, 0.03530727382156629
==========
iteration 9
==========
weights [-0.5868067  -0.02628692 -0.22383965 -0.35914696 -0.68844222  0.04385757]
expeced value MDP LP -1.8970149022680243
demonstration
[(32, 3), (40, 1), (41, 3), (49, 3), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.28690205 -0.76153552 -0.23234913  0.30487542  0.3325741  -0.28321406]
w_map [-0.61221787 -0.21710019 -0.20310307 -0.42023249 -0.59379905 -0.08725412] loglik -0.6931472256626989
accepted/total = 1675/3000 = 0.5583333333333333
-------
true weights [-0.5868067  -0.02628692 -0.22383965 -0.35914696 -0.68844222  0.04385757]
features
2 	1 	3 	2 	4 	2 	0 	0 	
2 	2 	1 	4 	1 	1 	2 	0 	
2 	3 	3 	1 	1 	2 	3 	4 	
0 	2 	1 	1 	4 	3 	4 	1 	
4 	2 	1 	4 	4 	2 	2 	2 	
2 	2 	2 	0 	3 	3 	3 	3 	
3 	2 	4 	0 	2 	0 	3 	4 	
4 	1 	3 	3 	2 	2 	3 	5 	
optimal policy
>	v	v	v	v	v	<	v	
>	>	v	>	v	v	<	v	
>	v	>	>	>	v	v	v	
>	>	>	^	>	v	>	v	
>	v	^	v	v	>	v	v	
>	v	<	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.75	-2.56	-2.69	-2.88	-2.68	-2.21	-2.77	-3.03	
-2.75	-2.55	-2.35	-2.68	-2.01	-2.01	-2.21	-2.47	
-2.76	-2.57	-2.35	-2.01	-2.01	-2.00	-2.24	-1.90	
-2.79	-2.23	-2.02	-2.02	-2.46	-1.79	-1.90	-1.23	
-2.76	-2.09	-2.03	-2.56	-2.00	-1.45	-1.24	-1.21	
-2.09	-1.88	-2.09	-1.89	-1.32	-1.37	-1.02	-1.00	
-2.02	-1.68	-2.13	-1.55	-0.97	-1.12	-0.67	-0.65	
-2.14	-1.47	-1.45	-1.11	-0.75	-0.54	-0.32	0.04	
map_weights [-0.61221787 -0.21710019 -0.20310307 -0.42023249 -0.59379905 -0.08725412]
MAP reward
-0.20	-0.22	-0.42	-0.20	-0.59	-0.20	-0.61	-0.61	
-0.20	-0.20	-0.22	-0.59	-0.22	-0.22	-0.20	-0.61	
-0.20	-0.42	-0.42	-0.22	-0.22	-0.20	-0.42	-0.59	
-0.61	-0.20	-0.22	-0.22	-0.59	-0.42	-0.59	-0.22	
-0.59	-0.20	-0.22	-0.59	-0.59	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.61	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.20	-0.59	-0.61	-0.20	-0.61	-0.42	-0.59	
-0.59	-0.22	-0.42	-0.42	-0.20	-0.20	-0.42	-0.09	
Map policy
v	v	v	>	>	v	v	v	
v	v	v	>	>	v	<	v	
>	v	>	>	>	v	v	v	
>	v	v	^	>	v	v	v	
v	v	v	>	v	>	>	v	
>	v	>	>	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8315265885911653
mean w [-0.58328346 -0.19353746 -0.16251538 -0.27400649 -0.51259778 -0.21961061]
Mean policy from posterior
v	v	v	>	>	v	v	v	
v	v	v	>	v	v	<	v	
>	v	>	>	>	v	v	v	
>	v	<	>	>	v	v	v	
v	v	v	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.16	-0.19	-0.27	-0.16	-0.51	-0.16	-0.58	-0.58	
-0.16	-0.16	-0.19	-0.51	-0.19	-0.19	-0.16	-0.58	
-0.16	-0.27	-0.27	-0.19	-0.19	-0.16	-0.27	-0.51	
-0.58	-0.16	-0.19	-0.19	-0.51	-0.27	-0.51	-0.19	
-0.51	-0.16	-0.19	-0.51	-0.51	-0.16	-0.16	-0.16	
-0.16	-0.16	-0.16	-0.58	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.16	-0.51	-0.58	-0.16	-0.58	-0.27	-0.51	
-0.51	-0.19	-0.27	-0.27	-0.16	-0.16	-0.27	-0.22	
mean = 0.04418142940037795, map = 0.09548614950697054
CVaR policy
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	>	>	v	>	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	>	>	v	>	v	
v	v	v	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	v	v	>	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	<	v	
v	v	v	>	v	v	<	v	
>	v	>	>	>	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.0290355524353747, 0.022355283085842448, 0.03979188211303786, 0.04892899660838346, 0.04130206295288774
==========
iteration 10
==========
weights [-0.34467623 -0.45326995 -0.65711793 -0.41425282 -0.26255775  0.05829845]
expeced value MDP LP -2.4697658965533784
demonstration
[(32, 3), (40, 3), (48, 1), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.41395995 -0.11385427 -0.53606707  0.50377618  0.27484225  0.44606923]
w_map [-0.42110833 -0.40025804 -0.65738796 -0.42170086 -0.21108919 -0.0889499 ] loglik -2.6427485977364995e-08
accepted/total = 1721/3000 = 0.5736666666666667
-------
true weights [-0.34467623 -0.45326995 -0.65711793 -0.41425282 -0.26255775  0.05829845]
features
0 	1 	1 	4 	4 	2 	2 	1 	
2 	4 	2 	1 	2 	4 	1 	4 	
4 	4 	0 	0 	4 	4 	1 	2 	
3 	1 	3 	4 	1 	1 	1 	0 	
3 	2 	1 	4 	2 	3 	4 	3 	
1 	3 	1 	3 	1 	1 	0 	1 	
4 	0 	1 	4 	3 	1 	0 	2 	
1 	4 	3 	2 	4 	1 	3 	5 	
optimal policy
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.40	-4.10	-3.98	-3.56	-3.47	-3.24	-3.23	-3.08	
-4.30	-3.68	-3.85	-3.33	-3.24	-2.61	-2.60	-2.66	
-3.68	-3.45	-3.22	-2.91	-2.61	-2.37	-2.16	-2.42	
-3.78	-3.40	-2.98	-2.59	-2.56	-2.13	-1.73	-1.78	
-3.52	-3.41	-2.78	-2.35	-2.33	-1.69	-1.29	-1.45	
-3.14	-2.86	-2.54	-2.11	-1.90	-1.48	-1.04	-1.05	
-2.71	-2.47	-2.15	-1.71	-1.46	-1.14	-0.70	-0.60	
-2.78	-2.35	-2.10	-1.71	-1.06	-0.81	-0.36	0.06	
map_weights [-0.42110833 -0.40025804 -0.65738796 -0.42170086 -0.21108919 -0.0889499 ]
MAP reward
-0.42	-0.40	-0.40	-0.21	-0.21	-0.66	-0.66	-0.40	
-0.66	-0.21	-0.66	-0.40	-0.66	-0.21	-0.40	-0.21	
-0.21	-0.21	-0.42	-0.42	-0.21	-0.21	-0.40	-0.66	
-0.42	-0.40	-0.42	-0.21	-0.40	-0.40	-0.40	-0.42	
-0.42	-0.66	-0.40	-0.21	-0.66	-0.42	-0.21	-0.42	
-0.40	-0.42	-0.40	-0.42	-0.40	-0.40	-0.42	-0.40	
-0.21	-0.42	-0.40	-0.21	-0.42	-0.40	-0.42	-0.66	
-0.40	-0.21	-0.42	-0.66	-0.21	-0.40	-0.42	-0.09	
Map policy
>	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	v	>	v	v	
v	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7802501179791927
mean w [-0.2802143  -0.28588851 -0.71617594 -0.41140222 -0.10613487  0.09269192]
Mean policy from posterior
>	v	>	v	<	v	v	v	
v	v	v	v	>	v	v	<	
>	>	>	>	>	v	v	v	
^	>	>	v	>	>	v	<	
v	>	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.28	-0.29	-0.29	-0.11	-0.11	-0.72	-0.72	-0.29	
-0.72	-0.11	-0.72	-0.29	-0.72	-0.11	-0.29	-0.11	
-0.11	-0.11	-0.28	-0.28	-0.11	-0.11	-0.29	-0.72	
-0.41	-0.29	-0.41	-0.11	-0.29	-0.29	-0.29	-0.28	
-0.41	-0.72	-0.29	-0.11	-0.72	-0.41	-0.11	-0.41	
-0.29	-0.41	-0.29	-0.41	-0.29	-0.29	-0.28	-0.29	
-0.11	-0.28	-0.29	-0.11	-0.41	-0.29	-0.28	-0.72	
-0.29	-0.11	-0.41	-0.72	-0.11	-0.29	-0.41	0.09	
mean = 0.05065605484380198, map = 0.033256115149669085
CVaR policy
>	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	<	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	<	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	v	v	v	>	v	v	<	
>	>	>	>	>	v	v	v	
^	>	>	v	>	>	v	<	
v	>	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	v	v	v	>	v	v	<	
>	>	>	>	>	v	v	v	
^	>	>	v	>	>	v	<	
v	>	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	^	>	>	>	>	>	.	
cvar = , 0.008746190710917201, 0.020414309953881382, 0.020414309706231926, 0.05065605344656099, 0.050656062130916
==========
iteration 11
==========
weights [-0.58041721 -0.34975756 -0.46086771 -0.14345249 -0.29632741  0.46903935]
expeced value MDP LP -1.45901076468911
demonstration
[(32, 3), (40, 3), (48, 3), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.86602977  0.3154729   0.12769767 -0.1450357  -0.33398932 -0.03972872]
w_map [-0.5412148  -0.47907552 -0.30304047 -0.28593781 -0.55131983  0.00506718] loglik -2.447819724693545e-11
accepted/total = 1906/3000 = 0.6353333333333333
-------
true weights [-0.58041721 -0.34975756 -0.46086771 -0.14345249 -0.29632741  0.46903935]
features
0 	3 	2 	2 	3 	2 	1 	0 	
3 	0 	2 	0 	3 	1 	3 	1 	
4 	3 	0 	2 	3 	0 	4 	2 	
1 	2 	0 	1 	1 	0 	4 	4 	
4 	0 	1 	0 	4 	3 	2 	1 	
4 	4 	3 	1 	1 	2 	2 	0 	
3 	4 	4 	4 	4 	3 	0 	2 	
3 	3 	4 	2 	3 	4 	3 	5 	
optimal policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.12	-2.73	-2.62	-2.18	-1.73	-2.18	-2.20	-2.55	
-2.56	-3.10	-2.61	-2.17	-1.61	-1.94	-1.87	-1.99	
-2.44	-2.54	-2.49	-1.92	-1.48	-1.86	-1.74	-1.66	
-2.17	-2.42	-2.04	-1.68	-1.35	-1.29	-1.46	-1.21	
-1.84	-1.98	-1.47	-1.58	-1.01	-0.72	-1.17	-0.92	
-1.56	-1.42	-1.13	-1.05	-0.76	-0.58	-0.72	-0.58	
-1.27	-1.29	-1.00	-0.71	-0.42	-0.12	-0.26	0.00	
-1.14	-1.01	-0.87	-0.58	-0.12	0.02	0.32	0.47	
map_weights [-0.5412148  -0.47907552 -0.30304047 -0.28593781 -0.55131983  0.00506718]
MAP reward
-0.54	-0.29	-0.30	-0.30	-0.29	-0.30	-0.48	-0.54	
-0.29	-0.54	-0.30	-0.54	-0.29	-0.48	-0.29	-0.48	
-0.55	-0.29	-0.54	-0.30	-0.29	-0.54	-0.55	-0.30	
-0.48	-0.30	-0.54	-0.48	-0.48	-0.54	-0.55	-0.55	
-0.55	-0.54	-0.48	-0.54	-0.55	-0.29	-0.30	-0.48	
-0.55	-0.55	-0.29	-0.48	-0.48	-0.30	-0.30	-0.54	
-0.29	-0.55	-0.55	-0.55	-0.55	-0.29	-0.54	-0.30	
-0.29	-0.29	-0.55	-0.30	-0.29	-0.55	-0.29	0.01	
Map policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8429190026760516
mean w [-0.40099398 -0.56819742 -0.27039396 -0.12984097 -0.35037051 -0.09329056]
Mean policy from posterior
>	>	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.40	-0.13	-0.27	-0.27	-0.13	-0.27	-0.57	-0.40	
-0.13	-0.40	-0.27	-0.40	-0.13	-0.57	-0.13	-0.57	
-0.35	-0.13	-0.40	-0.27	-0.13	-0.40	-0.35	-0.27	
-0.57	-0.27	-0.40	-0.57	-0.57	-0.40	-0.35	-0.35	
-0.35	-0.40	-0.57	-0.40	-0.35	-0.13	-0.27	-0.57	
-0.35	-0.35	-0.13	-0.57	-0.57	-0.27	-0.27	-0.40	
-0.13	-0.35	-0.35	-0.35	-0.35	-0.13	-0.40	-0.27	
-0.13	-0.13	-0.35	-0.27	-0.13	-0.35	-0.13	-0.09	
mean = 0.17350579992602633, map = 0.28816381239198363
CVaR policy
>	>	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05405782283834881, 0.14559680447561218, 0.17350579887578954, 0.1735057987718489, 0.17350579876794936
==========
iteration 12
==========
weights [-0.52224542 -0.36797392 -0.51471427 -0.42126538 -0.30986084  0.23118365]
expeced value MDP LP -2.5537975529811376
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 3), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.37495203 -0.17478605  0.54049818  0.60692228 -0.34031888  0.22923989]
w_map [-0.6904734  -0.23037456 -0.56372384 -0.29120461 -0.23282247 -0.11568514] loglik -0.6931471469896167
accepted/total = 1760/3000 = 0.5866666666666667
-------
true weights [-0.52224542 -0.36797392 -0.51471427 -0.42126538 -0.30986084  0.23118365]
features
2 	1 	3 	1 	0 	4 	0 	3 	
1 	3 	4 	4 	4 	2 	0 	2 	
0 	2 	0 	4 	4 	0 	1 	0 	
2 	4 	4 	3 	1 	2 	3 	4 	
2 	0 	4 	3 	4 	0 	0 	2 	
1 	2 	0 	4 	3 	1 	2 	0 	
1 	0 	1 	0 	3 	0 	4 	0 	
0 	1 	2 	3 	1 	1 	2 	5 	
optimal policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.78	-4.31	-3.98	-3.65	-3.53	-3.63	-3.36	-3.01	
-4.31	-3.98	-3.59	-3.32	-3.04	-3.35	-2.86	-2.62	
-4.33	-3.85	-3.53	-3.04	-2.75	-2.86	-2.37	-2.12	
-3.85	-3.37	-3.09	-2.87	-2.47	-2.46	-2.02	-1.62	
-3.78	-3.30	-2.81	-2.52	-2.12	-1.97	-1.61	-1.32	
-3.45	-3.11	-2.62	-2.12	-1.83	-1.46	-1.10	-0.81	
-3.11	-2.77	-2.27	-1.93	-1.42	-1.11	-0.59	-0.29	
-2.77	-2.27	-1.92	-1.42	-1.01	-0.65	-0.29	0.23	
map_weights [-0.6904734  -0.23037456 -0.56372384 -0.29120461 -0.23282247 -0.11568514]
MAP reward
-0.56	-0.23	-0.29	-0.23	-0.69	-0.23	-0.69	-0.29	
-0.23	-0.29	-0.23	-0.23	-0.23	-0.56	-0.69	-0.56	
-0.69	-0.56	-0.69	-0.23	-0.23	-0.69	-0.23	-0.69	
-0.56	-0.23	-0.23	-0.29	-0.23	-0.56	-0.29	-0.23	
-0.56	-0.69	-0.23	-0.29	-0.23	-0.69	-0.69	-0.56	
-0.23	-0.56	-0.69	-0.23	-0.29	-0.23	-0.56	-0.69	
-0.23	-0.69	-0.23	-0.69	-0.29	-0.69	-0.23	-0.69	
-0.69	-0.23	-0.56	-0.29	-0.23	-0.23	-0.56	-0.12	
Map policy
>	>	>	v	v	v	<	v	
>	>	>	v	v	<	v	v	
>	v	>	>	v	<	v	v	
>	>	>	>	v	<	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0965801652293523
mean w [-0.60517829 -0.26653265 -0.5386202  -0.13764471 -0.29555087  0.09474536]
Mean policy from posterior
>	>	>	v	v	v	v	v	
>	>	>	v	v	<	v	v	
>	v	v	v	v	<	v	v	
>	>	>	v	v	<	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	<	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.54	-0.27	-0.14	-0.27	-0.61	-0.30	-0.61	-0.14	
-0.27	-0.14	-0.30	-0.30	-0.30	-0.54	-0.61	-0.54	
-0.61	-0.54	-0.61	-0.30	-0.30	-0.61	-0.27	-0.61	
-0.54	-0.30	-0.30	-0.14	-0.27	-0.54	-0.14	-0.30	
-0.54	-0.61	-0.30	-0.14	-0.30	-0.61	-0.61	-0.54	
-0.27	-0.54	-0.61	-0.30	-0.14	-0.27	-0.54	-0.61	
-0.27	-0.61	-0.27	-0.61	-0.14	-0.61	-0.30	-0.61	
-0.61	-0.27	-0.54	-0.14	-0.27	-0.27	-0.54	0.09	
mean = 0.07915559686364393, map = 0.04152482455573292
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	v	v	<	v	v	
>	v	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	v	v	<	v	v	
>	v	v	v	v	<	v	v	
>	>	>	v	v	<	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	v	v	<	v	v	
>	v	v	v	v	<	v	v	
>	>	>	v	v	<	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	<	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	v	v	<	v	v	
>	v	v	v	v	<	v	v	
>	>	>	v	v	<	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	<	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.038439838106020474, 0.04373492508791177, 0.05671607964074843, 0.0791555992949684, 0.07915559518841775
==========
iteration 13
==========
weights [-0.16465125 -0.39456712 -0.30432771 -0.20456629 -0.82626103  0.00606123]
expeced value MDP LP -1.792204919258439
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.49320386 -0.41444286  0.45963641 -0.39923176  0.40340147  0.22716225]
w_map [-0.29346143 -0.35671085 -0.41914464 -0.23633757 -0.74362592  0.0460497 ] loglik -1.4031666495384343e-08
accepted/total = 1797/3000 = 0.599
-------
true weights [-0.16465125 -0.39456712 -0.30432771 -0.20456629 -0.82626103  0.00606123]
features
3 	3 	1 	2 	1 	3 	0 	0 	
2 	1 	3 	1 	2 	2 	1 	0 	
3 	4 	3 	3 	4 	3 	2 	1 	
0 	0 	2 	4 	2 	0 	1 	4 	
0 	4 	4 	1 	0 	4 	0 	2 	
2 	0 	2 	1 	2 	2 	0 	3 	
0 	4 	4 	3 	2 	0 	3 	3 	
3 	3 	0 	1 	3 	1 	4 	5 	
optimal policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	<	
v	<	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
optimal values
-3.08	-3.13	-2.95	-2.59	-2.31	-1.93	-1.93	-2.07	
-2.91	-2.95	-2.59	-2.40	-2.03	-1.74	-1.78	-1.93	
-2.63	-3.39	-2.63	-2.45	-2.27	-1.45	-1.40	-1.78	
-2.45	-2.59	-2.64	-2.36	-1.55	-1.26	-1.11	-1.52	
-2.31	-2.69	-2.50	-1.69	-1.31	-1.54	-0.72	-0.70	
-2.16	-1.88	-1.73	-1.44	-1.16	-0.86	-0.56	-0.40	
-2.11	-2.59	-1.87	-1.06	-0.86	-0.56	-0.40	-0.20	
-1.97	-1.78	-1.59	-1.44	-1.06	-0.95	-0.82	0.01	
map_weights [-0.29346143 -0.35671085 -0.41914464 -0.23633757 -0.74362592  0.0460497 ]
MAP reward
-0.24	-0.24	-0.36	-0.42	-0.36	-0.24	-0.29	-0.29	
-0.42	-0.36	-0.24	-0.36	-0.42	-0.42	-0.36	-0.29	
-0.24	-0.74	-0.24	-0.24	-0.74	-0.24	-0.42	-0.36	
-0.29	-0.29	-0.42	-0.74	-0.42	-0.29	-0.36	-0.74	
-0.29	-0.74	-0.74	-0.36	-0.29	-0.74	-0.29	-0.42	
-0.42	-0.29	-0.42	-0.36	-0.42	-0.42	-0.29	-0.24	
-0.29	-0.74	-0.74	-0.24	-0.42	-0.29	-0.24	-0.24	
-0.24	-0.24	-0.29	-0.36	-0.24	-0.36	-0.74	0.05	
Map policy
>	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7856345285008501
mean w [-0.35673949 -0.32498082 -0.25019327 -0.13089399 -0.65811205 -0.06375758]
Mean policy from posterior
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
Mean rewards
-0.13	-0.13	-0.32	-0.25	-0.32	-0.13	-0.36	-0.36	
-0.25	-0.32	-0.13	-0.32	-0.25	-0.25	-0.32	-0.36	
-0.13	-0.66	-0.13	-0.13	-0.66	-0.13	-0.25	-0.32	
-0.36	-0.36	-0.25	-0.66	-0.25	-0.36	-0.32	-0.66	
-0.36	-0.66	-0.66	-0.32	-0.36	-0.66	-0.36	-0.25	
-0.25	-0.36	-0.25	-0.32	-0.25	-0.25	-0.36	-0.13	
-0.36	-0.66	-0.66	-0.13	-0.25	-0.36	-0.13	-0.13	
-0.13	-0.13	-0.36	-0.32	-0.13	-0.32	-0.66	-0.06	
mean = 0.11677329960843164, map = 0.054011616899970694
CVaR policy
>	>	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
cvar = , 0.17247271206157144, 0.15210417223770767, 0.15210417091565964, 0.11677330437996924, 0.11677329956694726
==========
iteration 14
==========
weights [-0.67693597 -0.56792764 -0.00178009 -0.1635496  -0.26157735  0.35219531]
expeced value MDP LP -0.6461380456201834
demonstration
[(32, 3), (40, 3), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0)]
[-0.3885703   0.02021908  0.42607052 -0.1144785   0.60191146  0.54006063]
w_map [-0.54484533  0.05931519  0.64064033  0.1441826   0.21227501  0.47260547] loglik 0.0
accepted/total = 2755/3000 = 0.9183333333333333
-------
true weights [-0.67693597 -0.56792764 -0.00178009 -0.1635496  -0.26157735  0.35219531]
features
0 	4 	1 	1 	2 	0 	4 	3 	
2 	2 	3 	1 	4 	4 	2 	1 	
1 	1 	3 	0 	3 	3 	1 	0 	
4 	4 	1 	3 	3 	3 	1 	1 	
3 	4 	1 	1 	3 	0 	3 	1 	
3 	4 	2 	0 	2 	2 	0 	2 	
2 	4 	2 	1 	0 	3 	2 	1 	
3 	2 	2 	0 	0 	0 	1 	5 	
optimal policy
v	v	v	>	^	<	v	<	
>	<	<	<	^	<	<	<	
^	^	^	<	^	<	^	^	
v	v	^	>	v	<	<	v	
v	v	v	>	v	v	>	v	
v	>	v	>	>	<	>	>	
<	v	v	<	^	^	>	v	
>	v	<	<	<	>	>	.	
optimal values
-0.85	-0.44	-0.90	-0.74	-0.18	-0.85	-0.94	-1.10	
-0.18	-0.18	-0.34	-0.90	-0.44	-0.70	-0.69	-1.25	
-0.74	-0.74	-0.50	-1.17	-0.60	-0.75	-1.25	-1.92	
-0.76	-0.95	-1.06	-0.66	-0.50	-0.66	-1.22	-1.30	
-0.50	-0.70	-0.74	-0.90	-0.34	-0.85	-0.90	-0.74	
-0.34	-0.44	-0.18	-0.85	-0.18	-0.18	-0.85	-0.18	
-0.18	-0.44	-0.18	-0.74	-0.85	-0.34	-0.22	-0.22	
-0.34	-0.18	-0.18	-0.85	-1.52	-0.89	-0.22	0.35	
map_weights [-0.54484533  0.05931519  0.64064033  0.1441826   0.21227501  0.47260547]
MAP reward
-0.54	0.21	0.06	0.06	0.64	-0.54	0.21	0.14	
0.64	0.64	0.14	0.06	0.21	0.21	0.64	0.06	
0.06	0.06	0.14	-0.54	0.14	0.14	0.06	-0.54	
0.21	0.21	0.06	0.14	0.14	0.14	0.06	0.06	
0.14	0.21	0.06	0.06	0.14	-0.54	0.14	0.06	
0.14	0.21	0.64	-0.54	0.64	0.64	-0.54	0.64	
0.64	0.21	0.64	0.06	-0.54	0.14	0.64	0.06	
0.14	0.64	0.64	-0.54	-0.54	-0.54	0.06	0.47	
Map policy
v	v	<	>	^	<	v	<	
<	<	<	>	^	<	<	<	
^	^	^	>	^	^	^	v	
^	^	v	>	v	<	v	v	
v	v	v	>	v	v	>	v	
v	>	v	<	>	<	<	>	
<	>	v	<	^	^	<	^	
>	v	<	<	<	^	^	.	
expeced value MDP LP 39.833798235828375
mean w [-0.1869027  -0.26021415  0.41340012 -0.11667593 -0.39739254  0.0462758 ]
Mean policy from posterior
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	<	>	
<	>	v	<	^	^	<	^	
>	v	<	<	<	^	^	.	
Mean rewards
-0.19	-0.40	-0.26	-0.26	0.41	-0.19	-0.40	-0.12	
0.41	0.41	-0.12	-0.26	-0.40	-0.40	0.41	-0.26	
-0.26	-0.26	-0.12	-0.19	-0.12	-0.12	-0.26	-0.19	
-0.40	-0.40	-0.26	-0.12	-0.12	-0.12	-0.26	-0.26	
-0.12	-0.40	-0.26	-0.26	-0.12	-0.19	-0.12	-0.26	
-0.12	-0.40	0.41	-0.19	0.41	0.41	-0.19	0.41	
0.41	-0.40	0.41	-0.26	-0.19	-0.12	0.41	-0.26	
-0.12	0.41	0.41	-0.19	-0.19	-0.19	-0.26	0.05	
mean = 0.08582470229107408, map = 0.041547566704250416
CVaR policy
v	v	v	>	^	<	<	<	
>	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	<	>	
<	>	^	<	^	^	<	^	
>	>	^	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	v	^	<	^	^	<	^	
>	v	v	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	>	v	<	^	^	<	^	
>	v	^	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
>	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	v	v	<	^	^	<	^	
>	v	v	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
>	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	>	>	<	>	>	
<	v	v	<	^	^	<	^	
>	v	v	<	<	^	^	.	
cvar = , 0.08582507545781637, 0.08583967632441702, 0.08582801018111375, 0.0858404254102646, 0.08582599756323617
==========
iteration 15
==========
weights [-0.17112211 -0.63055689 -0.18926621 -0.15706279 -0.03268814  0.71523164]
expeced value MDP LP -0.3725553197038518
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 3), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.04448336 -0.19479622  0.00876614  0.13439286  0.50684707 -0.8276735 ]
w_map [-0.30677022 -0.60586074 -0.59323783 -0.33823382 -0.15473594  0.22033688] loglik -4.985845967198088e-07
accepted/total = 1576/3000 = 0.5253333333333333
-------
true weights [-0.17112211 -0.63055689 -0.18926621 -0.15706279 -0.03268814  0.71523164]
features
1 	4 	3 	3 	4 	4 	1 	2 	
2 	3 	0 	1 	2 	1 	2 	2 	
4 	1 	1 	2 	4 	0 	3 	0 	
4 	3 	1 	0 	4 	1 	1 	2 	
1 	1 	1 	1 	1 	4 	0 	0 	
0 	0 	4 	2 	4 	1 	2 	3 	
0 	4 	1 	3 	3 	4 	2 	4 	
1 	2 	1 	2 	0 	0 	4 	5 	
optimal policy
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.54	-0.92	-0.90	-0.75	-0.60	-0.62	-1.00	-0.41	
-1.14	-1.07	-1.06	-1.20	-0.57	-0.98	-0.37	-0.22	
-0.96	-1.53	-1.20	-0.57	-0.38	-0.36	-0.19	-0.03	
-0.94	-0.91	-1.19	-0.57	-0.40	-0.51	-0.47	0.14	
-0.93	-0.76	-0.59	-0.56	-0.37	0.13	0.16	0.34	
-0.30	-0.13	0.04	0.07	0.26	-0.18	0.32	0.51	
-0.34	-0.17	-0.49	0.14	0.30	0.46	0.48	0.68	
-0.96	-0.35	-0.50	0.13	0.32	0.50	0.68	0.72	
map_weights [-0.30677022 -0.60586074 -0.59323783 -0.33823382 -0.15473594  0.22033688]
MAP reward
-0.61	-0.15	-0.34	-0.34	-0.15	-0.15	-0.61	-0.59	
-0.59	-0.34	-0.31	-0.61	-0.59	-0.61	-0.59	-0.59	
-0.15	-0.61	-0.61	-0.59	-0.15	-0.31	-0.34	-0.31	
-0.15	-0.34	-0.61	-0.31	-0.15	-0.61	-0.61	-0.59	
-0.61	-0.61	-0.61	-0.61	-0.61	-0.15	-0.31	-0.31	
-0.31	-0.31	-0.15	-0.59	-0.15	-0.61	-0.59	-0.34	
-0.31	-0.15	-0.61	-0.34	-0.34	-0.15	-0.59	-0.15	
-0.61	-0.59	-0.61	-0.59	-0.31	-0.31	-0.15	0.22	
Map policy
>	>	>	>	v	<	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.6191820188208133
mean w [-0.2275651  -0.64901753 -0.38907886 -0.33961343 -0.09673787  0.07684442]
Mean policy from posterior
>	>	>	>	v	<	v	v	
v	^	>	v	v	v	v	v	
v	v	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.65	-0.10	-0.34	-0.34	-0.10	-0.10	-0.65	-0.39	
-0.39	-0.34	-0.23	-0.65	-0.39	-0.65	-0.39	-0.39	
-0.10	-0.65	-0.65	-0.39	-0.10	-0.23	-0.34	-0.23	
-0.10	-0.34	-0.65	-0.23	-0.10	-0.65	-0.65	-0.39	
-0.65	-0.65	-0.65	-0.65	-0.65	-0.10	-0.23	-0.23	
-0.23	-0.23	-0.10	-0.39	-0.10	-0.65	-0.39	-0.34	
-0.23	-0.10	-0.65	-0.34	-0.34	-0.10	-0.39	-0.10	
-0.65	-0.39	-0.65	-0.39	-0.23	-0.23	-0.10	0.08	
mean = 0.01922445755137614, map = 0.052257743017254166
CVaR policy
>	>	>	>	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	^	>	v	v	v	v	v	
v	v	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	^	>	v	v	v	v	v	
v	v	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.06238969404429562, 0.04244374803227291, 0.03137632015369507, 0.01922445317011945, 0.0192244531836967
==========
iteration 16
==========
weights [-0.2406627  -0.2448106  -0.93441745 -0.05926605 -0.02083348  0.07118127]
expeced value MDP LP -0.8498426005386726
demonstration
[(32, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 3), (36, 1), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.87975243  0.06075543  0.20028042  0.38981922  0.14180727 -0.10081596]
w_map [-0.71622015 -0.16369717 -0.59147303 -0.18881393 -0.26819846 -0.05301384] loglik -1.3862946533228921
accepted/total = 1618/3000 = 0.5393333333333333
-------
true weights [-0.2406627  -0.2448106  -0.93441745 -0.05926605 -0.02083348  0.07118127]
features
4 	1 	1 	2 	2 	4 	3 	0 	
1 	4 	1 	3 	2 	4 	3 	1 	
3 	1 	3 	0 	3 	1 	4 	1 	
3 	4 	3 	1 	3 	1 	2 	3 	
2 	2 	2 	3 	1 	4 	3 	3 	
3 	4 	2 	3 	0 	0 	2 	4 	
4 	2 	3 	0 	0 	1 	1 	1 	
3 	1 	1 	0 	4 	4 	1 	5 	
optimal policy
v	v	v	v	>	v	v	<	
v	v	v	v	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	<	>	^	v	^	>	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.26	-1.40	-1.43	-1.88	-1.58	-0.65	-0.67	-0.91	
-1.26	-1.17	-1.19	-0.96	-1.56	-0.63	-0.62	-0.79	
-1.02	-1.16	-0.96	-0.91	-0.67	-0.80	-0.56	-0.55	
-0.97	-0.92	-0.91	-0.86	-0.62	-0.57	-1.24	-0.31	
-1.90	-1.85	-1.55	-0.62	-0.57	-0.33	-0.31	-0.25	
-1.04	-1.05	-1.60	-0.67	-0.69	-0.56	-1.13	-0.19	
-0.99	-1.67	-0.74	-0.69	-0.45	-0.44	-0.42	-0.17	
-0.98	-0.93	-0.69	-0.45	-0.21	-0.19	-0.17	0.07	
map_weights [-0.71622015 -0.16369717 -0.59147303 -0.18881393 -0.26819846 -0.05301384]
MAP reward
-0.27	-0.16	-0.16	-0.59	-0.59	-0.27	-0.19	-0.72	
-0.16	-0.27	-0.16	-0.19	-0.59	-0.27	-0.19	-0.16	
-0.19	-0.16	-0.19	-0.72	-0.19	-0.16	-0.27	-0.16	
-0.19	-0.27	-0.19	-0.16	-0.19	-0.16	-0.59	-0.19	
-0.59	-0.59	-0.59	-0.19	-0.16	-0.27	-0.19	-0.19	
-0.19	-0.27	-0.59	-0.19	-0.72	-0.72	-0.59	-0.27	
-0.27	-0.59	-0.19	-0.72	-0.72	-0.16	-0.16	-0.16	
-0.19	-0.16	-0.16	-0.72	-0.27	-0.27	-0.16	-0.05	
Map policy
>	>	v	<	>	>	v	v	
v	v	v	<	v	>	>	v	
>	>	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	>	>	^	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.189071958028466
mean w [-0.5710942  -0.20706648 -0.56346724 -0.10451607 -0.1592456   0.05773088]
Mean policy from posterior
v	v	v	v	>	>	v	v	
v	v	v	<	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	>	>	^	^	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.16	-0.21	-0.21	-0.56	-0.56	-0.16	-0.10	-0.57	
-0.21	-0.16	-0.21	-0.10	-0.56	-0.16	-0.10	-0.21	
-0.10	-0.21	-0.10	-0.57	-0.10	-0.21	-0.16	-0.21	
-0.10	-0.16	-0.10	-0.21	-0.10	-0.21	-0.56	-0.10	
-0.56	-0.56	-0.56	-0.10	-0.21	-0.16	-0.10	-0.10	
-0.10	-0.16	-0.56	-0.10	-0.57	-0.57	-0.56	-0.16	
-0.16	-0.56	-0.10	-0.57	-0.57	-0.21	-0.21	-0.21	
-0.10	-0.21	-0.21	-0.57	-0.16	-0.16	-0.21	0.06	
mean = 0.02668024561099025, map = 0.10380922686108907
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
v	>	>	^	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
v	>	>	^	^	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	>	>	^	^	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	<	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	>	>	^	^	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	<	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	>	>	^	^	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.02573290639507042, 0.027526780740621537, 0.01782485999312544, 0.0266802491845346, 0.026680245378596368
==========
iteration 17
==========
weights [-0.79093218 -0.13423246 -0.04159454 -0.24763618 -0.37166243  0.3939812 ]
expeced value MDP LP -0.9781668881702649
demonstration
[(32, 1), (33, 1), (34, 1), (35, 3), (43, 3), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[ 0.36379713 -0.16110592 -0.1009652   0.32457513  0.50064094 -0.68957392]
w_map [-0.64475983 -0.33635737 -0.12709816 -0.56915736 -0.35244759  0.08267486] loglik -3.3305873614608572e-09
accepted/total = 1696/3000 = 0.5653333333333334
-------
true weights [-0.79093218 -0.13423246 -0.04159454 -0.24763618 -0.37166243  0.3939812 ]
features
4 	2 	4 	3 	4 	2 	0 	1 	
4 	3 	0 	4 	3 	0 	0 	3 	
2 	0 	4 	3 	1 	3 	1 	0 	
1 	2 	4 	1 	4 	1 	0 	1 	
1 	2 	3 	1 	2 	4 	1 	2 	
4 	0 	3 	2 	0 	1 	0 	0 	
0 	1 	0 	4 	2 	2 	1 	3 	
0 	2 	4 	0 	1 	0 	1 	5 	
optimal policy
v	v	>	v	v	<	<	v	
v	<	v	v	v	v	v	v	
v	v	>	v	>	v	<	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	<	<	
^	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
optimal values
-1.70	-1.60	-1.83	-1.48	-1.53	-1.55	-2.33	-1.86	
-1.34	-1.57	-2.02	-1.24	-1.17	-1.58	-1.71	-1.74	
-0.98	-1.60	-1.24	-0.88	-0.93	-0.80	-0.93	-1.51	
-0.94	-0.82	-1.00	-0.64	-0.83	-0.56	-1.35	-0.72	
-0.91	-0.78	-0.75	-0.51	-0.47	-0.43	-0.56	-0.60	
-1.27	-1.40	-0.62	-0.38	-0.76	-0.06	-0.67	-0.65	
-2.03	-1.25	-1.13	-0.34	0.03	0.08	0.12	0.14	
-2.06	-1.28	-1.25	-0.89	-0.10	-0.54	0.26	0.39	
map_weights [-0.64475983 -0.33635737 -0.12709816 -0.56915736 -0.35244759  0.08267486]
MAP reward
-0.35	-0.13	-0.35	-0.57	-0.35	-0.13	-0.64	-0.34	
-0.35	-0.57	-0.64	-0.35	-0.57	-0.64	-0.64	-0.57	
-0.13	-0.64	-0.35	-0.57	-0.34	-0.57	-0.34	-0.64	
-0.34	-0.13	-0.35	-0.34	-0.35	-0.34	-0.64	-0.34	
-0.34	-0.13	-0.57	-0.34	-0.13	-0.35	-0.34	-0.13	
-0.35	-0.64	-0.57	-0.13	-0.64	-0.34	-0.64	-0.64	
-0.64	-0.34	-0.64	-0.35	-0.13	-0.13	-0.34	-0.57	
-0.64	-0.13	-0.35	-0.64	-0.34	-0.64	-0.34	0.08	
Map policy
v	<	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
expeced value MDP LP -2.1205937361142437
mean w [-0.59284065 -0.28557397 -0.12988196 -0.4460054  -0.28983129 -0.21674905]
Mean policy from posterior
v	<	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
Mean rewards
-0.29	-0.13	-0.29	-0.45	-0.29	-0.13	-0.59	-0.29	
-0.29	-0.45	-0.59	-0.29	-0.45	-0.59	-0.59	-0.45	
-0.13	-0.59	-0.29	-0.45	-0.29	-0.45	-0.29	-0.59	
-0.29	-0.13	-0.29	-0.29	-0.29	-0.29	-0.59	-0.29	
-0.29	-0.13	-0.45	-0.29	-0.13	-0.29	-0.29	-0.13	
-0.29	-0.59	-0.45	-0.13	-0.59	-0.29	-0.59	-0.59	
-0.59	-0.29	-0.59	-0.29	-0.13	-0.13	-0.29	-0.45	
-0.59	-0.13	-0.29	-0.59	-0.29	-0.59	-0.29	-0.22	
mean = 0.04571513488437906, map = 0.09576424138702133
CVaR policy
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
CVaR policy
v	<	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
cvar = , 0.15420184688522498, 0.15420184689559047, 0.1451107270673101, 0.08668574366009507, 0.07206821953092424
==========
iteration 18
==========
weights [-0.44412316 -0.0720415  -0.15141447 -0.47629387 -0.45369036  0.58476277]
expeced value MDP LP -1.0142718814466793
demonstration
[(32, 1), (33, 3), (41, 1), (42, 3), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.10579818 -0.25606618 -0.68460321  0.59479321  0.25387722  0.19058512]
w_map [-0.54991235 -0.20704692 -0.20671343 -0.60283277 -0.41337459  0.27876807] loglik -0.6931472029964407
accepted/total = 1776/3000 = 0.592
-------
true weights [-0.44412316 -0.0720415  -0.15141447 -0.47629387 -0.45369036  0.58476277]
features
3 	1 	1 	3 	4 	3 	4 	4 	
4 	0 	1 	1 	2 	0 	3 	4 	
4 	3 	0 	4 	0 	3 	4 	2 	
0 	0 	4 	1 	0 	0 	4 	3 	
2 	1 	3 	2 	0 	4 	3 	4 	
0 	1 	4 	4 	1 	0 	2 	3 	
2 	3 	1 	1 	1 	4 	0 	0 	
1 	2 	2 	4 	4 	1 	1 	5 	
optimal policy
>	>	v	v	v	v	v	v	
>	>	>	v	<	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-1.96	-1.50	-1.44	-1.79	-1.91	-2.35	-2.36	-2.28	
-2.25	-1.82	-1.39	-1.33	-1.47	-1.90	-1.92	-1.84	
-1.84	-1.72	-1.70	-1.27	-1.49	-1.82	-1.46	-1.40	
-1.40	-1.26	-1.27	-0.82	-1.05	-1.36	-1.02	-1.26	
-0.97	-0.82	-1.16	-0.76	-0.61	-0.92	-0.57	-0.79	
-1.19	-0.76	-0.69	-0.62	-0.17	-0.47	-0.09	-0.34	
-0.75	-0.72	-0.24	-0.17	-0.10	-0.03	0.06	0.13	
-0.60	-0.54	-0.39	-0.48	-0.03	0.43	0.51	0.58	
map_weights [-0.54991235 -0.20704692 -0.20671343 -0.60283277 -0.41337459  0.27876807]
MAP reward
-0.60	-0.21	-0.21	-0.60	-0.41	-0.60	-0.41	-0.41	
-0.41	-0.55	-0.21	-0.21	-0.21	-0.55	-0.60	-0.41	
-0.41	-0.60	-0.55	-0.41	-0.55	-0.60	-0.41	-0.21	
-0.55	-0.55	-0.41	-0.21	-0.55	-0.55	-0.41	-0.60	
-0.21	-0.21	-0.60	-0.21	-0.55	-0.41	-0.60	-0.41	
-0.55	-0.21	-0.41	-0.41	-0.21	-0.55	-0.21	-0.60	
-0.21	-0.60	-0.21	-0.21	-0.21	-0.41	-0.55	-0.55	
-0.21	-0.21	-0.21	-0.41	-0.41	-0.21	-0.21	0.28	
Map policy
>	>	v	v	v	<	v	v	
>	>	>	v	<	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4825257116420678
mean w [-0.36361324 -0.10527618 -0.46207199 -0.48907938 -0.25934627  0.00082867]
Mean policy from posterior
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.49	-0.11	-0.11	-0.49	-0.26	-0.49	-0.26	-0.26	
-0.26	-0.36	-0.11	-0.11	-0.46	-0.36	-0.49	-0.26	
-0.26	-0.49	-0.36	-0.26	-0.36	-0.49	-0.26	-0.46	
-0.36	-0.36	-0.26	-0.11	-0.36	-0.36	-0.26	-0.49	
-0.46	-0.11	-0.49	-0.46	-0.36	-0.26	-0.49	-0.26	
-0.36	-0.11	-0.26	-0.26	-0.11	-0.36	-0.46	-0.49	
-0.46	-0.49	-0.11	-0.11	-0.11	-0.26	-0.36	-0.36	
-0.11	-0.46	-0.46	-0.26	-0.26	-0.11	-0.11	0.00	
mean = 0.044072302761302806, map = 0.021949789813743736
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.03041450046998273, 0.04407230849892585, 0.04407230393815875, 0.04407230321454492, 0.04407230374529747
==========
iteration 19
==========
weights [-3.86537140e-01 -3.90661519e-01 -3.04991611e-01 -4.46380362e-05
 -6.17544866e-01  4.72854175e-01]
expeced value MDP LP -0.437465840610313
demonstration
[(32, 3), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0)]
[-0.17316451  0.21474849 -0.69486615  0.00992958 -0.43170079  0.50457309]
w_map [-0.2220891   0.03531573 -0.63768031  0.42725274 -0.39490524  0.45199336] loglik 0.0
accepted/total = 2804/3000 = 0.9346666666666666
-------
true weights [-3.86537140e-01 -3.90661519e-01 -3.04991611e-01 -4.46380362e-05
 -6.17544866e-01  4.72854175e-01]
features
3 	4 	4 	0 	2 	3 	4 	2 	
4 	2 	0 	0 	0 	4 	1 	0 	
2 	0 	3 	3 	4 	1 	3 	1 	
1 	1 	1 	4 	0 	0 	1 	2 	
1 	2 	1 	3 	4 	3 	2 	3 	
3 	0 	0 	4 	3 	3 	0 	4 	
0 	1 	4 	1 	3 	0 	3 	2 	
1 	0 	3 	2 	1 	4 	2 	5 	
optimal policy
<	<	v	>	>	^	<	<	
^	>	v	v	^	^	<	v	
>	>	>	<	<	v	v	v	
v	^	^	^	>	v	v	v	
v	v	^	>	v	v	>	>	
<	<	<	>	v	<	v	v	
^	v	v	>	^	>	v	v	
>	>	v	<	^	>	>	.	
optimal values
-0.00	-0.62	-1.00	-0.69	-0.31	-0.00	-0.62	-0.92	
-0.62	-0.69	-0.39	-0.39	-0.69	-0.62	-1.01	-1.08	
-0.69	-0.39	-0.00	-0.00	-0.62	-0.78	-0.69	-0.70	
-0.78	-0.78	-0.40	-0.62	-0.77	-0.39	-0.70	-0.31	
-0.40	-0.69	-0.78	-0.62	-0.62	-0.00	-0.31	-0.00	
-0.00	-0.39	-0.77	-0.62	-0.00	-0.00	-0.23	-0.46	
-0.39	-0.78	-0.62	-0.40	-0.00	-0.23	0.16	0.16	
-0.78	-0.39	-0.00	-0.31	-0.40	-0.46	0.16	0.47	
map_weights [-0.2220891   0.03531573 -0.63768031  0.42725274 -0.39490524  0.45199336]
MAP reward
0.43	-0.39	-0.39	-0.22	-0.64	0.43	-0.39	-0.64	
-0.39	-0.64	-0.22	-0.22	-0.22	-0.39	0.04	-0.22	
-0.64	-0.22	0.43	0.43	-0.39	0.04	0.43	0.04	
0.04	0.04	0.04	-0.39	-0.22	-0.22	0.04	-0.64	
0.04	-0.64	0.04	0.43	-0.39	0.43	-0.64	0.43	
0.43	-0.22	-0.22	-0.39	0.43	0.43	-0.22	-0.39	
-0.22	0.04	-0.39	0.04	0.43	-0.22	0.43	-0.64	
0.04	-0.22	0.43	-0.64	0.04	-0.39	-0.64	0.45	
Map policy
<	<	v	v	>	^	<	<	
^	>	v	v	<	^	<	<	
>	>	>	<	<	v	v	<	
v	>	^	^	>	v	<	v	
v	<	^	<	v	v	<	>	
<	<	<	>	v	<	<	^	
^	v	v	>	^	^	<	<	
>	>	v	<	^	<	^	.	
expeced value MDP LP 48.36777180332956
mean w [-0.22001064 -0.08852327 -0.16746797  0.49906379 -0.11726855 -0.08725429]
Mean policy from posterior
<	<	<	>	>	^	<	<	
^	^	v	v	v	^	^	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	v	v	<	>	
<	<	>	>	>	^	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
Mean rewards
0.50	-0.12	-0.12	-0.22	-0.17	0.50	-0.12	-0.17	
-0.12	-0.17	-0.22	-0.22	-0.22	-0.12	-0.09	-0.22	
-0.17	-0.22	0.50	0.50	-0.12	-0.09	0.50	-0.09	
-0.09	-0.09	-0.09	-0.12	-0.22	-0.22	-0.09	-0.17	
-0.09	-0.17	-0.09	0.50	-0.12	0.50	-0.17	0.50	
0.50	-0.22	-0.22	-0.12	0.50	0.50	-0.22	-0.12	
-0.22	-0.09	-0.12	-0.09	0.50	-0.22	0.50	-0.17	
-0.09	-0.22	0.50	-0.17	-0.09	-0.12	-0.17	-0.09	
mean = 0.0947188203789372, map = 0.07108027456660448
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	^	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	>	>	v	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	^	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	^	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	>	>	v	v	<	>	
<	<	>	>	v	^	<	^	
^	>	v	>	^	<	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	^	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	>	^	>	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	<	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	^	>	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	<	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	^	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	^	v	v	<	>	
<	<	>	>	v	^	<	^	
^	>	v	>	^	<	<	^	
^	>	v	<	^	<	^	.	
cvar = , 0.09813280888465725, 0.09816344795662346, 0.09813326298451286, 0.09476721546456413, 0.09479226305965405
==========
iteration 20
==========
weights [-0.56426135 -0.10031456 -0.05486098 -0.05723787 -0.54609201  0.60584135]
expeced value MDP LP -0.14838274413885394
demonstration
[(32, 1), (33, 2), (25, 1), (26, 1), (27, 1), (28, 1), (29, 3), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.23964788  0.28498866 -0.05181443 -0.82305432  0.42486942 -0.02707735]
w_map [-0.68437276 -0.2335234  -0.12359125 -0.26595172 -0.62063809  0.07683749] loglik -0.6931740875229231
accepted/total = 1279/3000 = 0.42633333333333334
-------
true weights [-0.56426135 -0.10031456 -0.05486098 -0.05723787 -0.54609201  0.60584135]
features
4 	2 	4 	1 	2 	2 	1 	4 	
4 	0 	4 	2 	1 	2 	3 	4 	
0 	3 	3 	1 	0 	4 	3 	0 	
4 	2 	1 	2 	3 	3 	3 	4 	
0 	4 	1 	3 	4 	2 	2 	2 	
1 	0 	1 	2 	3 	4 	4 	1 	
0 	2 	3 	3 	0 	3 	2 	3 	
3 	1 	2 	4 	3 	2 	3 	5 	
optimal policy
>	>	>	>	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	<	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	v	
>	>	>	^	<	v	v	v	
>	>	>	^	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.26	-0.72	-0.67	-0.13	-0.03	0.03	0.04	-0.51	
-1.23	-0.69	-0.62	-0.07	-0.02	0.08	0.14	-0.41	
-0.69	-0.13	-0.07	-0.02	-0.42	-0.35	0.20	-0.37	
-0.62	-0.07	-0.02	0.09	0.14	0.20	0.26	-0.17	
-1.17	-0.62	-0.07	0.03	-0.29	0.26	0.32	0.38	
-0.78	-0.69	-0.13	-0.03	-0.08	-0.13	-0.07	0.44	
-0.76	-0.19	-0.14	-0.08	-0.15	0.42	0.48	0.54	
-0.34	-0.28	-0.18	-0.13	0.42	0.48	0.54	0.61	
map_weights [-0.68437276 -0.2335234  -0.12359125 -0.26595172 -0.62063809  0.07683749]
MAP reward
-0.62	-0.12	-0.62	-0.23	-0.12	-0.12	-0.23	-0.62	
-0.62	-0.68	-0.62	-0.12	-0.23	-0.12	-0.27	-0.62	
-0.68	-0.27	-0.27	-0.23	-0.68	-0.62	-0.27	-0.68	
-0.62	-0.12	-0.23	-0.12	-0.27	-0.27	-0.27	-0.62	
-0.68	-0.62	-0.23	-0.27	-0.62	-0.12	-0.12	-0.12	
-0.23	-0.68	-0.23	-0.12	-0.27	-0.62	-0.62	-0.23	
-0.68	-0.12	-0.27	-0.27	-0.68	-0.27	-0.12	-0.27	
-0.27	-0.23	-0.12	-0.62	-0.27	-0.12	-0.27	0.08	
Map policy
>	>	>	v	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	>	>	v	
>	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.161996645312033
mean w [-0.62181008 -0.20414698 -0.05767307 -0.22846766 -0.51411857  0.13199548]
Mean policy from posterior
>	>	>	v	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	<	
>	>	>	>	>	v	v	v	
^	^	^	^	>	>	>	v	
>	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.51	-0.06	-0.51	-0.20	-0.06	-0.06	-0.20	-0.51	
-0.51	-0.62	-0.51	-0.06	-0.20	-0.06	-0.23	-0.51	
-0.62	-0.23	-0.23	-0.20	-0.62	-0.51	-0.23	-0.62	
-0.51	-0.06	-0.20	-0.06	-0.23	-0.23	-0.23	-0.51	
-0.62	-0.51	-0.20	-0.23	-0.51	-0.06	-0.06	-0.06	
-0.20	-0.62	-0.20	-0.06	-0.23	-0.51	-0.51	-0.20	
-0.62	-0.06	-0.23	-0.23	-0.62	-0.23	-0.06	-0.23	
-0.23	-0.20	-0.06	-0.51	-0.23	-0.06	-0.23	0.13	
mean = 0.025237469446649785, map = 0.03675097954334697
CVaR policy
>	>	>	>	>	v	v	<	
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	v	<	
>	v	>	v	>	v	v	<	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	>	>	v	
>	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	v	<	
>	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	^	^	>	>	>	v	
>	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.11129876993857896, 0.07160521515635981, 0.04180178916444968, 0.03675097955467027, 0.030976078623307424
==========
iteration 21
==========
weights [-0.10255413 -0.17061703 -0.26391579 -0.67031573 -0.31661541  0.58408257]
expeced value MDP LP -0.7636550177943935
demonstration
[(32, 1), (33, 1), (34, 1), (35, 3), (43, 1), (44, 1), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.08845383 -0.05649582  0.84922535 -0.3168793   0.13875516 -0.38488305]
w_map [-0.14642431 -0.36720706 -0.26279368 -0.65487729 -0.58803035  0.00379616] loglik -9.218297236657236e-10
accepted/total = 1468/3000 = 0.48933333333333334
-------
true weights [-0.10255413 -0.17061703 -0.26391579 -0.67031573 -0.31661541  0.58408257]
features
0 	0 	1 	2 	4 	3 	3 	4 	
3 	2 	3 	3 	4 	3 	2 	0 	
1 	2 	0 	1 	4 	0 	4 	1 	
4 	4 	0 	0 	0 	1 	4 	0 	
1 	2 	2 	1 	1 	3 	3 	1 	
3 	1 	4 	0 	4 	0 	0 	1 	
1 	2 	2 	2 	2 	4 	0 	3 	
0 	4 	0 	3 	4 	2 	1 	5 	
optimal policy
>	v	<	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	<	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
optimal values
-1.47	-1.38	-1.54	-1.67	-1.42	-2.07	-1.44	-0.83	
-1.86	-1.29	-1.45	-1.42	-1.11	-1.42	-0.77	-0.52	
-1.20	-1.04	-0.78	-0.76	-0.81	-0.76	-0.73	-0.42	
-1.30	-1.00	-0.69	-0.59	-0.49	-0.66	-0.56	-0.25	
-1.17	-1.01	-0.75	-0.49	-0.39	-0.58	-0.48	-0.15	
-1.47	-0.80	-0.64	-0.33	-0.23	0.09	0.20	0.02	
-1.22	-1.06	-0.80	-0.54	-0.28	-0.02	0.30	-0.09	
-1.29	-1.20	-0.90	-0.85	-0.18	0.14	0.41	0.58	
map_weights [-0.14642431 -0.36720706 -0.26279368 -0.65487729 -0.58803035  0.00379616]
MAP reward
-0.15	-0.15	-0.37	-0.26	-0.59	-0.65	-0.65	-0.59	
-0.65	-0.26	-0.65	-0.65	-0.59	-0.65	-0.26	-0.15	
-0.37	-0.26	-0.15	-0.37	-0.59	-0.15	-0.59	-0.37	
-0.59	-0.59	-0.15	-0.15	-0.15	-0.37	-0.59	-0.15	
-0.37	-0.26	-0.26	-0.37	-0.37	-0.65	-0.65	-0.37	
-0.65	-0.37	-0.59	-0.15	-0.59	-0.15	-0.15	-0.37	
-0.37	-0.26	-0.26	-0.26	-0.26	-0.59	-0.15	-0.65	
-0.15	-0.59	-0.15	-0.65	-0.59	-0.26	-0.37	0.00	
Map policy
>	v	<	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
expeced value MDP LP -1.2848705068625228
mean w [-0.09604033 -0.24144206 -0.23694095 -0.58405154 -0.43190218  0.21127572]
Mean policy from posterior
>	v	<	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
Mean rewards
-0.10	-0.10	-0.24	-0.24	-0.43	-0.58	-0.58	-0.43	
-0.58	-0.24	-0.58	-0.58	-0.43	-0.58	-0.24	-0.10	
-0.24	-0.24	-0.10	-0.24	-0.43	-0.10	-0.43	-0.24	
-0.43	-0.43	-0.10	-0.10	-0.10	-0.24	-0.43	-0.10	
-0.24	-0.24	-0.24	-0.24	-0.24	-0.58	-0.58	-0.24	
-0.58	-0.24	-0.43	-0.10	-0.43	-0.10	-0.10	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	-0.43	-0.10	-0.58	
-0.10	-0.43	-0.10	-0.58	-0.43	-0.24	-0.24	0.21	
mean = 0.006943261942390366, map = 0.010466715117653513
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
cvar = , 0.1799227152587486, 0.052013200497884715, 0.013298940910790424, 0.011851234368922325, 0.010466713906837288
==========
iteration 22
==========
weights [-0.73515571 -0.56581614 -0.06623291 -0.25651433 -0.24663134  0.09156831]
expeced value MDP LP -1.9062027144048383
demonstration
[(32, 1), (33, 1), (34, 3), (42, 1), (43, 1), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.51286494  0.06770713  0.45167666  0.21472287  0.02548777  0.693987  ]
w_map [-0.67023973 -0.35426392 -0.05985308 -0.55662965 -0.32506354 -0.07867981] loglik -0.48540784342006305
accepted/total = 1535/3000 = 0.5116666666666667
-------
true weights [-0.73515571 -0.56581614 -0.06623291 -0.25651433 -0.24663134  0.09156831]
features
0 	2 	2 	3 	4 	1 	3 	3 	
1 	1 	1 	4 	4 	2 	4 	4 	
1 	2 	3 	3 	2 	0 	4 	4 	
1 	1 	1 	2 	4 	2 	1 	2 	
3 	2 	4 	3 	1 	3 	1 	4 	
3 	0 	2 	1 	2 	1 	4 	0 	
3 	0 	0 	3 	3 	0 	4 	0 	
2 	2 	1 	2 	2 	4 	1 	5 	
optimal policy
>	>	>	>	v	v	v	v	
v	v	>	>	v	<	v	v	
>	>	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.39	-2.68	-2.64	-2.60	-2.37	-2.73	-2.57	-2.35	
-3.51	-2.97	-2.91	-2.37	-2.14	-2.19	-2.34	-2.11	
-2.97	-2.43	-2.39	-2.15	-1.91	-2.52	-2.11	-1.89	
-2.74	-2.51	-2.46	-1.91	-1.87	-1.80	-2.06	-1.66	
-2.20	-1.96	-1.92	-1.88	-1.64	-1.75	-1.51	-1.61	
-1.98	-2.40	-1.69	-1.64	-1.08	-1.51	-0.96	-1.37	
-1.74	-2.17	-1.81	-1.08	-1.02	-1.45	-0.72	-0.64	
-1.50	-1.44	-1.39	-0.83	-0.78	-0.72	-0.48	0.09	
map_weights [-0.67023973 -0.35426392 -0.05985308 -0.55662965 -0.32506354 -0.07867981]
MAP reward
-0.67	-0.06	-0.06	-0.56	-0.33	-0.35	-0.56	-0.56	
-0.35	-0.35	-0.35	-0.33	-0.33	-0.06	-0.33	-0.33	
-0.35	-0.06	-0.56	-0.56	-0.06	-0.67	-0.33	-0.33	
-0.35	-0.35	-0.35	-0.06	-0.33	-0.06	-0.35	-0.06	
-0.56	-0.06	-0.33	-0.56	-0.35	-0.56	-0.35	-0.33	
-0.56	-0.67	-0.06	-0.35	-0.06	-0.35	-0.33	-0.67	
-0.56	-0.67	-0.67	-0.56	-0.56	-0.67	-0.33	-0.67	
-0.06	-0.06	-0.35	-0.06	-0.06	-0.33	-0.35	-0.08	
Map policy
>	v	<	>	v	v	v	v	
v	v	>	>	v	>	v	v	
>	v	v	v	v	v	v	v	
>	v	>	>	v	>	v	v	
>	>	v	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6515004196017977
mean w [-0.66472196 -0.38989244 -0.10288329 -0.35200369 -0.21852936  0.12430283]
Mean policy from posterior
>	>	>	>	v	v	v	v	
v	v	>	>	v	>	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	<	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.66	-0.10	-0.10	-0.35	-0.22	-0.39	-0.35	-0.35	
-0.39	-0.39	-0.39	-0.22	-0.22	-0.10	-0.22	-0.22	
-0.39	-0.10	-0.35	-0.35	-0.10	-0.66	-0.22	-0.22	
-0.39	-0.39	-0.39	-0.10	-0.22	-0.10	-0.39	-0.10	
-0.35	-0.10	-0.22	-0.35	-0.39	-0.35	-0.39	-0.22	
-0.35	-0.66	-0.10	-0.39	-0.10	-0.39	-0.22	-0.66	
-0.35	-0.66	-0.66	-0.35	-0.35	-0.66	-0.22	-0.66	
-0.10	-0.10	-0.39	-0.10	-0.10	-0.22	-0.39	0.12	
mean = 0.037681789883824646, map = 0.07299350496992174
CVaR policy
>	v	>	>	v	v	v	v	
v	v	>	>	v	>	v	v	
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	>	>	v	>	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	<	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	<	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.029084410080309375, 0.027102830075895756, 0.02710282665234831, 0.03768179972240593, 0.037681779896374
==========
iteration 23
==========
weights [-0.07457546 -0.5154494  -0.65889459 -0.23983411 -0.44147547  0.20539556]
expeced value MDP LP -1.8144946294694744
demonstration
[(32, 3), (40, 1), (41, 3), (49, 3), (57, 1), (58, 1), (59, 2), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.89431929 -0.11502645  0.05621552 -0.34599478 -0.02287607  0.25212307]
w_map [-0.10411907 -0.53011381 -0.62631451 -0.25613825 -0.39288201  0.30968634] loglik -0.0006583715231798237
accepted/total = 923/3000 = 0.30766666666666664
-------
true weights [-0.07457546 -0.5154494  -0.65889459 -0.23983411 -0.44147547  0.20539556]
features
1 	2 	1 	0 	0 	0 	3 	0 	
4 	0 	2 	1 	1 	0 	0 	1 	
4 	2 	3 	1 	1 	1 	1 	2 	
4 	3 	0 	4 	0 	3 	1 	0 	
2 	3 	3 	4 	2 	0 	2 	1 	
0 	3 	3 	4 	2 	2 	4 	2 	
4 	0 	1 	4 	3 	0 	0 	1 	
0 	0 	0 	3 	1 	4 	2 	5 	
optimal policy
>	>	>	>	>	v	v	<	
>	v	v	^	>	v	<	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
optimal values
-3.73	-3.25	-2.61	-2.12	-2.07	-2.01	-2.23	-2.28	
-3.26	-2.85	-2.80	-2.61	-2.45	-1.96	-2.01	-2.51	
-3.00	-2.80	-2.16	-2.38	-1.96	-1.90	-2.40	-2.18	
-2.58	-2.16	-1.94	-1.89	-1.46	-1.40	-1.90	-1.53	
-2.46	-1.98	-2.00	-1.98	-1.82	-1.17	-1.47	-1.47	
-1.82	-1.76	-1.78	-1.55	-1.34	-1.11	-0.82	-0.97	
-1.96	-1.53	-1.63	-1.12	-0.69	-0.45	-0.38	-0.31	
-1.53	-1.47	-1.41	-1.35	-1.20	-0.89	-0.46	0.21	
map_weights [-0.10411907 -0.53011381 -0.62631451 -0.25613825 -0.39288201  0.30968634]
MAP reward
-0.53	-0.63	-0.53	-0.10	-0.10	-0.10	-0.26	-0.10	
-0.39	-0.10	-0.63	-0.53	-0.53	-0.10	-0.10	-0.53	
-0.39	-0.63	-0.26	-0.53	-0.53	-0.53	-0.53	-0.63	
-0.39	-0.26	-0.10	-0.39	-0.10	-0.26	-0.53	-0.10	
-0.63	-0.26	-0.26	-0.39	-0.63	-0.10	-0.63	-0.53	
-0.10	-0.26	-0.26	-0.39	-0.63	-0.63	-0.39	-0.63	
-0.39	-0.10	-0.53	-0.39	-0.26	-0.10	-0.10	-0.53	
-0.10	-0.10	-0.10	-0.26	-0.53	-0.39	-0.63	0.31	
Map policy
v	>	>	>	>	v	v	<	
>	v	v	^	>	v	<	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	>	>	.	
expeced value MDP LP -1.1092441628532264
mean w [-0.05364829 -0.43260115 -0.62168814 -0.14150434 -0.28613339  0.47119391]
Mean policy from posterior
v	v	>	>	>	v	v	<	
v	v	v	^	>	v	<	<	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
Mean rewards
-0.43	-0.62	-0.43	-0.05	-0.05	-0.05	-0.14	-0.05	
-0.29	-0.05	-0.62	-0.43	-0.43	-0.05	-0.05	-0.43	
-0.29	-0.62	-0.14	-0.43	-0.43	-0.43	-0.43	-0.62	
-0.29	-0.14	-0.05	-0.29	-0.05	-0.14	-0.43	-0.05	
-0.62	-0.14	-0.14	-0.29	-0.62	-0.05	-0.62	-0.43	
-0.05	-0.14	-0.14	-0.29	-0.62	-0.62	-0.29	-0.62	
-0.29	-0.05	-0.43	-0.29	-0.14	-0.05	-0.05	-0.43	
-0.05	-0.05	-0.05	-0.14	-0.43	-0.29	-0.62	0.47	
mean = 0.018203913152550433, map = 0.004974471307937156
CVaR policy
v	>	>	>	>	v	v	<	
v	v	v	^	>	v	<	<	
v	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	>	>	.	
CVaR policy
v	>	>	>	>	v	v	<	
v	v	v	^	>	v	<	<	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
CVaR policy
v	>	>	>	>	v	v	<	
v	v	v	^	>	v	<	<	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
CVaR policy
v	v	>	>	>	v	v	<	
v	v	v	^	>	v	<	<	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
CVaR policy
v	v	>	>	>	v	v	<	
v	v	v	^	>	v	<	<	
v	v	v	v	v	v	<	v	
>	v	v	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
cvar = , 0.011258764758389495, 0.014041786696720626, 0.014041779839576218, 0.01820391031474311, 0.018203910328222772
==========
iteration 24
==========
weights [-0.3701546  -0.72728912 -0.3132146  -0.20491508 -0.35992136  0.25376989]
expeced value MDP LP -1.807155645393347
demonstration
[(32, 1), (33, 1), (34, 1), (35, 3), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.01028128 -0.61121081  0.25327404  0.4586244  -0.42697045  0.41173753]
w_map [-0.20055443 -0.71147573 -0.47030332 -0.20817058 -0.43454452  0.01520052] loglik -1.4330989728250643e-08
accepted/total = 1572/3000 = 0.524
-------
true weights [-0.3701546  -0.72728912 -0.3132146  -0.20491508 -0.35992136  0.25376989]
features
1 	2 	2 	3 	2 	2 	4 	2 	
0 	1 	1 	2 	4 	4 	4 	4 	
0 	0 	0 	3 	1 	0 	0 	2 	
3 	4 	1 	0 	0 	4 	2 	4 	
3 	0 	2 	3 	1 	2 	1 	2 	
4 	1 	2 	4 	1 	3 	2 	1 	
2 	0 	4 	4 	0 	3 	3 	3 	
3 	4 	0 	1 	3 	0 	2 	5 	
optimal policy
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.86	-3.16	-2.88	-2.59	-2.51	-2.22	-2.53	-2.27	
-3.32	-3.51	-3.12	-2.41	-2.27	-1.92	-2.19	-1.98	
-2.98	-2.81	-2.47	-2.12	-2.29	-1.58	-1.85	-1.64	
-2.63	-2.61	-2.63	-1.93	-1.58	-1.22	-1.49	-1.34	
-2.45	-2.27	-1.92	-1.62	-1.59	-0.87	-1.19	-0.99	
-2.42	-2.44	-1.73	-1.43	-1.29	-0.56	-0.47	-0.68	
-2.08	-1.79	-1.43	-1.08	-0.73	-0.36	-0.16	0.05	
-2.24	-2.05	-1.71	-1.35	-0.63	-0.43	-0.06	0.25	
map_weights [-0.20055443 -0.71147573 -0.47030332 -0.20817058 -0.43454452  0.01520052]
MAP reward
-0.71	-0.47	-0.47	-0.21	-0.47	-0.47	-0.43	-0.47	
-0.20	-0.71	-0.71	-0.47	-0.43	-0.43	-0.43	-0.43	
-0.20	-0.20	-0.20	-0.21	-0.71	-0.20	-0.20	-0.47	
-0.21	-0.43	-0.71	-0.20	-0.20	-0.43	-0.47	-0.43	
-0.21	-0.20	-0.47	-0.21	-0.71	-0.47	-0.71	-0.47	
-0.43	-0.71	-0.47	-0.43	-0.71	-0.21	-0.47	-0.71	
-0.47	-0.20	-0.43	-0.43	-0.20	-0.21	-0.21	-0.21	
-0.21	-0.43	-0.20	-0.71	-0.21	-0.20	-0.47	0.02	
Map policy
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	<	v	
v	v	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5610921911351188
mean w [-0.2628716  -0.69873504 -0.34589078 -0.11750282 -0.23738057 -0.07923435]
Mean policy from posterior
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	<	v	
v	>	>	v	v	v	<	v	
v	v	>	v	>	v	<	v	
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.70	-0.35	-0.35	-0.12	-0.35	-0.35	-0.24	-0.35	
-0.26	-0.70	-0.70	-0.35	-0.24	-0.24	-0.24	-0.24	
-0.26	-0.26	-0.26	-0.12	-0.70	-0.26	-0.26	-0.35	
-0.12	-0.24	-0.70	-0.26	-0.26	-0.24	-0.35	-0.24	
-0.12	-0.26	-0.35	-0.12	-0.70	-0.35	-0.70	-0.35	
-0.24	-0.70	-0.35	-0.24	-0.70	-0.12	-0.35	-0.70	
-0.35	-0.26	-0.24	-0.24	-0.26	-0.12	-0.12	-0.12	
-0.12	-0.24	-0.26	-0.70	-0.12	-0.26	-0.35	-0.08	
mean = 0.019502875479745274, map = 0.03836266200725724
CVaR policy
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	<	v	
v	v	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	<	v	
v	>	>	v	v	v	<	v	
v	v	>	v	>	v	<	v	
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	<	v	
v	>	>	v	v	v	<	v	
v	v	>	v	>	v	<	v	
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	v	v	v	>	v	<	v	
v	>	>	v	v	v	<	v	
v	v	>	v	>	v	<	v	
>	>	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
cvar = , 0.022063645904407903, 0.029571475318887153, 0.019502875632242844, 0.019502878764219744, 0.019502875484496363
==========
iteration 25
==========
weights [-0.14094762 -0.52527219 -0.49991509 -0.08408736 -0.62976705  0.22501239]
expeced value MDP LP -1.1743076201594294
demonstration
[(32, 1), (33, 1), (34, 3), (42, 1), (43, 1), (44, 3), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.47225102  0.06371376 -0.11742312 -0.16142987  0.84543695  0.13530743]
w_map [-0.2386908  -0.57636642 -0.2861692  -0.24491888 -0.60751483  0.31603184] loglik -2.757882811010859e-11
accepted/total = 2022/3000 = 0.674
-------
true weights [-0.14094762 -0.52527219 -0.49991509 -0.08408736 -0.62976705  0.22501239]
features
4 	3 	1 	1 	2 	4 	1 	4 	
3 	1 	0 	3 	0 	4 	3 	3 	
2 	1 	0 	4 	1 	2 	1 	2 	
4 	0 	3 	1 	2 	3 	4 	0 	
2 	4 	0 	4 	0 	4 	0 	2 	
1 	1 	0 	3 	2 	1 	1 	3 	
0 	2 	0 	1 	0 	3 	3 	3 	
0 	0 	2 	3 	0 	3 	4 	5 	
optimal policy
>	>	v	v	v	>	v	v	
>	>	v	<	<	>	>	v	
>	v	v	<	v	v	>	v	
>	>	v	<	v	v	v	v	
>	>	v	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
optimal values
-2.53	-1.92	-1.86	-1.93	-2.03	-2.35	-1.74	-1.77	
-1.92	-1.86	-1.35	-1.42	-1.54	-1.84	-1.22	-1.15	
-2.21	-1.73	-1.22	-1.83	-1.81	-1.74	-1.59	-1.08	
-1.83	-1.22	-1.09	-1.60	-1.30	-1.25	-1.21	-0.58	
-2.12	-1.63	-1.01	-1.37	-0.80	-1.18	-0.58	-0.45	
-1.74	-1.40	-0.88	-0.75	-0.67	-0.56	-0.47	0.05	
-1.23	-1.32	-0.83	-0.70	-0.17	-0.03	0.05	0.14	
-1.10	-0.97	-0.83	-0.34	-0.25	-0.12	-0.41	0.23	
map_weights [-0.2386908  -0.57636642 -0.2861692  -0.24491888 -0.60751483  0.31603184]
MAP reward
-0.61	-0.24	-0.58	-0.58	-0.29	-0.61	-0.58	-0.61	
-0.24	-0.58	-0.24	-0.24	-0.24	-0.61	-0.24	-0.24	
-0.29	-0.58	-0.24	-0.61	-0.58	-0.29	-0.58	-0.29	
-0.61	-0.24	-0.24	-0.58	-0.29	-0.24	-0.61	-0.24	
-0.29	-0.61	-0.24	-0.61	-0.24	-0.61	-0.24	-0.29	
-0.58	-0.58	-0.24	-0.24	-0.29	-0.58	-0.58	-0.24	
-0.24	-0.29	-0.24	-0.58	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.29	-0.24	-0.24	-0.24	-0.61	0.32	
Map policy
>	>	v	v	v	>	v	v	
>	>	v	>	v	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5889864230057058
mean w [-0.23934243 -0.67049248 -0.26264672 -0.16787866 -0.46921168  0.04639018]
Mean policy from posterior
v	>	v	v	v	v	v	v	
v	>	v	>	>	>	>	v	
v	v	v	<	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	>	v	
v	>	>	>	v	v	>	v	
v	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.47	-0.17	-0.67	-0.67	-0.26	-0.47	-0.67	-0.47	
-0.17	-0.67	-0.24	-0.17	-0.24	-0.47	-0.17	-0.17	
-0.26	-0.67	-0.24	-0.47	-0.67	-0.26	-0.67	-0.26	
-0.47	-0.24	-0.17	-0.67	-0.26	-0.17	-0.47	-0.24	
-0.26	-0.47	-0.24	-0.47	-0.24	-0.47	-0.24	-0.26	
-0.67	-0.67	-0.24	-0.17	-0.26	-0.67	-0.67	-0.17	
-0.24	-0.26	-0.24	-0.67	-0.24	-0.17	-0.17	-0.17	
-0.24	-0.24	-0.26	-0.17	-0.24	-0.17	-0.47	0.05	
mean = 0.1141461951266991, map = 0.08551105461673059
CVaR policy
v	v	v	>	v	v	v	v	
v	>	v	>	>	>	>	v	
v	>	v	v	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	v	v	
v	>	v	>	>	>	>	v	
v	>	v	v	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	>	v	
v	>	>	>	v	v	v	v	
>	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	v	v	
v	>	v	>	>	>	>	v	
v	>	v	v	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	>	v	
v	>	>	>	v	v	>	v	
>	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	>	v	>	>	>	>	v	
v	>	v	<	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	>	v	
v	>	>	>	v	v	v	v	
v	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	>	v	>	>	>	>	v	
v	>	v	<	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	>	v	
v	>	>	>	v	v	v	v	
v	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.13250340242589842, 0.13434444928529565, 0.1343444489225325, 0.11414619903955514, 0.11414619720539343
==========
iteration 26
==========
weights [-0.5936157  -0.01014504 -0.01389096 -0.2426591  -0.22906451  0.73210008]
expeced value MDP LP -0.33357797309954346
demonstration
[(32, 3), (40, 3), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.09081601  0.80109188  0.41732979 -0.16848211 -0.22175162 -0.31349657]
w_map [-0.63833035 -0.40525774 -0.25541095 -0.45272053 -0.31467739  0.24308031] loglik -1.3923516117841928e-08
accepted/total = 1938/3000 = 0.646
-------
true weights [-0.5936157  -0.01014504 -0.01389096 -0.2426591  -0.22906451  0.73210008]
features
4 	1 	1 	1 	4 	1 	0 	3 	
3 	2 	1 	3 	4 	2 	0 	1 	
0 	4 	1 	2 	3 	2 	1 	3 	
1 	3 	0 	4 	0 	4 	4 	0 	
3 	4 	1 	4 	0 	1 	4 	0 	
2 	0 	3 	4 	4 	0 	1 	0 	
2 	2 	4 	2 	3 	2 	0 	1 	
1 	1 	0 	0 	3 	3 	2 	5 	
optimal policy
>	>	v	<	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	<	v	v	>	v	v	<	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
optimal values
-0.76	-0.53	-0.53	-0.53	-0.61	-0.39	-0.98	-0.84	
-0.77	-0.53	-0.52	-0.75	-0.61	-0.38	-0.95	-0.61	
-0.93	-0.74	-0.52	-0.51	-0.61	-0.37	-0.36	-0.60	
-0.34	-0.58	-0.88	-0.51	-0.95	-0.36	-0.36	-0.95	
-0.33	-0.51	-0.29	-0.28	-0.63	-0.14	-0.13	-0.48	
-0.09	-0.66	-0.29	-0.05	-0.04	-0.16	0.10	0.11	
-0.08	-0.06	-0.05	0.18	0.20	0.44	0.11	0.71	
-0.08	-0.07	-0.64	-0.38	0.21	0.46	0.71	0.73	
map_weights [-0.63833035 -0.40525774 -0.25541095 -0.45272053 -0.31467739  0.24308031]
MAP reward
-0.31	-0.41	-0.41	-0.41	-0.31	-0.41	-0.64	-0.45	
-0.45	-0.26	-0.41	-0.45	-0.31	-0.26	-0.64	-0.41	
-0.64	-0.31	-0.41	-0.26	-0.45	-0.26	-0.41	-0.45	
-0.41	-0.45	-0.64	-0.31	-0.64	-0.31	-0.31	-0.64	
-0.45	-0.31	-0.41	-0.31	-0.64	-0.41	-0.31	-0.64	
-0.26	-0.64	-0.45	-0.31	-0.31	-0.64	-0.41	-0.64	
-0.26	-0.26	-0.31	-0.26	-0.45	-0.26	-0.64	-0.41	
-0.41	-0.41	-0.64	-0.64	-0.45	-0.45	-0.26	0.24	
Map policy
>	v	v	v	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	>	v	v	<	
v	v	>	v	>	>	v	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.709155025172771
mean w [-0.54894234 -0.40607487 -0.12240244 -0.31259618 -0.32557904  0.1651629 ]
Mean policy from posterior
v	v	>	v	v	v	<	v	
>	>	>	v	>	v	<	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	<	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.33	-0.41	-0.41	-0.41	-0.33	-0.41	-0.55	-0.31	
-0.31	-0.12	-0.41	-0.31	-0.33	-0.12	-0.55	-0.41	
-0.55	-0.33	-0.41	-0.12	-0.31	-0.12	-0.41	-0.31	
-0.41	-0.31	-0.55	-0.33	-0.55	-0.33	-0.33	-0.55	
-0.31	-0.33	-0.41	-0.33	-0.55	-0.41	-0.33	-0.55	
-0.12	-0.55	-0.31	-0.33	-0.33	-0.55	-0.41	-0.55	
-0.12	-0.12	-0.33	-0.12	-0.31	-0.12	-0.55	-0.41	
-0.41	-0.41	-0.55	-0.55	-0.31	-0.31	-0.12	0.17	
mean = 0.08827307337002294, map = 0.07669315438373714
CVaR policy
>	v	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	<	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	<	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	<	v	
>	>	>	v	>	v	<	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	<	v	
>	>	>	v	>	v	<	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.10384433085051187, 0.09743668754824303, 0.09756404816676528, 0.09827323349375572, 0.08827307522663469
==========
iteration 27
==========
weights [-0.01850592 -0.18489183 -0.17904333 -0.63184099 -0.50212388  0.53109754]
expeced value MDP LP -0.5220450183958818
demonstration
[(32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.15996162 -0.45915217  0.73721584 -0.07200262 -0.2675415  -0.37860471]
w_map [-0.22742997 -0.46052988 -0.20562698 -0.68408069 -0.4428461  -0.172703  ] loglik -2.9654216859853477e-09
accepted/total = 1827/3000 = 0.609
-------
true weights [-0.01850592 -0.18489183 -0.17904333 -0.63184099 -0.50212388  0.53109754]
features
0 	2 	3 	1 	3 	4 	3 	4 	
0 	3 	3 	4 	4 	4 	0 	1 	
2 	1 	3 	4 	1 	0 	1 	1 	
0 	2 	4 	3 	3 	2 	1 	2 	
0 	1 	2 	4 	2 	2 	2 	1 	
3 	2 	1 	3 	3 	1 	4 	3 	
0 	2 	2 	1 	2 	0 	1 	4 	
4 	4 	3 	3 	1 	0 	0 	5 	
optimal policy
v	<	<	v	v	v	v	v	
v	<	>	>	v	v	v	<	
v	v	>	>	>	v	<	<	
v	v	v	v	v	v	v	<	
>	v	v	>	>	v	<	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
optimal values
-0.84	-1.01	-1.64	-1.46	-1.42	-1.11	-0.94	-0.99	
-0.83	-1.46	-1.90	-1.29	-0.79	-0.61	-0.31	-0.49	
-0.82	-0.98	-1.42	-0.79	-0.29	-0.11	-0.29	-0.47	
-0.65	-0.80	-0.95	-1.22	-0.72	-0.09	-0.27	-0.45	
-0.64	-0.63	-0.45	-0.59	-0.09	0.09	-0.09	-0.27	
-0.91	-0.45	-0.27	-0.54	-0.36	0.27	-0.19	-0.61	
-0.28	-0.27	-0.09	0.09	0.28	0.46	0.32	0.02	
-0.78	-0.77	-0.72	-0.34	0.29	0.48	0.51	0.53	
map_weights [-0.22742997 -0.46052988 -0.20562698 -0.68408069 -0.4428461  -0.172703  ]
MAP reward
-0.23	-0.21	-0.68	-0.46	-0.68	-0.44	-0.68	-0.44	
-0.23	-0.68	-0.68	-0.44	-0.44	-0.44	-0.23	-0.46	
-0.21	-0.46	-0.68	-0.44	-0.46	-0.23	-0.46	-0.46	
-0.23	-0.21	-0.44	-0.68	-0.68	-0.21	-0.46	-0.21	
-0.23	-0.46	-0.21	-0.44	-0.21	-0.21	-0.21	-0.46	
-0.68	-0.21	-0.46	-0.68	-0.68	-0.46	-0.44	-0.68	
-0.23	-0.21	-0.21	-0.46	-0.21	-0.23	-0.46	-0.44	
-0.44	-0.44	-0.68	-0.68	-0.46	-0.23	-0.23	-0.17	
Map policy
v	<	>	v	>	v	v	v	
v	v	>	>	>	v	<	v	
v	v	v	>	>	v	<	v	
>	v	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
expeced value MDP LP -1.7983290309437487
mean w [-0.1850766  -0.38263008 -0.15315261 -0.62948108 -0.41993703 -0.07056852]
Mean policy from posterior
v	<	<	v	v	v	v	v	
v	v	>	v	v	v	v	<	
v	v	v	>	>	v	<	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.19	-0.15	-0.63	-0.38	-0.63	-0.42	-0.63	-0.42	
-0.19	-0.63	-0.63	-0.42	-0.42	-0.42	-0.19	-0.38	
-0.15	-0.38	-0.63	-0.42	-0.38	-0.19	-0.38	-0.38	
-0.19	-0.15	-0.42	-0.63	-0.63	-0.15	-0.38	-0.15	
-0.19	-0.38	-0.15	-0.42	-0.15	-0.15	-0.15	-0.38	
-0.63	-0.15	-0.38	-0.63	-0.63	-0.38	-0.42	-0.63	
-0.19	-0.15	-0.15	-0.38	-0.15	-0.19	-0.38	-0.42	
-0.42	-0.42	-0.63	-0.63	-0.38	-0.19	-0.19	-0.07	
mean = 0.0564211375986039, map = 0.1396905137536798
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	<	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	<	<	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	v	v	v	>	v	v	v	
>	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	<	<	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	v	v	v	>	v	v	v	
>	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
cvar = , 0.11962117744266865, 0.11033689255942702, 0.10196606035166211, 0.08184434024260501, 0.08184434019845199
==========
iteration 28
==========
weights [-0.5100235  -0.34968658 -0.25484199 -0.37175441 -0.63466652  0.10792566]
expeced value MDP LP -2.4444006017462887
demonstration
[(32, 1), (33, 1), (34, 1), (35, 3), (43, 1), (44, 3), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.10316213  0.54972129  0.35304982 -0.47706229 -0.0500638   0.57656316]
w_map [-0.5329226  -0.33247877 -0.17654402 -0.17080512 -0.72002255 -0.16333008] loglik -0.6931477298736723
accepted/total = 1308/3000 = 0.436
-------
true weights [-0.5100235  -0.34968658 -0.25484199 -0.37175441 -0.63466652  0.10792566]
features
2 	2 	3 	3 	3 	1 	2 	2 	
4 	2 	0 	1 	0 	4 	0 	0 	
4 	1 	1 	4 	4 	3 	3 	4 	
4 	4 	0 	2 	4 	1 	4 	2 	
2 	4 	2 	1 	4 	4 	4 	4 	
1 	0 	0 	3 	0 	2 	0 	3 	
2 	4 	2 	4 	2 	2 	2 	3 	
4 	4 	4 	1 	2 	4 	0 	5 	
optimal policy
>	v	>	v	>	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
optimal values
-4.21	-3.99	-4.04	-3.70	-3.59	-3.26	-3.07	-2.84	
-4.37	-3.78	-3.72	-3.36	-3.42	-2.94	-2.96	-2.61	
-4.16	-3.56	-3.24	-3.04	-2.94	-2.32	-2.47	-2.12	
-3.87	-3.53	-2.92	-2.43	-2.59	-1.97	-2.12	-1.50	
-3.27	-3.04	-2.43	-2.20	-2.13	-1.64	-1.65	-1.26	
-3.05	-2.85	-2.36	-1.87	-1.51	-1.01	-1.02	-0.63	
-2.72	-2.49	-1.88	-1.64	-1.01	-0.77	-0.52	-0.26	
-3.33	-2.83	-2.21	-1.60	-1.26	-1.03	-0.40	0.11	
map_weights [-0.5329226  -0.33247877 -0.17654402 -0.17080512 -0.72002255 -0.16333008]
MAP reward
-0.18	-0.18	-0.17	-0.17	-0.17	-0.33	-0.18	-0.18	
-0.72	-0.18	-0.53	-0.33	-0.53	-0.72	-0.53	-0.53	
-0.72	-0.33	-0.33	-0.72	-0.72	-0.17	-0.17	-0.72	
-0.72	-0.72	-0.53	-0.18	-0.72	-0.33	-0.72	-0.18	
-0.18	-0.72	-0.18	-0.33	-0.72	-0.72	-0.72	-0.72	
-0.33	-0.53	-0.53	-0.17	-0.53	-0.18	-0.53	-0.17	
-0.18	-0.72	-0.18	-0.72	-0.18	-0.18	-0.18	-0.17	
-0.72	-0.72	-0.72	-0.33	-0.18	-0.72	-0.53	-0.16	
Map policy
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	^	>	.	
expeced value MDP LP -1.7984591642273662
mean w [-0.46517397 -0.22290369 -0.15869862 -0.13171257 -0.59380738 -0.09040962]
Mean policy from posterior
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	^	>	.	
Mean rewards
-0.16	-0.16	-0.13	-0.13	-0.13	-0.22	-0.16	-0.16	
-0.59	-0.16	-0.47	-0.22	-0.47	-0.59	-0.47	-0.47	
-0.59	-0.22	-0.22	-0.59	-0.59	-0.13	-0.13	-0.59	
-0.59	-0.59	-0.47	-0.16	-0.59	-0.22	-0.59	-0.16	
-0.16	-0.59	-0.16	-0.22	-0.59	-0.59	-0.59	-0.59	
-0.22	-0.47	-0.47	-0.13	-0.47	-0.16	-0.47	-0.13	
-0.16	-0.59	-0.16	-0.59	-0.16	-0.16	-0.16	-0.13	
-0.59	-0.59	-0.59	-0.22	-0.16	-0.59	-0.47	-0.09	
mean = 0.020224989931718174, map = 0.022165231506845462
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
cvar = , 0.010487510395359667, 0.007041207283185802, 0.005428770293554042, 0.014600632384993428, 0.014600632411478909
==========
iteration 29
==========
weights [-0.52871357 -0.78382907 -0.29487581 -0.01777343 -0.12584647  0.05448833]
expeced value MDP LP -1.3249246001259218
demonstration
[(32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0)]
[-0.38488636 -0.19556881 -0.22263601 -0.73710554 -0.31245287  0.35085206]
w_map [-0.51752259 -0.28118477 -0.2291812   0.12286364 -0.35980726  0.67529603] loglik 0.0
accepted/total = 2768/3000 = 0.9226666666666666
-------
true weights [-0.52871357 -0.78382907 -0.29487581 -0.01777343 -0.12584647  0.05448833]
features
3 	4 	2 	4 	1 	3 	1 	1 	
1 	3 	2 	0 	1 	0 	0 	1 	
1 	0 	2 	0 	1 	3 	2 	2 	
2 	3 	0 	0 	3 	4 	4 	2 	
3 	1 	1 	4 	0 	1 	4 	2 	
0 	1 	4 	1 	2 	3 	3 	4 	
1 	2 	0 	2 	4 	2 	4 	0 	
2 	4 	4 	4 	1 	1 	4 	5 	
optimal policy
<	<	<	>	>	v	<	v	
^	^	<	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	v	<	
<	^	>	>	v	v	v	v	
^	>	>	>	>	>	v	<	
>	v	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
optimal values
-1.78	-1.89	-2.16	-2.00	-1.89	-1.12	-1.89	-2.58	
-2.54	-1.88	-2.16	-2.15	-1.89	-1.12	-1.27	-1.81	
-2.68	-2.15	-1.91	-1.63	-1.37	-0.59	-0.75	-1.04	
-1.91	-1.63	-1.63	-1.12	-0.59	-0.58	-0.46	-0.75	
-1.78	-2.40	-1.93	-1.16	-1.04	-1.01	-0.34	-0.63	
-2.29	-2.18	-1.41	-1.30	-0.52	-0.23	-0.21	-0.34	
-2.30	-1.53	-1.42	-0.90	-0.61	-0.49	-0.20	-0.47	
-1.53	-1.25	-1.13	-1.02	-1.39	-0.86	-0.07	0.05	
map_weights [-0.51752259 -0.28118477 -0.2291812   0.12286364 -0.35980726  0.67529603]
MAP reward
0.12	-0.36	-0.23	-0.36	-0.28	0.12	-0.28	-0.28	
-0.28	0.12	-0.23	-0.52	-0.28	-0.52	-0.52	-0.28	
-0.28	-0.52	-0.23	-0.52	-0.28	0.12	-0.23	-0.23	
-0.23	0.12	-0.52	-0.52	0.12	-0.36	-0.36	-0.23	
0.12	-0.28	-0.28	-0.36	-0.52	-0.28	-0.36	-0.23	
-0.52	-0.28	-0.36	-0.28	-0.23	0.12	0.12	-0.36	
-0.28	-0.23	-0.52	-0.23	-0.36	-0.23	-0.36	-0.52	
-0.23	-0.36	-0.36	-0.36	-0.28	-0.28	-0.36	0.68	
Map policy
<	<	<	>	>	^	<	<	
^	<	<	<	^	^	^	^	
v	v	^	>	>	^	<	<	
v	<	<	>	>	v	v	v	
<	<	<	v	v	v	v	<	
^	^	>	>	>	>	<	<	
^	^	>	^	>	^	^	<	
^	^	>	^	>	^	^	.	
expeced value MDP LP 39.29586716430508
mean w [-0.18596659 -0.11031624 -0.28572299  0.41019836 -0.18510741  0.13041273]
Mean policy from posterior
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	<	
<	<	<	<	>	v	v	<	
^	^	^	>	>	>	<	<	
^	^	^	^	^	^	^	^	
^	^	^	>	>	^	^	.	
Mean rewards
0.41	-0.19	-0.29	-0.19	-0.11	0.41	-0.11	-0.11	
-0.11	0.41	-0.29	-0.19	-0.11	-0.19	-0.19	-0.11	
-0.11	-0.19	-0.29	-0.19	-0.11	0.41	-0.29	-0.29	
-0.29	0.41	-0.19	-0.19	0.41	-0.19	-0.19	-0.29	
0.41	-0.11	-0.11	-0.19	-0.19	-0.11	-0.19	-0.29	
-0.19	-0.11	-0.19	-0.11	-0.29	0.41	0.41	-0.19	
-0.11	-0.29	-0.19	-0.29	-0.19	-0.29	-0.19	-0.19	
-0.29	-0.19	-0.19	-0.19	-0.11	-0.11	-0.19	0.13	
mean = 1.3776135477921054, map = 1.2806201365706567
CVaR policy
<	<	<	>	>	^	<	<	
^	^	<	<	>	^	<	<	
v	v	^	>	>	^	<	<	
v	<	<	>	v	^	v	v	
<	<	<	>	v	v	v	<	
^	<	>	>	>	>	<	<	
^	<	>	>	>	^	^	<	
^	<	>	^	^	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	^	<	<	>	^	<	<	
v	^	<	>	>	^	<	<	
v	<	<	>	>	^	v	<	
<	<	<	>	v	v	v	v	
^	<	<	>	>	>	<	<	
^	^	>	>	^	^	^	^	
^	^	>	^	^	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	<	
<	<	<	<	>	v	v	v	
^	^	^	>	>	>	<	<	
^	^	^	^	^	^	^	^	
^	^	^	>	>	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	<	
<	<	<	<	>	v	v	v	
^	^	^	>	>	>	<	<	
^	^	^	^	^	^	^	^	
^	^	^	>	>	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	<	
<	<	<	<	>	v	v	<	
^	^	^	>	>	>	<	<	
^	^	^	^	^	^	^	^	
^	^	^	>	>	^	^	.	
cvar = , 1.1519916878538312, 1.148556039188472, 1.3776064300893418, 1.3776089463746592, 1.3776125999938886
==========
iteration 30
==========
weights [-0.31004306 -0.29634479 -0.21476876 -0.59271379 -0.28141103  0.58260248]
expeced value MDP LP -1.3929046319665854
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.81671575  0.31098816 -0.21700337  0.08125557 -0.06557139 -0.42221939]
w_map [-0.30285524 -0.46210822 -0.22788226 -0.7535488  -0.24356798  0.12507284] loglik -2.5667219460956403e-09
accepted/total = 1790/3000 = 0.5966666666666667
-------
true weights [-0.31004306 -0.29634479 -0.21476876 -0.59271379 -0.28141103  0.58260248]
features
0 	3 	2 	3 	2 	4 	3 	2 	
3 	3 	2 	3 	0 	1 	0 	2 	
1 	3 	3 	1 	3 	1 	1 	2 	
1 	2 	2 	1 	2 	4 	0 	1 	
2 	1 	3 	4 	4 	4 	2 	0 	
4 	4 	0 	2 	1 	0 	3 	4 	
3 	4 	0 	3 	2 	3 	4 	0 	
1 	3 	1 	1 	2 	0 	0 	5 	
optimal policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.32	-3.25	-2.68	-2.65	-2.08	-1.88	-1.82	-1.24	
-3.04	-3.06	-2.49	-2.38	-1.91	-1.62	-1.34	-1.04	
-2.47	-2.50	-2.30	-1.81	-1.82	-1.37	-1.12	-0.83	
-2.20	-1.92	-1.73	-1.53	-1.24	-1.09	-0.84	-0.62	
-2.00	-1.82	-1.82	-1.24	-1.04	-0.81	-0.54	-0.33	
-1.81	-1.54	-1.27	-0.97	-0.76	-0.91	-0.61	-0.02	
-1.99	-1.42	-1.15	-1.06	-0.47	-0.61	-0.02	0.27	
-1.71	-1.43	-0.84	-0.55	-0.26	-0.05	0.27	0.58	
map_weights [-0.30285524 -0.46210822 -0.22788226 -0.7535488  -0.24356798  0.12507284]
MAP reward
-0.30	-0.75	-0.23	-0.75	-0.23	-0.24	-0.75	-0.23	
-0.75	-0.75	-0.23	-0.75	-0.30	-0.46	-0.30	-0.23	
-0.46	-0.75	-0.75	-0.46	-0.75	-0.46	-0.46	-0.23	
-0.46	-0.23	-0.23	-0.46	-0.23	-0.24	-0.30	-0.46	
-0.23	-0.46	-0.75	-0.24	-0.24	-0.24	-0.23	-0.30	
-0.24	-0.24	-0.30	-0.23	-0.46	-0.30	-0.75	-0.24	
-0.75	-0.24	-0.30	-0.75	-0.23	-0.75	-0.24	-0.30	
-0.46	-0.75	-0.46	-0.46	-0.23	-0.30	-0.30	0.13	
Map policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.021193742226311
mean w [-0.30080788 -0.42636919 -0.15747481 -0.69206904 -0.27204476 -0.05232223]
Mean policy from posterior
>	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.30	-0.69	-0.16	-0.69	-0.16	-0.27	-0.69	-0.16	
-0.69	-0.69	-0.16	-0.69	-0.30	-0.43	-0.30	-0.16	
-0.43	-0.69	-0.69	-0.43	-0.69	-0.43	-0.43	-0.16	
-0.43	-0.16	-0.16	-0.43	-0.16	-0.27	-0.30	-0.43	
-0.16	-0.43	-0.69	-0.27	-0.27	-0.27	-0.16	-0.30	
-0.27	-0.27	-0.30	-0.16	-0.43	-0.30	-0.69	-0.27	
-0.69	-0.27	-0.30	-0.69	-0.16	-0.69	-0.27	-0.30	
-0.43	-0.69	-0.43	-0.43	-0.16	-0.30	-0.30	-0.05	
mean = 0.026825460225989195, map = 0.02026006149480475
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.035290302136040896, 0.026825460218940833, 0.026825462107370246, 0.02682546421967169, 0.026825460191895578
==========
iteration 31
==========
weights [-0.05358524 -0.25940048 -0.6119391  -0.55979138 -0.36199438  0.33311294]
expeced value MDP LP -1.3212044259615796
demonstration
[(32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.77759312  0.35829653  0.27809404  0.31089256  0.13765598 -0.27208984]
w_map [-0.22312306 -0.20794643 -0.68269062 -0.50136363 -0.41197334  0.14078487] loglik -0.6931471951799244
accepted/total = 1807/3000 = 0.6023333333333334
-------
true weights [-0.05358524 -0.25940048 -0.6119391  -0.55979138 -0.36199438  0.33311294]
features
4 	1 	0 	2 	1 	0 	0 	0 	
0 	1 	4 	2 	0 	4 	0 	0 	
0 	0 	3 	3 	0 	0 	3 	2 	
0 	2 	1 	4 	1 	2 	1 	0 	
1 	1 	2 	1 	2 	4 	3 	0 	
1 	4 	4 	1 	1 	4 	2 	4 	
2 	1 	1 	0 	4 	3 	4 	4 	
2 	0 	3 	2 	0 	1 	1 	5 	
optimal policy
v	>	>	>	>	>	>	v	
v	v	>	>	v	>	>	v	
v	<	v	>	>	>	v	v	
v	>	>	v	>	>	>	v	
v	v	>	v	v	>	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-2.43	-2.38	-2.14	-2.11	-1.51	-1.27	-1.23	-1.18	
-2.09	-2.33	-2.36	-2.02	-1.42	-1.53	-1.18	-1.14	
-2.06	-2.09	-2.29	-1.93	-1.38	-1.34	-1.30	-1.10	
-2.02	-2.34	-1.75	-1.50	-1.60	-1.35	-0.75	-0.49	
-1.99	-1.75	-1.75	-1.15	-1.46	-1.35	-1.00	-0.44	
-1.75	-1.50	-1.25	-0.90	-0.85	-1.10	-0.90	-0.39	
-1.75	-1.15	-0.90	-0.65	-0.60	-0.75	-0.29	-0.03	
-1.79	-1.19	-1.40	-0.85	-0.24	-0.19	0.07	0.33	
map_weights [-0.22312306 -0.20794643 -0.68269062 -0.50136363 -0.41197334  0.14078487]
MAP reward
-0.41	-0.21	-0.22	-0.68	-0.21	-0.22	-0.22	-0.22	
-0.22	-0.21	-0.41	-0.68	-0.22	-0.41	-0.22	-0.22	
-0.22	-0.22	-0.50	-0.50	-0.22	-0.22	-0.50	-0.68	
-0.22	-0.68	-0.21	-0.41	-0.21	-0.68	-0.21	-0.22	
-0.21	-0.21	-0.68	-0.21	-0.68	-0.41	-0.50	-0.22	
-0.21	-0.41	-0.41	-0.21	-0.21	-0.41	-0.68	-0.41	
-0.68	-0.21	-0.21	-0.22	-0.41	-0.50	-0.41	-0.41	
-0.68	-0.22	-0.50	-0.68	-0.22	-0.21	-0.21	0.14	
Map policy
v	v	v	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	>	v	v	
v	>	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.5927359088694835
mean w [-0.15152797 -0.17167875 -0.64169631 -0.36583098 -0.36851664  0.01005316]
Mean policy from posterior
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.37	-0.17	-0.15	-0.64	-0.17	-0.15	-0.15	-0.15	
-0.15	-0.17	-0.37	-0.64	-0.15	-0.37	-0.15	-0.15	
-0.15	-0.15	-0.37	-0.37	-0.15	-0.15	-0.37	-0.64	
-0.15	-0.64	-0.17	-0.37	-0.17	-0.64	-0.17	-0.15	
-0.17	-0.17	-0.64	-0.17	-0.64	-0.37	-0.37	-0.15	
-0.17	-0.37	-0.37	-0.17	-0.17	-0.37	-0.64	-0.37	
-0.64	-0.17	-0.17	-0.15	-0.37	-0.37	-0.37	-0.37	
-0.64	-0.15	-0.37	-0.64	-0.15	-0.17	-0.17	0.01	
mean = 0.045210089977292256, map = 0.11584452541395551
CVaR policy
v	v	>	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	>	>	>	v	v	
v	>	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	>	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
cvar = , 0.049535647052415754, 0.05291659474966348, 0.04521008856198927, 0.04521008861555753, 0.04521008861024223
==========
iteration 32
==========
weights [-0.4094745  -0.40134772 -0.10730514 -0.40448845 -0.6221851   0.33016821]
expeced value MDP LP -1.3553792537867473
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[ 0.43917294 -0.24293219 -0.028694    0.282863    0.67266457  0.46346374]
w_map [-0.48221407 -0.52404739 -0.06037215 -0.2553283  -0.59786822  0.25799259] loglik -1.1855784433883514
accepted/total = 1855/3000 = 0.6183333333333333
-------
true weights [-0.4094745  -0.40134772 -0.10730514 -0.40448845 -0.6221851   0.33016821]
features
3 	1 	0 	2 	1 	0 	2 	3 	
2 	3 	4 	0 	3 	1 	2 	1 	
1 	1 	2 	4 	0 	0 	3 	1 	
1 	4 	0 	0 	1 	2 	1 	1 	
3 	2 	3 	1 	2 	2 	0 	2 	
3 	1 	0 	3 	3 	2 	2 	4 	
3 	3 	1 	0 	3 	1 	1 	2 	
1 	1 	2 	2 	4 	2 	2 	5 	
optimal policy
v	v	>	>	v	v	v	<	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	^	v	>	>	>	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.09	-3.01	-2.64	-2.25	-2.17	-1.79	-1.58	-1.96	
-2.71	-2.64	-2.48	-2.18	-1.78	-1.39	-1.48	-1.69	
-2.63	-2.26	-1.87	-2.00	-1.39	-1.00	-1.39	-1.30	
-2.26	-2.09	-1.79	-1.39	-0.99	-0.60	-0.99	-0.90	
-1.87	-1.48	-1.39	-0.99	-0.60	-0.50	-0.70	-0.51	
-2.26	-1.87	-1.51	-1.19	-0.79	-0.39	-0.29	-0.40	
-1.89	-1.50	-1.11	-1.02	-0.69	-0.29	-0.18	0.22	
-1.50	-1.11	-0.72	-0.62	-0.51	0.11	0.22	0.33	
map_weights [-0.48221407 -0.52404739 -0.06037215 -0.2553283  -0.59786822  0.25799259]
MAP reward
-0.26	-0.52	-0.48	-0.06	-0.52	-0.48	-0.06	-0.26	
-0.06	-0.26	-0.60	-0.48	-0.26	-0.52	-0.06	-0.52	
-0.52	-0.52	-0.06	-0.60	-0.48	-0.48	-0.26	-0.52	
-0.52	-0.60	-0.48	-0.48	-0.52	-0.06	-0.52	-0.52	
-0.26	-0.06	-0.26	-0.52	-0.06	-0.06	-0.48	-0.06	
-0.26	-0.52	-0.48	-0.26	-0.26	-0.06	-0.06	-0.60	
-0.26	-0.26	-0.52	-0.48	-0.26	-0.52	-0.52	-0.06	
-0.52	-0.52	-0.06	-0.06	-0.60	-0.06	-0.06	0.26	
Map policy
v	v	>	v	v	>	v	<	
>	v	v	>	v	v	v	<	
v	>	v	>	>	v	<	v	
v	v	v	>	v	v	<	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.2783315863671245
mean w [-0.48311256 -0.27510224 -0.11101831 -0.35182117 -0.54853637  0.17779803]
Mean policy from posterior
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	^	v	>	>	>	v	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.35	-0.28	-0.48	-0.11	-0.28	-0.48	-0.11	-0.35	
-0.11	-0.35	-0.55	-0.48	-0.35	-0.28	-0.11	-0.28	
-0.28	-0.28	-0.11	-0.55	-0.48	-0.48	-0.35	-0.28	
-0.28	-0.55	-0.48	-0.48	-0.28	-0.11	-0.28	-0.28	
-0.35	-0.11	-0.35	-0.28	-0.11	-0.11	-0.48	-0.11	
-0.35	-0.28	-0.48	-0.35	-0.35	-0.11	-0.11	-0.55	
-0.35	-0.35	-0.28	-0.48	-0.35	-0.28	-0.28	-0.11	
-0.28	-0.28	-0.11	-0.11	-0.55	-0.11	-0.11	0.18	
mean = 0.0029119459178625906, map = 0.009688970049152923
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	v	v	>	>	>	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	v	v	>	>	>	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	v	v	>	>	>	v	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	^	v	>	>	>	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	^	v	>	>	>	v	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.01441942877633462, 0.014419446494182697, 0.003566172055587735, 0.002911951012768199, 0.0029119606053975566
==========
iteration 33
==========
weights [-0.5241945  -0.77962364 -0.22290199 -0.13972452 -0.17733602  0.12942492]
expeced value MDP LP -1.9563915891969281
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.25883669 -0.17049245 -0.4819607  -0.59749238  0.28604309  0.48252666]
w_map [-0.46712435 -0.7367512  -0.17553044 -0.32249391 -0.25086838  0.20308696] loglik -0.6931471820346822
accepted/total = 1970/3000 = 0.6566666666666666
-------
true weights [-0.5241945  -0.77962364 -0.22290199 -0.13972452 -0.17733602  0.12942492]
features
2 	1 	4 	3 	1 	2 	3 	0 	
1 	0 	0 	0 	4 	3 	2 	1 	
0 	3 	0 	1 	0 	0 	3 	4 	
1 	4 	2 	1 	1 	3 	3 	4 	
3 	1 	2 	1 	1 	2 	3 	2 	
1 	2 	3 	4 	3 	0 	0 	1 	
1 	3 	3 	1 	1 	0 	0 	1 	
2 	0 	4 	1 	1 	4 	3 	5 	
optimal policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	^	v	v	<	
>	>	v	<	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-3.64	-3.45	-2.70	-2.55	-2.69	-1.98	-1.77	-2.28	
-3.58	-2.83	-2.93	-2.43	-1.93	-1.77	-1.65	-2.36	
-2.83	-2.33	-2.56	-3.19	-2.43	-1.95	-1.44	-1.60	
-2.97	-2.21	-2.05	-2.81	-2.20	-1.44	-1.31	-1.48	
-2.73	-2.61	-1.85	-2.28	-2.12	-1.39	-1.18	-1.39	
-2.61	-1.85	-1.64	-1.52	-1.36	-1.23	-1.05	-1.42	
-2.65	-1.89	-1.77	-2.25	-1.48	-0.71	-0.54	-0.65	
-2.59	-2.39	-1.90	-1.74	-0.97	-0.19	-0.01	0.13	
map_weights [-0.46712435 -0.7367512  -0.17553044 -0.32249391 -0.25086838  0.20308696]
MAP reward
-0.18	-0.74	-0.25	-0.32	-0.74	-0.18	-0.32	-0.47	
-0.74	-0.47	-0.47	-0.47	-0.25	-0.32	-0.18	-0.74	
-0.47	-0.32	-0.47	-0.74	-0.47	-0.47	-0.32	-0.25	
-0.74	-0.25	-0.18	-0.74	-0.74	-0.32	-0.32	-0.25	
-0.32	-0.74	-0.18	-0.74	-0.74	-0.18	-0.32	-0.18	
-0.74	-0.18	-0.32	-0.25	-0.32	-0.47	-0.47	-0.74	
-0.74	-0.32	-0.32	-0.74	-0.74	-0.47	-0.47	-0.74	
-0.18	-0.47	-0.25	-0.74	-0.74	-0.25	-0.32	0.20	
Map policy
v	v	v	v	>	>	v	<	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	v	v	
>	>	v	<	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	^	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.122308995883171
mean w [-0.35618833 -0.65774587 -0.2678226  -0.21653619 -0.15237211 -0.20196945]
Mean policy from posterior
>	>	>	v	v	>	v	<	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	<	
>	v	v	v	v	v	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.27	-0.66	-0.15	-0.22	-0.66	-0.27	-0.22	-0.36	
-0.66	-0.36	-0.36	-0.36	-0.15	-0.22	-0.27	-0.66	
-0.36	-0.22	-0.36	-0.66	-0.36	-0.36	-0.22	-0.15	
-0.66	-0.15	-0.27	-0.66	-0.66	-0.22	-0.22	-0.15	
-0.22	-0.66	-0.27	-0.66	-0.66	-0.27	-0.22	-0.27	
-0.66	-0.27	-0.22	-0.15	-0.22	-0.36	-0.36	-0.66	
-0.66	-0.22	-0.22	-0.66	-0.66	-0.36	-0.36	-0.66	
-0.27	-0.36	-0.15	-0.66	-0.66	-0.15	-0.22	-0.20	
mean = 0.01779913884861095, map = 0.05301831138495228
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	<	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	<	
>	v	v	v	v	v	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	<	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	<	
>	v	v	v	v	v	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	<	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	<	
>	v	v	v	v	v	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.12171405797829715, 0.11173743900333077, 0.02175164003222929, 0.02175163999665619, 0.021751640027522612
==========
iteration 34
==========
weights [-0.62439498 -0.16258313 -0.103938   -0.22088694 -0.30853585  0.65491153]
expeced value MDP LP -0.671030005140768
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[ 0.33903884 -0.36968764 -0.1946346  -0.44126167  0.71279923  0.08778657]
w_map [-0.31370756 -0.33884906 -0.32277078 -0.31246118 -0.69674607  0.31543753] loglik -1.9739765377835283e-13
accepted/total = 2143/3000 = 0.7143333333333334
-------
true weights [-0.62439498 -0.16258313 -0.103938   -0.22088694 -0.30853585  0.65491153]
features
3 	4 	4 	0 	4 	4 	4 	1 	
4 	4 	4 	0 	1 	2 	4 	2 	
1 	3 	3 	3 	0 	0 	2 	2 	
0 	3 	2 	1 	1 	3 	0 	1 	
1 	4 	1 	4 	0 	1 	4 	1 	
2 	1 	4 	2 	2 	3 	4 	0 	
4 	2 	3 	1 	2 	2 	2 	4 	
4 	1 	3 	2 	3 	4 	3 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
v	>	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	>	>	>	.	
optimal values
-1.75	-1.69	-1.48	-2.01	-1.55	-1.40	-1.25	-0.96	
-1.54	-1.39	-1.18	-1.40	-1.26	-1.11	-1.10	-0.80	
-1.25	-1.09	-0.88	-0.78	-1.17	-1.01	-0.80	-0.70	
-1.41	-0.88	-0.67	-0.57	-0.55	-0.39	-0.92	-0.61	
-0.79	-0.84	-0.57	-0.41	-0.62	-0.17	-0.30	-0.45	
-0.64	-0.54	-0.41	-0.10	0.00	-0.01	0.01	-0.29	
-0.68	-0.38	-0.28	-0.06	0.11	0.21	0.32	0.34	
-0.84	-0.54	-0.38	-0.16	-0.11	0.11	0.43	0.65	
map_weights [-0.31370756 -0.33884906 -0.32277078 -0.31246118 -0.69674607  0.31543753]
MAP reward
-0.31	-0.70	-0.70	-0.31	-0.70	-0.70	-0.70	-0.34	
-0.70	-0.70	-0.70	-0.31	-0.34	-0.32	-0.70	-0.32	
-0.34	-0.31	-0.31	-0.31	-0.31	-0.31	-0.32	-0.32	
-0.31	-0.31	-0.32	-0.34	-0.34	-0.31	-0.31	-0.34	
-0.34	-0.70	-0.34	-0.70	-0.31	-0.34	-0.70	-0.34	
-0.32	-0.34	-0.70	-0.32	-0.32	-0.31	-0.70	-0.31	
-0.70	-0.32	-0.31	-0.34	-0.32	-0.32	-0.32	-0.70	
-0.70	-0.34	-0.31	-0.32	-0.31	-0.70	-0.31	0.32	
Map policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	v	>	v	
v	v	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8595686050390565
mean w [-0.46076798 -0.18869389 -0.18802367 -0.29062616 -0.63992567 -0.04431956]
Mean policy from posterior
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	>	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
Mean rewards
-0.29	-0.64	-0.64	-0.46	-0.64	-0.64	-0.64	-0.19	
-0.64	-0.64	-0.64	-0.46	-0.19	-0.19	-0.64	-0.19	
-0.19	-0.29	-0.29	-0.29	-0.46	-0.46	-0.19	-0.19	
-0.46	-0.29	-0.19	-0.19	-0.19	-0.29	-0.46	-0.19	
-0.19	-0.64	-0.19	-0.64	-0.46	-0.19	-0.64	-0.19	
-0.19	-0.19	-0.64	-0.19	-0.19	-0.29	-0.64	-0.46	
-0.64	-0.19	-0.29	-0.19	-0.19	-0.19	-0.19	-0.64	
-0.64	-0.19	-0.29	-0.19	-0.29	-0.64	-0.29	-0.04	
mean = 0.041537163731556026, map = 0.32845573451372556
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	>	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
cvar = , 0.09574134300286685, 0.06615287453936491, 0.050243207050373107, 0.04842869476838274, 0.04512963490187083
==========
iteration 35
==========
weights [-0.06684751 -0.23871016 -0.10837398 -0.73125052 -0.41222717  0.47132302]
expeced value MDP LP -0.6096881849589867
demonstration
[(32, 1), (33, 1), (34, 3), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.5979709  -0.04942892 -0.67643094 -0.33109991 -0.10126099  0.25009563]
w_map [-0.2265586  -0.06888226 -0.41422621 -0.64224056 -0.59648172 -0.06387261] loglik -2.3562336082250113e-08
accepted/total = 1806/3000 = 0.602
-------
true weights [-0.06684751 -0.23871016 -0.10837398 -0.73125052 -0.41222717  0.47132302]
features
1 	1 	4 	0 	1 	1 	0 	3 	
0 	3 	4 	0 	4 	1 	0 	4 	
0 	1 	4 	2 	1 	3 	4 	4 	
0 	2 	3 	0 	1 	0 	3 	2 	
3 	0 	1 	4 	2 	2 	4 	3 	
2 	4 	2 	0 	2 	0 	4 	3 	
3 	0 	4 	4 	3 	2 	2 	2 	
4 	3 	2 	4 	0 	0 	2 	5 	
optimal policy
v	<	>	v	<	<	<	<	
v	<	>	v	<	v	<	<	
v	v	>	v	v	v	<	v	
>	v	>	>	>	v	<	<	
>	>	v	v	>	v	<	v	
>	>	>	>	>	v	v	v	
>	>	^	^	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-0.99	-1.22	-1.00	-0.60	-0.83	-1.06	-1.12	-1.84	
-0.76	-1.48	-0.94	-0.54	-0.94	-1.03	-1.09	-1.49	
-0.70	-0.81	-0.88	-0.47	-0.54	-0.80	-1.20	-1.30	
-0.64	-0.58	-1.10	-0.37	-0.31	-0.07	-0.80	-0.90	
-1.20	-0.48	-0.41	-0.48	-0.11	-0.00	-0.41	-1.10	
-0.69	-0.59	-0.18	-0.07	-0.00	0.11	-0.17	-0.38	
-1.37	-0.65	-0.59	-0.48	-0.52	0.18	0.25	0.36	
-1.43	-1.03	-0.30	-0.20	0.22	0.29	0.36	0.47	
map_weights [-0.2265586  -0.06888226 -0.41422621 -0.64224056 -0.59648172 -0.06387261]
MAP reward
-0.07	-0.07	-0.60	-0.23	-0.07	-0.07	-0.23	-0.64	
-0.23	-0.64	-0.60	-0.23	-0.60	-0.07	-0.23	-0.60	
-0.23	-0.07	-0.60	-0.41	-0.07	-0.64	-0.60	-0.60	
-0.23	-0.41	-0.64	-0.23	-0.07	-0.23	-0.64	-0.41	
-0.64	-0.23	-0.07	-0.60	-0.41	-0.41	-0.60	-0.64	
-0.41	-0.60	-0.41	-0.23	-0.41	-0.23	-0.60	-0.64	
-0.64	-0.23	-0.60	-0.60	-0.64	-0.41	-0.41	-0.41	
-0.60	-0.64	-0.41	-0.60	-0.23	-0.23	-0.41	-0.06	
Map policy
v	>	>	v	v	v	<	<	
v	v	>	v	v	v	<	v	
>	v	>	>	v	v	<	v	
>	v	>	>	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4027578552722688
mean w [-0.12380817 -0.21114426 -0.24944944 -0.54105024 -0.53163097  0.20454921]
Mean policy from posterior
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	<	
v	v	>	v	v	v	<	v	
>	v	>	>	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.21	-0.21	-0.53	-0.12	-0.21	-0.21	-0.12	-0.54	
-0.12	-0.54	-0.53	-0.12	-0.53	-0.21	-0.12	-0.53	
-0.12	-0.21	-0.53	-0.25	-0.21	-0.54	-0.53	-0.53	
-0.12	-0.25	-0.54	-0.12	-0.21	-0.12	-0.54	-0.25	
-0.54	-0.12	-0.21	-0.53	-0.25	-0.25	-0.53	-0.54	
-0.25	-0.53	-0.25	-0.12	-0.25	-0.12	-0.53	-0.54	
-0.54	-0.12	-0.53	-0.53	-0.54	-0.25	-0.25	-0.25	
-0.53	-0.54	-0.25	-0.53	-0.12	-0.12	-0.25	0.20	
mean = 0.022100382047730016, map = 0.06952914454276049
CVaR policy
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	>	v	v	<	
v	v	>	v	v	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	<	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	>	v	<	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	<	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.10572971677247656, 0.057614511901020515, 0.04741068259240877, 0.02210038169515849, 0.022100381612101816
==========
iteration 36
==========
weights [-0.01995221 -0.56123947 -0.57293475 -0.57445404 -0.15730936  0.04017778]
expeced value MDP LP -1.6125031222031943
demonstration
[(32, 3), (40, 1), (41, 3), (49, 3), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.47625962  0.4097119   0.0653375  -0.36084516  0.1487206   0.66986335]
w_map [-0.26900178 -0.34893072 -0.52118726 -0.42232826 -0.42484958 -0.41879698] loglik -1.3862943257859968
accepted/total = 1915/3000 = 0.6383333333333333
-------
true weights [-0.01995221 -0.56123947 -0.57293475 -0.57445404 -0.15730936  0.04017778]
features
4 	2 	2 	4 	1 	2 	0 	4 	
0 	4 	0 	4 	2 	3 	1 	2 	
2 	2 	4 	3 	3 	4 	3 	4 	
3 	0 	2 	4 	1 	0 	3 	4 	
0 	0 	4 	2 	4 	3 	4 	3 	
0 	3 	3 	1 	3 	0 	4 	2 	
3 	0 	2 	1 	3 	3 	1 	4 	
2 	0 	0 	3 	1 	0 	0 	5 	
optimal policy
v	<	v	v	<	v	^	<	
<	<	<	<	v	v	v	v	
^	v	^	v	>	v	<	v	
v	v	<	>	>	v	v	v	
v	v	<	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.13	-2.68	-2.68	-2.40	-2.94	-2.43	-2.00	-2.13	
-2.00	-2.13	-2.13	-2.27	-2.43	-1.87	-2.42	-2.10	
-2.55	-2.28	-2.27	-2.41	-1.87	-1.31	-1.87	-1.54	
-2.28	-1.72	-2.28	-1.86	-1.72	-1.17	-1.41	-1.40	
-1.72	-1.72	-1.86	-1.86	-1.30	-1.16	-0.84	-1.26	
-1.72	-1.72	-2.26	-1.71	-1.16	-0.59	-0.69	-0.69	
-1.72	-1.16	-1.70	-1.68	-1.13	-0.57	-0.54	-0.12	
-1.71	-1.15	-1.14	-1.13	-0.56	-0.00	0.02	0.04	
map_weights [-0.26900178 -0.34893072 -0.52118726 -0.42232826 -0.42484958 -0.41879698]
MAP reward
-0.42	-0.52	-0.52	-0.42	-0.35	-0.52	-0.27	-0.42	
-0.27	-0.42	-0.27	-0.42	-0.52	-0.42	-0.35	-0.52	
-0.52	-0.52	-0.42	-0.42	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.27	-0.52	-0.42	-0.35	-0.27	-0.42	-0.42	
-0.27	-0.27	-0.42	-0.52	-0.42	-0.42	-0.42	-0.42	
-0.27	-0.42	-0.42	-0.35	-0.42	-0.27	-0.42	-0.52	
-0.42	-0.27	-0.52	-0.35	-0.42	-0.42	-0.35	-0.42	
-0.52	-0.27	-0.27	-0.42	-0.35	-0.27	-0.27	-0.42	
Map policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4984498149063539
mean w [-0.10628486 -0.31082057 -0.46387511 -0.41477022 -0.41139347  0.24683997]
Mean policy from posterior
v	v	v	>	v	v	v	<	
>	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	<	>	>	v	v	v	
v	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.41	-0.46	-0.46	-0.41	-0.31	-0.46	-0.11	-0.41	
-0.11	-0.41	-0.11	-0.41	-0.46	-0.41	-0.31	-0.46	
-0.46	-0.46	-0.41	-0.41	-0.41	-0.41	-0.41	-0.41	
-0.41	-0.11	-0.46	-0.41	-0.31	-0.11	-0.41	-0.41	
-0.11	-0.11	-0.41	-0.46	-0.41	-0.41	-0.41	-0.41	
-0.11	-0.41	-0.41	-0.31	-0.41	-0.11	-0.41	-0.46	
-0.41	-0.11	-0.46	-0.31	-0.41	-0.41	-0.31	-0.41	
-0.46	-0.11	-0.11	-0.41	-0.31	-0.11	-0.11	0.25	
mean = 0.10250985390353518, map = 0.15114912275115944
CVaR policy
v	v	v	>	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
v	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	<	>	>	v	v	v	
v	v	<	v	>	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
v	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	<	>	>	v	v	v	
v	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	<	>	>	v	v	v	
v	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.15710882058593145, 0.1573742750796958, 0.12819115937277004, 0.12819115937753, 0.10251004381181028
==========
iteration 37
==========
weights [-0.13292463 -0.35914369 -0.00912155 -0.1627433  -0.90904404  0.02042576]
expeced value MDP LP -1.1796173475983402
demonstration
[(32, 1), (33, 3), (41, 1), (42, 3), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.49180797  0.06777035  0.32880741 -0.56143659  0.16820365  0.54946726]
w_map [-0.28499902 -0.46192296 -0.1348408  -0.51691452 -0.62135427  0.18422524] loglik -1.8674405710683573e-06
accepted/total = 1371/3000 = 0.457
-------
true weights [-0.13292463 -0.35914369 -0.00912155 -0.1627433  -0.90904404  0.02042576]
features
3 	3 	0 	0 	1 	1 	3 	3 	
0 	4 	1 	3 	2 	4 	4 	4 	
1 	3 	4 	3 	1 	1 	0 	2 	
1 	0 	3 	1 	3 	3 	3 	1 	
0 	2 	1 	4 	3 	2 	1 	4 	
4 	0 	1 	0 	4 	4 	3 	1 	
0 	4 	2 	0 	1 	4 	3 	0 	
0 	1 	3 	0 	3 	3 	2 	5 	
optimal policy
v	>	>	v	v	<	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	>	
v	v	<	>	v	v	v	<	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.93	-1.86	-1.71	-1.59	-1.67	-2.02	-1.99	-1.96	
-1.79	-2.22	-1.82	-1.48	-1.33	-2.08	-1.85	-1.81	
-1.67	-1.32	-2.22	-1.48	-1.33	-1.18	-0.95	-0.91	
-1.52	-1.17	-1.32	-1.33	-0.98	-0.83	-0.83	-1.18	
-1.17	-1.05	-1.28	-1.60	-0.83	-0.67	-0.67	-1.38	
-1.95	-1.05	-0.93	-0.70	-1.57	-1.22	-0.31	-0.47	
-1.20	-1.48	-0.57	-0.57	-0.67	-1.06	-0.15	-0.11	
-1.08	-0.95	-0.60	-0.44	-0.31	-0.15	0.01	0.02	
map_weights [-0.28499902 -0.46192296 -0.1348408  -0.51691452 -0.62135427  0.18422524]
MAP reward
-0.52	-0.52	-0.28	-0.28	-0.46	-0.46	-0.52	-0.52	
-0.28	-0.62	-0.46	-0.52	-0.13	-0.62	-0.62	-0.62	
-0.46	-0.52	-0.62	-0.52	-0.46	-0.46	-0.28	-0.13	
-0.46	-0.28	-0.52	-0.46	-0.52	-0.52	-0.52	-0.46	
-0.28	-0.13	-0.46	-0.62	-0.52	-0.13	-0.46	-0.62	
-0.62	-0.28	-0.46	-0.28	-0.62	-0.62	-0.52	-0.46	
-0.28	-0.62	-0.13	-0.28	-0.46	-0.62	-0.52	-0.28	
-0.28	-0.46	-0.52	-0.28	-0.52	-0.52	-0.13	0.18	
Map policy
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	>	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6337556415987842
mean w [-0.16835588 -0.3516239  -0.08064751 -0.36087394 -0.60823758  0.16347666]
Mean policy from posterior
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.36	-0.36	-0.17	-0.17	-0.35	-0.35	-0.36	-0.36	
-0.17	-0.61	-0.35	-0.36	-0.08	-0.61	-0.61	-0.61	
-0.35	-0.36	-0.61	-0.36	-0.35	-0.35	-0.17	-0.08	
-0.35	-0.17	-0.36	-0.35	-0.36	-0.36	-0.36	-0.35	
-0.17	-0.08	-0.35	-0.61	-0.36	-0.08	-0.35	-0.61	
-0.61	-0.17	-0.35	-0.17	-0.61	-0.61	-0.36	-0.35	
-0.17	-0.61	-0.08	-0.17	-0.35	-0.61	-0.36	-0.17	
-0.17	-0.35	-0.36	-0.17	-0.36	-0.36	-0.08	0.16	
mean = 0.12408759156304017, map = 0.31582835956265987
CVaR policy
v	>	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	>	>	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.3371182745831707, 0.3158283587046349, 0.16485436226274874, 0.1648543612361566, 0.12408759270057268
==========
iteration 38
==========
weights [-0.24018941 -0.09776579 -0.34570597 -0.20586074 -0.69557571  0.5357556 ]
expeced value MDP LP -0.8340895143841156
demonstration
[(32, 1), (33, 1), (34, 3), (42, 1), (43, 1), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.29021255  0.16940151 -0.21306869 -0.79086506 -0.41341127  0.21284999]
w_map [-0.38645153 -0.17939468 -0.58544433 -0.21556132 -0.65484642 -0.02090917] loglik -6.493656456996177e-08
accepted/total = 1750/3000 = 0.5833333333333334
-------
true weights [-0.24018941 -0.09776579 -0.34570597 -0.20586074 -0.69557571  0.5357556 ]
features
1 	2 	4 	3 	1 	1 	3 	3 	
2 	2 	4 	0 	1 	2 	1 	4 	
0 	3 	1 	4 	0 	4 	3 	0 	
0 	0 	1 	2 	2 	4 	4 	2 	
3 	1 	0 	4 	0 	4 	4 	2 	
4 	2 	0 	1 	1 	1 	4 	3 	
4 	2 	0 	0 	1 	0 	0 	0 	
0 	4 	4 	3 	1 	1 	3 	5 	
optimal policy
v	v	>	>	v	<	v	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	<	>	v	
v	v	v	>	v	<	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.67	-1.69	-1.94	-1.26	-1.07	-1.15	-1.32	-1.51	
-1.59	-1.36	-1.51	-1.21	-0.98	-1.31	-1.13	-1.53	
-1.25	-1.02	-0.83	-1.51	-0.89	-1.58	-1.04	-0.84	
-1.17	-0.97	-0.74	-0.99	-0.66	-1.34	-1.30	-0.61	
-0.93	-0.74	-0.65	-0.86	-0.31	-0.81	-0.96	-0.27	
-1.44	-0.75	-0.41	-0.17	-0.07	-0.12	-0.61	0.08	
-1.48	-0.80	-0.45	-0.22	0.02	-0.02	0.08	0.29	
-1.69	-1.47	-0.78	-0.08	0.12	0.22	0.32	0.54	
map_weights [-0.38645153 -0.17939468 -0.58544433 -0.21556132 -0.65484642 -0.02090917]
MAP reward
-0.18	-0.59	-0.65	-0.22	-0.18	-0.18	-0.22	-0.22	
-0.59	-0.59	-0.65	-0.39	-0.18	-0.59	-0.18	-0.65	
-0.39	-0.22	-0.18	-0.65	-0.39	-0.65	-0.22	-0.39	
-0.39	-0.39	-0.18	-0.59	-0.59	-0.65	-0.65	-0.59	
-0.22	-0.18	-0.39	-0.65	-0.39	-0.65	-0.65	-0.59	
-0.65	-0.59	-0.39	-0.18	-0.18	-0.18	-0.65	-0.22	
-0.65	-0.59	-0.39	-0.39	-0.18	-0.39	-0.39	-0.39	
-0.39	-0.65	-0.65	-0.22	-0.18	-0.18	-0.22	-0.02	
Map policy
v	v	>	>	v	<	v	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	<	>	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7618718721854132
mean w [-0.31073963 -0.12625476 -0.62058059 -0.25348006 -0.51555661 -0.08355586]
Mean policy from posterior
v	v	v	>	v	<	<	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.13	-0.62	-0.52	-0.25	-0.13	-0.13	-0.25	-0.25	
-0.62	-0.62	-0.52	-0.31	-0.13	-0.62	-0.13	-0.52	
-0.31	-0.25	-0.13	-0.52	-0.31	-0.52	-0.25	-0.31	
-0.31	-0.31	-0.13	-0.62	-0.62	-0.52	-0.52	-0.62	
-0.25	-0.13	-0.31	-0.52	-0.31	-0.52	-0.52	-0.62	
-0.52	-0.62	-0.31	-0.13	-0.13	-0.13	-0.52	-0.25	
-0.52	-0.62	-0.31	-0.31	-0.13	-0.31	-0.31	-0.31	
-0.31	-0.52	-0.52	-0.25	-0.13	-0.13	-0.25	-0.08	
mean = 0.07119073576061041, map = 0.005634482833701493
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	<	v	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	<	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.13755415321193154, 0.11532785489538411, 0.09095604539381297, 0.07400518148083757, 0.0711907286810497
==========
iteration 39
==========
weights [-0.04764988 -0.63173347 -0.26004631 -0.41853618 -0.52868348  0.27629598]
expeced value MDP LP -2.0470749509994812
demonstration
[(32, 1), (33, 1), (34, 3), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.20962744 -0.41874762  0.39491768  0.61909968  0.38783172  0.30174317]
w_map [-0.13386044 -0.63890392 -0.22460471 -0.53171044 -0.4388341   0.21941861] loglik -0.6931471868816601
accepted/total = 1661/3000 = 0.5536666666666666
-------
true weights [-0.04764988 -0.63173347 -0.26004631 -0.41853618 -0.52868348  0.27629598]
features
2 	0 	3 	1 	1 	0 	1 	1 	
4 	4 	2 	3 	2 	0 	0 	0 	
0 	3 	2 	1 	4 	0 	3 	2 	
2 	3 	0 	4 	4 	1 	4 	1 	
2 	2 	0 	4 	0 	3 	2 	1 	
3 	1 	2 	0 	2 	1 	2 	3 	
0 	0 	4 	4 	1 	0 	1 	1 	
2 	2 	3 	1 	4 	4 	3 	5 	
optimal policy
>	>	v	v	>	v	v	v	
v	>	v	<	>	v	v	<	
v	>	v	<	v	>	v	v	
>	>	v	<	v	v	v	v	
>	>	v	>	v	>	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
optimal values
-3.04	-2.81	-2.79	-3.40	-2.88	-2.27	-2.83	-2.85	
-3.08	-2.90	-2.40	-2.79	-2.48	-2.24	-2.22	-2.24	
-2.58	-2.56	-2.16	-2.77	-2.64	-2.22	-2.19	-2.25	
-2.55	-2.32	-1.92	-2.43	-2.13	-2.30	-1.79	-2.01	
-2.37	-2.13	-1.89	-2.13	-1.62	-1.68	-1.28	-1.40	
-2.76	-2.47	-1.86	-1.62	-1.58	-1.34	-1.03	-0.77	
-2.41	-2.39	-2.36	-1.85	-1.34	-0.71	-0.78	-0.36	
-2.65	-2.45	-2.21	-1.81	-1.19	-0.67	-0.15	0.28	
map_weights [-0.13386044 -0.63890392 -0.22460471 -0.53171044 -0.4388341   0.21941861]
MAP reward
-0.22	-0.13	-0.53	-0.64	-0.64	-0.13	-0.64	-0.64	
-0.44	-0.44	-0.22	-0.53	-0.22	-0.13	-0.13	-0.13	
-0.13	-0.53	-0.22	-0.64	-0.44	-0.13	-0.53	-0.22	
-0.22	-0.53	-0.13	-0.44	-0.44	-0.64	-0.44	-0.64	
-0.22	-0.22	-0.13	-0.44	-0.13	-0.53	-0.22	-0.64	
-0.53	-0.64	-0.22	-0.13	-0.22	-0.64	-0.22	-0.53	
-0.13	-0.13	-0.44	-0.44	-0.64	-0.13	-0.64	-0.64	
-0.22	-0.22	-0.53	-0.64	-0.44	-0.44	-0.53	0.22	
Map policy
>	v	v	v	>	v	v	v	
v	>	v	<	>	v	v	v	
v	>	v	<	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	v	>	v	>	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.3246739586259237
mean w [-0.13274468 -0.65149273 -0.17597152 -0.46484232 -0.33553103 -0.15867804]
Mean policy from posterior
>	v	v	v	>	v	v	v	
v	>	v	<	>	v	v	<	
v	>	v	<	v	>	v	<	
v	>	v	v	v	>	v	v	
>	>	v	>	v	>	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.18	-0.13	-0.46	-0.65	-0.65	-0.13	-0.65	-0.65	
-0.34	-0.34	-0.18	-0.46	-0.18	-0.13	-0.13	-0.13	
-0.13	-0.46	-0.18	-0.65	-0.34	-0.13	-0.46	-0.18	
-0.18	-0.46	-0.13	-0.34	-0.34	-0.65	-0.34	-0.65	
-0.18	-0.18	-0.13	-0.34	-0.13	-0.46	-0.18	-0.65	
-0.46	-0.65	-0.18	-0.13	-0.18	-0.65	-0.18	-0.46	
-0.13	-0.13	-0.34	-0.34	-0.65	-0.13	-0.65	-0.65	
-0.18	-0.18	-0.46	-0.65	-0.34	-0.34	-0.46	-0.16	
mean = 0.01410960448649723, map = 0.01252042312146262
CVaR policy
>	v	v	v	>	v	v	v	
v	>	v	>	>	v	v	v	
v	>	v	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
v	>	v	>	>	v	v	v	
v	>	v	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
v	>	v	>	>	v	v	<	
v	>	v	<	v	>	v	<	
v	>	v	v	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
v	>	v	<	>	v	v	<	
v	>	v	<	v	>	v	<	
v	>	v	v	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
v	>	v	<	>	v	v	<	
v	>	v	<	v	>	v	<	
v	>	v	v	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.02312021501315087, 0.023120217873712434, 0.01732524999828078, 0.014758528348531641, 0.014758507895069606
==========
iteration 40
==========
weights [-0.16999147 -0.30044224 -0.18595266 -0.18621804 -0.86748449  0.24300713]
expeced value MDP LP -1.2911925434252285
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.27245232  0.16908648  0.36629545  0.62289402 -0.59523352 -0.14390009]
w_map [-0.18213857 -0.57723449 -0.34669765 -0.55353676 -0.41092506  0.19535679] loglik -2.2114797104677564e-07
accepted/total = 1689/3000 = 0.563
-------
true weights [-0.16999147 -0.30044224 -0.18595266 -0.18621804 -0.86748449  0.24300713]
features
2 	4 	1 	0 	3 	4 	0 	0 	
2 	4 	1 	2 	3 	4 	3 	1 	
2 	0 	3 	3 	1 	2 	4 	2 	
3 	0 	2 	1 	2 	3 	3 	0 	
3 	3 	2 	1 	4 	3 	0 	2 	
2 	0 	1 	4 	1 	3 	1 	4 	
2 	2 	0 	2 	3 	2 	1 	0 	
0 	3 	3 	0 	3 	0 	1 	5 	
optimal policy
v	>	>	v	v	<	>	v	
v	v	v	v	v	v	>	v	
>	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	<	>	v	v	<	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.24	-3.04	-2.20	-1.91	-1.76	-2.61	-1.81	-1.65	
-2.08	-2.59	-2.01	-1.76	-1.59	-1.99	-1.67	-1.50	
-1.91	-1.74	-1.73	-1.59	-1.42	-1.13	-1.73	-1.21	
-1.76	-1.59	-1.56	-1.42	-1.13	-0.95	-0.87	-1.03	
-1.60	-1.43	-1.39	-1.67	-1.64	-0.78	-0.69	-0.87	
-1.43	-1.26	-1.21	-1.62	-0.89	-0.60	-0.53	-0.80	
-1.27	-1.10	-0.92	-0.76	-0.60	-0.41	-0.23	0.07	
-1.10	-0.94	-0.76	-0.58	-0.41	-0.23	-0.06	0.24	
map_weights [-0.18213857 -0.57723449 -0.34669765 -0.55353676 -0.41092506  0.19535679]
MAP reward
-0.35	-0.41	-0.58	-0.18	-0.55	-0.41	-0.18	-0.18	
-0.35	-0.41	-0.58	-0.35	-0.55	-0.41	-0.55	-0.58	
-0.35	-0.18	-0.55	-0.55	-0.58	-0.35	-0.41	-0.35	
-0.55	-0.18	-0.35	-0.58	-0.35	-0.55	-0.55	-0.18	
-0.55	-0.55	-0.35	-0.58	-0.41	-0.55	-0.18	-0.35	
-0.35	-0.18	-0.58	-0.41	-0.58	-0.55	-0.58	-0.41	
-0.35	-0.35	-0.18	-0.35	-0.55	-0.35	-0.58	-0.18	
-0.18	-0.55	-0.55	-0.18	-0.55	-0.18	-0.58	0.20	
Map policy
v	v	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8024001511400973
mean w [-0.11075731 -0.43025581 -0.23389331 -0.56796154 -0.36638525  0.02585154]
Mean policy from posterior
v	v	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	>	v	>	v	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	v	>	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.23	-0.37	-0.43	-0.11	-0.57	-0.37	-0.11	-0.11	
-0.23	-0.37	-0.43	-0.23	-0.57	-0.37	-0.57	-0.43	
-0.23	-0.11	-0.57	-0.57	-0.43	-0.23	-0.37	-0.23	
-0.57	-0.11	-0.23	-0.43	-0.23	-0.57	-0.57	-0.11	
-0.57	-0.57	-0.23	-0.43	-0.37	-0.57	-0.11	-0.23	
-0.23	-0.11	-0.43	-0.37	-0.43	-0.57	-0.43	-0.37	
-0.23	-0.23	-0.11	-0.23	-0.57	-0.23	-0.43	-0.11	
-0.11	-0.57	-0.57	-0.11	-0.57	-0.11	-0.43	0.03	
mean = 0.3661925259063534, map = 0.3402699150509787
CVaR policy
>	v	>	>	>	>	>	v	
>	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	>	v	>	v	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	>	v	>	v	>	>	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.4283492790406007, 0.34545529582075174, 0.34545529926728724, 0.3609506302883452, 0.36095062068013917
==========
iteration 41
==========
weights [-0.31750265 -0.52318306 -0.21493703 -0.01627862 -0.06694747  0.75797537]
expeced value MDP LP -0.4478254943158449
demonstration
[(32, 1), (33, 3), (41, 1), (42, 3), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.03747044 -0.31313184  0.76866315 -0.49434908  0.11126374 -0.23008856]
w_map [-0.44530168 -0.64993003 -0.47516677 -0.16132539 -0.33345861  0.12764554] loglik -3.591440574268745e-06
accepted/total = 1462/3000 = 0.48733333333333334
-------
true weights [-0.31750265 -0.52318306 -0.21493703 -0.01627862 -0.06694747  0.75797537]
features
4 	4 	3 	1 	3 	3 	0 	3 	
3 	4 	1 	4 	1 	4 	1 	4 	
1 	0 	1 	4 	0 	0 	2 	3 	
2 	4 	3 	0 	1 	3 	1 	3 	
3 	3 	1 	0 	2 	0 	0 	3 	
4 	4 	4 	1 	3 	0 	4 	2 	
1 	1 	2 	3 	2 	2 	2 	1 	
3 	4 	3 	1 	4 	2 	0 	5 	
optimal policy
v	v	<	>	>	>	>	v	
>	v	<	v	^	^	>	v	
v	v	v	v	>	>	>	v	
v	v	<	<	v	v	>	v	
>	v	v	>	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
optimal values
-0.94	-0.93	-0.94	-0.98	-0.46	-0.45	-0.44	-0.12	
-0.88	-0.87	-1.39	-0.94	-0.98	-0.51	-0.63	-0.11	
-1.18	-0.82	-1.03	-0.89	-0.88	-0.57	-0.25	-0.04	
-0.66	-0.50	-0.51	-0.83	-0.82	-0.50	-0.55	-0.02	
-0.45	-0.44	-0.88	-0.62	-0.30	-0.49	-0.17	-0.01	
-0.49	-0.43	-0.37	-0.61	-0.09	-0.17	0.14	0.01	
-0.91	-0.82	-0.30	-0.09	-0.07	-0.00	0.21	0.23	
-0.39	-0.38	-0.31	-0.38	0.14	0.21	0.43	0.76	
map_weights [-0.44530168 -0.64993003 -0.47516677 -0.16132539 -0.33345861  0.12764554]
MAP reward
-0.33	-0.33	-0.16	-0.65	-0.16	-0.16	-0.45	-0.16	
-0.16	-0.33	-0.65	-0.33	-0.65	-0.33	-0.65	-0.33	
-0.65	-0.45	-0.65	-0.33	-0.45	-0.45	-0.48	-0.16	
-0.48	-0.33	-0.16	-0.45	-0.65	-0.16	-0.65	-0.16	
-0.16	-0.16	-0.65	-0.45	-0.48	-0.45	-0.45	-0.16	
-0.33	-0.33	-0.33	-0.65	-0.16	-0.45	-0.33	-0.48	
-0.65	-0.65	-0.48	-0.16	-0.48	-0.48	-0.48	-0.65	
-0.16	-0.33	-0.16	-0.65	-0.33	-0.48	-0.45	0.13	
Map policy
>	>	>	>	>	>	>	v	
>	v	^	v	^	v	>	v	
v	v	>	>	>	>	>	v	
v	v	>	v	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0266630922624023
mean w [-0.39186418 -0.67099741 -0.35711133 -0.13431972 -0.2239713   0.0203988 ]
Mean policy from posterior
v	v	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	>	>	>	>	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.22	-0.22	-0.13	-0.67	-0.13	-0.13	-0.39	-0.13	
-0.13	-0.22	-0.67	-0.22	-0.67	-0.22	-0.67	-0.22	
-0.67	-0.39	-0.67	-0.22	-0.39	-0.39	-0.36	-0.13	
-0.36	-0.22	-0.13	-0.39	-0.67	-0.13	-0.67	-0.13	
-0.13	-0.13	-0.67	-0.39	-0.36	-0.39	-0.39	-0.13	
-0.22	-0.22	-0.22	-0.67	-0.13	-0.39	-0.22	-0.36	
-0.67	-0.67	-0.36	-0.13	-0.36	-0.36	-0.36	-0.67	
-0.13	-0.22	-0.13	-0.67	-0.22	-0.36	-0.39	0.02	
mean = 0.015053260735945262, map = 0.043742213914196415
CVaR policy
>	>	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	>	>	>	>	>	v	
v	v	>	v	>	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	>	>	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	>	>	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	>	>	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	>	>	>	>	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.03684073961452827, 0.032308613432485866, 0.02982470250259084, 0.029824702487102506, 0.01677056053039344
==========
iteration 42
==========
weights [-0.48168162 -0.00285144 -0.40146165 -0.62732878 -0.19438844  0.41889732]
expeced value MDP LP -0.6802611196993763
demonstration
[(32, 1), (33, 3), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0)]
[ 0.26156956  0.17585796  0.76115433 -0.22572511 -0.39827927 -0.33424725]
w_map [-0.24320973  0.56150266 -0.6900454  -0.29581523 -0.23700991 -0.0756359 ] loglik -1.0985880000007455
accepted/total = 2823/3000 = 0.941
-------
true weights [-0.48168162 -0.00285144 -0.40146165 -0.62732878 -0.19438844  0.41889732]
features
4 	2 	3 	4 	2 	4 	2 	0 	
3 	2 	2 	2 	3 	3 	1 	3 	
1 	1 	3 	1 	3 	2 	1 	2 	
2 	2 	3 	1 	3 	1 	4 	0 	
3 	2 	3 	1 	1 	1 	3 	3 	
2 	1 	1 	4 	3 	0 	4 	1 	
2 	4 	4 	4 	4 	0 	4 	3 	
2 	4 	3 	4 	2 	2 	0 	5 	
optimal policy
v	v	>	v	>	>	v	<	
v	v	>	v	<	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	v	v	v	<	<	
v	v	>	>	>	<	<	v	
>	>	<	^	^	^	>	v	
>	^	^	^	<	>	v	v	
>	^	^	^	>	>	>	.	
optimal values
-1.09	-1.08	-1.49	-0.87	-1.26	-0.87	-0.68	-1.16	
-0.91	-0.68	-1.08	-0.68	-1.30	-0.91	-0.29	-0.91	
-0.29	-0.29	-0.91	-0.29	-0.91	-0.68	-0.29	-0.68	
-0.68	-0.68	-0.91	-0.29	-0.91	-0.29	-0.48	-0.95	
-1.30	-0.68	-0.91	-0.29	-0.29	-0.29	-0.91	-0.84	
-0.68	-0.29	-0.29	-0.48	-0.91	-0.76	-0.41	-0.21	
-0.87	-0.48	-0.48	-0.67	-0.85	-0.74	-0.26	-0.21	
-1.06	-0.67	-1.10	-0.85	-0.86	-0.47	-0.07	0.42	
map_weights [-0.24320973  0.56150266 -0.6900454  -0.29581523 -0.23700991 -0.0756359 ]
MAP reward
-0.24	-0.69	-0.30	-0.24	-0.69	-0.24	-0.69	-0.24	
-0.30	-0.69	-0.69	-0.69	-0.30	-0.30	0.56	-0.30	
0.56	0.56	-0.30	0.56	-0.30	-0.69	0.56	-0.69	
-0.69	-0.69	-0.30	0.56	-0.30	0.56	-0.24	-0.24	
-0.30	-0.69	-0.30	0.56	0.56	0.56	-0.30	-0.30	
-0.69	0.56	0.56	-0.24	-0.30	-0.24	-0.24	0.56	
-0.69	-0.24	-0.24	-0.24	-0.24	-0.24	-0.24	-0.30	
-0.69	-0.24	-0.30	-0.24	-0.69	-0.69	-0.24	-0.08	
Map policy
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	v	v	v	<	<	
^	v	>	>	<	<	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
expeced value MDP LP 53.26688953119425
mean w [-0.03349465  0.5499335  -0.07536867 -0.17262437 -0.26908133 -0.14507074]
Mean policy from posterior
v	v	v	v	v	>	v	<	
v	v	>	v	<	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	v	<	v	<	^	
^	v	>	>	>	<	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
Mean rewards
-0.27	-0.08	-0.17	-0.27	-0.08	-0.27	-0.08	-0.03	
-0.17	-0.08	-0.08	-0.08	-0.17	-0.17	0.55	-0.17	
0.55	0.55	-0.17	0.55	-0.17	-0.08	0.55	-0.08	
-0.08	-0.08	-0.17	0.55	-0.17	0.55	-0.27	-0.03	
-0.17	-0.08	-0.17	0.55	0.55	0.55	-0.17	-0.17	
-0.08	0.55	0.55	-0.27	-0.17	-0.03	-0.27	0.55	
-0.08	-0.27	-0.27	-0.27	-0.27	-0.03	-0.27	-0.17	
-0.08	-0.27	-0.17	-0.27	-0.08	-0.08	-0.03	-0.15	
mean = 0.11942786392557359, map = 0.09415843463374951
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
<	<	<	v	<	v	^	<	
^	^	>	v	<	v	<	v	
^	v	>	>	<	^	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	v	<	v	<	v	
^	v	>	>	<	<	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	v	>	v	<	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	^	>	v	<	^	
^	v	>	>	<	<	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	v	>	v	<	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	^	<	v	^	^	
^	v	>	>	<	^	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	v	>	v	<	>	v	<	
>	<	>	v	<	>	^	<	
^	^	>	^	<	v	<	^	
^	v	>	>	>	^	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
cvar = , 0.14381516302649267, 0.14385998963905255, 0.1194279682064725, 0.11943136004033839, 0.11945625500806878
==========
iteration 43
==========
weights [-0.43255586 -0.48379263 -0.00750224 -0.64402311 -0.34845398  0.20639255]
expeced value MDP LP -1.2029239591397993
demonstration
[(32, 3), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0)]
[ 0.3906694  -0.54578307  0.46643941  0.46318181 -0.33190676  0.08504734]
w_map [-0.06266215 -0.45328459  0.78540042 -0.11031249 -0.29128405  0.27701529] loglik -0.6930801614480515
accepted/total = 2744/3000 = 0.9146666666666666
-------
true weights [-0.43255586 -0.48379263 -0.00750224 -0.64402311 -0.34845398  0.20639255]
features
4 	2 	3 	3 	1 	3 	2 	0 	
2 	3 	2 	0 	2 	3 	0 	4 	
2 	0 	2 	1 	2 	4 	4 	3 	
2 	1 	0 	2 	0 	0 	3 	2 	
0 	1 	3 	3 	3 	3 	1 	4 	
2 	0 	1 	3 	1 	0 	2 	3 	
1 	4 	1 	4 	1 	3 	3 	0 	
1 	4 	0 	3 	4 	4 	4 	5 	
optimal policy
>	^	<	v	v	>	^	<	
v	^	v	>	v	<	^	^	
^	>	^	>	^	<	<	v	
^	<	^	>	^	^	>	>	
v	<	^	^	^	v	v	^	
<	<	<	v	>	>	v	v	
^	^	<	>	v	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.09	-0.75	-1.39	-1.81	-1.23	-1.39	-0.75	-1.18	
-0.75	-1.39	-0.75	-1.18	-0.75	-1.39	-1.18	-1.51	
-0.75	-1.18	-0.75	-1.23	-0.75	-1.09	-1.43	-1.39	
-0.75	-1.23	-1.18	-1.17	-1.18	-1.51	-1.39	-0.75	
-1.18	-1.65	-1.81	-1.80	-1.81	-1.84	-1.26	-1.09	
-0.75	-1.18	-1.65	-2.27	-1.68	-1.21	-0.79	-0.87	
-1.23	-1.51	-1.98	-1.65	-1.31	-1.13	-0.79	-0.23	
-1.70	-1.85	-1.89	-1.47	-0.83	-0.49	-0.14	0.21	
map_weights [-0.06266215 -0.45328459  0.78540042 -0.11031249 -0.29128405  0.27701529]
MAP reward
-0.29	0.79	-0.11	-0.11	-0.45	-0.11	0.79	-0.06	
0.79	-0.11	0.79	-0.06	0.79	-0.11	-0.06	-0.29	
0.79	-0.06	0.79	-0.45	0.79	-0.29	-0.29	-0.11	
0.79	-0.45	-0.06	0.79	-0.06	-0.06	-0.11	0.79	
-0.06	-0.45	-0.11	-0.11	-0.11	-0.11	-0.45	-0.29	
0.79	-0.06	-0.45	-0.11	-0.45	-0.06	0.79	-0.11	
-0.45	-0.29	-0.45	-0.29	-0.45	-0.11	-0.11	-0.06	
-0.45	-0.29	-0.06	-0.11	-0.29	-0.29	-0.29	0.28	
Map policy
v	^	<	v	v	>	^	<	
v	<	v	>	v	<	^	^	
<	<	^	>	^	<	^	v	
^	<	^	<	^	<	>	>	
v	<	^	^	^	^	^	^	
<	<	<	^	^	>	>	^	
^	^	<	^	^	^	^	^	
^	^	<	^	^	^	^	.	
expeced value MDP LP 50.34002613970702
mean w [ 0.04490195 -0.16362856  0.52168481 -0.15557097 -0.11610297 -0.04196249]
Mean policy from posterior
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
^	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	^	^	>	>	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
Mean rewards
-0.12	0.52	-0.16	-0.16	-0.16	-0.16	0.52	0.04	
0.52	-0.16	0.52	0.04	0.52	-0.16	0.04	-0.12	
0.52	0.04	0.52	-0.16	0.52	-0.12	-0.12	-0.16	
0.52	-0.16	0.04	0.52	0.04	0.04	-0.16	0.52	
0.04	-0.16	-0.16	-0.16	-0.16	-0.16	-0.16	-0.12	
0.52	0.04	-0.16	-0.16	-0.16	0.04	0.52	-0.16	
-0.16	-0.12	-0.16	-0.12	-0.16	-0.16	-0.16	0.04	
-0.16	-0.12	0.04	-0.16	-0.12	-0.12	-0.12	-0.04	
mean = 0.3215871633082312, map = 0.34074451593684874
CVaR policy
v	^	<	v	v	>	^	<	
<	<	v	<	v	<	^	<	
v	<	^	<	^	<	<	v	
^	<	^	<	^	^	>	>	
^	<	^	^	^	^	>	^	
<	<	<	^	^	>	>	^	
^	^	<	<	^	^	^	^	
^	^	<	<	<	>	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
<	<	v	<	v	<	^	^	
<	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	^	^	>	>	^	
^	^	<	<	^	^	^	^	
^	^	<	<	<	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
<	<	v	>	v	<	^	^	
<	<	^	<	^	<	^	v	
<	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	^	^	>	>	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
v	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	^	^	>	>	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
v	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	^	^	>	>	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
cvar = , 0.3186301341683544, 0.32388016839458644, 0.3215871452408263, 0.3216030085840442, 0.3215895376978448
==========
iteration 44
==========
weights [-0.20169497 -0.5384595  -0.10666749 -0.08781692 -0.71497981  0.3729539 ]
expeced value MDP LP -1.6882638322738717
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.51921774  0.45841349  0.51120622 -0.4149192   0.25305863 -0.15080317]
w_map [-0.57341025 -0.5909519  -0.23675498 -0.25208887 -0.42516946  0.14698897] loglik -2.0794415008916936
accepted/total = 2008/3000 = 0.6693333333333333
-------
true weights [-0.20169497 -0.5384595  -0.10666749 -0.08781692 -0.71497981  0.3729539 ]
features
0 	2 	3 	0 	0 	3 	0 	0 	
4 	4 	1 	0 	2 	3 	0 	1 	
1 	3 	3 	0 	4 	2 	4 	4 	
4 	3 	2 	0 	1 	4 	1 	1 	
2 	0 	4 	1 	4 	0 	3 	0 	
0 	3 	1 	2 	1 	0 	4 	1 	
0 	4 	1 	1 	0 	2 	4 	0 	
3 	0 	4 	1 	0 	4 	3 	5 	
optimal policy
>	>	>	>	>	v	<	<	
^	v	>	>	>	v	<	v	
>	v	v	^	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.48	-2.30	-2.22	-2.15	-1.97	-1.79	-1.97	-2.15	
-3.17	-2.96	-2.51	-1.99	-1.81	-1.72	-1.90	-2.33	
-2.78	-2.27	-2.22	-2.17	-2.34	-1.65	-1.89	-1.81	
-2.90	-2.20	-2.15	-2.07	-2.08	-1.55	-1.18	-1.10	
-2.22	-2.14	-2.58	-1.88	-1.55	-0.85	-0.65	-0.57	
-2.14	-1.95	-1.88	-1.36	-1.27	-0.73	-1.08	-0.37	
-2.30	-2.49	-1.79	-1.27	-0.73	-0.54	-0.44	0.17	
-2.12	-2.05	-1.87	-1.17	-0.63	-0.44	0.28	0.37	
map_weights [-0.57341025 -0.5909519  -0.23675498 -0.25208887 -0.42516946  0.14698897]
MAP reward
-0.57	-0.24	-0.25	-0.57	-0.57	-0.25	-0.57	-0.57	
-0.43	-0.43	-0.59	-0.57	-0.24	-0.25	-0.57	-0.59	
-0.59	-0.25	-0.25	-0.57	-0.43	-0.24	-0.43	-0.43	
-0.43	-0.25	-0.24	-0.57	-0.59	-0.43	-0.59	-0.59	
-0.24	-0.57	-0.43	-0.59	-0.43	-0.57	-0.25	-0.57	
-0.57	-0.25	-0.59	-0.24	-0.59	-0.57	-0.43	-0.59	
-0.57	-0.43	-0.59	-0.59	-0.57	-0.24	-0.43	-0.57	
-0.25	-0.57	-0.43	-0.59	-0.57	-0.43	-0.25	0.15	
Map policy
>	>	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.48529125168347
mean w [-0.50969573 -0.42069766 -0.17340777 -0.22457702 -0.51359223 -0.00301089]
Mean policy from posterior
>	>	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.51	-0.17	-0.22	-0.51	-0.51	-0.22	-0.51	-0.51	
-0.51	-0.51	-0.42	-0.51	-0.17	-0.22	-0.51	-0.42	
-0.42	-0.22	-0.22	-0.51	-0.51	-0.17	-0.51	-0.51	
-0.51	-0.22	-0.17	-0.51	-0.42	-0.51	-0.42	-0.42	
-0.17	-0.51	-0.51	-0.42	-0.51	-0.51	-0.22	-0.51	
-0.51	-0.22	-0.42	-0.17	-0.42	-0.51	-0.51	-0.42	
-0.51	-0.51	-0.42	-0.42	-0.51	-0.17	-0.51	-0.51	
-0.22	-0.51	-0.51	-0.42	-0.51	-0.51	-0.22	-0.00	
mean = 0.23773008482816005, map = 0.2794270759411652
CVaR policy
>	>	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.20751414642096333, 0.238955195566404, 0.23773008482331548, 0.2377300848618642, 0.23773008490653114
==========
iteration 45
==========
weights [-0.13132706 -0.72845255 -0.31441088 -0.54012427 -0.2334518   0.08379686]
expeced value MDP LP -2.3290516589831314
demonstration
[(32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.14994606 -0.16006181  0.84841119  0.27735365 -0.07218581 -0.38724541]
w_map [-0.06276747 -0.56832322 -0.48725844 -0.53080788 -0.06130329  0.38747009] loglik -0.8048926586238281
accepted/total = 1537/3000 = 0.5123333333333333
-------
true weights [-0.13132706 -0.72845255 -0.31441088 -0.54012427 -0.2334518   0.08379686]
features
3 	3 	1 	4 	1 	2 	2 	1 	
1 	2 	3 	4 	1 	4 	0 	4 	
3 	0 	2 	4 	3 	1 	1 	2 	
2 	2 	4 	1 	2 	1 	3 	1 	
2 	2 	3 	0 	2 	3 	1 	2 	
2 	4 	1 	3 	3 	3 	4 	2 	
1 	0 	3 	3 	3 	2 	4 	4 	
0 	2 	1 	1 	0 	1 	2 	5 	
optimal policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	v	v	>	v	^	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
optimal values
-4.49	-3.99	-4.09	-3.40	-3.33	-2.62	-2.41	-2.72	
-4.17	-3.48	-3.61	-3.19	-3.04	-2.33	-2.12	-2.01	
-3.71	-3.20	-3.10	-2.99	-2.78	-3.04	-2.50	-1.79	
-3.38	-3.10	-2.81	-2.79	-2.27	-2.39	-1.86	-1.49	
-3.17	-2.88	-2.60	-2.08	-1.97	-1.67	-1.33	-0.77	
-2.88	-2.59	-2.90	-2.20	-1.67	-1.15	-0.61	-0.46	
-3.09	-2.39	-2.28	-1.75	-1.23	-0.69	-0.38	-0.15	
-2.78	-2.68	-2.51	-1.80	-1.08	-0.96	-0.23	0.08	
map_weights [-0.06276747 -0.56832322 -0.48725844 -0.53080788 -0.06130329  0.38747009]
MAP reward
-0.53	-0.53	-0.57	-0.06	-0.57	-0.49	-0.49	-0.57	
-0.57	-0.49	-0.53	-0.06	-0.57	-0.06	-0.06	-0.06	
-0.53	-0.06	-0.49	-0.06	-0.53	-0.57	-0.57	-0.49	
-0.49	-0.49	-0.06	-0.57	-0.49	-0.57	-0.53	-0.57	
-0.49	-0.49	-0.53	-0.06	-0.49	-0.53	-0.57	-0.49	
-0.49	-0.06	-0.57	-0.53	-0.53	-0.53	-0.06	-0.49	
-0.57	-0.06	-0.53	-0.53	-0.53	-0.49	-0.06	-0.06	
-0.06	-0.49	-0.57	-0.57	-0.06	-0.57	-0.49	0.39	
Map policy
>	>	>	v	>	v	v	v	
>	v	>	v	>	>	v	<	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9573734019746063
mean w [-0.22042209 -0.63110354 -0.26304317 -0.38376446 -0.13294357 -0.15230988]
Mean policy from posterior
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.38	-0.38	-0.63	-0.13	-0.63	-0.26	-0.26	-0.63	
-0.63	-0.26	-0.38	-0.13	-0.63	-0.13	-0.22	-0.13	
-0.38	-0.22	-0.26	-0.13	-0.38	-0.63	-0.63	-0.26	
-0.26	-0.26	-0.13	-0.63	-0.26	-0.63	-0.38	-0.63	
-0.26	-0.26	-0.38	-0.22	-0.26	-0.38	-0.63	-0.26	
-0.26	-0.13	-0.63	-0.38	-0.38	-0.38	-0.13	-0.26	
-0.63	-0.22	-0.38	-0.38	-0.38	-0.26	-0.13	-0.13	
-0.22	-0.26	-0.63	-0.63	-0.22	-0.63	-0.26	-0.15	
mean = 0.010800712043881333, map = 0.10525096410540868
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	>	>	.	
cvar = , 0.007682815510038132, 0.0033808821723160243, 0.003380881596515284, 0.010800706805520033, 0.010800706828341333
==========
iteration 46
==========
weights [-0.28003463 -0.43393688 -0.70423907 -0.3280981  -0.15792812  0.32363106]
expeced value MDP LP -1.7189552378788981
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 3), (46, 3), (54, 3), (62, 1), (63, None)]
[ 0.72264547 -0.4880824  -0.31377553  0.29831954  0.01654979 -0.22767425]
w_map [-0.26984874 -0.35073406 -0.7283936  -0.40546826 -0.14518056 -0.29686383] loglik -0.6931471661600312
accepted/total = 1848/3000 = 0.616
-------
true weights [-0.28003463 -0.43393688 -0.70423907 -0.3280981  -0.15792812  0.32363106]
features
3 	1 	3 	1 	3 	2 	2 	1 	
0 	4 	4 	4 	4 	4 	4 	3 	
3 	0 	3 	0 	0 	0 	2 	0 	
3 	3 	2 	1 	0 	4 	4 	2 	
3 	2 	4 	1 	3 	0 	4 	3 	
3 	2 	3 	1 	1 	1 	0 	2 	
3 	1 	2 	0 	2 	3 	3 	3 	
0 	2 	1 	1 	1 	3 	3 	5 	
optimal policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.61	-2.45	-2.21	-2.18	-1.93	-2.17	-2.31	-2.35	
-2.30	-2.04	-1.90	-1.76	-1.62	-1.48	-1.62	-1.93	
-2.61	-2.30	-2.17	-1.86	-1.60	-1.33	-1.61	-1.87	
-2.91	-2.61	-2.44	-1.75	-1.33	-1.06	-0.91	-1.61	
-2.90	-2.60	-1.91	-1.77	-1.35	-1.04	-0.76	-1.03	
-3.16	-2.87	-2.19	-1.88	-1.46	-1.04	-0.61	-0.71	
-2.86	-2.56	-2.15	-1.46	-1.36	-0.66	-0.34	-0.01	
-2.56	-2.30	-1.61	-1.19	-0.77	-0.34	-0.01	0.32	
map_weights [-0.26984874 -0.35073406 -0.7283936  -0.40546826 -0.14518056 -0.29686383]
MAP reward
-0.41	-0.35	-0.41	-0.35	-0.41	-0.73	-0.73	-0.35	
-0.27	-0.15	-0.15	-0.15	-0.15	-0.15	-0.15	-0.41	
-0.41	-0.27	-0.41	-0.27	-0.27	-0.27	-0.73	-0.27	
-0.41	-0.41	-0.73	-0.35	-0.27	-0.15	-0.15	-0.73	
-0.41	-0.73	-0.15	-0.35	-0.41	-0.27	-0.15	-0.41	
-0.41	-0.73	-0.41	-0.35	-0.35	-0.35	-0.27	-0.73	
-0.41	-0.35	-0.73	-0.27	-0.73	-0.41	-0.41	-0.41	
-0.27	-0.73	-0.35	-0.35	-0.35	-0.41	-0.41	-0.30	
Map policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
>	^	^	>	v	v	v	<	
^	^	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
v	>	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.3155682238932243
mean w [-0.28774902 -0.49500248 -0.59561524 -0.36679179 -0.24466157 -0.00451246]
Mean policy from posterior
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.37	-0.50	-0.37	-0.50	-0.37	-0.60	-0.60	-0.50	
-0.29	-0.24	-0.24	-0.24	-0.24	-0.24	-0.24	-0.37	
-0.37	-0.29	-0.37	-0.29	-0.29	-0.29	-0.60	-0.29	
-0.37	-0.37	-0.60	-0.50	-0.29	-0.24	-0.24	-0.60	
-0.37	-0.60	-0.24	-0.50	-0.37	-0.29	-0.24	-0.37	
-0.37	-0.60	-0.37	-0.50	-0.50	-0.50	-0.29	-0.60	
-0.37	-0.50	-0.60	-0.29	-0.60	-0.37	-0.37	-0.37	
-0.29	-0.60	-0.50	-0.50	-0.50	-0.37	-0.37	-0.00	
mean = 0.047576709116756044, map = 0.0022410869652587095
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
v	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.046928583988930406, 0.03941216269046799, 0.04757670989703833, 0.04757670931801328, 0.0475767144514474
==========
iteration 47
==========
weights [-0.32654197 -0.65989334 -0.15157256 -0.37933918 -0.338548    0.42002847]
expeced value MDP LP -1.788102963274963
demonstration
[(32, 1), (33, 3), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.46577358 -0.52784099 -0.48960173  0.03582056 -0.51308685 -0.01370301]
w_map [-0.17573474 -0.74437639 -0.21189794 -0.48928573 -0.34082563 -0.12065535] loglik -1.1776143438169129e-07
accepted/total = 1704/3000 = 0.568
-------
true weights [-0.32654197 -0.65989334 -0.15157256 -0.37933918 -0.338548    0.42002847]
features
3 	1 	2 	4 	1 	2 	2 	4 	
0 	2 	0 	3 	0 	2 	1 	1 	
1 	4 	2 	1 	1 	1 	2 	4 	
0 	3 	3 	0 	1 	4 	0 	2 	
2 	0 	3 	3 	3 	4 	2 	0 	
4 	2 	4 	0 	0 	4 	4 	4 	
0 	3 	4 	0 	0 	1 	0 	3 	
3 	2 	2 	4 	1 	4 	0 	5 	
optimal policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.65	-3.63	-3.00	-2.94	-2.73	-2.09	-1.96	-2.07	
-3.30	-3.00	-2.88	-2.63	-2.27	-1.96	-1.83	-1.75	
-3.36	-2.89	-2.58	-2.73	-2.47	-1.83	-1.18	-1.10	
-2.73	-2.66	-2.45	-2.09	-2.01	-1.37	-1.04	-0.77	
-2.43	-2.30	-2.15	-1.79	-1.42	-1.05	-0.72	-0.63	
-2.31	-1.99	-1.86	-1.54	-1.22	-0.91	-0.57	-0.30	
-2.19	-1.88	-1.70	-1.53	-1.21	-0.90	-0.24	0.04	
-1.88	-1.51	-1.38	-1.24	-0.91	-0.25	0.09	0.42	
map_weights [-0.17573474 -0.74437639 -0.21189794 -0.48928573 -0.34082563 -0.12065535]
MAP reward
-0.49	-0.74	-0.21	-0.34	-0.74	-0.21	-0.21	-0.34	
-0.18	-0.21	-0.18	-0.49	-0.18	-0.21	-0.74	-0.74	
-0.74	-0.34	-0.21	-0.74	-0.74	-0.74	-0.21	-0.34	
-0.18	-0.49	-0.49	-0.18	-0.74	-0.34	-0.18	-0.21	
-0.21	-0.18	-0.49	-0.49	-0.49	-0.34	-0.21	-0.18	
-0.34	-0.21	-0.34	-0.18	-0.18	-0.34	-0.34	-0.34	
-0.18	-0.49	-0.34	-0.18	-0.18	-0.74	-0.18	-0.49	
-0.49	-0.21	-0.21	-0.34	-0.74	-0.34	-0.18	-0.12	
Map policy
v	>	v	v	v	>	v	<	
>	>	v	>	>	v	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.409370595448672
mean w [-0.10934918 -0.58999838 -0.22952931 -0.58728641 -0.21756357  0.0631612 ]
Mean policy from posterior
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	<	
^	>	>	>	^	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.59	-0.59	-0.23	-0.22	-0.59	-0.23	-0.23	-0.22	
-0.11	-0.23	-0.11	-0.59	-0.11	-0.23	-0.59	-0.59	
-0.59	-0.22	-0.23	-0.59	-0.59	-0.59	-0.23	-0.22	
-0.11	-0.59	-0.59	-0.11	-0.59	-0.22	-0.11	-0.23	
-0.23	-0.11	-0.59	-0.59	-0.59	-0.22	-0.23	-0.11	
-0.22	-0.23	-0.22	-0.11	-0.11	-0.22	-0.22	-0.22	
-0.11	-0.59	-0.22	-0.11	-0.11	-0.59	-0.11	-0.59	
-0.59	-0.23	-0.23	-0.22	-0.59	-0.22	-0.11	0.06	
mean = 0.1494086344734913, map = 0.044772636730785464
CVaR policy
v	v	v	v	>	>	v	v	
>	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	<	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	<	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	v	>	v	<	
>	>	>	>	>	>	v	<	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	<	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	v	>	v	<	
>	>	>	>	>	>	v	<	
^	>	>	^	^	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	<	
^	>	>	>	^	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.03164873745090602, 0.05176083576070978, 0.08667055972787763, 0.10897309244663034, 0.14940863205542732
==========
iteration 48
==========
weights [-0.38330247 -0.46518363 -0.7242588  -0.08270866 -0.18571323  0.26608733]
expeced value MDP LP -1.5625245722155583
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.16320687 -0.05280776 -0.14477097 -0.49723021 -0.71613979 -0.4353414 ]
w_map [-0.69765551 -0.43854736 -0.28070637 -0.19485774 -0.44938348 -0.04734864] loglik -3.255706815252779e-11
accepted/total = 1887/3000 = 0.629
-------
true weights [-0.38330247 -0.46518363 -0.7242588  -0.08270866 -0.18571323  0.26608733]
features
0 	0 	0 	4 	2 	4 	1 	4 	
1 	2 	3 	1 	3 	0 	4 	2 	
0 	2 	4 	0 	3 	2 	3 	4 	
0 	2 	1 	0 	1 	2 	1 	3 	
0 	0 	3 	3 	2 	4 	3 	4 	
1 	1 	1 	2 	4 	0 	3 	4 	
3 	0 	0 	4 	4 	0 	0 	1 	
0 	1 	1 	4 	0 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	<	
>	>	>	>	>	>	v	v	
v	>	>	>	^	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.11	-2.75	-2.39	-2.13	-2.22	-1.62	-1.53	-1.70	
-3.17	-2.73	-2.03	-1.97	-1.52	-1.45	-1.07	-1.54	
-3.02	-2.82	-2.12	-1.95	-1.58	-1.61	-0.90	-0.82	
-2.67	-2.65	-2.03	-1.88	-1.89	-1.44	-1.00	-0.64	
-2.31	-1.94	-1.58	-1.51	-1.44	-0.72	-0.54	-0.57	
-2.37	-2.31	-1.94	-1.73	-1.02	-0.84	-0.46	-0.39	
-1.92	-1.86	-1.49	-1.12	-0.94	-0.76	-0.58	-0.20	
-2.21	-1.85	-1.40	-0.94	-0.76	-0.39	-0.20	0.27	
map_weights [-0.69765551 -0.43854736 -0.28070637 -0.19485774 -0.44938348 -0.04734864]
MAP reward
-0.70	-0.70	-0.70	-0.45	-0.28	-0.45	-0.44	-0.45	
-0.44	-0.28	-0.19	-0.44	-0.19	-0.70	-0.45	-0.28	
-0.70	-0.28	-0.45	-0.70	-0.19	-0.28	-0.19	-0.45	
-0.70	-0.28	-0.44	-0.70	-0.44	-0.28	-0.44	-0.19	
-0.70	-0.70	-0.19	-0.19	-0.28	-0.45	-0.19	-0.45	
-0.44	-0.44	-0.44	-0.28	-0.45	-0.70	-0.19	-0.45	
-0.19	-0.70	-0.70	-0.45	-0.45	-0.70	-0.70	-0.44	
-0.70	-0.44	-0.44	-0.45	-0.70	-0.45	-0.44	-0.05	
Map policy
v	v	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
>	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.2164243246823174
mean w [-0.51464412 -0.48997016 -0.37316567 -0.1200399  -0.27515792 -0.19396752]
Mean policy from posterior
v	v	v	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.51	-0.51	-0.51	-0.28	-0.37	-0.28	-0.49	-0.28	
-0.49	-0.37	-0.12	-0.49	-0.12	-0.51	-0.28	-0.37	
-0.51	-0.37	-0.28	-0.51	-0.12	-0.37	-0.12	-0.28	
-0.51	-0.37	-0.49	-0.51	-0.49	-0.37	-0.49	-0.12	
-0.51	-0.51	-0.12	-0.12	-0.37	-0.28	-0.12	-0.28	
-0.49	-0.49	-0.49	-0.37	-0.28	-0.51	-0.12	-0.28	
-0.12	-0.51	-0.51	-0.28	-0.28	-0.51	-0.51	-0.49	
-0.51	-0.49	-0.49	-0.28	-0.51	-0.28	-0.49	-0.19	
mean = 0.08117696788206619, map = 0.18543313742158807
CVaR policy
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	v	v	>	>	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.23025367720892898, 0.1488798376487388, 0.0867378108269663, 0.0811769670897764, 0.08117696543901909
==========
iteration 49
==========
weights [-0.44520993 -0.27171281 -0.45007951 -0.36066169 -0.62605234  0.05805433]
expeced value MDP LP -2.4933815319654355
demonstration
[(32, 3), (40, 1), (41, 1), (42, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.05464748  0.17040784 -0.31198462 -0.24916806  0.89915501 -0.00871556]
w_map [-0.66051375 -0.16159473 -0.63866812 -0.29814786 -0.17838025  0.09486894] loglik -1.386294239159426
accepted/total = 2143/3000 = 0.7143333333333334
-------
true weights [-0.44520993 -0.27171281 -0.45007951 -0.36066169 -0.62605234  0.05805433]
features
3 	4 	0 	4 	1 	0 	4 	0 	
4 	0 	4 	3 	0 	4 	4 	4 	
4 	2 	2 	0 	1 	4 	2 	2 	
1 	3 	4 	3 	0 	2 	4 	0 	
3 	2 	2 	1 	4 	4 	1 	0 	
3 	3 	1 	3 	4 	1 	2 	1 	
1 	2 	3 	1 	1 	1 	3 	2 	
3 	1 	2 	0 	0 	0 	2 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.87	-4.64	-4.22	-3.82	-3.64	-3.93	-3.59	-2.99	
-4.56	-4.06	-3.82	-3.22	-3.40	-3.52	-3.00	-2.57	
-3.97	-3.65	-3.31	-2.89	-2.99	-2.92	-2.40	-1.97	
-3.38	-3.23	-3.07	-2.47	-2.74	-2.32	-1.97	-1.53	
-3.14	-2.90	-2.56	-2.13	-2.50	-1.89	-1.36	-1.10	
-2.81	-2.47	-2.13	-1.88	-1.89	-1.28	-1.10	-0.66	
-2.56	-2.31	-1.88	-1.53	-1.28	-1.01	-0.75	-0.39	
-2.72	-2.39	-2.14	-1.70	-1.27	-0.83	-0.39	0.06	
map_weights [-0.66051375 -0.16159473 -0.63866812 -0.29814786 -0.17838025  0.09486894]
MAP reward
-0.30	-0.18	-0.66	-0.18	-0.16	-0.66	-0.18	-0.66	
-0.18	-0.66	-0.18	-0.30	-0.66	-0.18	-0.18	-0.18	
-0.18	-0.64	-0.64	-0.66	-0.16	-0.18	-0.64	-0.64	
-0.16	-0.30	-0.18	-0.30	-0.66	-0.64	-0.18	-0.66	
-0.30	-0.64	-0.64	-0.16	-0.18	-0.18	-0.16	-0.66	
-0.30	-0.30	-0.16	-0.30	-0.18	-0.16	-0.64	-0.16	
-0.16	-0.64	-0.30	-0.16	-0.16	-0.16	-0.30	-0.64	
-0.30	-0.16	-0.64	-0.66	-0.66	-0.66	-0.64	0.09	
Map policy
v	<	v	v	v	v	v	v	
v	<	v	v	v	v	v	<	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	v	v	<	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	^	^	^	>	>	.	
expeced value MDP LP -1.8796115649373246
mean w [-0.47274284 -0.16292373 -0.4906809  -0.16501022 -0.53076085  0.04040235]
Mean policy from posterior
v	v	v	v	v	<	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	<	>	v	<	v	v	v	
v	v	v	v	<	v	>	v	
>	>	v	v	>	v	>	v	
^	>	>	>	>	>	v	v	
^	>	^	^	>	>	>	.	
Mean rewards
-0.17	-0.53	-0.47	-0.53	-0.16	-0.47	-0.53	-0.47	
-0.53	-0.47	-0.53	-0.17	-0.47	-0.53	-0.53	-0.53	
-0.53	-0.49	-0.49	-0.47	-0.16	-0.53	-0.49	-0.49	
-0.16	-0.17	-0.53	-0.17	-0.47	-0.49	-0.53	-0.47	
-0.17	-0.49	-0.49	-0.16	-0.53	-0.53	-0.16	-0.47	
-0.17	-0.17	-0.16	-0.17	-0.53	-0.16	-0.49	-0.16	
-0.16	-0.49	-0.17	-0.16	-0.16	-0.16	-0.17	-0.49	
-0.17	-0.16	-0.49	-0.47	-0.47	-0.47	-0.49	0.04	
mean = 0.0743795862387926, map = 0.37766937758254127
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	<	v	v	v	
v	>	>	v	<	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	<	v	v	v	
v	v	>	v	<	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	>	.	
CVaR policy
v	v	v	v	v	<	v	v	
v	v	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	<	>	v	<	v	v	v	
v	v	>	v	<	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	>	.	
CVaR policy
v	v	>	v	v	<	v	v	
v	v	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	<	>	v	<	v	v	v	
v	v	>	v	<	v	>	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
cvar = , 0.028568501187614892, 0.0324577693153838, 0.03111434719594186, 0.06454548700517027, 0.07984214833876013
==========
iteration 50
==========
weights [-0.89793034 -0.16824879 -0.15900828 -0.23235499 -0.27964434  0.0891068 ]
expeced value MDP LP -1.471682357898469
demonstration
[(32, 1), (33, 3), (41, 1), (42, 1), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.66701249  0.11535398 -0.44346386  0.10750448  0.56999646 -0.09313656]
w_map [-0.72360335 -0.25462772 -0.26809449 -0.27687086 -0.49506533 -0.13394441] loglik -9.083720442504273e-10
accepted/total = 1769/3000 = 0.5896666666666667
-------
true weights [-0.89793034 -0.16824879 -0.15900828 -0.23235499 -0.27964434  0.0891068 ]
features
2 	1 	2 	3 	3 	0 	0 	0 	
1 	2 	4 	4 	2 	3 	1 	2 	
2 	0 	3 	3 	2 	4 	0 	4 	
3 	1 	0 	1 	2 	4 	0 	0 	
1 	4 	4 	2 	0 	2 	4 	3 	
0 	3 	2 	2 	0 	1 	0 	0 	
1 	3 	0 	1 	1 	4 	3 	4 	
4 	3 	3 	0 	2 	2 	2 	5 	
optimal policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	<	
v	>	>	v	v	v	<	^	
v	v	>	v	>	v	v	v	
>	v	v	v	>	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-2.30	-2.16	-2.02	-1.88	-1.75	-2.46	-2.61	-2.75	
-2.16	-2.02	-1.88	-1.66	-1.53	-1.58	-1.73	-1.87	
-2.01	-2.49	-1.61	-1.39	-1.39	-1.36	-2.25	-2.14	
-1.87	-1.66	-2.06	-1.17	-1.24	-1.09	-1.98	-2.19	
-1.66	-1.50	-1.28	-1.02	-1.71	-0.82	-1.09	-1.31	
-2.12	-1.24	-1.02	-0.86	-1.44	-0.67	-1.20	-1.09	
-1.61	-1.46	-1.60	-0.71	-0.55	-0.51	-0.30	-0.19	
-1.87	-1.68	-1.50	-1.28	-0.39	-0.23	-0.07	0.09	
map_weights [-0.72360335 -0.25462772 -0.26809449 -0.27687086 -0.49506533 -0.13394441]
MAP reward
-0.27	-0.25	-0.27	-0.28	-0.28	-0.72	-0.72	-0.72	
-0.25	-0.27	-0.50	-0.50	-0.27	-0.28	-0.25	-0.27	
-0.27	-0.72	-0.28	-0.28	-0.27	-0.50	-0.72	-0.50	
-0.28	-0.25	-0.72	-0.25	-0.27	-0.50	-0.72	-0.72	
-0.25	-0.50	-0.50	-0.27	-0.72	-0.27	-0.50	-0.28	
-0.72	-0.28	-0.27	-0.27	-0.72	-0.25	-0.72	-0.72	
-0.25	-0.28	-0.72	-0.25	-0.25	-0.50	-0.28	-0.50	
-0.50	-0.28	-0.28	-0.72	-0.27	-0.27	-0.27	-0.13	
Map policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.3929286616636405
mean w [-0.66935059 -0.26256705 -0.13598282 -0.2291257  -0.40699145  0.28417722]
Mean policy from posterior
>	>	>	>	v	v	v	v	
v	>	v	v	v	<	<	<	
v	>	>	v	v	v	<	v	
v	v	>	v	<	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.14	-0.26	-0.14	-0.23	-0.23	-0.67	-0.67	-0.67	
-0.26	-0.14	-0.41	-0.41	-0.14	-0.23	-0.26	-0.14	
-0.14	-0.67	-0.23	-0.23	-0.14	-0.41	-0.67	-0.41	
-0.23	-0.26	-0.67	-0.26	-0.14	-0.41	-0.67	-0.67	
-0.26	-0.41	-0.41	-0.14	-0.67	-0.14	-0.41	-0.23	
-0.67	-0.23	-0.14	-0.14	-0.67	-0.26	-0.67	-0.67	
-0.26	-0.23	-0.67	-0.26	-0.26	-0.41	-0.23	-0.41	
-0.41	-0.23	-0.23	-0.67	-0.14	-0.14	-0.14	0.28	
mean = 0.056603612005547976, map = 0.0674165401368716
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	v	v	<	v	
v	v	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	v	v	<	v	
>	v	>	v	<	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	>	v	v	v	<	<	<	
v	>	>	v	v	v	<	v	
>	v	>	v	<	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.09954299580760617, 0.07089655169974929, 0.040883413992602424, 0.04575453401378793, 0.056603619214330436
==========
iteration 51
==========
weights [-0.08889064 -0.06545644 -0.44693923 -0.79932443 -0.37125299  0.10635271]
expeced value MDP LP -2.034645332989533
demonstration
[(32, 3), (40, 1), (41, 1), (42, 3), (50, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.02576216  0.24137813 -0.38546745 -0.35494832  0.80434996  0.13971607]
w_map [-0.27637923 -0.23982238 -0.52138061 -0.55187372 -0.52804895  0.10421952] loglik -7.818812264304142e-10
accepted/total = 1883/3000 = 0.6276666666666667
-------
true weights [-0.08889064 -0.06545644 -0.44693923 -0.79932443 -0.37125299  0.10635271]
features
3 	2 	3 	0 	2 	4 	4 	0 	
3 	4 	4 	4 	4 	4 	3 	1 	
2 	2 	3 	4 	0 	2 	2 	1 	
2 	1 	3 	3 	1 	4 	4 	3 	
3 	2 	2 	1 	0 	3 	3 	4 	
1 	2 	1 	2 	2 	4 	2 	2 	
4 	4 	0 	2 	3 	3 	0 	2 	
3 	4 	0 	0 	4 	0 	3 	5 	
optimal policy
>	v	>	v	v	>	>	v	
>	v	>	v	v	<	>	v	
v	v	>	>	v	<	>	v	
>	v	v	v	v	<	v	v	
v	>	v	>	v	v	v	v	
>	>	v	<	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.27	-3.51	-3.40	-2.62	-2.64	-2.79	-2.45	-2.10	
-3.86	-3.09	-2.91	-2.56	-2.21	-2.56	-2.81	-2.03	
-3.17	-2.75	-2.99	-2.21	-1.86	-2.29	-2.41	-1.98	
-2.75	-2.33	-2.63	-2.57	-1.79	-2.14	-2.02	-1.94	
-2.68	-2.28	-1.85	-1.79	-1.74	-2.02	-1.66	-1.15	
-1.90	-1.85	-1.42	-1.85	-1.67	-1.23	-0.87	-0.79	
-2.08	-1.73	-1.37	-1.65	-1.93	-1.22	-0.43	-0.34	
-2.43	-1.65	-1.29	-1.22	-1.14	-0.78	-0.69	0.11	
map_weights [-0.27637923 -0.23982238 -0.52138061 -0.55187372 -0.52804895  0.10421952]
MAP reward
-0.55	-0.52	-0.55	-0.28	-0.52	-0.53	-0.53	-0.28	
-0.55	-0.53	-0.53	-0.53	-0.53	-0.53	-0.55	-0.24	
-0.52	-0.52	-0.55	-0.53	-0.28	-0.52	-0.52	-0.24	
-0.52	-0.24	-0.55	-0.55	-0.24	-0.53	-0.53	-0.55	
-0.55	-0.52	-0.52	-0.24	-0.28	-0.55	-0.55	-0.53	
-0.24	-0.52	-0.24	-0.52	-0.52	-0.53	-0.52	-0.52	
-0.53	-0.53	-0.28	-0.52	-0.55	-0.55	-0.28	-0.52	
-0.55	-0.53	-0.28	-0.28	-0.53	-0.28	-0.55	0.10	
Map policy
>	v	>	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	>	>	v	
>	v	v	v	v	>	v	v	
v	>	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1519772299345146
mean w [-0.14143916 -0.21738438 -0.49972249 -0.41744442 -0.56252247  0.15594831]
Mean policy from posterior
v	v	>	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	<	v	v	
>	>	v	v	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	v	<	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.42	-0.50	-0.42	-0.14	-0.50	-0.56	-0.56	-0.14	
-0.42	-0.56	-0.56	-0.56	-0.56	-0.56	-0.42	-0.22	
-0.50	-0.50	-0.42	-0.56	-0.14	-0.50	-0.50	-0.22	
-0.50	-0.22	-0.42	-0.42	-0.22	-0.56	-0.56	-0.42	
-0.42	-0.50	-0.50	-0.22	-0.14	-0.42	-0.42	-0.56	
-0.22	-0.50	-0.22	-0.50	-0.50	-0.56	-0.50	-0.50	
-0.56	-0.56	-0.14	-0.50	-0.42	-0.42	-0.14	-0.50	
-0.42	-0.56	-0.14	-0.14	-0.56	-0.14	-0.42	0.16	
mean = 0.3775116034583683, map = 0.049291809611675674
CVaR policy
v	v	>	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	>	>	v	
>	v	v	v	v	v	v	v	
v	v	v	>	v	>	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	>	v	>	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	<	v	v	
>	>	v	v	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	<	v	v	
>	>	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	<	v	v	
>	>	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	<	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.24374013959576502, 0.3415557888618457, 0.3810745961517079, 0.3810745853176858, 0.3775115912761233
==========
iteration 52
==========
weights [-0.2303422  -0.27617581 -0.58030475 -0.3484603  -0.38072288  0.51724394]
expeced value MDP LP -1.4949577628705846
demonstration
[(32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.38463963 -0.22835104 -0.01301562  0.87228965 -0.15944597  0.1158728 ]
w_map [-0.13312044 -0.10241311 -0.69665597 -0.50748767 -0.44929284 -0.16447845] loglik -0.5130665469009017
accepted/total = 1729/3000 = 0.5763333333333334
-------
true weights [-0.2303422  -0.27617581 -0.58030475 -0.3484603  -0.38072288  0.51724394]
features
3 	1 	0 	0 	1 	0 	2 	0 	
3 	1 	1 	1 	0 	1 	3 	0 	
3 	1 	2 	4 	2 	2 	1 	1 	
2 	4 	0 	0 	4 	1 	1 	4 	
1 	1 	2 	1 	4 	0 	4 	3 	
3 	3 	4 	4 	1 	4 	0 	1 	
1 	0 	4 	0 	0 	2 	4 	0 	
1 	2 	2 	2 	1 	1 	3 	5 	
optimal policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	>	v	
^	^	>	>	>	>	>	.	
optimal values
-3.20	-2.88	-2.63	-2.42	-2.21	-1.95	-2.00	-1.43	
-3.04	-2.72	-2.47	-2.21	-1.95	-1.74	-1.48	-1.21	
-2.83	-2.51	-2.46	-2.05	-2.03	-1.67	-1.14	-0.99	
-2.82	-2.26	-1.90	-1.68	-1.47	-1.10	-0.88	-0.72	
-2.28	-2.02	-2.03	-1.47	-1.20	-0.83	-0.61	-0.35	
-2.02	-1.76	-1.58	-1.21	-0.88	-0.61	-0.23	0.00	
-1.69	-1.43	-1.21	-0.84	-0.62	-0.68	-0.10	0.28	
-1.95	-2.00	-1.54	-0.97	-0.39	-0.11	0.16	0.52	
map_weights [-0.13312044 -0.10241311 -0.69665597 -0.50748767 -0.44929284 -0.16447845]
MAP reward
-0.51	-0.10	-0.13	-0.13	-0.10	-0.13	-0.70	-0.13	
-0.51	-0.10	-0.10	-0.10	-0.13	-0.10	-0.51	-0.13	
-0.51	-0.10	-0.70	-0.45	-0.70	-0.70	-0.10	-0.10	
-0.70	-0.45	-0.13	-0.13	-0.45	-0.10	-0.10	-0.45	
-0.10	-0.10	-0.70	-0.10	-0.45	-0.13	-0.45	-0.51	
-0.51	-0.51	-0.45	-0.45	-0.10	-0.45	-0.13	-0.10	
-0.10	-0.13	-0.45	-0.13	-0.13	-0.70	-0.45	-0.13	
-0.10	-0.70	-0.70	-0.70	-0.10	-0.10	-0.51	-0.16	
Map policy
>	v	v	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	^	v	v	v	>	v	<	
>	>	>	v	>	>	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	v	>	>	v	
^	^	^	>	>	>	>	.	
expeced value MDP LP -1.7249176277181872
mean w [-0.13234148 -0.17689214 -0.55054382 -0.30332959 -0.53604064 -0.14888342]
Mean policy from posterior
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	^	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.30	-0.18	-0.13	-0.13	-0.18	-0.13	-0.55	-0.13	
-0.30	-0.18	-0.18	-0.18	-0.13	-0.18	-0.30	-0.13	
-0.30	-0.18	-0.55	-0.54	-0.55	-0.55	-0.18	-0.18	
-0.55	-0.54	-0.13	-0.13	-0.54	-0.18	-0.18	-0.54	
-0.18	-0.18	-0.55	-0.18	-0.54	-0.13	-0.54	-0.30	
-0.30	-0.30	-0.54	-0.54	-0.18	-0.54	-0.13	-0.18	
-0.18	-0.13	-0.54	-0.13	-0.13	-0.55	-0.54	-0.13	
-0.18	-0.55	-0.55	-0.55	-0.18	-0.18	-0.30	-0.15	
mean = 0.016662697799308956, map = 0.0572425595307231
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
cvar = , 0.05281517367689026, 0.01367280550126515, 0.0028039866351774467, 0.0028039857649211353, 0.0028039856318458067
==========
iteration 53
==========
weights [-0.31907605 -0.02261283 -0.25637322 -0.8144772  -0.35621239  0.20418503]
expeced value MDP LP -1.159931651599198
demonstration
[(32, 1), (33, 3), (41, 1), (42, 3), (50, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.22589051  0.51402832 -0.73830992 -0.18107879 -0.19132456 -0.26505134]
w_map [-0.37181325 -0.1912702  -0.37516872 -0.76480548 -0.31457108 -0.02316597] loglik -9.874359818695666e-08
accepted/total = 1679/3000 = 0.5596666666666666
-------
true weights [-0.31907605 -0.02261283 -0.25637322 -0.8144772  -0.35621239  0.20418503]
features
0 	0 	3 	3 	3 	3 	0 	0 	
0 	0 	2 	1 	3 	3 	2 	3 	
0 	4 	4 	0 	4 	1 	1 	2 	
1 	2 	4 	4 	2 	3 	4 	1 	
4 	1 	3 	2 	0 	1 	1 	2 	
2 	1 	0 	2 	0 	3 	0 	3 	
3 	3 	2 	0 	0 	4 	2 	4 	
0 	1 	1 	1 	1 	1 	0 	5 	
optimal policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	v	v	>	>	v	v	
>	v	<	v	v	v	v	v	
>	v	v	v	>	>	v	<	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.98	-2.00	-2.69	-2.45	-3.01	-2.41	-1.62	-1.92	
-1.67	-1.70	-1.90	-1.66	-2.22	-1.88	-1.31	-2.01	
-1.37	-1.39	-1.74	-1.65	-1.42	-1.08	-1.06	-1.21	
-1.06	-1.05	-1.39	-1.34	-1.28	-1.53	-1.05	-0.97	
-1.15	-0.80	-1.58	-1.00	-1.03	-0.72	-0.70	-0.95	
-1.03	-0.79	-0.77	-0.75	-0.79	-1.30	-0.69	-0.97	
-1.35	-1.03	-0.46	-0.50	-0.48	-0.49	-0.37	-0.15	
-0.54	-0.22	-0.20	-0.18	-0.16	-0.14	-0.12	0.20	
map_weights [-0.37181325 -0.1912702  -0.37516872 -0.76480548 -0.31457108 -0.02316597]
MAP reward
-0.37	-0.37	-0.76	-0.76	-0.76	-0.76	-0.37	-0.37	
-0.37	-0.37	-0.38	-0.19	-0.76	-0.76	-0.38	-0.76	
-0.37	-0.31	-0.31	-0.37	-0.31	-0.19	-0.19	-0.38	
-0.19	-0.38	-0.31	-0.31	-0.38	-0.76	-0.31	-0.19	
-0.31	-0.19	-0.76	-0.38	-0.37	-0.19	-0.19	-0.38	
-0.38	-0.19	-0.37	-0.38	-0.37	-0.76	-0.37	-0.76	
-0.76	-0.76	-0.38	-0.37	-0.37	-0.31	-0.38	-0.31	
-0.37	-0.19	-0.19	-0.19	-0.19	-0.19	-0.37	-0.02	
Map policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	>	>	>	>	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5933560937604547
mean w [-0.27351431 -0.12707031 -0.30352511 -0.65148818 -0.34113021  0.00121421]
Mean policy from posterior
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	>	v	<	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.27	-0.27	-0.65	-0.65	-0.65	-0.65	-0.27	-0.27	
-0.27	-0.27	-0.30	-0.13	-0.65	-0.65	-0.30	-0.65	
-0.27	-0.34	-0.34	-0.27	-0.34	-0.13	-0.13	-0.30	
-0.13	-0.30	-0.34	-0.34	-0.30	-0.65	-0.34	-0.13	
-0.34	-0.13	-0.65	-0.30	-0.27	-0.13	-0.13	-0.30	
-0.30	-0.13	-0.27	-0.30	-0.27	-0.65	-0.27	-0.65	
-0.65	-0.65	-0.30	-0.27	-0.27	-0.34	-0.30	-0.34	
-0.27	-0.13	-0.13	-0.13	-0.13	-0.13	-0.27	0.00	
mean = 0.010772966996008382, map = 0.05082818332265959
CVaR policy
v	v	v	v	v	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	>	>	>	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	>	>	>	>	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	>	v	<	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05741199176213607, 0.033938103842775735, 0.026850937261917673, 0.026850937490584537, 0.01077303535275198
==========
iteration 54
==========
weights [-0.0212071  -0.22766981 -0.10325627 -0.04689663 -0.34228555  0.90426554]
expeced value MDP LP 0.24349806328469975
demonstration
[(32, 3), (40, 3), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.28644876 -0.53067634  0.56236297  0.30370297  0.30334872 -0.36853994]
w_map [-0.12385978 -0.63628362 -0.25206231 -0.2398206  -0.62810534 -0.25344873] loglik -8.174892165158099e-05
accepted/total = 1131/3000 = 0.377
-------
true weights [-0.0212071  -0.22766981 -0.10325627 -0.04689663 -0.34228555  0.90426554]
features
3 	1 	4 	2 	3 	1 	1 	1 	
2 	2 	2 	3 	3 	4 	3 	2 	
4 	2 	1 	2 	0 	1 	4 	2 	
4 	3 	1 	1 	3 	4 	2 	3 	
0 	2 	1 	0 	1 	2 	2 	3 	
2 	4 	2 	1 	4 	0 	3 	3 	
3 	3 	2 	1 	2 	2 	3 	2 	
1 	4 	0 	4 	4 	0 	3 	5 	
optimal policy
v	v	>	>	v	<	v	v	
>	>	>	>	v	>	>	v	
>	^	>	>	v	<	v	v	
v	v	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
optimal values
-0.17	-0.25	-0.26	0.09	0.19	-0.04	0.13	0.18	
-0.12	-0.02	0.09	0.19	0.24	0.01	0.36	0.41	
-0.46	-0.12	-0.05	0.18	0.29	0.06	0.17	0.52	
-0.31	-0.04	-0.12	0.11	0.31	0.25	0.52	0.63	
0.03	0.01	0.11	0.34	0.37	0.60	0.63	0.68	
0.05	-0.14	0.15	0.13	0.36	0.71	0.74	0.74	
0.16	0.21	0.26	0.36	0.60	0.71	0.79	0.79	
-0.07	-0.11	0.23	0.12	0.47	0.82	0.85	0.90	
map_weights [-0.12385978 -0.63628362 -0.25206231 -0.2398206  -0.62810534 -0.25344873]
MAP reward
-0.24	-0.64	-0.63	-0.25	-0.24	-0.64	-0.64	-0.64	
-0.25	-0.25	-0.25	-0.24	-0.24	-0.63	-0.24	-0.25	
-0.63	-0.25	-0.64	-0.25	-0.12	-0.64	-0.63	-0.25	
-0.63	-0.24	-0.64	-0.64	-0.24	-0.63	-0.25	-0.24	
-0.12	-0.25	-0.64	-0.12	-0.64	-0.25	-0.25	-0.24	
-0.25	-0.63	-0.25	-0.64	-0.63	-0.12	-0.24	-0.24	
-0.24	-0.24	-0.25	-0.64	-0.25	-0.25	-0.24	-0.25	
-0.64	-0.63	-0.12	-0.63	-0.63	-0.12	-0.24	-0.25	
Map policy
v	v	>	>	v	v	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	v	v	v	v	
v	v	>	v	>	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.2290742083528146
mean w [-0.10249245 -0.50768054 -0.17592466 -0.17928552 -0.63999376  0.24972836]
Mean policy from posterior
v	v	>	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	^	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
^	>	^	>	>	>	>	.	
Mean rewards
-0.18	-0.51	-0.64	-0.18	-0.18	-0.51	-0.51	-0.51	
-0.18	-0.18	-0.18	-0.18	-0.18	-0.64	-0.18	-0.18	
-0.64	-0.18	-0.51	-0.18	-0.10	-0.51	-0.64	-0.18	
-0.64	-0.18	-0.51	-0.51	-0.18	-0.64	-0.18	-0.18	
-0.10	-0.18	-0.51	-0.10	-0.51	-0.18	-0.18	-0.18	
-0.18	-0.64	-0.18	-0.51	-0.64	-0.10	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.51	-0.18	-0.18	-0.18	-0.18	
-0.51	-0.64	-0.10	-0.64	-0.64	-0.10	-0.18	0.25	
mean = 0.028005844866591245, map = 0.04891985759138101
CVaR policy
v	v	>	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
^	>	^	>	>	>	>	.	
cvar = , 0.03933574636018933, 0.03123687132772529, 0.02707684147680245, 0.027076841439002908, 0.027076841360651638
==========
iteration 55
==========
weights [-0.47728428 -0.33526691 -0.16233473 -0.18336267 -0.06880032  0.7714194 ]
expeced value MDP LP -0.3700508609062212
demonstration
[(32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.19162643  0.11597242  0.82803725 -0.24506988 -0.45135564  0.02007147]
w_map [-0.56466293 -0.4789594  -0.40131148 -0.396106   -0.19923951  0.30676771] loglik -1.960437430170714e-07
accepted/total = 1804/3000 = 0.6013333333333334
-------
true weights [-0.47728428 -0.33526691 -0.16233473 -0.18336267 -0.06880032  0.7714194 ]
features
4 	3 	0 	4 	0 	3 	2 	3 	
2 	4 	2 	4 	4 	2 	2 	1 	
4 	4 	3 	2 	3 	4 	1 	0 	
4 	4 	3 	4 	3 	2 	0 	4 	
4 	2 	4 	2 	0 	2 	0 	1 	
0 	4 	2 	0 	4 	3 	4 	3 	
1 	4 	4 	1 	3 	4 	0 	2 	
2 	1 	3 	1 	3 	3 	3 	5 	
optimal policy
v	v	>	v	v	v	v	<	
>	>	>	>	>	v	<	v	
v	v	>	v	>	v	<	v	
v	v	>	>	>	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-0.96	-0.92	-1.06	-0.59	-0.94	-0.58	-0.72	-0.89	
-0.90	-0.75	-0.69	-0.53	-0.47	-0.40	-0.56	-0.80	
-0.75	-0.69	-0.76	-0.58	-0.42	-0.24	-0.57	-0.47	
-0.69	-0.62	-0.60	-0.42	-0.36	-0.17	-0.47	0.00	
-0.62	-0.56	-0.50	-0.55	-0.40	-0.01	-0.14	0.07	
-0.88	-0.40	-0.43	-0.40	0.08	0.15	0.34	0.41	
-0.67	-0.34	-0.27	-0.20	0.13	0.32	0.12	0.60	
-0.80	-0.65	-0.32	-0.13	0.20	0.39	0.58	0.77	
map_weights [-0.56466293 -0.4789594  -0.40131148 -0.396106   -0.19923951  0.30676771]
MAP reward
-0.20	-0.40	-0.56	-0.20	-0.56	-0.40	-0.40	-0.40	
-0.40	-0.20	-0.40	-0.20	-0.20	-0.40	-0.40	-0.48	
-0.20	-0.20	-0.40	-0.40	-0.40	-0.20	-0.48	-0.56	
-0.20	-0.20	-0.40	-0.20	-0.40	-0.40	-0.56	-0.20	
-0.20	-0.40	-0.20	-0.40	-0.56	-0.40	-0.56	-0.48	
-0.56	-0.20	-0.40	-0.56	-0.20	-0.40	-0.20	-0.40	
-0.48	-0.20	-0.20	-0.48	-0.40	-0.20	-0.56	-0.40	
-0.40	-0.48	-0.40	-0.48	-0.40	-0.40	-0.40	0.31	
Map policy
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7002572938259173
mean w [-0.65261002 -0.34070905 -0.40513307 -0.3079639  -0.1178993   0.14954823]
Mean policy from posterior
>	v	>	v	v	v	>	v	
v	v	<	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.12	-0.31	-0.65	-0.12	-0.65	-0.31	-0.41	-0.31	
-0.41	-0.12	-0.41	-0.12	-0.12	-0.41	-0.41	-0.34	
-0.12	-0.12	-0.31	-0.41	-0.31	-0.12	-0.34	-0.65	
-0.12	-0.12	-0.31	-0.12	-0.31	-0.41	-0.65	-0.12	
-0.12	-0.41	-0.12	-0.41	-0.65	-0.41	-0.65	-0.34	
-0.65	-0.12	-0.41	-0.65	-0.12	-0.31	-0.12	-0.31	
-0.34	-0.12	-0.12	-0.34	-0.31	-0.12	-0.65	-0.41	
-0.41	-0.34	-0.31	-0.34	-0.31	-0.31	-0.31	0.15	
mean = 0.04209183552434742, map = 0.03931657092257951
CVaR policy
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	>	v	
v	v	<	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	>	v	
v	v	<	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	>	v	
v	v	<	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.06135330360516339, 0.03931657115203535, 0.0420918358701049, 0.04209183692415014, 0.04209183548576523
==========
iteration 56
==========
weights [-0.44050408 -0.11585265 -0.4575773  -0.53940492 -0.49653111  0.21367386]
expeced value MDP LP -1.8480005185521684
demonstration
[(32, 1), (33, 1), (34, 3), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.3077692   0.63745125 -0.21585505 -0.45274389 -0.4903187   0.0833736 ]
w_map [-0.46796306 -0.16651027 -0.35179713 -0.6150049  -0.43663322  0.24626021] loglik -0.6931562544771488
accepted/total = 1208/3000 = 0.4026666666666667
-------
true weights [-0.44050408 -0.11585265 -0.4575773  -0.53940492 -0.49653111  0.21367386]
features
2 	0 	2 	4 	2 	1 	3 	0 	
3 	4 	3 	0 	0 	2 	0 	4 	
3 	1 	0 	1 	4 	3 	1 	3 	
2 	1 	2 	4 	2 	2 	1 	1 	
4 	3 	1 	3 	1 	0 	3 	2 	
0 	4 	4 	1 	0 	0 	3 	1 	
3 	4 	1 	0 	3 	0 	1 	0 	
3 	2 	4 	3 	3 	1 	1 	5 	
optimal policy
>	v	>	>	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.93	-3.51	-3.42	-2.99	-2.52	-2.08	-2.07	-2.34	
-3.61	-3.10	-3.15	-2.64	-2.41	-1.99	-1.55	-1.92	
-3.14	-2.63	-2.64	-2.22	-2.13	-1.65	-1.12	-1.43	
-2.97	-2.54	-2.45	-2.35	-1.87	-1.46	-1.01	-0.90	
-3.00	-2.53	-2.01	-1.96	-1.43	-1.33	-1.09	-0.80	
-2.81	-2.39	-1.91	-1.43	-1.33	-0.90	-0.56	-0.34	
-2.53	-2.01	-1.53	-1.43	-1.00	-0.46	-0.02	-0.23	
-2.54	-2.02	-1.58	-1.09	-0.56	-0.02	0.10	0.21	
map_weights [-0.46796306 -0.16651027 -0.35179713 -0.6150049  -0.43663322  0.24626021]
MAP reward
-0.35	-0.47	-0.35	-0.44	-0.35	-0.17	-0.62	-0.47	
-0.62	-0.44	-0.62	-0.47	-0.47	-0.35	-0.47	-0.44	
-0.62	-0.17	-0.47	-0.17	-0.44	-0.62	-0.17	-0.62	
-0.35	-0.17	-0.35	-0.44	-0.35	-0.35	-0.17	-0.17	
-0.44	-0.62	-0.17	-0.62	-0.17	-0.47	-0.62	-0.35	
-0.47	-0.44	-0.44	-0.17	-0.47	-0.47	-0.62	-0.17	
-0.62	-0.44	-0.17	-0.47	-0.62	-0.47	-0.17	-0.47	
-0.62	-0.35	-0.44	-0.62	-0.62	-0.17	-0.17	0.25	
Map policy
>	v	>	>	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9201758202247046
mean w [-0.30143617 -0.11742675 -0.51332911 -0.48716961 -0.39233673 -0.18322162]
Mean policy from posterior
>	v	>	v	>	>	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	>	>	v	v	
>	v	v	v	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.51	-0.30	-0.51	-0.39	-0.51	-0.12	-0.49	-0.30	
-0.49	-0.39	-0.49	-0.30	-0.30	-0.51	-0.30	-0.39	
-0.49	-0.12	-0.30	-0.12	-0.39	-0.49	-0.12	-0.49	
-0.51	-0.12	-0.51	-0.39	-0.51	-0.51	-0.12	-0.12	
-0.39	-0.49	-0.12	-0.49	-0.12	-0.30	-0.49	-0.51	
-0.30	-0.39	-0.39	-0.12	-0.30	-0.30	-0.49	-0.12	
-0.49	-0.39	-0.12	-0.30	-0.49	-0.30	-0.12	-0.30	
-0.49	-0.51	-0.39	-0.49	-0.49	-0.12	-0.12	-0.18	
mean = 0.05038757541209904, map = 0.07038552766835626
CVaR policy
>	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	>	v	v	
>	v	v	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	v	v	v	v	>	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	>	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	>	>	v	v	
>	v	v	v	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.0008445351083359309, 0.0036037058050675252, 0.003603705026060444, 0.017313769682539393, 0.05038758048394443
==========
iteration 57
==========
weights [-0.2725645  -0.34215258 -0.8763279  -0.08535261 -0.12101183  0.13696965]
expeced value MDP LP -1.312996948050642
demonstration
[(32, 3), (40, 3), (48, 1), (49, 3), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.36669671  0.46431885 -0.12850432 -0.57389622  0.52430211  0.17081743]
w_map [-0.44016934 -0.43850417 -0.70058822 -0.2163643  -0.21199194  0.17716405] loglik -1.237091851180594e-08
accepted/total = 1763/3000 = 0.5876666666666667
-------
true weights [-0.2725645  -0.34215258 -0.8763279  -0.08535261 -0.12101183  0.13696965]
features
3 	1 	0 	2 	3 	2 	0 	1 	
4 	2 	1 	1 	4 	0 	4 	4 	
1 	0 	3 	1 	4 	1 	0 	4 	
0 	3 	2 	3 	2 	1 	1 	3 	
3 	0 	0 	2 	0 	1 	4 	4 	
0 	2 	0 	1 	0 	1 	2 	1 	
0 	3 	1 	3 	4 	0 	4 	0 	
0 	3 	3 	3 	1 	3 	1 	5 	
optimal policy
v	<	v	>	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	>	>	^	>	>	v	
v	v	v	^	v	>	>	v	
v	<	v	v	v	>	>	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.28	-2.60	-2.29	-2.32	-1.46	-2.14	-1.28	-1.23	
-2.22	-2.89	-2.04	-1.71	-1.38	-1.28	-1.01	-0.90	
-2.12	-2.11	-1.89	-1.82	-1.49	-1.38	-1.05	-0.79	
-1.79	-1.86	-2.44	-1.89	-2.04	-1.34	-1.01	-0.67	
-1.54	-1.79	-1.58	-1.92	-1.17	-1.04	-0.71	-0.59	
-1.46	-1.81	-1.32	-1.06	-0.91	-0.86	-1.13	-0.48	
-1.20	-0.94	-1.06	-0.72	-0.64	-0.53	-0.26	-0.14	
-1.13	-0.86	-0.79	-0.71	-0.63	-0.29	-0.21	0.14	
map_weights [-0.44016934 -0.43850417 -0.70058822 -0.2163643  -0.21199194  0.17716405]
MAP reward
-0.22	-0.44	-0.44	-0.70	-0.22	-0.70	-0.44	-0.44	
-0.21	-0.70	-0.44	-0.44	-0.21	-0.44	-0.21	-0.21	
-0.44	-0.44	-0.22	-0.44	-0.21	-0.44	-0.44	-0.21	
-0.44	-0.22	-0.70	-0.22	-0.70	-0.44	-0.44	-0.22	
-0.22	-0.44	-0.44	-0.70	-0.44	-0.44	-0.21	-0.21	
-0.44	-0.70	-0.44	-0.44	-0.44	-0.44	-0.70	-0.44	
-0.44	-0.22	-0.44	-0.22	-0.21	-0.44	-0.21	-0.44	
-0.44	-0.22	-0.22	-0.22	-0.44	-0.22	-0.44	0.18	
Map policy
v	>	v	>	v	>	v	v	
v	>	>	>	>	>	>	v	
v	>	>	>	>	>	>	v	
v	v	v	v	>	>	v	v	
v	v	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5254519668574607
mean w [-0.30788392 -0.28841594 -0.57038682 -0.13049508 -0.48252668  0.21603777]
Mean policy from posterior
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.13	-0.29	-0.31	-0.57	-0.13	-0.57	-0.31	-0.29	
-0.48	-0.57	-0.29	-0.29	-0.48	-0.31	-0.48	-0.48	
-0.29	-0.31	-0.13	-0.29	-0.48	-0.29	-0.31	-0.48	
-0.31	-0.13	-0.57	-0.13	-0.57	-0.29	-0.29	-0.13	
-0.13	-0.31	-0.31	-0.57	-0.31	-0.29	-0.48	-0.48	
-0.31	-0.57	-0.31	-0.29	-0.31	-0.29	-0.57	-0.29	
-0.31	-0.13	-0.29	-0.13	-0.48	-0.31	-0.48	-0.31	
-0.31	-0.13	-0.13	-0.13	-0.29	-0.13	-0.29	0.22	
mean = 0.2246192111578753, map = 0.03147077843974899
CVaR policy
v	v	v	>	v	>	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	>	>	v	
v	v	v	v	v	>	>	v	
v	>	v	>	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.19853902936083356, 0.21021423077043888, 0.21272977835339835, 0.22461920997896723, 0.22461920986120543
==========
iteration 58
==========
weights [-0.69611314 -0.19058184 -0.24465214 -0.32533667 -0.22025134  0.51468028]
expeced value MDP LP -1.3038652479637114
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.08944529  0.49347353  0.1855475   0.45127536  0.55548513  0.4492687 ]
w_map [-0.66012413 -0.48744245 -0.33663422 -0.36821435 -0.27871655  0.00697587] loglik -1.999467258428922e-11
accepted/total = 1959/3000 = 0.653
-------
true weights [-0.69611314 -0.19058184 -0.24465214 -0.32533667 -0.22025134  0.51468028]
features
4 	3 	0 	3 	1 	2 	1 	2 	
2 	1 	3 	4 	3 	0 	0 	0 	
4 	2 	1 	4 	1 	0 	1 	4 	
3 	3 	2 	2 	0 	1 	4 	3 	
4 	0 	1 	1 	4 	3 	2 	1 	
3 	3 	2 	4 	3 	4 	4 	2 	
3 	2 	4 	2 	0 	3 	0 	2 	
2 	0 	4 	0 	2 	1 	3 	5 	
optimal policy
v	v	v	v	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.54	-2.42	-2.70	-2.08	-2.08	-1.91	-1.68	-1.63	
-2.34	-2.12	-2.03	-1.78	-1.99	-2.19	-1.50	-1.40	
-2.15	-1.94	-1.72	-1.57	-1.68	-1.50	-0.82	-0.71	
-2.16	-1.85	-1.54	-1.36	-1.50	-0.82	-0.63	-0.50	
-2.02	-1.99	-1.31	-1.13	-0.95	-0.74	-0.42	-0.17	
-1.81	-1.50	-1.19	-0.96	-0.74	-0.42	-0.20	0.02	
-1.91	-1.60	-1.37	-1.18	-0.95	-0.33	-0.43	0.26	
-2.07	-1.84	-1.16	-0.95	-0.25	-0.01	0.18	0.51	
map_weights [-0.66012413 -0.48744245 -0.33663422 -0.36821435 -0.27871655  0.00697587]
MAP reward
-0.28	-0.37	-0.66	-0.37	-0.49	-0.34	-0.49	-0.34	
-0.34	-0.49	-0.37	-0.28	-0.37	-0.66	-0.66	-0.66	
-0.28	-0.34	-0.49	-0.28	-0.49	-0.66	-0.49	-0.28	
-0.37	-0.37	-0.34	-0.34	-0.66	-0.49	-0.28	-0.37	
-0.28	-0.66	-0.49	-0.49	-0.28	-0.37	-0.34	-0.49	
-0.37	-0.37	-0.34	-0.28	-0.37	-0.28	-0.28	-0.34	
-0.37	-0.34	-0.28	-0.34	-0.66	-0.37	-0.66	-0.34	
-0.34	-0.66	-0.28	-0.66	-0.34	-0.49	-0.37	0.01	
Map policy
v	v	>	v	v	>	v	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4615770068129115
mean w [-0.59493114 -0.46731024 -0.15095669 -0.26618156 -0.17247148  0.1033678 ]
Mean policy from posterior
v	<	v	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	v	^	>	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.17	-0.27	-0.59	-0.27	-0.47	-0.15	-0.47	-0.15	
-0.15	-0.47	-0.27	-0.17	-0.27	-0.59	-0.59	-0.59	
-0.17	-0.15	-0.47	-0.17	-0.47	-0.59	-0.47	-0.17	
-0.27	-0.27	-0.15	-0.15	-0.59	-0.47	-0.17	-0.27	
-0.17	-0.59	-0.47	-0.47	-0.17	-0.27	-0.15	-0.47	
-0.27	-0.27	-0.15	-0.17	-0.27	-0.17	-0.17	-0.15	
-0.27	-0.15	-0.17	-0.15	-0.59	-0.27	-0.59	-0.15	
-0.15	-0.59	-0.17	-0.59	-0.15	-0.47	-0.27	0.10	
mean = 0.0913014555674343, map = 0.050549227391317286
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	v	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	<	>	v	v	
v	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	^	v	^	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	<	v	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	v	^	>	v	
^	^	^	>	>	>	>	.	
cvar = , 0.06029232003024765, 0.05506439012638076, 0.06128421809605977, 0.08196452414187805, 0.0913014555890832
==========
iteration 59
==========
weights [-0.02402604 -0.14085619 -0.41371611 -0.05490963 -0.51419023  0.73553695]
expeced value MDP LP -0.3784134816102686
demonstration
[(32, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 3), (37, 3), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.34559725  0.01969014 -0.13067472  0.15071442 -0.16318588 -0.90208342]
w_map [-0.16450157 -0.11705767 -0.43407109 -0.31969299 -0.81274144 -0.08981518] loglik -9.367267495008491e-07
accepted/total = 1412/3000 = 0.4706666666666667
-------
true weights [-0.02402604 -0.14085619 -0.41371611 -0.05490963 -0.51419023  0.73553695]
features
4 	1 	2 	3 	1 	4 	4 	0 	
2 	1 	3 	1 	0 	3 	2 	3 	
1 	0 	0 	2 	4 	2 	1 	4 	
0 	0 	1 	3 	1 	3 	4 	0 	
3 	2 	3 	2 	4 	2 	4 	3 	
1 	3 	2 	4 	2 	0 	1 	2 	
1 	0 	1 	4 	2 	2 	1 	3 	
3 	1 	1 	4 	4 	0 	0 	5 	
optimal policy
>	v	v	v	v	v	>	v	
v	v	v	<	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
^	<	^	>	>	>	v	v	
^	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.24	-0.73	-0.90	-0.68	-0.71	-1.06	-0.93	-0.42	
-1.00	-0.60	-0.49	-0.63	-0.57	-0.55	-0.81	-0.40	
-0.60	-0.46	-0.44	-0.69	-0.74	-0.50	-0.48	-0.35	
-0.46	-0.44	-0.42	-0.28	-0.23	-0.09	-0.35	0.17	
-0.51	-0.85	-0.47	-0.69	-0.55	-0.04	-0.11	0.20	
-0.65	-0.69	-0.88	-0.55	-0.04	0.38	0.41	0.25	
-0.78	-0.65	-0.64	-0.68	-0.16	0.25	0.56	0.67	
-0.69	-0.64	-0.50	-0.36	0.15	0.67	0.70	0.74	
map_weights [-0.16450157 -0.11705767 -0.43407109 -0.31969299 -0.81274144 -0.08981518]
MAP reward
-0.81	-0.12	-0.43	-0.32	-0.12	-0.81	-0.81	-0.16	
-0.43	-0.12	-0.32	-0.12	-0.16	-0.32	-0.43	-0.32	
-0.12	-0.16	-0.16	-0.43	-0.81	-0.43	-0.12	-0.81	
-0.16	-0.16	-0.12	-0.32	-0.12	-0.32	-0.81	-0.16	
-0.32	-0.43	-0.32	-0.43	-0.81	-0.43	-0.81	-0.32	
-0.12	-0.32	-0.43	-0.81	-0.43	-0.16	-0.12	-0.43	
-0.12	-0.16	-0.12	-0.81	-0.43	-0.43	-0.12	-0.32	
-0.32	-0.12	-0.12	-0.81	-0.81	-0.16	-0.16	-0.09	
Map policy
>	v	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	v	v	
^	^	^	^	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.875151892651639
mean w [-0.10990565 -0.13753132 -0.42682773 -0.2220915  -0.69075147 -0.34414961]
Mean policy from posterior
>	v	v	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
^	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.69	-0.14	-0.43	-0.22	-0.14	-0.69	-0.69	-0.11	
-0.43	-0.14	-0.22	-0.14	-0.11	-0.22	-0.43	-0.22	
-0.14	-0.11	-0.11	-0.43	-0.69	-0.43	-0.14	-0.69	
-0.11	-0.11	-0.14	-0.22	-0.14	-0.22	-0.69	-0.11	
-0.22	-0.43	-0.22	-0.43	-0.69	-0.43	-0.69	-0.22	
-0.14	-0.22	-0.43	-0.69	-0.43	-0.11	-0.14	-0.43	
-0.14	-0.11	-0.14	-0.69	-0.43	-0.43	-0.14	-0.22	
-0.22	-0.14	-0.14	-0.69	-0.69	-0.11	-0.11	-0.34	
mean = 0.010073229654856464, map = 0.05949895143640782
CVaR policy
>	v	v	>	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	<	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
^	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
^	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.013673933615180822, 0.009548082768450206, 0.01437186177503591, 0.010073232974567525, 0.010073229770846792
==========
iteration 60
==========
weights [-0.24478746 -0.68042635 -0.21964141 -0.62989695 -0.14757166  0.10153406]
expeced value MDP LP -2.078471539219937
demonstration
[(32, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 3), (37, 3), (45, 3), (53, 1), (54, 3), (62, 1), (63, None)]
[ 0.2394752   0.63077599 -0.34547186  0.5881734  -0.03160102  0.28013552]
w_map [-0.35670007 -0.65334232 -0.41322396 -0.44640194 -0.21647621 -0.17034723] loglik -0.6931468086802681
accepted/total = 1891/3000 = 0.6303333333333333
-------
true weights [-0.24478746 -0.68042635 -0.21964141 -0.62989695 -0.14757166  0.10153406]
features
1 	4 	3 	3 	0 	3 	3 	1 	
3 	1 	2 	0 	0 	2 	2 	1 	
1 	1 	0 	1 	1 	1 	4 	3 	
4 	4 	2 	2 	4 	4 	3 	4 	
0 	4 	1 	3 	3 	0 	0 	3 	
0 	1 	1 	0 	0 	0 	1 	1 	
4 	1 	1 	3 	1 	3 	4 	3 	
1 	0 	1 	3 	2 	1 	4 	5 	
optimal policy
>	>	v	v	v	v	v	<	
v	>	v	<	v	>	v	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	^	v	v	v	v	<	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.76	-3.11	-3.00	-3.21	-2.67	-2.87	-2.67	-3.32	
-3.48	-3.05	-2.39	-2.61	-2.45	-2.26	-2.06	-2.72	
-2.88	-2.75	-2.19	-2.43	-2.23	-2.09	-1.86	-2.47	
-2.22	-2.09	-1.97	-1.76	-1.56	-1.43	-1.73	-1.86	
-2.44	-2.22	-2.63	-2.14	-1.91	-1.29	-1.11	-1.73	
-2.66	-2.85	-2.19	-1.52	-1.29	-1.06	-0.87	-1.20	
-2.79	-3.10	-2.77	-2.11	-1.49	-0.82	-0.19	-0.53	
-3.10	-2.45	-2.22	-1.56	-0.94	-0.73	-0.05	0.10	
map_weights [-0.35670007 -0.65334232 -0.41322396 -0.44640194 -0.21647621 -0.17034723]
MAP reward
-0.65	-0.22	-0.45	-0.45	-0.36	-0.45	-0.45	-0.65	
-0.45	-0.65	-0.41	-0.36	-0.36	-0.41	-0.41	-0.65	
-0.65	-0.65	-0.36	-0.65	-0.65	-0.65	-0.22	-0.45	
-0.22	-0.22	-0.41	-0.41	-0.22	-0.22	-0.45	-0.22	
-0.36	-0.22	-0.65	-0.45	-0.45	-0.36	-0.36	-0.45	
-0.36	-0.65	-0.65	-0.36	-0.36	-0.36	-0.65	-0.65	
-0.22	-0.65	-0.65	-0.45	-0.65	-0.45	-0.22	-0.45	
-0.65	-0.36	-0.65	-0.45	-0.41	-0.65	-0.22	-0.17	
Map policy
>	>	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.869960019705429
mean w [-0.26286911 -0.70076654 -0.24191416 -0.38082457 -0.14044381  0.08125921]
Mean policy from posterior
>	>	v	v	v	v	v	<	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.70	-0.14	-0.38	-0.38	-0.26	-0.38	-0.38	-0.70	
-0.38	-0.70	-0.24	-0.26	-0.26	-0.24	-0.24	-0.70	
-0.70	-0.70	-0.26	-0.70	-0.70	-0.70	-0.14	-0.38	
-0.14	-0.14	-0.24	-0.24	-0.14	-0.14	-0.38	-0.14	
-0.26	-0.14	-0.70	-0.38	-0.38	-0.26	-0.26	-0.38	
-0.26	-0.70	-0.70	-0.26	-0.26	-0.26	-0.70	-0.70	
-0.14	-0.70	-0.70	-0.38	-0.70	-0.38	-0.14	-0.38	
-0.70	-0.26	-0.70	-0.38	-0.24	-0.70	-0.14	0.08	
mean = 0.016023150408313036, map = 0.049153255845975075
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	<	
v	>	v	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
v	>	v	v	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.052321366579370476, 0.027441119686834714, 0.0170170799273639, 0.016023150347170834, 0.01602315040087321
==========
iteration 61
==========
weights [-0.48174809 -0.04079798 -0.24348761 -0.71022041 -0.00888151  0.44997352]
expeced value MDP LP -0.6802533721258843
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.22883779  0.10193011 -0.14408504  0.0752998  -0.05078882  0.95301283]
w_map [-0.41904591 -0.19215381 -0.53543156 -0.60913404 -0.33731627  0.1263485 ] loglik -1.3862943175783151
accepted/total = 1817/3000 = 0.6056666666666667
-------
true weights [-0.48174809 -0.04079798 -0.24348761 -0.71022041 -0.00888151  0.44997352]
features
0 	1 	4 	0 	1 	3 	4 	2 	
1 	2 	4 	3 	2 	1 	0 	4 	
0 	1 	3 	3 	3 	2 	2 	3 	
0 	3 	1 	2 	0 	4 	4 	3 	
2 	0 	4 	3 	3 	2 	0 	1 	
0 	4 	2 	0 	1 	3 	1 	2 	
4 	4 	4 	0 	0 	2 	4 	0 	
2 	2 	3 	1 	0 	4 	4 	5 	
optimal policy
>	>	v	>	v	v	v	<	
>	>	^	>	>	v	v	<	
>	^	v	v	>	v	v	<	
v	>	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	<	
>	>	>	v	>	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.39	-0.92	-0.89	-1.15	-0.68	-1.11	-0.84	-1.07	
-1.15	-1.12	-0.89	-1.35	-0.64	-0.40	-0.84	-0.84	
-1.62	-1.15	-1.56	-1.54	-1.07	-0.37	-0.36	-1.06	
-1.77	-1.56	-0.86	-0.84	-0.60	-0.12	-0.12	-0.62	
-1.30	-1.07	-0.82	-1.53	-1.05	-0.35	-0.11	0.09	
-1.07	-0.59	-0.82	-0.82	-0.35	-0.34	0.38	0.13	
-0.59	-0.59	-0.59	-0.58	-0.31	0.18	0.42	-0.04	
-0.83	-0.83	-0.81	-0.10	-0.06	0.42	0.44	0.45	
map_weights [-0.41904591 -0.19215381 -0.53543156 -0.60913404 -0.33731627  0.1263485 ]
MAP reward
-0.42	-0.19	-0.34	-0.42	-0.19	-0.61	-0.34	-0.54	
-0.19	-0.54	-0.34	-0.61	-0.54	-0.19	-0.42	-0.34	
-0.42	-0.19	-0.61	-0.61	-0.61	-0.54	-0.54	-0.61	
-0.42	-0.61	-0.19	-0.54	-0.42	-0.34	-0.34	-0.61	
-0.54	-0.42	-0.34	-0.61	-0.61	-0.54	-0.42	-0.19	
-0.42	-0.34	-0.54	-0.42	-0.19	-0.61	-0.19	-0.54	
-0.34	-0.34	-0.34	-0.42	-0.42	-0.54	-0.34	-0.42	
-0.54	-0.54	-0.61	-0.19	-0.42	-0.34	-0.34	0.13	
Map policy
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
v	>	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.3763059124174635
mean w [-0.2734818  -0.12119382 -0.43177951 -0.63837272 -0.16432744  0.19089652]
Mean policy from posterior
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
v	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.27	-0.12	-0.16	-0.27	-0.12	-0.64	-0.16	-0.43	
-0.12	-0.43	-0.16	-0.64	-0.43	-0.12	-0.27	-0.16	
-0.27	-0.12	-0.64	-0.64	-0.64	-0.43	-0.43	-0.64	
-0.27	-0.64	-0.12	-0.43	-0.27	-0.16	-0.16	-0.64	
-0.43	-0.27	-0.16	-0.64	-0.64	-0.43	-0.27	-0.12	
-0.27	-0.16	-0.43	-0.27	-0.12	-0.64	-0.12	-0.43	
-0.16	-0.16	-0.16	-0.27	-0.27	-0.43	-0.16	-0.27	
-0.43	-0.43	-0.64	-0.12	-0.27	-0.16	-0.16	0.19	
mean = 0.11727619704539172, map = 0.13727624993642618
CVaR policy
v	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	<	
v	v	v	>	>	v	v	<	
>	v	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
v	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	<	
v	v	v	>	>	v	v	<	
v	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	<	
v	v	v	>	>	v	v	<	
v	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.1881271832852076, 0.13168556883470983, 0.1207166066761427, 0.11727619748438534, 0.11727619705756265
==========
iteration 62
==========
weights [-0.1408033  -0.10463389 -0.60361841 -0.16941572 -0.70980778  0.26896509]
expeced value MDP LP -1.2165179906560697
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.26108125 -0.59686512  0.24022779 -0.40010654 -0.08173841  0.59254772]
w_map [-0.34058848 -0.31447582 -0.59837453 -0.25776318 -0.60015894  0.02048803] loglik -0.6931471808134013
accepted/total = 2027/3000 = 0.6756666666666666
-------
true weights [-0.1408033  -0.10463389 -0.60361841 -0.16941572 -0.70980778  0.26896509]
features
0 	4 	4 	0 	3 	3 	0 	2 	
2 	1 	4 	3 	0 	4 	4 	1 	
1 	1 	4 	4 	4 	3 	1 	4 	
3 	2 	2 	1 	2 	4 	2 	2 	
1 	2 	1 	1 	1 	4 	2 	1 	
0 	0 	3 	3 	4 	2 	3 	1 	
0 	4 	2 	1 	0 	0 	4 	1 	
3 	0 	2 	4 	0 	3 	3 	5 	
optimal policy
v	v	>	v	v	<	>	v	
v	v	<	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	>	v	v	
v	>	>	v	<	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	>	v	>	v	
^	<	>	>	>	>	>	.	
optimal values
-2.10	-2.26	-2.49	-1.79	-1.95	-2.10	-2.16	-2.04	
-1.98	-1.57	-2.26	-1.67	-1.79	-2.19	-2.04	-1.45	
-1.39	-1.48	-2.11	-1.52	-2.11	-1.50	-1.34	-1.36	
-1.30	-1.89	-1.41	-0.81	-1.41	-1.95	-1.25	-0.65	
-1.14	-1.41	-0.81	-0.72	-0.81	-1.36	-0.65	-0.05	
-1.05	-0.91	-0.78	-0.62	-1.06	-0.72	-0.11	0.06	
-1.18	-1.62	-1.05	-0.45	-0.35	-0.21	-0.55	0.16	
-1.33	-1.46	-1.52	-0.92	-0.21	-0.07	0.10	0.27	
map_weights [-0.34058848 -0.31447582 -0.59837453 -0.25776318 -0.60015894  0.02048803]
MAP reward
-0.34	-0.60	-0.60	-0.34	-0.26	-0.26	-0.34	-0.60	
-0.60	-0.31	-0.60	-0.26	-0.34	-0.60	-0.60	-0.31	
-0.31	-0.31	-0.60	-0.60	-0.60	-0.26	-0.31	-0.60	
-0.26	-0.60	-0.60	-0.31	-0.60	-0.60	-0.60	-0.60	
-0.31	-0.60	-0.31	-0.31	-0.31	-0.60	-0.60	-0.31	
-0.34	-0.34	-0.26	-0.26	-0.60	-0.60	-0.26	-0.31	
-0.34	-0.60	-0.60	-0.31	-0.34	-0.34	-0.60	-0.31	
-0.26	-0.34	-0.60	-0.60	-0.34	-0.26	-0.26	0.02	
Map policy
v	v	>	v	>	v	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	>	>	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8503089621874818
mean w [-0.16031428 -0.26691789 -0.49135202 -0.17317315 -0.63689405 -0.10879235]
Mean policy from posterior
v	v	>	v	v	<	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	v	v	v	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.16	-0.64	-0.64	-0.16	-0.17	-0.17	-0.16	-0.49	
-0.49	-0.27	-0.64	-0.17	-0.16	-0.64	-0.64	-0.27	
-0.27	-0.27	-0.64	-0.64	-0.64	-0.17	-0.27	-0.64	
-0.17	-0.49	-0.49	-0.27	-0.49	-0.64	-0.49	-0.49	
-0.27	-0.49	-0.27	-0.27	-0.27	-0.64	-0.49	-0.27	
-0.16	-0.16	-0.17	-0.17	-0.64	-0.49	-0.17	-0.27	
-0.16	-0.64	-0.49	-0.27	-0.16	-0.16	-0.64	-0.27	
-0.17	-0.16	-0.49	-0.64	-0.16	-0.17	-0.17	-0.11	
mean = 0.04388679503756432, map = 0.07499207586957968
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
cvar = , 0.12515176796137384, 0.11357826708505048, 0.05746375974045259, 0.05027324457822613, 0.05027324450548187
==========
iteration 63
==========
weights [-0.53480199 -0.71108202 -0.26845986 -0.03036318 -0.15956608  0.33150451]
expeced value MDP LP -1.3665223606979628
demonstration
[(32, 1), (33, 1), (34, 1), (35, 3), (43, 1), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.27722393 -0.15666176  0.05226306 -0.12878509  0.04800167  0.9364736 ]
w_map [-0.59310207 -0.57739023 -0.28471131 -0.29921603 -0.30299732  0.22902472] loglik -1.1965539670200087e-11
accepted/total = 1989/3000 = 0.663
-------
true weights [-0.53480199 -0.71108202 -0.26845986 -0.03036318 -0.15956608  0.33150451]
features
1 	0 	4 	3 	2 	3 	2 	4 	
1 	1 	2 	4 	3 	0 	3 	0 	
2 	3 	0 	3 	0 	1 	3 	4 	
1 	2 	2 	3 	2 	1 	2 	4 	
4 	2 	2 	4 	1 	0 	3 	1 	
0 	0 	0 	2 	3 	4 	4 	3 	
1 	3 	2 	2 	1 	2 	1 	1 	
2 	3 	3 	0 	1 	0 	0 	5 	
optimal policy
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	<	
>	v	>	v	<	>	v	<	
>	>	>	v	<	>	v	<	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	^	^	v	v	
>	>	^	^	>	>	>	.	
optimal values
-2.71	-2.02	-1.50	-1.35	-1.43	-1.17	-1.16	-1.30	
-2.64	-2.29	-1.59	-1.34	-1.35	-1.42	-0.90	-1.42	
-1.95	-1.70	-1.71	-1.19	-1.71	-1.58	-0.87	-1.02	
-2.38	-1.68	-1.43	-1.17	-1.43	-1.55	-0.85	-1.00	
-1.81	-1.66	-1.41	-1.15	-1.45	-1.12	-0.59	-1.12	
-2.32	-2.05	-1.53	-1.00	-0.74	-0.72	-0.56	-0.41	
-2.23	-1.53	-1.52	-1.26	-1.45	-0.98	-0.92	-0.38	
-1.80	-1.55	-1.53	-1.78	-1.44	-0.74	-0.21	0.33	
map_weights [-0.59310207 -0.57739023 -0.28471131 -0.29921603 -0.30299732  0.22902472]
MAP reward
-0.58	-0.59	-0.30	-0.30	-0.28	-0.30	-0.28	-0.30	
-0.58	-0.58	-0.28	-0.30	-0.30	-0.59	-0.30	-0.59	
-0.28	-0.30	-0.59	-0.30	-0.59	-0.58	-0.30	-0.30	
-0.58	-0.28	-0.28	-0.30	-0.28	-0.58	-0.28	-0.30	
-0.30	-0.28	-0.28	-0.30	-0.58	-0.59	-0.30	-0.58	
-0.59	-0.59	-0.59	-0.28	-0.30	-0.30	-0.30	-0.30	
-0.58	-0.30	-0.28	-0.28	-0.58	-0.28	-0.58	-0.58	
-0.28	-0.30	-0.30	-0.59	-0.58	-0.59	-0.59	0.23	
Map policy
v	>	>	>	>	>	v	<	
v	v	>	v	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7686505274804811
mean w [-0.6044585  -0.52206259 -0.27706108 -0.17730604 -0.21188515  0.23812607]
Mean policy from posterior
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	v	
>	v	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	^	>	>	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.52	-0.60	-0.21	-0.18	-0.28	-0.18	-0.28	-0.21	
-0.52	-0.52	-0.28	-0.21	-0.18	-0.60	-0.18	-0.60	
-0.28	-0.18	-0.60	-0.18	-0.60	-0.52	-0.18	-0.21	
-0.52	-0.28	-0.28	-0.18	-0.28	-0.52	-0.28	-0.21	
-0.21	-0.28	-0.28	-0.21	-0.52	-0.60	-0.18	-0.52	
-0.60	-0.60	-0.60	-0.28	-0.18	-0.21	-0.21	-0.18	
-0.52	-0.18	-0.28	-0.28	-0.52	-0.28	-0.52	-0.52	
-0.28	-0.18	-0.18	-0.60	-0.52	-0.60	-0.60	0.24	
mean = 0.03961350672982, map = 0.19106758563668613
CVaR policy
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
>	v	>	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
>	v	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	<	
v	>	>	v	>	>	v	v	
>	v	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	<	
v	>	>	v	>	>	v	v	
>	v	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	<	
v	>	>	v	>	>	v	v	
>	v	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	^	>	>	v	
>	>	^	>	>	>	>	.	
cvar = , 0.1558799679155456, 0.08279801024645073, 0.0707745432354765, 0.05027889138025787, 0.040909276061177335
==========
iteration 64
==========
weights [-0.48701902 -0.16726154 -0.51772544 -0.39916227 -0.45686498  0.31422967]
expeced value MDP LP -2.404170670378075
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.70884852  0.44622357 -0.03986817 -0.30640626  0.34087976 -0.29452508]
w_map [-0.60983606 -0.25436793 -0.52600821 -0.34240072 -0.36043824  0.19889273] loglik -1.3862943632217082
accepted/total = 1767/3000 = 0.589
-------
true weights [-0.48701902 -0.16726154 -0.51772544 -0.39916227 -0.45686498  0.31422967]
features
1 	3 	2 	1 	2 	0 	0 	3 	
1 	2 	2 	4 	2 	2 	2 	3 	
3 	1 	0 	4 	4 	0 	0 	0 	
4 	1 	4 	3 	1 	2 	2 	2 	
1 	1 	0 	0 	0 	4 	3 	2 	
1 	3 	4 	1 	0 	4 	4 	2 	
2 	3 	2 	4 	1 	2 	4 	0 	
3 	0 	2 	2 	2 	3 	4 	5 	
optimal policy
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.68	-3.89	-4.12	-3.64	-3.71	-3.79	-3.36	-2.93	
-3.54	-3.53	-3.98	-3.51	-3.23	-3.33	-2.91	-2.56	
-3.41	-3.04	-3.50	-3.08	-2.74	-2.85	-2.41	-2.18	
-3.33	-2.90	-3.08	-2.65	-2.30	-2.38	-1.94	-1.71	
-2.90	-2.76	-2.71	-2.28	-2.16	-1.88	-1.44	-1.20	
-2.76	-2.62	-2.25	-1.81	-1.69	-1.50	-1.05	-0.69	
-3.03	-2.54	-2.16	-1.66	-1.21	-1.06	-0.60	-0.18	
-2.91	-2.53	-2.07	-1.56	-1.06	-0.54	-0.15	0.31	
map_weights [-0.60983606 -0.25436793 -0.52600821 -0.34240072 -0.36043824  0.19889273]
MAP reward
-0.25	-0.34	-0.53	-0.25	-0.53	-0.61	-0.61	-0.34	
-0.25	-0.53	-0.53	-0.36	-0.53	-0.53	-0.53	-0.34	
-0.34	-0.25	-0.61	-0.36	-0.36	-0.61	-0.61	-0.61	
-0.36	-0.25	-0.36	-0.34	-0.25	-0.53	-0.53	-0.53	
-0.25	-0.25	-0.61	-0.61	-0.61	-0.36	-0.34	-0.53	
-0.25	-0.34	-0.36	-0.25	-0.61	-0.36	-0.36	-0.53	
-0.53	-0.34	-0.53	-0.36	-0.25	-0.53	-0.36	-0.61	
-0.34	-0.61	-0.53	-0.53	-0.53	-0.34	-0.36	0.20	
Map policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0759541861672357
mean w [-0.64162996 -0.16452234 -0.52443605 -0.18088998 -0.31225934  0.0878758 ]
Mean policy from posterior
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	<	
>	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	<	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.16	-0.18	-0.52	-0.16	-0.52	-0.64	-0.64	-0.18	
-0.16	-0.52	-0.52	-0.31	-0.52	-0.52	-0.52	-0.18	
-0.18	-0.16	-0.64	-0.31	-0.31	-0.64	-0.64	-0.64	
-0.31	-0.16	-0.31	-0.18	-0.16	-0.52	-0.52	-0.52	
-0.16	-0.16	-0.64	-0.64	-0.64	-0.31	-0.18	-0.52	
-0.16	-0.18	-0.31	-0.16	-0.64	-0.31	-0.31	-0.52	
-0.52	-0.18	-0.52	-0.31	-0.16	-0.52	-0.31	-0.64	
-0.18	-0.64	-0.52	-0.52	-0.52	-0.18	-0.31	0.09	
mean = 0.10999536734171622, map = 0.0520989017629212
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	<	
>	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	<	
>	>	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.014548147872088357, 0.047676293922711555, 0.04767629794557182, 0.04994105051019382, 0.10389477826272975
==========
iteration 65
==========
weights [-0.50408971 -0.18216383 -0.26848375 -0.1616642  -0.43175195  0.65427924]
expeced value MDP LP -1.0195725831184053
demonstration
[(32, 1), (33, 1), (34, 3), (42, 3), (50, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.65135246 -0.16956531  0.24752782  0.55873193  0.0614933  -0.41201305]
w_map [-0.55860932 -0.44449086 -0.37254402 -0.20038794 -0.54569669  0.11685139] loglik -0.693147300043897
accepted/total = 1440/3000 = 0.48
-------
true weights [-0.50408971 -0.18216383 -0.26848375 -0.1616642  -0.43175195  0.65427924]
features
0 	4 	3 	2 	3 	3 	4 	4 	
2 	3 	4 	4 	3 	0 	0 	4 	
4 	3 	0 	3 	4 	4 	4 	0 	
4 	2 	3 	1 	2 	3 	2 	0 	
3 	2 	1 	2 	4 	1 	3 	3 	
0 	1 	3 	1 	4 	4 	2 	2 	
2 	4 	2 	0 	0 	3 	1 	2 	
4 	2 	2 	1 	3 	3 	4 	5 	
optimal policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.43	-2.11	-1.93	-1.79	-1.54	-1.62	-1.82	-1.90	
-1.95	-1.69	-1.98	-1.56	-1.39	-1.48	-1.41	-1.48	
-1.96	-1.55	-1.64	-1.14	-1.24	-0.98	-0.91	-1.06	
-1.82	-1.40	-1.14	-0.99	-0.82	-0.55	-0.48	-0.56	
-1.55	-1.40	-1.15	-1.08	-0.82	-0.40	-0.22	-0.06	
-1.64	-1.15	-0.97	-0.97	-0.83	-0.40	-0.08	0.11	
-1.50	-1.24	-0.82	-0.79	-0.47	0.03	0.19	0.38	
-1.24	-0.82	-0.56	-0.29	-0.11	0.05	0.22	0.65	
map_weights [-0.55860932 -0.44449086 -0.37254402 -0.20038794 -0.54569669  0.11685139]
MAP reward
-0.56	-0.55	-0.20	-0.37	-0.20	-0.20	-0.55	-0.55	
-0.37	-0.20	-0.55	-0.55	-0.20	-0.56	-0.56	-0.55	
-0.55	-0.20	-0.56	-0.20	-0.55	-0.55	-0.55	-0.56	
-0.55	-0.37	-0.20	-0.44	-0.37	-0.20	-0.37	-0.56	
-0.20	-0.37	-0.44	-0.37	-0.55	-0.44	-0.20	-0.20	
-0.56	-0.44	-0.20	-0.44	-0.55	-0.55	-0.37	-0.37	
-0.37	-0.55	-0.37	-0.56	-0.56	-0.20	-0.44	-0.37	
-0.55	-0.37	-0.37	-0.44	-0.20	-0.20	-0.55	0.12	
Map policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	>	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.680003885735275
mean w [-0.60842841 -0.29792506 -0.28608162 -0.13863986 -0.41828673  0.06247188]
Mean policy from posterior
v	v	>	>	v	<	v	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.61	-0.42	-0.14	-0.29	-0.14	-0.14	-0.42	-0.42	
-0.29	-0.14	-0.42	-0.42	-0.14	-0.61	-0.61	-0.42	
-0.42	-0.14	-0.61	-0.14	-0.42	-0.42	-0.42	-0.61	
-0.42	-0.29	-0.14	-0.30	-0.29	-0.14	-0.29	-0.61	
-0.14	-0.29	-0.30	-0.29	-0.42	-0.30	-0.14	-0.14	
-0.61	-0.30	-0.14	-0.30	-0.42	-0.42	-0.29	-0.29	
-0.29	-0.42	-0.29	-0.61	-0.61	-0.14	-0.30	-0.29	
-0.42	-0.29	-0.29	-0.30	-0.14	-0.14	-0.42	0.06	
mean = 0.04442356912939682, map = 0.04077094246692381
CVaR policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	<	v	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	<	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.013487893563305153, 0.03071820623783239, 0.04347416733939502, 0.04442355275598642, 0.04442355221560801
==========
iteration 66
==========
weights [-0.30989402 -0.1210639  -0.0412376  -0.20132667 -0.15194379  0.90773859]
expeced value MDP LP 0.00484872204110065
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[ 0.04138643  0.39262096 -0.19918549 -0.42577242 -0.78489021  0.08441735]
w_map [-0.46247632 -0.19145589 -0.32750962 -0.65448249 -0.38326023  0.25877019] loglik -0.6931471965410445
accepted/total = 1577/3000 = 0.5256666666666666
-------
true weights [-0.30989402 -0.1210639  -0.0412376  -0.20132667 -0.15194379  0.90773859]
features
0 	4 	4 	1 	4 	0 	4 	2 	
4 	2 	0 	3 	2 	0 	1 	0 	
1 	4 	1 	1 	1 	3 	3 	0 	
2 	2 	1 	1 	0 	4 	3 	2 	
0 	4 	4 	4 	2 	1 	4 	3 	
1 	4 	4 	2 	3 	1 	2 	4 	
1 	4 	0 	1 	4 	3 	1 	1 	
3 	0 	2 	0 	1 	3 	1 	5 	
optimal policy
v	v	>	v	v	>	v	<	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-0.75	-0.47	-0.56	-0.41	-0.32	-0.55	-0.24	-0.28	
-0.44	-0.32	-0.52	-0.30	-0.17	-0.32	-0.09	-0.26	
-0.29	-0.29	-0.22	-0.10	-0.13	-0.01	0.03	0.05	
-0.18	-0.14	-0.10	0.03	-0.01	0.19	0.24	0.37	
-0.47	-0.16	-0.00	0.15	0.30	0.35	0.44	0.41	
-0.20	-0.08	0.07	0.22	0.27	0.47	0.60	0.62	
-0.32	-0.23	-0.15	0.16	0.29	0.44	0.65	0.78	
-0.42	-0.23	0.08	0.13	0.44	0.57	0.78	0.91	
map_weights [-0.46247632 -0.19145589 -0.32750962 -0.65448249 -0.38326023  0.25877019]
MAP reward
-0.46	-0.38	-0.38	-0.19	-0.38	-0.46	-0.38	-0.33	
-0.38	-0.33	-0.46	-0.65	-0.33	-0.46	-0.19	-0.46	
-0.19	-0.38	-0.19	-0.19	-0.19	-0.65	-0.65	-0.46	
-0.33	-0.33	-0.19	-0.19	-0.46	-0.38	-0.65	-0.33	
-0.46	-0.38	-0.38	-0.38	-0.33	-0.19	-0.38	-0.65	
-0.19	-0.38	-0.38	-0.33	-0.65	-0.19	-0.33	-0.38	
-0.19	-0.38	-0.46	-0.19	-0.38	-0.65	-0.19	-0.19	
-0.65	-0.46	-0.33	-0.46	-0.19	-0.65	-0.19	0.26	
Map policy
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6593052824287686
mean w [-0.47698018 -0.17406332 -0.25179591 -0.59658129 -0.26512815  0.05072282]
Mean policy from posterior
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.48	-0.27	-0.27	-0.17	-0.27	-0.48	-0.27	-0.25	
-0.27	-0.25	-0.48	-0.60	-0.25	-0.48	-0.17	-0.48	
-0.17	-0.27	-0.17	-0.17	-0.17	-0.60	-0.60	-0.48	
-0.25	-0.25	-0.17	-0.17	-0.48	-0.27	-0.60	-0.25	
-0.48	-0.27	-0.27	-0.27	-0.25	-0.17	-0.27	-0.60	
-0.17	-0.27	-0.27	-0.25	-0.60	-0.17	-0.25	-0.27	
-0.17	-0.27	-0.48	-0.17	-0.27	-0.60	-0.17	-0.17	
-0.60	-0.48	-0.25	-0.48	-0.17	-0.60	-0.17	0.05	
mean = 0.018577614677699694, map = 0.023770382909088894
CVaR policy
v	v	>	>	v	v	>	v	
v	v	v	>	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	>	v	
v	v	v	v	v	v	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05059605091830277, 0.012652184406053757, 0.009860512341609229, 0.018577612733503113, 0.018577611477700873
==========
iteration 67
==========
weights [-0.11013276 -0.67319968 -0.20101732 -0.12934998 -0.68243955  0.1086731 ]
expeced value MDP LP -1.7041313103468287
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.48315704  0.14631808  0.07939621  0.03226485  0.50177374 -0.69715754]
w_map [-0.27738038 -0.48477481 -0.29792804 -0.30063223 -0.68411171 -0.20224691] loglik -1.3182237523778895e-08
accepted/total = 1965/3000 = 0.655
-------
true weights [-0.11013276 -0.67319968 -0.20101732 -0.12934998 -0.68243955  0.1086731 ]
features
3 	1 	0 	4 	4 	0 	4 	1 	
4 	3 	2 	1 	1 	3 	3 	2 	
1 	2 	4 	1 	0 	3 	3 	3 	
2 	0 	3 	4 	1 	1 	1 	3 	
0 	4 	1 	2 	2 	3 	3 	4 	
1 	2 	1 	4 	4 	0 	1 	2 	
4 	0 	0 	0 	1 	3 	4 	0 	
0 	1 	4 	2 	3 	1 	2 	5 	
optimal policy
>	v	v	>	>	v	v	v	
>	v	<	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
optimal values
-3.29	-3.20	-2.81	-2.90	-2.24	-1.57	-2.03	-1.97	
-3.21	-2.55	-2.72	-2.77	-2.11	-1.47	-1.36	-1.31	
-3.09	-2.44	-2.78	-2.11	-1.46	-1.36	-1.24	-1.12	
-2.44	-2.27	-2.18	-2.08	-1.88	-1.69	-1.66	-1.00	
-2.31	-2.23	-2.07	-1.41	-1.22	-1.03	-1.00	-0.88	
-2.22	-1.56	-1.94	-1.85	-1.58	-0.91	-0.87	-0.20	
-2.04	-1.37	-1.28	-1.18	-1.47	-0.81	-0.68	-0.00	
-2.12	-2.03	-1.75	-1.08	-0.89	-0.77	-0.09	0.11	
map_weights [-0.27738038 -0.48477481 -0.29792804 -0.30063223 -0.68411171 -0.20224691]
MAP reward
-0.30	-0.48	-0.28	-0.68	-0.68	-0.28	-0.68	-0.48	
-0.68	-0.30	-0.30	-0.48	-0.48	-0.30	-0.30	-0.30	
-0.48	-0.30	-0.68	-0.48	-0.28	-0.30	-0.30	-0.30	
-0.30	-0.28	-0.30	-0.68	-0.48	-0.48	-0.48	-0.30	
-0.28	-0.68	-0.48	-0.30	-0.30	-0.30	-0.30	-0.68	
-0.48	-0.30	-0.48	-0.68	-0.68	-0.28	-0.48	-0.30	
-0.68	-0.28	-0.28	-0.28	-0.48	-0.30	-0.68	-0.28	
-0.28	-0.48	-0.68	-0.30	-0.30	-0.48	-0.30	-0.20	
Map policy
>	v	v	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8586858056352797
mean w [-0.16206327 -0.38184607 -0.163873   -0.34041897 -0.60800628 -0.08511475]
Mean policy from posterior
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.34	-0.38	-0.16	-0.61	-0.61	-0.16	-0.61	-0.38	
-0.61	-0.34	-0.16	-0.38	-0.38	-0.34	-0.34	-0.16	
-0.38	-0.16	-0.61	-0.38	-0.16	-0.34	-0.34	-0.34	
-0.16	-0.16	-0.34	-0.61	-0.38	-0.38	-0.38	-0.34	
-0.16	-0.61	-0.38	-0.16	-0.16	-0.34	-0.34	-0.61	
-0.38	-0.16	-0.38	-0.61	-0.61	-0.16	-0.38	-0.16	
-0.61	-0.16	-0.16	-0.16	-0.38	-0.34	-0.61	-0.16	
-0.16	-0.38	-0.61	-0.16	-0.34	-0.38	-0.16	-0.09	
mean = 0.1725322173095616, map = 0.16645317340141053
CVaR policy
>	v	v	v	>	v	v	v	
>	v	>	>	v	>	>	v	
v	v	v	>	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
cvar = , 0.11929104211669284, 0.16045958235686664, 0.15844049205638666, 0.15844049053981224, 0.1584404923060152
==========
iteration 68
==========
weights [-0.4487679  -0.35687817 -0.53436972 -0.42553897 -0.44563112  0.07761331]
expeced value MDP LP -2.8139914937535053
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.20402433  0.22023726 -0.55015973  0.60547593  0.3649617   0.32771292]
w_map [-0.310659   -0.28554489 -0.76873852 -0.34482729 -0.32854247 -0.06442189] loglik -1.6342482922482304e-13
accepted/total = 2087/3000 = 0.6956666666666667
-------
true weights [-0.4487679  -0.35687817 -0.53436972 -0.42553897 -0.44563112  0.07761331]
features
3 	4 	1 	1 	3 	3 	1 	4 	
4 	0 	4 	0 	4 	2 	1 	2 	
2 	3 	3 	2 	1 	4 	3 	3 	
3 	1 	4 	4 	0 	4 	4 	3 	
4 	2 	1 	0 	3 	2 	1 	2 	
3 	3 	4 	1 	4 	0 	2 	3 	
2 	0 	4 	3 	2 	0 	1 	0 	
2 	0 	3 	3 	2 	4 	2 	5 	
optimal policy
>	>	>	>	>	>	v	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-5.29	-4.91	-4.51	-4.20	-3.88	-3.49	-3.09	-3.07	
-5.05	-4.65	-4.33	-4.01	-3.59	-3.27	-2.76	-2.65	
-4.74	-4.24	-3.93	-3.68	-3.18	-2.85	-2.43	-2.14	
-4.24	-3.86	-3.54	-3.21	-2.86	-2.45	-2.03	-1.73	
-3.99	-3.62	-3.12	-2.79	-2.44	-2.11	-1.60	-1.32	
-3.58	-3.19	-2.79	-2.37	-2.03	-1.60	-1.25	-0.79	
-3.45	-2.95	-2.52	-2.10	-1.69	-1.17	-0.73	-0.37	
-3.18	-2.67	-2.24	-1.84	-1.42	-0.90	-0.46	0.08	
map_weights [-0.310659   -0.28554489 -0.76873852 -0.34482729 -0.32854247 -0.06442189]
MAP reward
-0.34	-0.33	-0.29	-0.29	-0.34	-0.34	-0.29	-0.33	
-0.33	-0.31	-0.33	-0.31	-0.33	-0.77	-0.29	-0.77	
-0.77	-0.34	-0.34	-0.77	-0.29	-0.33	-0.34	-0.34	
-0.34	-0.29	-0.33	-0.33	-0.31	-0.33	-0.33	-0.34	
-0.33	-0.77	-0.29	-0.31	-0.34	-0.77	-0.29	-0.77	
-0.34	-0.34	-0.33	-0.29	-0.33	-0.31	-0.77	-0.34	
-0.77	-0.31	-0.33	-0.34	-0.77	-0.31	-0.29	-0.31	
-0.77	-0.31	-0.34	-0.34	-0.77	-0.33	-0.77	-0.06	
Map policy
>	>	>	v	v	>	v	<	
>	v	v	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8787318906969048
mean w [-0.25362115 -0.28625157 -0.71723383 -0.22731228 -0.24910258  0.11229204]
Mean policy from posterior
>	v	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
v	>	>	v	v	>	>	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.23	-0.25	-0.29	-0.29	-0.23	-0.23	-0.29	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	-0.72	-0.29	-0.72	
-0.72	-0.23	-0.23	-0.72	-0.29	-0.25	-0.23	-0.23	
-0.23	-0.29	-0.25	-0.25	-0.25	-0.25	-0.25	-0.23	
-0.25	-0.72	-0.29	-0.25	-0.23	-0.72	-0.29	-0.72	
-0.23	-0.23	-0.25	-0.29	-0.25	-0.25	-0.72	-0.23	
-0.72	-0.25	-0.25	-0.23	-0.72	-0.25	-0.29	-0.25	
-0.72	-0.25	-0.23	-0.23	-0.72	-0.25	-0.72	0.11	
mean = 0.0692496146489594, map = 0.018972472416987518
CVaR policy
>	>	v	>	v	>	v	v	
>	v	v	>	v	>	v	v	
v	v	v	>	v	>	>	v	
>	>	v	v	v	>	v	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
v	>	v	v	v	>	>	v	
v	>	>	v	v	>	>	v	
v	v	v	>	v	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
v	>	v	v	v	>	>	v	
v	>	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
v	>	>	v	v	>	>	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
v	>	>	v	v	>	>	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.020574806708346216, 0.05683584356015947, 0.06789446900527762, 0.06924961683552144, 0.06924961589957945
==========
iteration 69
==========
weights [-0.37050983 -0.25833116 -0.23439414 -0.08713256 -0.79646328  0.31480316]
expeced value MDP LP -1.8806386965914959
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.20560544 -0.15427472  0.60859789  0.25066837  0.63867915 -0.30461226]
w_map [-0.45671489 -0.4431187  -0.28378362 -0.20030574 -0.68873872  0.00638477] loglik -7.039720344437228e-07
accepted/total = 1416/3000 = 0.472
-------
true weights [-0.37050983 -0.25833116 -0.23439414 -0.08713256 -0.79646328  0.31480316]
features
3 	2 	2 	2 	1 	4 	0 	4 	
4 	0 	4 	4 	0 	4 	4 	3 	
2 	2 	1 	3 	3 	4 	3 	1 	
4 	2 	3 	2 	0 	1 	2 	0 	
3 	2 	1 	0 	1 	3 	1 	1 	
4 	1 	4 	2 	2 	4 	3 	2 	
3 	2 	2 	0 	1 	2 	1 	4 	
0 	0 	4 	1 	0 	0 	4 	5 	
optimal policy
>	v	>	>	v	<	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
optimal values
-2.93	-2.87	-2.80	-2.60	-2.38	-3.16	-2.48	-2.42	
-3.30	-2.66	-2.88	-2.64	-2.15	-2.90	-2.13	-1.64	
-2.53	-2.32	-2.10	-1.86	-1.80	-2.13	-1.34	-1.57	
-3.00	-2.22	-2.01	-1.94	-1.73	-1.37	-1.27	-1.33	
-2.25	-2.18	-1.97	-1.73	-1.37	-1.12	-1.04	-0.97	
-2.85	-2.25	-2.44	-1.66	-1.44	-1.58	-0.79	-0.71	
-2.07	-2.01	-1.79	-1.57	-1.21	-0.97	-0.74	-0.48	
-2.42	-2.36	-2.24	-1.46	-1.21	-0.85	-0.48	0.31	
map_weights [-0.45671489 -0.4431187  -0.28378362 -0.20030574 -0.68873872  0.00638477]
MAP reward
-0.20	-0.28	-0.28	-0.28	-0.44	-0.69	-0.46	-0.69	
-0.69	-0.46	-0.69	-0.69	-0.46	-0.69	-0.69	-0.20	
-0.28	-0.28	-0.44	-0.20	-0.20	-0.69	-0.20	-0.44	
-0.69	-0.28	-0.20	-0.28	-0.46	-0.44	-0.28	-0.46	
-0.20	-0.28	-0.44	-0.46	-0.44	-0.20	-0.44	-0.44	
-0.69	-0.44	-0.69	-0.28	-0.28	-0.69	-0.20	-0.28	
-0.20	-0.28	-0.28	-0.46	-0.44	-0.28	-0.44	-0.69	
-0.46	-0.46	-0.69	-0.44	-0.46	-0.46	-0.69	0.01	
Map policy
>	v	>	v	v	>	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -2.448365368652329
mean w [-0.42617496 -0.36275412 -0.28511011 -0.12773974 -0.60773084 -0.14405242]
Mean policy from posterior
>	v	>	v	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.13	-0.29	-0.29	-0.29	-0.36	-0.61	-0.43	-0.61	
-0.61	-0.43	-0.61	-0.61	-0.43	-0.61	-0.61	-0.13	
-0.29	-0.29	-0.36	-0.13	-0.13	-0.61	-0.13	-0.36	
-0.61	-0.29	-0.13	-0.29	-0.43	-0.36	-0.29	-0.43	
-0.13	-0.29	-0.36	-0.43	-0.36	-0.13	-0.36	-0.36	
-0.61	-0.36	-0.61	-0.29	-0.29	-0.61	-0.13	-0.29	
-0.13	-0.29	-0.29	-0.43	-0.36	-0.29	-0.36	-0.61	
-0.43	-0.43	-0.61	-0.36	-0.43	-0.43	-0.61	-0.14	
mean = 0.012938236357642241, map = 0.023945613067376748
CVaR policy
>	v	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
cvar = , 0.051277223198279875, 0.03783853745035204, 0.029886061198224123, 0.02009200389664123, 0.012938222224313733
==========
iteration 70
==========
weights [-0.22513359 -0.28905385 -0.03580942 -0.69498026 -0.53881297  0.30193286]
expeced value MDP LP -1.2175531702620517
demonstration
[(32, 3), (40, 1), (41, 1), (42, 2), (34, 1), (35, 1), (36, 1), (37, 3), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.70453797 -0.23644976 -0.0702577  -0.0061609  -0.19736508  0.63544526]
w_map [-0.24769052 -0.24392684 -0.12008998 -0.61754476 -0.67002991  0.18554216] loglik -4.455026609928581e-05
accepted/total = 1111/3000 = 0.37033333333333335
-------
true weights [-0.22513359 -0.28905385 -0.03580942 -0.69498026 -0.53881297  0.30193286]
features
2 	2 	1 	2 	1 	2 	1 	4 	
1 	4 	1 	4 	1 	4 	3 	2 	
0 	1 	1 	0 	1 	0 	3 	2 	
0 	0 	2 	0 	1 	2 	2 	1 	
0 	4 	2 	2 	0 	1 	3 	1 	
2 	2 	0 	1 	3 	0 	3 	1 	
0 	3 	0 	0 	4 	0 	2 	1 	
4 	0 	1 	0 	4 	4 	4 	5 	
optimal policy
>	>	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
optimal values
-1.91	-1.90	-1.88	-1.84	-1.83	-1.55	-1.71	-1.43	
-1.96	-2.07	-1.61	-1.95	-1.56	-1.53	-1.59	-0.90	
-1.69	-1.54	-1.33	-1.43	-1.28	-1.00	-1.50	-0.88	
-1.48	-1.27	-1.05	-1.22	-1.07	-0.79	-0.81	-0.85	
-1.50	-1.56	-1.03	-1.00	-0.98	-0.76	-1.25	-0.57	
-1.29	-1.27	-1.24	-1.28	-1.16	-0.47	-0.72	-0.28	
-1.50	-1.90	-1.22	-1.00	-0.79	-0.25	-0.03	0.01	
-2.02	-1.71	-1.50	-1.22	-1.31	-0.78	-0.24	0.30	
map_weights [-0.24769052 -0.24392684 -0.12008998 -0.61754476 -0.67002991  0.18554216]
MAP reward
-0.12	-0.12	-0.24	-0.12	-0.24	-0.12	-0.24	-0.67	
-0.24	-0.67	-0.24	-0.67	-0.24	-0.67	-0.62	-0.12	
-0.25	-0.24	-0.24	-0.25	-0.24	-0.25	-0.62	-0.12	
-0.25	-0.25	-0.12	-0.25	-0.24	-0.12	-0.12	-0.24	
-0.25	-0.67	-0.12	-0.12	-0.25	-0.24	-0.62	-0.24	
-0.12	-0.12	-0.25	-0.24	-0.62	-0.25	-0.62	-0.24	
-0.25	-0.62	-0.25	-0.25	-0.67	-0.25	-0.12	-0.24	
-0.67	-0.25	-0.24	-0.25	-0.67	-0.67	-0.67	0.19	
Map policy
>	>	v	>	v	>	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	>	v	
>	>	v	>	>	>	>	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
expeced value MDP LP -0.8679515053563995
mean w [-0.16281713 -0.22895636 -0.05828341 -0.50853114 -0.5676272   0.41247138]
Mean policy from posterior
v	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	^	>	>	>	>	>	v	
^	>	^	^	^	^	>	.	
Mean rewards
-0.06	-0.06	-0.23	-0.06	-0.23	-0.06	-0.23	-0.57	
-0.23	-0.57	-0.23	-0.57	-0.23	-0.57	-0.51	-0.06	
-0.16	-0.23	-0.23	-0.16	-0.23	-0.16	-0.51	-0.06	
-0.16	-0.16	-0.06	-0.16	-0.23	-0.06	-0.06	-0.23	
-0.16	-0.57	-0.06	-0.06	-0.16	-0.23	-0.51	-0.23	
-0.06	-0.06	-0.16	-0.23	-0.51	-0.16	-0.51	-0.23	
-0.16	-0.51	-0.16	-0.16	-0.57	-0.16	-0.06	-0.23	
-0.57	-0.16	-0.23	-0.16	-0.57	-0.57	-0.57	0.41	
mean = 0.004659862233888479, map = 0.048583211475696775
CVaR policy
>	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	^	^	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	>	v	
>	>	^	>	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	^	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	^	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	^	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	^	>	>	>	>	>	v	
^	>	^	^	>	^	>	.	
cvar = , 0.009390461475424106, 0.007384915778634538, 0.0029391713766457883, 0.003949177785975033, 0.004659855533110768
==========
iteration 71
==========
weights [-0.61278202 -0.25491264 -0.35505055 -0.54428494 -0.317345    0.19105732]
expeced value MDP LP -2.3003258457556166
demonstration
[(32, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 3), (37, 3), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.66707742  0.13621466 -0.22262127 -0.65759905 -0.01237102 -0.23303113]
w_map [-0.52032304 -0.22730276 -0.11513761 -0.76408729 -0.05901138 -0.27754098] loglik -0.4122500003741436
accepted/total = 1874/3000 = 0.6246666666666667
-------
true weights [-0.61278202 -0.25491264 -0.35505055 -0.54428494 -0.317345    0.19105732]
features
3 	1 	2 	1 	1 	1 	3 	3 	
2 	4 	2 	1 	0 	2 	3 	4 	
3 	1 	1 	4 	1 	2 	4 	1 	
1 	2 	4 	2 	1 	4 	3 	1 	
2 	3 	0 	3 	3 	4 	0 	1 	
3 	3 	3 	0 	3 	2 	0 	0 	
0 	4 	3 	0 	4 	2 	3 	4 	
0 	0 	3 	4 	4 	4 	3 	5 	
optimal policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
^	^	>	>	>	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.11	-3.60	-3.42	-3.10	-2.91	-2.68	-2.83	-2.30	
-3.70	-3.38	-3.20	-2.87	-2.94	-2.45	-2.30	-1.78	
-3.61	-3.10	-2.87	-2.64	-2.35	-2.11	-1.78	-1.47	
-3.38	-3.15	-2.83	-2.53	-2.20	-1.97	-1.76	-1.23	
-3.70	-3.67	-3.30	-2.72	-2.19	-1.67	-1.59	-0.99	
-3.67	-3.16	-2.87	-2.41	-1.82	-1.36	-1.28	-0.74	
-3.22	-2.64	-2.34	-1.89	-1.29	-1.02	-0.67	-0.13	
-3.00	-2.41	-1.82	-1.29	-0.98	-0.67	-0.36	0.19	
map_weights [-0.52032304 -0.22730276 -0.11513761 -0.76408729 -0.05901138 -0.27754098]
MAP reward
-0.76	-0.23	-0.12	-0.23	-0.23	-0.23	-0.76	-0.76	
-0.12	-0.06	-0.12	-0.23	-0.52	-0.12	-0.76	-0.06	
-0.76	-0.23	-0.23	-0.06	-0.23	-0.12	-0.06	-0.23	
-0.23	-0.12	-0.06	-0.12	-0.23	-0.06	-0.76	-0.23	
-0.12	-0.76	-0.52	-0.76	-0.76	-0.06	-0.52	-0.23	
-0.76	-0.76	-0.76	-0.52	-0.76	-0.12	-0.52	-0.52	
-0.52	-0.06	-0.76	-0.52	-0.06	-0.12	-0.76	-0.06	
-0.52	-0.52	-0.76	-0.06	-0.06	-0.06	-0.76	-0.28	
Map policy
v	v	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
>	v	>	v	>	v	<	v	
>	>	>	>	>	v	>	v	
^	^	^	^	>	v	>	v	
^	v	>	v	v	v	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0397299087727943
mean w [-0.60535636 -0.26533198 -0.15322638 -0.52810098 -0.19816088 -0.18517069]
Mean policy from posterior
v	>	v	>	>	v	<	v	
>	>	v	v	>	v	<	v	
>	v	>	v	>	v	<	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	v	>	v	
^	v	>	>	>	v	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.53	-0.27	-0.15	-0.27	-0.27	-0.27	-0.53	-0.53	
-0.15	-0.20	-0.15	-0.27	-0.61	-0.15	-0.53	-0.20	
-0.53	-0.27	-0.27	-0.20	-0.27	-0.15	-0.20	-0.27	
-0.27	-0.15	-0.20	-0.15	-0.27	-0.20	-0.53	-0.27	
-0.15	-0.53	-0.61	-0.53	-0.53	-0.20	-0.61	-0.27	
-0.53	-0.53	-0.53	-0.61	-0.53	-0.15	-0.61	-0.61	
-0.61	-0.20	-0.53	-0.61	-0.20	-0.15	-0.53	-0.20	
-0.61	-0.61	-0.53	-0.20	-0.20	-0.20	-0.53	-0.19	
mean = 0.11647903627903444, map = 0.12282283703073338
CVaR policy
>	>	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	v	>	v	
^	^	>	>	>	v	>	v	
v	v	>	>	>	v	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	<	v	
>	>	v	v	>	v	v	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	v	>	v	
^	v	>	>	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	v	<	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	v	>	v	
^	v	>	>	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	v	<	v	
>	>	v	v	>	v	<	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	v	>	v	
^	v	>	>	>	v	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	v	<	v	
>	>	v	v	>	v	<	v	
>	v	>	v	>	v	<	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	v	>	v	
^	v	>	>	>	v	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.07787654324849047, 0.09291415153384541, 0.09687831958038018, 0.10368453301568437, 0.11647903533920934
==========
iteration 72
==========
weights [-0.17961962 -0.64037079 -0.66922549 -0.27033157 -0.18633629  0.04470917]
expeced value MDP LP -1.6678488754885077
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.40312934 -0.22238748 -0.11776132 -0.73474915 -0.22532124 -0.42841199]
w_map [-0.19394875 -0.64359553 -0.43678791 -0.40837505 -0.43647099  0.01038693] loglik -1.1817093081845087e-08
accepted/total = 1705/3000 = 0.5683333333333334
-------
true weights [-0.17961962 -0.64037079 -0.66922549 -0.27033157 -0.18633629  0.04470917]
features
0 	0 	0 	4 	2 	1 	0 	4 	
2 	2 	4 	4 	0 	3 	0 	4 	
2 	2 	4 	3 	1 	4 	2 	2 	
2 	3 	4 	3 	3 	3 	2 	3 	
3 	4 	1 	4 	0 	0 	3 	4 	
0 	4 	3 	2 	1 	0 	3 	1 	
1 	3 	3 	1 	3 	1 	4 	0 	
3 	4 	2 	1 	4 	3 	4 	5 	
optimal policy
>	>	v	v	v	v	v	<	
^	>	v	>	>	v	<	<	
>	>	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	^	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.52	-2.36	-2.20	-2.12	-2.44	-2.25	-1.95	-2.12	
-3.16	-2.69	-2.04	-1.96	-1.79	-1.63	-1.79	-1.96	
-3.17	-2.53	-1.88	-1.79	-1.99	-1.37	-2.02	-1.87	
-2.61	-1.96	-1.71	-1.54	-1.36	-1.19	-1.51	-1.21	
-2.32	-2.07	-1.91	-1.28	-1.10	-0.93	-0.85	-0.95	
-2.32	-2.16	-2.00	-1.94	-1.39	-0.76	-0.59	-0.77	
-2.62	-2.00	-1.74	-1.49	-0.86	-0.96	-0.32	-0.14	
-2.30	-2.05	-1.88	-1.23	-0.59	-0.41	-0.14	0.04	
map_weights [-0.19394875 -0.64359553 -0.43678791 -0.40837505 -0.43647099  0.01038693]
MAP reward
-0.19	-0.19	-0.19	-0.44	-0.44	-0.64	-0.19	-0.44	
-0.44	-0.44	-0.44	-0.44	-0.19	-0.41	-0.19	-0.44	
-0.44	-0.44	-0.44	-0.41	-0.64	-0.44	-0.44	-0.44	
-0.44	-0.41	-0.44	-0.41	-0.41	-0.41	-0.44	-0.41	
-0.41	-0.44	-0.64	-0.44	-0.19	-0.19	-0.41	-0.44	
-0.19	-0.44	-0.41	-0.44	-0.64	-0.19	-0.41	-0.64	
-0.64	-0.41	-0.41	-0.64	-0.41	-0.64	-0.44	-0.19	
-0.41	-0.44	-0.44	-0.64	-0.44	-0.41	-0.44	0.01	
Map policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8562693647615627
mean w [-0.12071673 -0.54019212 -0.44817904 -0.36803233 -0.25935884 -0.06823718]
Mean policy from posterior
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	<	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	^	>	>	v	v	
^	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.12	-0.12	-0.12	-0.26	-0.45	-0.54	-0.12	-0.26	
-0.45	-0.45	-0.26	-0.26	-0.12	-0.37	-0.12	-0.26	
-0.45	-0.45	-0.26	-0.37	-0.54	-0.26	-0.45	-0.45	
-0.45	-0.37	-0.26	-0.37	-0.37	-0.37	-0.45	-0.37	
-0.37	-0.26	-0.54	-0.26	-0.12	-0.12	-0.37	-0.26	
-0.12	-0.26	-0.37	-0.45	-0.54	-0.12	-0.37	-0.54	
-0.54	-0.37	-0.37	-0.54	-0.37	-0.54	-0.26	-0.12	
-0.37	-0.26	-0.45	-0.54	-0.26	-0.37	-0.26	-0.07	
mean = 0.021066956257692615, map = 0.11059739486208287
CVaR policy
>	>	>	>	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	^	>	>	v	v	
>	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	^	>	>	v	v	
>	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.16655847734475948, 0.055592010641185574, 0.04675621769009597, 0.03232397908602347, 0.032323979916734746
==========
iteration 73
==========
weights [-0.0065879  -0.31779271 -0.10417501 -0.6774825  -0.03263814  0.65426614]
expeced value MDP LP 0.05408183179890491
demonstration
[(32, 1), (33, 1), (34, 3), (42, 1), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.084242    0.34888941  0.70741048 -0.40135185  0.03908171 -0.45622273]
w_map [-0.21735179 -0.52213236 -0.52439584 -0.54728676 -0.26520658 -0.18785012] loglik -1.0238920822303044e-10
accepted/total = 2061/3000 = 0.687
-------
true weights [-0.0065879  -0.31779271 -0.10417501 -0.6774825  -0.03263814  0.65426614]
features
3 	3 	0 	4 	1 	2 	2 	0 	
4 	2 	3 	0 	0 	4 	3 	2 	
4 	4 	0 	3 	4 	4 	2 	3 	
2 	2 	4 	2 	1 	0 	3 	2 	
4 	4 	4 	1 	1 	3 	0 	0 	
2 	2 	4 	4 	3 	1 	2 	4 	
1 	1 	1 	0 	0 	3 	3 	0 	
0 	4 	4 	3 	4 	4 	2 	5 	
optimal policy
v	v	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	<	v	v	v	v	
v	v	v	<	<	v	v	v	
>	>	v	v	<	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
optimal values
-0.50	-0.53	-0.20	-0.20	-0.48	-0.28	-0.38	-0.31	
0.18	0.15	-0.39	-0.17	-0.16	-0.17	-0.85	-0.31	
0.22	0.25	0.29	-0.39	-0.16	-0.14	-0.21	-0.20	
0.16	0.19	0.30	0.19	-0.13	-0.11	-0.11	0.48	
0.26	0.30	0.34	0.09	-0.23	-0.11	0.58	0.59	
0.16	0.26	0.37	0.41	-0.23	0.17	0.49	0.60	
-0.16	-0.06	0.12	0.45	0.46	-0.18	-0.04	0.64	
0.05	0.06	0.09	-0.21	0.47	0.51	0.54	0.65	
map_weights [-0.21735179 -0.52213236 -0.52439584 -0.54728676 -0.26520658 -0.18785012]
MAP reward
-0.55	-0.55	-0.22	-0.27	-0.52	-0.52	-0.52	-0.22	
-0.27	-0.52	-0.55	-0.22	-0.22	-0.27	-0.55	-0.52	
-0.27	-0.27	-0.22	-0.55	-0.27	-0.27	-0.52	-0.55	
-0.52	-0.52	-0.27	-0.52	-0.52	-0.22	-0.55	-0.52	
-0.27	-0.27	-0.27	-0.52	-0.52	-0.55	-0.22	-0.22	
-0.52	-0.52	-0.27	-0.27	-0.55	-0.52	-0.52	-0.27	
-0.52	-0.52	-0.52	-0.22	-0.22	-0.55	-0.55	-0.22	
-0.22	-0.27	-0.27	-0.55	-0.27	-0.27	-0.52	-0.19	
Map policy
v	>	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	v	>	>	>	v	
>	>	>	v	v	>	>	v	
v	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7079459650802924
mean w [-0.24378307 -0.47164692 -0.38677307 -0.5657764  -0.12585712 -0.10220777]
Mean policy from posterior
v	v	v	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	<	>	v	v	v	
v	v	v	<	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.57	-0.57	-0.24	-0.13	-0.47	-0.39	-0.39	-0.24	
-0.13	-0.39	-0.57	-0.24	-0.24	-0.13	-0.57	-0.39	
-0.13	-0.13	-0.24	-0.57	-0.13	-0.13	-0.39	-0.57	
-0.39	-0.39	-0.13	-0.39	-0.47	-0.24	-0.57	-0.39	
-0.13	-0.13	-0.13	-0.47	-0.47	-0.57	-0.24	-0.24	
-0.39	-0.39	-0.13	-0.13	-0.57	-0.47	-0.39	-0.13	
-0.47	-0.47	-0.47	-0.24	-0.24	-0.57	-0.57	-0.24	
-0.24	-0.13	-0.13	-0.57	-0.13	-0.13	-0.39	-0.10	
mean = 0.04343056035250448, map = 0.07446847125416955
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	<	>	v	v	v	
v	v	v	<	>	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	<	>	v	v	v	
v	v	v	<	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	<	>	v	v	v	
v	v	v	<	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.07182088557113353, 0.055745794031365925, 0.04527428265734568, 0.04343055920859193, 0.043430559442468736
==========
iteration 74
==========
weights [-0.30557272 -0.31797187 -0.23414576 -0.67327985 -0.14898029  0.52458944]
expeced value MDP LP -1.2535152997443926
demonstration
[(32, 3), (40, 3), (48, 1), (49, 1), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.1145916   0.30031325  0.52014097 -0.66244356 -0.06966633 -0.42714075]
w_map [-0.74244548 -0.37936846 -0.10912389 -0.52528157 -0.07712426 -0.1052491 ] loglik -0.44379066762244435
accepted/total = 1376/3000 = 0.45866666666666667
-------
true weights [-0.30557272 -0.31797187 -0.23414576 -0.67327985 -0.14898029  0.52458944]
features
4 	0 	1 	4 	1 	4 	1 	3 	
2 	2 	2 	4 	3 	0 	1 	0 	
0 	2 	4 	0 	3 	1 	3 	4 	
1 	1 	0 	2 	2 	4 	4 	3 	
1 	0 	2 	2 	0 	2 	0 	0 	
4 	0 	2 	2 	2 	1 	1 	0 	
2 	0 	2 	2 	3 	3 	3 	4 	
0 	1 	1 	2 	0 	1 	2 	5 	
optimal policy
v	>	>	v	>	v	<	v	
>	>	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	>	v	
v	>	>	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.48	-2.42	-2.14	-1.84	-1.87	-1.57	-1.87	-2.01	
-2.35	-2.14	-1.93	-1.71	-2.10	-1.44	-1.66	-1.35	
-2.21	-1.93	-1.71	-1.58	-1.72	-1.14	-1.36	-1.06	
-2.18	-1.88	-1.58	-1.28	-1.06	-0.83	-0.69	-0.92	
-1.98	-1.76	-1.47	-1.25	-1.07	-0.78	-0.55	-0.25	
-1.68	-1.54	-1.25	-1.03	-0.80	-0.57	-0.26	0.06	
-1.54	-1.32	-1.03	-0.80	-1.01	-0.71	-0.31	0.37	
-1.49	-1.19	-0.88	-0.57	-0.34	-0.04	0.29	0.52	
map_weights [-0.74244548 -0.37936846 -0.10912389 -0.52528157 -0.07712426 -0.1052491 ]
MAP reward
-0.08	-0.74	-0.38	-0.08	-0.38	-0.08	-0.38	-0.53	
-0.11	-0.11	-0.11	-0.08	-0.53	-0.74	-0.38	-0.74	
-0.74	-0.11	-0.08	-0.74	-0.53	-0.38	-0.53	-0.08	
-0.38	-0.38	-0.74	-0.11	-0.11	-0.08	-0.08	-0.53	
-0.38	-0.74	-0.11	-0.11	-0.74	-0.11	-0.74	-0.74	
-0.08	-0.74	-0.11	-0.11	-0.11	-0.38	-0.38	-0.74	
-0.11	-0.74	-0.11	-0.11	-0.53	-0.53	-0.53	-0.08	
-0.74	-0.38	-0.38	-0.11	-0.74	-0.38	-0.11	-0.11	
Map policy
v	v	>	v	<	v	v	<	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	>	>	v	>	>	v	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.068892454042615
mean w [-0.45468748 -0.44026281 -0.18732488 -0.60946676 -0.12852908 -0.06431158]
Mean policy from posterior
v	v	>	v	<	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.13	-0.45	-0.44	-0.13	-0.44	-0.13	-0.44	-0.61	
-0.19	-0.19	-0.19	-0.13	-0.61	-0.45	-0.44	-0.45	
-0.45	-0.19	-0.13	-0.45	-0.61	-0.44	-0.61	-0.13	
-0.44	-0.44	-0.45	-0.19	-0.19	-0.13	-0.13	-0.61	
-0.44	-0.45	-0.19	-0.19	-0.45	-0.19	-0.45	-0.45	
-0.13	-0.45	-0.19	-0.19	-0.19	-0.44	-0.44	-0.45	
-0.19	-0.45	-0.19	-0.19	-0.61	-0.61	-0.61	-0.13	
-0.45	-0.44	-0.44	-0.19	-0.45	-0.44	-0.19	-0.06	
mean = 0.06032016930697304, map = 0.27736406580854034
CVaR policy
v	v	>	v	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	>	>	>	v	
v	>	v	v	>	>	>	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	>	>	>	v	
v	>	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	<	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	<	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05589807514666889, 0.05070293180292618, 0.05291667820588186, 0.060320173466022364, 0.06032016967748488
==========
iteration 75
==========
weights [-0.08194328 -0.39520843 -0.45167315 -0.29919102 -0.07673882  0.73326861]
expeced value MDP LP -0.7526650300552158
demonstration
[(32, 1), (33, 1), (34, 3), (42, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[ 0.33091881  0.23460898  0.15416357  0.601917   -0.64913912  0.1673299 ]
w_map [-0.17232149 -0.56479866 -0.6288179  -0.42548938 -0.13103369  0.24017656] loglik -4.463764469164744e-08
accepted/total = 1687/3000 = 0.5623333333333334
-------
true weights [-0.08194328 -0.39520843 -0.45167315 -0.29919102 -0.07673882  0.73326861]
features
4 	3 	2 	0 	1 	2 	3 	3 	
2 	3 	4 	2 	2 	4 	2 	4 	
2 	1 	1 	4 	4 	1 	4 	0 	
4 	0 	4 	0 	3 	1 	2 	0 	
1 	0 	3 	2 	2 	4 	1 	2 	
0 	2 	4 	2 	1 	0 	2 	1 	
0 	2 	0 	4 	3 	2 	0 	2 	
2 	1 	3 	1 	1 	3 	0 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
optimal values
-1.87	-1.82	-1.68	-1.40	-1.65	-1.36	-1.37	-1.09	
-1.82	-1.53	-1.25	-1.33	-1.26	-0.91	-1.24	-0.80	
-1.38	-1.25	-1.18	-0.89	-0.82	-0.85	-0.80	-0.73	
-0.94	-0.87	-0.79	-0.83	-0.75	-0.46	-0.75	-0.65	
-1.19	-0.80	-0.72	-0.96	-0.51	-0.06	-0.30	-0.57	
-0.95	-0.88	-0.43	-0.73	-0.38	0.02	0.10	-0.12	
-0.88	-0.80	-0.36	-0.28	-0.20	0.10	0.56	0.27	
-1.32	-1.04	-0.65	-0.45	-0.06	0.34	0.64	0.73	
map_weights [-0.17232149 -0.56479866 -0.6288179  -0.42548938 -0.13103369  0.24017656]
MAP reward
-0.13	-0.43	-0.63	-0.17	-0.56	-0.63	-0.43	-0.43	
-0.63	-0.43	-0.13	-0.63	-0.63	-0.13	-0.63	-0.13	
-0.63	-0.56	-0.56	-0.13	-0.13	-0.56	-0.13	-0.17	
-0.13	-0.17	-0.13	-0.17	-0.43	-0.56	-0.63	-0.17	
-0.56	-0.17	-0.43	-0.63	-0.63	-0.13	-0.56	-0.63	
-0.17	-0.63	-0.13	-0.63	-0.56	-0.17	-0.63	-0.56	
-0.17	-0.63	-0.17	-0.13	-0.43	-0.63	-0.17	-0.63	
-0.63	-0.56	-0.43	-0.56	-0.56	-0.43	-0.17	0.24	
Map policy
>	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
expeced value MDP LP -1.8218076043434417
mean w [-0.15906016 -0.62983454 -0.51200875 -0.2953206  -0.17652557  0.16428307]
Mean policy from posterior
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
Mean rewards
-0.18	-0.30	-0.51	-0.16	-0.63	-0.51	-0.30	-0.30	
-0.51	-0.30	-0.18	-0.51	-0.51	-0.18	-0.51	-0.18	
-0.51	-0.63	-0.63	-0.18	-0.18	-0.63	-0.18	-0.16	
-0.18	-0.16	-0.18	-0.16	-0.30	-0.63	-0.51	-0.16	
-0.63	-0.16	-0.30	-0.51	-0.51	-0.18	-0.63	-0.51	
-0.16	-0.51	-0.18	-0.51	-0.63	-0.16	-0.51	-0.63	
-0.16	-0.51	-0.16	-0.18	-0.30	-0.51	-0.16	-0.51	
-0.51	-0.63	-0.30	-0.63	-0.63	-0.30	-0.16	0.16	
mean = 0.007998522100715943, map = 8.06596970970297e-06
CVaR policy
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
cvar = , 0.013914670896511883, 0.009900872452026865, 0.007457450322793879, 0.007998519545530103, 0.007998519517224523
==========
iteration 76
==========
weights [-0.79956774 -0.37369359 -0.32428528 -0.04562172 -0.14464077  0.30476431]
expeced value MDP LP -0.8024153454466554
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.2900322   0.26146586 -0.08393265  0.81089532  0.38588259 -0.1844333 ]
w_map [-0.59901846 -0.55919544 -0.38088646 -0.19548805 -0.37594811  0.06205068] loglik -6.867454516168436e-08
accepted/total = 1684/3000 = 0.5613333333333334
-------
true weights [-0.79956774 -0.37369359 -0.32428528 -0.04562172 -0.14464077  0.30476431]
features
4 	4 	3 	3 	2 	4 	4 	2 	
3 	3 	3 	3 	1 	4 	3 	1 	
4 	4 	3 	3 	3 	1 	1 	4 	
0 	4 	4 	1 	2 	3 	4 	0 	
1 	0 	2 	3 	2 	1 	3 	4 	
2 	4 	2 	3 	4 	2 	2 	0 	
1 	3 	3 	3 	4 	2 	4 	1 	
1 	1 	0 	0 	1 	3 	4 	5 	
optimal policy
v	>	>	v	<	v	v	<	
>	>	v	v	v	>	v	<	
^	>	>	v	v	v	v	<	
>	>	v	v	>	>	v	v	
v	v	>	v	<	>	v	<	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
optimal values
-1.17	-1.13	-1.00	-0.96	-1.28	-1.17	-1.04	-1.35	
-1.03	-1.00	-0.96	-0.93	-1.26	-1.04	-0.90	-1.27	
-1.17	-1.06	-0.93	-0.89	-0.89	-0.91	-0.87	-1.00	
-1.86	-1.07	-0.94	-0.85	-0.86	-0.54	-0.50	-1.29	
-1.31	-1.42	-0.80	-0.48	-0.80	-0.73	-0.36	-0.50	
-0.94	-0.62	-0.76	-0.44	-0.50	-0.54	-0.31	-0.87	
-0.85	-0.48	-0.44	-0.40	-0.36	-0.22	0.01	-0.07	
-1.22	-0.85	-1.24	-1.06	-0.26	0.11	0.16	0.30	
map_weights [-0.59901846 -0.55919544 -0.38088646 -0.19548805 -0.37594811  0.06205068]
MAP reward
-0.38	-0.38	-0.20	-0.20	-0.38	-0.38	-0.38	-0.38	
-0.20	-0.20	-0.20	-0.20	-0.56	-0.38	-0.20	-0.56	
-0.38	-0.38	-0.20	-0.20	-0.20	-0.56	-0.56	-0.38	
-0.60	-0.38	-0.38	-0.56	-0.38	-0.20	-0.38	-0.60	
-0.56	-0.60	-0.38	-0.20	-0.38	-0.56	-0.20	-0.38	
-0.38	-0.38	-0.38	-0.20	-0.38	-0.38	-0.38	-0.60	
-0.56	-0.20	-0.20	-0.20	-0.38	-0.38	-0.38	-0.56	
-0.56	-0.56	-0.60	-0.60	-0.56	-0.20	-0.38	0.06	
Map policy
v	>	v	v	<	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
expeced value MDP LP -1.5413904226895614
mean w [-0.50976656 -0.60125761 -0.27656321 -0.10444458 -0.2775318   0.04353081]
Mean policy from posterior
v	>	v	v	<	v	v	<	
>	>	v	v	v	>	v	<	
^	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	>	v	<	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.28	-0.28	-0.10	-0.10	-0.28	-0.28	-0.28	-0.28	
-0.10	-0.10	-0.10	-0.10	-0.60	-0.28	-0.10	-0.60	
-0.28	-0.28	-0.10	-0.10	-0.10	-0.60	-0.60	-0.28	
-0.51	-0.28	-0.28	-0.60	-0.28	-0.10	-0.28	-0.51	
-0.60	-0.51	-0.28	-0.10	-0.28	-0.60	-0.10	-0.28	
-0.28	-0.28	-0.28	-0.10	-0.28	-0.28	-0.28	-0.51	
-0.60	-0.10	-0.10	-0.10	-0.28	-0.28	-0.28	-0.60	
-0.60	-0.60	-0.51	-0.51	-0.60	-0.10	-0.28	0.04	
mean = 0.023366541344353786, map = 0.03914593739365668
CVaR policy
v	>	>	v	v	v	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	>	v	v	<	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	v	v	<	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	v	v	<	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
v	>	v	v	<	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	v	v	<	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	v	v	<	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	v	v	<	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
cvar = , 0.09876074718718053, 0.050264129856581574, 0.026611884776961414, 0.026611884774578987, 0.026611884774276784
==========
iteration 77
==========
weights [-0.23061708 -0.30649039 -0.59327348 -0.4776827  -0.23756431  0.46506819]
expeced value MDP LP -1.6328825901316075
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.13944577 -0.04013367  0.56613516 -0.48159764  0.57976994 -0.30060884]
w_map [-0.14396038 -0.23564643 -0.48517224 -0.7464306  -0.36099478  0.02963439] loglik -1.386294355614858
accepted/total = 1920/3000 = 0.64
-------
true weights [-0.23061708 -0.30649039 -0.59327348 -0.4776827  -0.23756431  0.46506819]
features
4 	2 	2 	4 	4 	3 	2 	2 	
3 	4 	0 	2 	0 	2 	2 	0 	
0 	4 	4 	4 	0 	2 	0 	3 	
0 	0 	2 	3 	3 	4 	3 	2 	
1 	0 	0 	1 	1 	0 	3 	3 	
3 	2 	4 	2 	0 	4 	0 	4 	
4 	2 	1 	1 	2 	4 	4 	1 	
0 	3 	3 	2 	3 	3 	3 	5 	
optimal policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	>	>	>	.	
optimal values
-3.09	-3.01	-2.91	-2.34	-2.12	-2.58	-2.63	-2.41	
-2.89	-2.45	-2.34	-2.48	-1.90	-2.16	-2.06	-1.83	
-2.43	-2.23	-2.13	-1.91	-1.69	-1.59	-1.48	-1.62	
-2.22	-2.01	-2.16	-1.83	-1.47	-1.00	-1.26	-1.15	
-2.09	-1.80	-1.59	-1.37	-1.07	-0.77	-0.79	-0.56	
-2.55	-2.16	-1.58	-1.36	-0.77	-0.55	-0.31	-0.09	
-2.30	-2.08	-1.50	-1.21	-0.91	-0.32	-0.09	0.15	
-2.51	-2.42	-1.97	-1.55	-0.97	-0.49	-0.02	0.47	
map_weights [-0.14396038 -0.23564643 -0.48517224 -0.7464306  -0.36099478  0.02963439]
MAP reward
-0.36	-0.49	-0.49	-0.36	-0.36	-0.75	-0.49	-0.49	
-0.75	-0.36	-0.14	-0.49	-0.14	-0.49	-0.49	-0.14	
-0.14	-0.36	-0.36	-0.36	-0.14	-0.49	-0.14	-0.75	
-0.14	-0.14	-0.49	-0.75	-0.75	-0.36	-0.75	-0.49	
-0.24	-0.14	-0.14	-0.24	-0.24	-0.14	-0.75	-0.75	
-0.75	-0.49	-0.36	-0.49	-0.14	-0.36	-0.14	-0.36	
-0.36	-0.49	-0.24	-0.24	-0.49	-0.36	-0.36	-0.24	
-0.14	-0.75	-0.75	-0.49	-0.75	-0.75	-0.75	0.03	
Map policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
expeced value MDP LP -2.1294233558135414
mean w [-0.14910389 -0.20582113 -0.5324585  -0.58575061 -0.33099196 -0.20153425]
Mean policy from posterior
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
Mean rewards
-0.33	-0.53	-0.53	-0.33	-0.33	-0.59	-0.53	-0.53	
-0.59	-0.33	-0.15	-0.53	-0.15	-0.53	-0.53	-0.15	
-0.15	-0.33	-0.33	-0.33	-0.15	-0.53	-0.15	-0.59	
-0.15	-0.15	-0.53	-0.59	-0.59	-0.33	-0.59	-0.53	
-0.21	-0.15	-0.15	-0.21	-0.21	-0.15	-0.59	-0.59	
-0.59	-0.53	-0.33	-0.53	-0.15	-0.33	-0.15	-0.33	
-0.33	-0.53	-0.21	-0.21	-0.53	-0.33	-0.33	-0.21	
-0.15	-0.59	-0.59	-0.53	-0.59	-0.59	-0.59	-0.20	
mean = 0.03241062416992069, map = 0.05444945209180463
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
cvar = , 0.05382328298728467, 0.021492643835990588, 0.028585072281208657, 0.02858507264566912, 0.02858507256958953
==========
iteration 78
==========
weights [-0.56973416 -0.21635767 -0.19048689 -0.55703297 -0.51310729  0.13690242]
expeced value MDP LP -2.1029111882317597
demonstration
[(32, 3), (40, 3), (48, 3), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.1593698   0.52285045 -0.20458775  0.74197749  0.31342008 -0.10300379]
w_map [-0.53541642 -0.27721466 -0.25292059 -0.46545504 -0.59327049 -0.0624035 ] loglik -1.8002239698944322e-09
accepted/total = 1851/3000 = 0.617
-------
true weights [-0.56973416 -0.21635767 -0.19048689 -0.55703297 -0.51310729  0.13690242]
features
0 	0 	0 	4 	1 	2 	4 	2 	
0 	4 	0 	2 	3 	3 	4 	0 	
1 	4 	3 	0 	4 	3 	3 	1 	
1 	4 	3 	0 	4 	0 	1 	4 	
3 	4 	0 	3 	0 	2 	4 	1 	
4 	0 	2 	3 	0 	1 	4 	1 	
1 	4 	0 	1 	3 	3 	0 	0 	
2 	0 	2 	2 	2 	1 	1 	5 	
optimal policy
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.27	-4.61	-4.14	-3.61	-3.13	-2.94	-2.78	-2.29	
-3.73	-4.08	-3.75	-3.22	-3.31	-2.87	-2.60	-2.12	
-3.20	-3.60	-3.21	-3.06	-2.78	-2.33	-2.10	-1.56	
-3.01	-3.12	-2.68	-2.52	-2.29	-1.80	-1.56	-1.36	
-2.82	-2.64	-2.14	-1.97	-1.80	-1.24	-1.36	-0.86	
-2.29	-2.14	-1.59	-1.43	-1.60	-1.06	-1.15	-0.65	
-1.79	-1.91	-1.41	-0.88	-1.04	-0.85	-0.65	-0.43	
-1.59	-1.41	-0.85	-0.67	-0.48	-0.30	-0.08	0.14	
map_weights [-0.53541642 -0.27721466 -0.25292059 -0.46545504 -0.59327049 -0.0624035 ]
MAP reward
-0.54	-0.54	-0.54	-0.59	-0.28	-0.25	-0.59	-0.25	
-0.54	-0.59	-0.54	-0.25	-0.47	-0.47	-0.59	-0.54	
-0.28	-0.59	-0.47	-0.54	-0.59	-0.47	-0.47	-0.28	
-0.28	-0.59	-0.47	-0.54	-0.59	-0.54	-0.28	-0.59	
-0.47	-0.59	-0.54	-0.47	-0.54	-0.25	-0.59	-0.28	
-0.59	-0.54	-0.25	-0.47	-0.54	-0.28	-0.59	-0.28	
-0.28	-0.59	-0.54	-0.28	-0.47	-0.47	-0.54	-0.54	
-0.25	-0.54	-0.25	-0.25	-0.25	-0.28	-0.28	-0.06	
Map policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6183709869393337
mean w [-0.44473169 -0.19123712 -0.13653983 -0.40899567 -0.46064575  0.17899244]
Mean policy from posterior
v	>	v	v	>	v	>	v	
v	>	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.44	-0.44	-0.44	-0.46	-0.19	-0.14	-0.46	-0.14	
-0.44	-0.46	-0.44	-0.14	-0.41	-0.41	-0.46	-0.44	
-0.19	-0.46	-0.41	-0.44	-0.46	-0.41	-0.41	-0.19	
-0.19	-0.46	-0.41	-0.44	-0.46	-0.44	-0.19	-0.46	
-0.41	-0.46	-0.44	-0.41	-0.44	-0.14	-0.46	-0.19	
-0.46	-0.44	-0.14	-0.41	-0.44	-0.19	-0.46	-0.19	
-0.19	-0.46	-0.44	-0.19	-0.41	-0.41	-0.44	-0.44	
-0.14	-0.44	-0.14	-0.14	-0.14	-0.19	-0.19	0.18	
mean = 0.01582833943176798, map = 0.01802366714352699
CVaR policy
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	>	v	
v	>	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	>	v	
v	>	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.01242156866543187, 0.01320926095481445, 0.013209288531719654, 0.015828339583891182, 0.015828339358806343
==========
iteration 79
==========
weights [-0.54965462 -0.14332739 -0.68734027 -0.28927885 -0.31835763  0.14094887]
expeced value MDP LP -1.958254054509639
demonstration
[(32, 3), (40, 3), (48, 3), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.11822031 -0.46453382  0.31611846  0.33862565  0.17641978  0.72423072]
w_map [-0.53500529 -0.34174968 -0.65052885 -0.29956573 -0.28909891  0.02170053] loglik -8.093081760307541e-12
accepted/total = 1993/3000 = 0.6643333333333333
-------
true weights [-0.54965462 -0.14332739 -0.68734027 -0.28927885 -0.31835763  0.14094887]
features
4 	0 	2 	3 	3 	4 	0 	4 	
0 	0 	4 	2 	0 	0 	2 	0 	
4 	1 	3 	2 	2 	4 	1 	4 	
0 	4 	1 	0 	4 	3 	3 	4 	
3 	4 	0 	4 	0 	2 	1 	2 	
1 	2 	0 	2 	0 	0 	1 	1 	
1 	2 	0 	1 	3 	0 	3 	0 	
3 	4 	1 	3 	1 	2 	4 	5 	
optimal policy
v	v	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	<	
v	<	>	>	>	>	v	v	
v	<	v	v	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.74	-3.69	-3.48	-2.83	-2.56	-2.30	-2.36	-2.30	
-3.46	-3.17	-2.82	-3.19	-2.53	-2.00	-1.83	-2.00	
-2.94	-2.65	-2.53	-2.80	-2.14	-1.46	-1.16	-1.46	
-3.02	-2.56	-2.26	-2.14	-1.61	-1.30	-1.02	-1.33	
-2.50	-2.79	-2.78	-2.26	-1.96	-1.42	-0.74	-1.23	
-2.23	-2.90	-2.47	-2.08	-1.69	-1.15	-0.60	-0.55	
-2.11	-2.38	-1.94	-1.41	-1.28	-1.01	-0.47	-0.41	
-1.99	-1.71	-1.41	-1.28	-1.00	-0.86	-0.18	0.14	
map_weights [-0.53500529 -0.34174968 -0.65052885 -0.29956573 -0.28909891  0.02170053]
MAP reward
-0.29	-0.54	-0.65	-0.30	-0.30	-0.29	-0.54	-0.29	
-0.54	-0.54	-0.29	-0.65	-0.54	-0.54	-0.65	-0.54	
-0.29	-0.34	-0.30	-0.65	-0.65	-0.29	-0.34	-0.29	
-0.54	-0.29	-0.34	-0.54	-0.29	-0.30	-0.30	-0.29	
-0.30	-0.29	-0.54	-0.29	-0.54	-0.65	-0.34	-0.65	
-0.34	-0.65	-0.54	-0.65	-0.54	-0.54	-0.34	-0.34	
-0.34	-0.65	-0.54	-0.34	-0.30	-0.54	-0.30	-0.54	
-0.30	-0.29	-0.34	-0.30	-0.34	-0.65	-0.29	0.02	
Map policy
v	v	>	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.582832702691769
mean w [-0.55848515 -0.1561185  -0.45909245 -0.23537181 -0.25960273  0.23578143]
Mean policy from posterior
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	<	
v	<	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.26	-0.56	-0.46	-0.24	-0.24	-0.26	-0.56	-0.26	
-0.56	-0.56	-0.26	-0.46	-0.56	-0.56	-0.46	-0.56	
-0.26	-0.16	-0.24	-0.46	-0.46	-0.26	-0.16	-0.26	
-0.56	-0.26	-0.16	-0.56	-0.26	-0.24	-0.24	-0.26	
-0.24	-0.26	-0.56	-0.26	-0.56	-0.46	-0.16	-0.46	
-0.16	-0.46	-0.56	-0.46	-0.56	-0.56	-0.16	-0.16	
-0.16	-0.46	-0.56	-0.16	-0.24	-0.56	-0.24	-0.56	
-0.24	-0.26	-0.16	-0.24	-0.16	-0.46	-0.26	0.24	
mean = 0.01669547350330558, map = 0.06652997143162032
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	v	
v	<	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	<	
v	<	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	<	
v	<	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	<	
v	<	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.04590658189378405, 0.019955538567293596, 0.016695473455782706, 0.016695473499620528, 0.016695473480555778
==========
iteration 80
==========
weights [-0.11326459 -0.49346822 -0.27584755 -0.3739412  -0.68835187  0.23218109]
expeced value MDP LP -1.878082627336454
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.30282806  0.62674603  0.21987241  0.14679489  0.29329629 -0.59964094]
w_map [-0.30789762 -0.222608   -0.17144435 -0.61358307 -0.46418249 -0.48404752] loglik -2.0794412206457764
accepted/total = 1922/3000 = 0.6406666666666667
-------
true weights [-0.11326459 -0.49346822 -0.27584755 -0.3739412  -0.68835187  0.23218109]
features
4 	0 	2 	1 	2 	3 	2 	0 	
1 	1 	4 	0 	0 	0 	2 	4 	
1 	3 	1 	0 	4 	4 	0 	1 	
2 	0 	4 	3 	4 	1 	4 	3 	
0 	3 	0 	1 	3 	0 	1 	0 	
3 	0 	4 	0 	1 	2 	2 	3 	
0 	2 	0 	2 	2 	1 	3 	4 	
0 	0 	2 	4 	1 	0 	3 	5 	
optimal policy
>	v	>	v	v	v	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-3.75	-3.09	-3.11	-2.87	-2.57	-2.57	-2.39	-2.47	
-3.38	-3.01	-3.06	-2.40	-2.31	-2.22	-2.13	-2.45	
-2.91	-2.54	-2.78	-2.31	-2.82	-2.27	-1.87	-1.78	
-2.44	-2.19	-2.62	-2.22	-2.15	-1.60	-1.95	-1.30	
-2.19	-2.10	-1.96	-1.86	-1.48	-1.12	-1.27	-0.93	
-2.10	-1.74	-2.06	-1.38	-1.50	-1.02	-0.79	-0.83	
-1.74	-1.64	-1.38	-1.28	-1.02	-0.75	-0.52	-0.46	
-1.84	-1.74	-1.64	-1.43	-0.75	-0.26	-0.14	0.23	
map_weights [-0.30789762 -0.222608   -0.17144435 -0.61358307 -0.46418249 -0.48404752]
MAP reward
-0.46	-0.31	-0.17	-0.22	-0.17	-0.61	-0.17	-0.31	
-0.22	-0.22	-0.46	-0.31	-0.31	-0.31	-0.17	-0.46	
-0.22	-0.61	-0.22	-0.31	-0.46	-0.46	-0.31	-0.22	
-0.17	-0.31	-0.46	-0.61	-0.46	-0.22	-0.46	-0.61	
-0.31	-0.61	-0.31	-0.22	-0.61	-0.31	-0.22	-0.31	
-0.61	-0.31	-0.46	-0.31	-0.22	-0.17	-0.17	-0.61	
-0.31	-0.17	-0.31	-0.17	-0.17	-0.22	-0.61	-0.46	
-0.31	-0.31	-0.17	-0.46	-0.22	-0.31	-0.61	-0.48	
Map policy
v	>	>	>	v	>	v	<	
v	>	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9672303738848353
mean w [-0.18537571 -0.39888563 -0.13680812 -0.41372671 -0.55342061 -0.04910391]
Mean policy from posterior
v	>	>	v	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.55	-0.19	-0.14	-0.40	-0.14	-0.41	-0.14	-0.19	
-0.40	-0.40	-0.55	-0.19	-0.19	-0.19	-0.14	-0.55	
-0.40	-0.41	-0.40	-0.19	-0.55	-0.55	-0.19	-0.40	
-0.14	-0.19	-0.55	-0.41	-0.55	-0.40	-0.55	-0.41	
-0.19	-0.41	-0.19	-0.40	-0.41	-0.19	-0.40	-0.19	
-0.41	-0.19	-0.55	-0.19	-0.40	-0.14	-0.14	-0.41	
-0.19	-0.14	-0.19	-0.14	-0.14	-0.40	-0.41	-0.55	
-0.19	-0.19	-0.14	-0.55	-0.40	-0.19	-0.41	-0.05	
mean = 0.037755201687584616, map = 0.1864730767352416
CVaR policy
v	>	>	>	v	>	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	>	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.1459771542166548, 0.05351242677049939, 0.03775520174271296, 0.037755202846040614, 0.037755204151882715
==========
iteration 81
==========
weights [-0.18275572 -0.13082037 -0.49748576 -0.01509867 -0.3286618   0.77055028]
expeced value MDP LP -0.13444942017680966
demonstration
[(32, 2), (24, 1), (25, 1), (26, 3), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.68050191  0.5518457   0.21284623 -0.20542443 -0.3719823  -0.08068414]
w_map [-0.27777928 -0.13718378 -0.83681452 -0.13574203 -0.4289356  -0.03673003] loglik -3.693335084875571e-06
accepted/total = 1198/3000 = 0.3993333333333333
-------
true weights [-0.18275572 -0.13082037 -0.49748576 -0.01509867 -0.3286618   0.77055028]
features
4 	4 	2 	3 	1 	1 	1 	2 	
2 	3 	1 	2 	4 	2 	0 	4 	
3 	4 	4 	3 	4 	2 	0 	0 	
4 	3 	1 	4 	4 	3 	0 	3 	
2 	2 	3 	1 	3 	4 	3 	0 	
0 	4 	3 	4 	3 	3 	0 	3 	
1 	4 	3 	4 	0 	1 	4 	0 	
0 	2 	2 	2 	3 	0 	4 	5 	
optimal policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
optimal values
-0.97	-0.65	-0.92	-0.59	-0.58	-0.46	-0.33	-0.66	
-0.82	-0.32	-0.43	-0.66	-0.67	-0.70	-0.20	-0.16	
-0.32	-0.31	-0.30	-0.16	-0.35	-0.35	-0.02	0.17	
-0.31	0.02	0.03	-0.15	-0.02	0.15	0.17	0.35	
-0.81	-0.34	0.16	0.18	0.31	0.02	0.35	0.37	
-0.36	-0.18	0.15	0.00	0.33	0.35	0.37	0.56	
-0.33	-0.20	0.13	-0.18	0.15	0.22	0.25	0.58	
-0.51	-0.69	-0.37	-0.27	0.23	0.25	0.43	0.77	
map_weights [-0.27777928 -0.13718378 -0.83681452 -0.13574203 -0.4289356  -0.03673003]
MAP reward
-0.43	-0.43	-0.84	-0.14	-0.14	-0.14	-0.14	-0.84	
-0.84	-0.14	-0.14	-0.84	-0.43	-0.84	-0.28	-0.43	
-0.14	-0.43	-0.43	-0.14	-0.43	-0.84	-0.28	-0.28	
-0.43	-0.14	-0.14	-0.43	-0.43	-0.14	-0.28	-0.14	
-0.84	-0.84	-0.14	-0.14	-0.14	-0.43	-0.14	-0.28	
-0.28	-0.43	-0.14	-0.43	-0.14	-0.14	-0.28	-0.14	
-0.14	-0.43	-0.14	-0.43	-0.28	-0.14	-0.43	-0.28	
-0.28	-0.84	-0.84	-0.84	-0.14	-0.28	-0.43	-0.04	
Map policy
>	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	>	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	^	^	>	>	>	>	.	
expeced value MDP LP -1.1004351053380519
mean w [-0.23223548 -0.09261384 -0.71666145 -0.07875325 -0.39738872  0.14116317]
Mean policy from posterior
>	v	v	>	>	>	v	v	
v	v	v	v	^	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	>	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.40	-0.40	-0.72	-0.08	-0.09	-0.09	-0.09	-0.72	
-0.72	-0.08	-0.09	-0.72	-0.40	-0.72	-0.23	-0.40	
-0.08	-0.40	-0.40	-0.08	-0.40	-0.72	-0.23	-0.23	
-0.40	-0.08	-0.09	-0.40	-0.40	-0.08	-0.23	-0.08	
-0.72	-0.72	-0.08	-0.09	-0.08	-0.40	-0.08	-0.23	
-0.23	-0.40	-0.08	-0.40	-0.08	-0.08	-0.23	-0.08	
-0.09	-0.40	-0.08	-0.40	-0.23	-0.09	-0.40	-0.23	
-0.23	-0.72	-0.72	-0.72	-0.08	-0.23	-0.40	0.14	
mean = 0.0036889041041887616, map = 0.011718540145316092
CVaR policy
>	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	>	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	>	v	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	^	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	>	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
cvar = , 0.015280780135608102, 0.011718540794519172, 0.004821765528590594, 0.004821767636183444, 0.002672271808330129
==========
iteration 82
==========
weights [-0.0970151  -0.30073936 -0.15961981 -0.07668966 -0.86338445  0.3512142 ]
expeced value MDP LP -1.1579534958900672
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.68880277 -0.25422331 -0.37738045  0.0375256  -0.54222338  0.15195684]
w_map [-0.42642824 -0.31877938 -0.43954996 -0.2434086  -0.64211274  0.22754776] loglik -9.689969715509505e-09
accepted/total = 1766/3000 = 0.5886666666666667
-------
true weights [-0.0970151  -0.30073936 -0.15961981 -0.07668966 -0.86338445  0.3512142 ]
features
0 	1 	3 	2 	0 	3 	4 	0 	
2 	2 	2 	3 	3 	0 	4 	2 	
1 	4 	0 	0 	4 	0 	4 	2 	
0 	3 	1 	4 	1 	4 	4 	3 	
1 	4 	2 	1 	0 	3 	1 	1 	
3 	3 	4 	0 	2 	1 	3 	0 	
2 	1 	2 	2 	2 	1 	4 	4 	
0 	4 	4 	2 	4 	0 	0 	5 	
optimal policy
v	>	v	v	v	v	>	v	
>	>	v	v	<	v	>	v	
v	v	v	<	v	v	>	v	
v	>	v	>	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	<	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
optimal values
-1.89	-1.87	-1.59	-1.67	-1.67	-1.61	-2.20	-1.35	
-1.81	-1.67	-1.52	-1.52	-1.59	-1.55	-2.12	-1.27	
-1.71	-2.21	-1.38	-1.46	-1.71	-1.46	-1.97	-1.12	
-1.42	-1.36	-1.30	-1.71	-0.85	-1.38	-1.67	-0.97	
-1.34	-1.84	-1.00	-0.85	-0.56	-0.52	-0.82	-0.90	
-1.05	-0.98	-1.42	-0.56	-0.47	-0.45	-0.52	-0.61	
-1.07	-0.92	-0.62	-0.47	-0.31	-0.15	-0.62	-0.52	
-1.15	-1.77	-1.48	-0.62	-0.71	0.15	0.25	0.35	
map_weights [-0.42642824 -0.31877938 -0.43954996 -0.2434086  -0.64211274  0.22754776]
MAP reward
-0.43	-0.32	-0.24	-0.44	-0.43	-0.24	-0.64	-0.43	
-0.44	-0.44	-0.44	-0.24	-0.24	-0.43	-0.64	-0.44	
-0.32	-0.64	-0.43	-0.43	-0.64	-0.43	-0.64	-0.44	
-0.43	-0.24	-0.32	-0.64	-0.32	-0.64	-0.64	-0.24	
-0.32	-0.64	-0.44	-0.32	-0.43	-0.24	-0.32	-0.32	
-0.24	-0.24	-0.64	-0.43	-0.44	-0.32	-0.24	-0.43	
-0.44	-0.32	-0.44	-0.44	-0.44	-0.32	-0.64	-0.64	
-0.43	-0.64	-0.64	-0.44	-0.64	-0.43	-0.43	0.23	
Map policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	v	>	v	v	>	v	
v	>	>	>	>	v	v	v	
>	v	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.7174170013594021
mean w [-0.28783756 -0.28379235 -0.26903308 -0.15101572 -0.65544879  0.31738809]
Mean policy from posterior
v	>	v	v	>	v	>	v	
v	>	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.29	-0.28	-0.15	-0.27	-0.29	-0.15	-0.66	-0.29	
-0.27	-0.27	-0.27	-0.15	-0.15	-0.29	-0.66	-0.27	
-0.28	-0.66	-0.29	-0.29	-0.66	-0.29	-0.66	-0.27	
-0.29	-0.15	-0.28	-0.66	-0.28	-0.66	-0.66	-0.15	
-0.28	-0.66	-0.27	-0.28	-0.29	-0.15	-0.28	-0.28	
-0.15	-0.15	-0.66	-0.29	-0.27	-0.28	-0.15	-0.29	
-0.27	-0.28	-0.27	-0.27	-0.27	-0.28	-0.66	-0.66	
-0.29	-0.66	-0.66	-0.27	-0.66	-0.29	-0.29	0.32	
mean = 0.040009653268038736, map = 0.11105165653301219
CVaR policy
v	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	>	v	
v	>	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	>	v	
v	>	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.11640791826565611, 0.0657622270916105, 0.053298766605313075, 0.040009685245667725, 0.040009637570957546
==========
iteration 83
==========
weights [-0.56886229 -0.39226077 -0.38195774 -0.41244511 -0.31038352  0.33194363]
expeced value MDP LP -2.414623707571926
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.03819368 -0.21652398 -0.42244354 -0.3923955   0.74802201 -0.24431314]
w_map [-0.67127229 -0.46858023 -0.2446859  -0.48695102 -0.18117155  0.00323496] loglik -0.6931473512936748
accepted/total = 1200/3000 = 0.4
-------
true weights [-0.56886229 -0.39226077 -0.38195774 -0.41244511 -0.31038352  0.33194363]
features
1 	3 	1 	1 	0 	3 	3 	2 	
0 	4 	0 	0 	2 	4 	0 	3 	
0 	1 	2 	2 	2 	3 	4 	0 	
0 	4 	4 	4 	0 	1 	4 	4 	
3 	0 	3 	1 	3 	0 	1 	2 	
3 	2 	3 	3 	0 	2 	1 	0 	
1 	4 	0 	0 	3 	0 	3 	3 	
1 	0 	1 	3 	4 	0 	3 	5 	
optimal policy
>	v	>	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.67	-4.33	-4.15	-3.79	-3.44	-2.93	-2.80	-2.64	
-4.48	-3.95	-3.86	-3.44	-2.90	-2.54	-2.41	-2.28	
-4.21	-3.68	-3.32	-2.97	-2.61	-2.25	-1.86	-1.88	
-3.87	-3.34	-3.06	-2.78	-2.49	-1.94	-1.56	-1.33	
-3.69	-3.47	-2.96	-2.58	-2.21	-1.81	-1.27	-1.03	
-3.32	-2.93	-2.60	-2.21	-1.81	-1.26	-0.88	-0.65	
-2.94	-2.58	-2.29	-1.91	-1.36	-1.06	-0.50	-0.08	
-2.66	-2.29	-1.74	-1.36	-0.96	-0.65	-0.08	0.33	
map_weights [-0.67127229 -0.46858023 -0.2446859  -0.48695102 -0.18117155  0.00323496]
MAP reward
-0.47	-0.49	-0.47	-0.47	-0.67	-0.49	-0.49	-0.24	
-0.67	-0.18	-0.67	-0.67	-0.24	-0.18	-0.67	-0.49	
-0.67	-0.47	-0.24	-0.24	-0.24	-0.49	-0.18	-0.67	
-0.67	-0.18	-0.18	-0.18	-0.67	-0.47	-0.18	-0.18	
-0.49	-0.67	-0.49	-0.47	-0.49	-0.67	-0.47	-0.24	
-0.49	-0.24	-0.49	-0.49	-0.67	-0.24	-0.47	-0.67	
-0.47	-0.18	-0.67	-0.67	-0.49	-0.67	-0.49	-0.49	
-0.47	-0.67	-0.47	-0.49	-0.18	-0.67	-0.49	0.00	
Map policy
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	>	>	>	v	
v	^	^	>	>	v	>	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.664644449809571
mean w [-0.59196188 -0.39631473 -0.27569407 -0.43427334 -0.19411172 -0.19293415]
Mean policy from posterior
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.40	-0.43	-0.40	-0.40	-0.59	-0.43	-0.43	-0.28	
-0.59	-0.19	-0.59	-0.59	-0.28	-0.19	-0.59	-0.43	
-0.59	-0.40	-0.28	-0.28	-0.28	-0.43	-0.19	-0.59	
-0.59	-0.19	-0.19	-0.19	-0.59	-0.40	-0.19	-0.19	
-0.43	-0.59	-0.43	-0.40	-0.43	-0.59	-0.40	-0.28	
-0.43	-0.28	-0.43	-0.43	-0.59	-0.28	-0.40	-0.59	
-0.40	-0.19	-0.59	-0.59	-0.43	-0.59	-0.43	-0.43	
-0.40	-0.59	-0.40	-0.43	-0.19	-0.59	-0.43	-0.19	
mean = 0.026296955644531383, map = 0.08747395851203343
CVaR policy
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	>	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.02853294038873022, 0.02629693320755422, 0.02629693337749206, 0.026296932855075728, 0.026296935941776844
==========
iteration 84
==========
weights [-0.05882344 -0.44639466 -0.61216112 -0.57223911 -0.20929647  0.22642385]
expeced value MDP LP -1.361321481278447
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.37758861  0.66243073  0.32204157 -0.09250901  0.48552974  0.26571518]
w_map [-0.15066564 -0.45391116 -0.58289145 -0.58859038 -0.28889827 -0.04001546] loglik -2.3170551877171874e-06
accepted/total = 1399/3000 = 0.4663333333333333
-------
true weights [-0.05882344 -0.44639466 -0.61216112 -0.57223911 -0.20929647  0.22642385]
features
2 	0 	4 	0 	3 	2 	3 	4 	
3 	1 	0 	2 	0 	4 	2 	3 	
0 	2 	0 	4 	0 	2 	4 	3 	
1 	2 	4 	2 	1 	1 	4 	3 	
2 	4 	3 	1 	4 	4 	1 	4 	
2 	2 	2 	0 	3 	4 	4 	0 	
0 	0 	1 	1 	0 	2 	4 	0 	
2 	1 	1 	4 	2 	2 	1 	5 	
optimal policy
>	>	v	<	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
optimal values
-2.33	-1.73	-1.69	-1.73	-1.82	-2.05	-2.12	-1.99	
-2.48	-1.93	-1.50	-1.86	-1.26	-1.45	-1.56	-1.80	
-2.09	-2.05	-1.45	-1.41	-1.21	-1.56	-0.96	-1.24	
-2.51	-2.24	-1.65	-1.76	-1.16	-0.96	-0.75	-0.68	
-2.51	-1.92	-1.72	-1.16	-0.72	-0.52	-0.55	-0.11	
-2.26	-2.13	-1.54	-0.93	-0.88	-0.31	-0.11	0.10	
-1.67	-1.63	-1.58	-1.15	-0.71	-0.66	-0.05	0.17	
-2.26	-2.06	-1.78	-1.35	-1.31	-0.83	-0.22	0.23	
map_weights [-0.15066564 -0.45391116 -0.58289145 -0.58859038 -0.28889827 -0.04001546]
MAP reward
-0.58	-0.15	-0.29	-0.15	-0.59	-0.58	-0.59	-0.29	
-0.59	-0.45	-0.15	-0.58	-0.15	-0.29	-0.58	-0.59	
-0.15	-0.58	-0.15	-0.29	-0.15	-0.58	-0.29	-0.59	
-0.45	-0.58	-0.29	-0.58	-0.45	-0.45	-0.29	-0.59	
-0.58	-0.29	-0.59	-0.45	-0.29	-0.29	-0.45	-0.29	
-0.58	-0.58	-0.58	-0.15	-0.59	-0.29	-0.29	-0.15	
-0.15	-0.15	-0.45	-0.45	-0.15	-0.58	-0.29	-0.15	
-0.58	-0.45	-0.45	-0.29	-0.58	-0.58	-0.45	-0.04	
Map policy
>	>	v	v	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
expeced value MDP LP -1.4675118550294286
mean w [-0.12473956 -0.40734166 -0.61017585 -0.45864928 -0.17504186  0.12815706]
Mean policy from posterior
>	>	v	<	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
Mean rewards
-0.61	-0.12	-0.18	-0.12	-0.46	-0.61	-0.46	-0.18	
-0.46	-0.41	-0.12	-0.61	-0.12	-0.18	-0.61	-0.46	
-0.12	-0.61	-0.12	-0.18	-0.12	-0.61	-0.18	-0.46	
-0.41	-0.61	-0.18	-0.61	-0.41	-0.41	-0.18	-0.46	
-0.61	-0.18	-0.46	-0.41	-0.18	-0.18	-0.41	-0.18	
-0.61	-0.61	-0.61	-0.12	-0.46	-0.18	-0.18	-0.12	
-0.12	-0.12	-0.41	-0.41	-0.12	-0.61	-0.18	-0.12	
-0.61	-0.41	-0.41	-0.18	-0.61	-0.61	-0.41	0.13	
mean = 0.0018965339739214215, map = 0.004460291571809716
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
cvar = , 0.05620158752063209, 0.04355572391511231, 0.018705238340396946, 0.003842747142726699, 0.003842747138520064
==========
iteration 85
==========
weights [-0.26584721 -0.30756227 -0.47704667 -0.36451169 -0.47507618  0.49858905]
expeced value MDP LP -1.85289582480823
demonstration
[(32, 1), (33, 3), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.90893154 -0.11242633 -0.13040196 -0.10726157 -0.26815018 -0.24655534]
w_map [-0.31029044 -0.20028372 -0.64063646 -0.47186404 -0.47255229 -0.08502856] loglik -4.668265773943858e-12
accepted/total = 2012/3000 = 0.6706666666666666
-------
true weights [-0.26584721 -0.30756227 -0.47704667 -0.36451169 -0.47507618  0.49858905]
features
1 	4 	2 	4 	4 	3 	4 	2 	
3 	1 	4 	3 	0 	3 	3 	4 	
1 	2 	4 	3 	1 	2 	4 	0 	
1 	3 	1 	3 	3 	4 	1 	4 	
0 	1 	3 	2 	3 	2 	1 	1 	
4 	1 	4 	1 	2 	2 	4 	4 	
1 	2 	2 	1 	0 	1 	3 	0 	
1 	3 	3 	3 	3 	3 	2 	5 	
optimal policy
v	v	>	v	v	v	v	v	
v	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.75	-3.85	-3.64	-3.20	-2.86	-2.65	-2.42	-2.20	
-3.48	-3.41	-3.20	-2.75	-2.41	-2.31	-1.97	-1.74	
-3.15	-3.13	-2.85	-2.46	-2.17	-2.08	-1.62	-1.28	
-2.87	-2.68	-2.40	-2.12	-1.88	-1.62	-1.16	-1.02	
-2.59	-2.34	-2.11	-1.77	-1.53	-1.33	-0.86	-0.55	
-2.51	-2.06	-1.77	-1.30	-1.18	-0.92	-0.61	-0.25	
-2.22	-1.94	-1.47	-1.01	-0.71	-0.45	-0.14	0.23	
-2.06	-1.77	-1.42	-1.07	-0.71	-0.35	0.02	0.50	
map_weights [-0.31029044 -0.20028372 -0.64063646 -0.47186404 -0.47255229 -0.08502856]
MAP reward
-0.20	-0.47	-0.64	-0.47	-0.47	-0.47	-0.47	-0.64	
-0.47	-0.20	-0.47	-0.47	-0.31	-0.47	-0.47	-0.47	
-0.20	-0.64	-0.47	-0.47	-0.20	-0.64	-0.47	-0.31	
-0.20	-0.47	-0.20	-0.47	-0.47	-0.47	-0.20	-0.47	
-0.31	-0.20	-0.47	-0.64	-0.47	-0.64	-0.20	-0.20	
-0.47	-0.20	-0.47	-0.20	-0.64	-0.64	-0.47	-0.47	
-0.20	-0.64	-0.64	-0.20	-0.31	-0.20	-0.47	-0.31	
-0.20	-0.47	-0.47	-0.47	-0.47	-0.47	-0.64	-0.09	
Map policy
v	v	v	v	v	v	v	v	
v	v	>	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	.	
expeced value MDP LP -1.9344041380306238
mean w [-0.2478999  -0.13682418 -0.61680616 -0.4413425  -0.37312744 -0.0843599 ]
Mean policy from posterior
v	v	>	>	v	>	v	v	
v	<	v	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
Mean rewards
-0.14	-0.37	-0.62	-0.37	-0.37	-0.44	-0.37	-0.62	
-0.44	-0.14	-0.37	-0.44	-0.25	-0.44	-0.44	-0.37	
-0.14	-0.62	-0.37	-0.44	-0.14	-0.62	-0.37	-0.25	
-0.14	-0.44	-0.14	-0.44	-0.44	-0.37	-0.14	-0.37	
-0.25	-0.14	-0.44	-0.62	-0.44	-0.62	-0.14	-0.14	
-0.37	-0.14	-0.37	-0.14	-0.62	-0.62	-0.37	-0.37	
-0.14	-0.62	-0.62	-0.14	-0.25	-0.14	-0.44	-0.25	
-0.14	-0.44	-0.44	-0.44	-0.44	-0.44	-0.62	-0.08	
mean = 0.08340261924972014, map = 0.03652086962475476
CVaR policy
v	v	>	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
v	<	v	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
CVaR policy
v	<	>	>	v	>	v	v	
v	<	v	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
CVaR policy
v	<	>	>	v	>	v	v	
v	<	v	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
cvar = , 0.02855733323826981, 0.04609071486041838, 0.07439296950806029, 0.07439296975908638, 0.07439296963197695
==========
iteration 86
==========
weights [-0.75097374 -0.0187764  -0.15173761 -0.13237438 -0.48458289  0.40039735]
expeced value MDP LP -1.0553840498000273
demonstration
[(32, 2), (24, 1), (25, 1), (26, 2), (18, 1), (19, 1), (20, 3), (28, 1), (29, 3), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.57637069 -0.02905017 -0.68324686 -0.39332681 -0.03628111  0.21001037]
w_map [-0.60464379 -0.07796155 -0.33782129 -0.064943   -0.68604425  0.19831881] loglik -0.4780639120407404
accepted/total = 1183/3000 = 0.3943333333333333
-------
true weights [-0.75097374 -0.0187764  -0.15173761 -0.13237438 -0.48458289  0.40039735]
features
3 	4 	3 	0 	0 	3 	3 	3 	
4 	3 	3 	2 	1 	3 	3 	0 	
2 	2 	2 	3 	1 	2 	1 	4 	
1 	2 	3 	0 	3 	2 	4 	4 	
1 	4 	4 	1 	4 	3 	1 	1 	
4 	2 	4 	2 	4 	0 	2 	0 	
0 	3 	1 	3 	1 	0 	0 	2 	
1 	2 	0 	2 	4 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	<	v	<	
>	>	>	>	v	v	<	<	
>	>	^	v	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-1.92	-1.81	-1.34	-1.83	-1.69	-1.20	-1.21	-1.33	
-1.81	-1.34	-1.21	-1.09	-0.95	-1.07	-1.08	-1.82	
-1.48	-1.35	-1.21	-1.06	-0.94	-0.95	-0.96	-1.44	
-1.47	-1.46	-1.33	-1.63	-0.93	-0.81	-1.02	-1.00	
-1.47	-1.49	-1.36	-0.89	-1.14	-0.66	-0.54	-0.52	
-1.49	-1.01	-1.22	-0.88	-1.09	-1.27	-0.53	-0.51	
-1.61	-0.87	-0.74	-0.73	-0.61	-0.86	-0.38	0.24	
-1.02	-1.01	-1.48	-0.74	-0.59	-0.11	0.38	0.40	
map_weights [-0.60464379 -0.07796155 -0.33782129 -0.064943   -0.68604425  0.19831881]
MAP reward
-0.06	-0.69	-0.06	-0.60	-0.60	-0.06	-0.06	-0.06	
-0.69	-0.06	-0.06	-0.34	-0.08	-0.06	-0.06	-0.60	
-0.34	-0.34	-0.34	-0.06	-0.08	-0.34	-0.08	-0.69	
-0.08	-0.34	-0.06	-0.60	-0.06	-0.34	-0.69	-0.69	
-0.08	-0.69	-0.69	-0.08	-0.69	-0.06	-0.08	-0.08	
-0.69	-0.34	-0.69	-0.34	-0.69	-0.60	-0.34	-0.60	
-0.60	-0.06	-0.08	-0.06	-0.08	-0.60	-0.60	-0.34	
-0.08	-0.34	-0.60	-0.34	-0.69	-0.69	-0.08	0.20	
Map policy
>	>	v	v	v	v	<	<	
>	>	>	v	v	<	<	<	
>	>	>	>	v	v	^	v	
>	>	^	>	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	^	>	>	>	.	
expeced value MDP LP -1.603845187725134
mean w [-0.57629969 -0.10322542 -0.21370831 -0.07464606 -0.67944728  0.00155992]
Mean policy from posterior
>	>	v	v	v	v	<	<	
>	>	>	v	v	v	<	<	
>	>	>	>	v	v	<	<	
>	>	^	>	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	^	>	>	>	.	
Mean rewards
-0.07	-0.68	-0.07	-0.58	-0.58	-0.07	-0.07	-0.07	
-0.68	-0.07	-0.07	-0.21	-0.10	-0.07	-0.07	-0.58	
-0.21	-0.21	-0.21	-0.07	-0.10	-0.21	-0.10	-0.68	
-0.10	-0.21	-0.07	-0.58	-0.07	-0.21	-0.68	-0.68	
-0.10	-0.68	-0.68	-0.10	-0.68	-0.07	-0.10	-0.10	
-0.68	-0.21	-0.68	-0.21	-0.68	-0.58	-0.21	-0.58	
-0.58	-0.07	-0.10	-0.07	-0.10	-0.58	-0.58	-0.21	
-0.10	-0.21	-0.58	-0.21	-0.68	-0.68	-0.10	0.00	
mean = 0.16307120085147608, map = 0.16742474750603797
CVaR policy
>	>	v	v	>	v	v	<	
>	>	>	v	v	v	<	<	
>	>	>	>	v	v	<	v	
>	>	^	>	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	<	
>	>	>	v	v	v	<	<	
>	>	>	>	v	v	<	v	
>	>	^	>	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	<	
>	>	>	v	v	v	<	<	
>	>	>	>	v	v	<	v	
>	>	^	>	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	<	
>	>	>	v	v	v	<	<	
>	>	>	>	v	v	<	v	
>	>	^	>	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	^	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	<	
>	>	>	v	v	v	<	<	
>	>	>	>	v	v	<	<	
>	>	^	>	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	^	>	>	>	.	
cvar = , 0.1471070878441143, 0.14548176336713703, 0.15351249340215212, 0.16370185955557193, 0.16307120551838628
==========
iteration 87
==========
weights [-0.02163477 -0.78721642 -0.11620583 -0.11232309 -0.39244965  0.44686155]
expeced value MDP LP -0.6133073751083885
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 3), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.01840591 -0.53119946 -0.49348567 -0.65679728  0.19613335 -0.06410376]
w_map [-0.1780165  -0.61934688 -0.35269079 -0.376719   -0.55866425  0.07940924] loglik -9.255890631720831e-08
accepted/total = 1466/3000 = 0.4886666666666667
-------
true weights [-0.02163477 -0.78721642 -0.11620583 -0.11232309 -0.39244965  0.44686155]
features
0 	2 	2 	4 	1 	4 	0 	1 	
4 	2 	0 	2 	4 	1 	4 	2 	
3 	4 	3 	0 	4 	2 	1 	0 	
4 	4 	0 	1 	1 	3 	0 	4 	
1 	2 	2 	2 	3 	2 	3 	4 	
1 	3 	3 	4 	0 	4 	2 	2 	
3 	0 	4 	4 	0 	4 	2 	1 	
2 	3 	1 	0 	0 	4 	2 	5 	
optimal policy
>	>	v	v	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	v	<	>	v	v	v	
>	>	v	v	>	>	v	<	
>	>	>	>	v	>	v	<	
>	>	^	>	v	>	v	<	
>	^	>	v	v	>	v	v	
>	^	>	>	>	>	>	.	
optimal values
-0.84	-0.83	-0.72	-1.10	-1.82	-1.35	-0.97	-1.35	
-1.10	-0.72	-0.61	-0.72	-1.05	-1.06	-0.95	-0.57	
-1.08	-0.98	-0.59	-0.61	-0.66	-0.27	-0.83	-0.46	
-1.26	-0.87	-0.49	-1.14	-0.94	-0.16	-0.05	-0.44	
-1.36	-0.58	-0.47	-0.36	-0.24	-0.14	-0.02	-0.42	
-1.46	-0.68	-0.58	-0.52	-0.13	-0.30	0.09	-0.03	
-0.80	-0.70	-0.89	-0.50	-0.11	-0.19	0.21	-0.34	
-0.91	-0.80	-0.90	-0.11	-0.09	-0.07	0.33	0.45	
map_weights [-0.1780165  -0.61934688 -0.35269079 -0.376719   -0.55866425  0.07940924]
MAP reward
-0.18	-0.35	-0.35	-0.56	-0.62	-0.56	-0.18	-0.62	
-0.56	-0.35	-0.18	-0.35	-0.56	-0.62	-0.56	-0.35	
-0.38	-0.56	-0.38	-0.18	-0.56	-0.35	-0.62	-0.18	
-0.56	-0.56	-0.18	-0.62	-0.62	-0.38	-0.18	-0.56	
-0.62	-0.35	-0.35	-0.35	-0.38	-0.35	-0.38	-0.56	
-0.62	-0.38	-0.38	-0.56	-0.18	-0.56	-0.35	-0.35	
-0.38	-0.18	-0.56	-0.56	-0.18	-0.56	-0.35	-0.62	
-0.35	-0.38	-0.62	-0.18	-0.18	-0.56	-0.35	0.08	
Map policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.02910362498692
mean w [-0.09892476 -0.48655831 -0.25196242 -0.37302518 -0.56080737 -0.13482805]
Mean policy from posterior
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.10	-0.25	-0.25	-0.56	-0.49	-0.56	-0.10	-0.49	
-0.56	-0.25	-0.10	-0.25	-0.56	-0.49	-0.56	-0.25	
-0.37	-0.56	-0.37	-0.10	-0.56	-0.25	-0.49	-0.10	
-0.56	-0.56	-0.10	-0.49	-0.49	-0.37	-0.10	-0.56	
-0.49	-0.25	-0.25	-0.25	-0.37	-0.25	-0.37	-0.56	
-0.49	-0.37	-0.37	-0.56	-0.10	-0.56	-0.25	-0.25	
-0.37	-0.10	-0.56	-0.56	-0.10	-0.56	-0.25	-0.49	
-0.25	-0.37	-0.49	-0.10	-0.10	-0.56	-0.25	-0.13	
mean = 0.11594699761400784, map = 0.11650224540287579
CVaR policy
>	>	v	v	>	>	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	<	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.16342056125278182, 0.14653014113073148, 0.1283112703710222, 0.11594699693044164, 0.1159469969464978
==========
iteration 88
==========
weights [-0.06630414 -0.07244412 -0.08869352 -0.20137503 -0.97022566  0.02448124]
expeced value MDP LP -0.928148503246208
demonstration
[(32, 1), (33, 2), (25, 1), (26, 1), (27, 2), (19, 1), (20, 1), (21, 1), (22, 3), (30, 3), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.05182758 -0.32608284  0.4724304   0.04482594  0.80935289 -0.10359504]
w_map [-0.2219932  -0.09952803 -0.37726271 -0.17897932 -0.87101848  0.08820003] loglik -4.158884360088619
accepted/total = 961/3000 = 0.32033333333333336
-------
true weights [-0.06630414 -0.07244412 -0.08869352 -0.20137503 -0.97022566  0.02448124]
features
0 	4 	4 	0 	4 	0 	1 	0 	
4 	1 	0 	4 	0 	2 	4 	3 	
0 	1 	2 	1 	2 	0 	1 	3 	
2 	1 	1 	2 	4 	4 	3 	1 	
0 	0 	4 	4 	1 	3 	1 	1 	
4 	0 	4 	3 	3 	4 	1 	0 	
1 	1 	0 	4 	0 	0 	0 	0 	
3 	1 	1 	4 	0 	4 	4 	5 	
optimal policy
v	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	>	^	v	v	v	v	
>	^	^	>	v	>	v	v	
^	^	>	>	v	v	v	v	
>	^	<	>	>	>	>	v	
^	^	^	>	^	^	>	.	
optimal values
-1.94	-1.89	-1.83	-1.73	-1.68	-0.72	-0.77	-0.71	
-1.89	-0.93	-0.87	-1.68	-0.72	-0.66	-1.48	-0.65	
-0.93	-0.87	-0.81	-0.73	-0.66	-0.58	-0.52	-0.45	
-1.01	-0.94	-0.87	-0.81	-1.47	-1.41	-0.45	-0.25	
-1.05	-0.99	-1.83	-1.47	-0.50	-0.45	-0.25	-0.18	
-2.01	-1.05	-1.60	-0.63	-0.44	-1.14	-0.18	-0.11	
-1.17	-1.11	-1.17	-1.21	-0.24	-0.17	-0.11	-0.04	
-1.36	-1.17	-1.23	-1.27	-0.30	-1.14	-0.95	0.02	
map_weights [-0.2219932  -0.09952803 -0.37726271 -0.17897932 -0.87101848  0.08820003]
MAP reward
-0.22	-0.87	-0.87	-0.22	-0.87	-0.22	-0.10	-0.22	
-0.87	-0.10	-0.22	-0.87	-0.22	-0.38	-0.87	-0.18	
-0.22	-0.10	-0.38	-0.10	-0.38	-0.22	-0.10	-0.18	
-0.38	-0.10	-0.10	-0.38	-0.87	-0.87	-0.18	-0.10	
-0.22	-0.22	-0.87	-0.87	-0.10	-0.18	-0.10	-0.10	
-0.87	-0.22	-0.87	-0.18	-0.18	-0.87	-0.10	-0.22	
-0.10	-0.10	-0.22	-0.87	-0.22	-0.22	-0.22	-0.22	
-0.18	-0.10	-0.10	-0.87	-0.22	-0.87	-0.87	0.09	
Map policy
v	v	v	>	>	>	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
>	^	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
expeced value MDP LP -1.4870255523273077
mean w [-0.15695137 -0.06079503 -0.24103595 -0.24782628 -0.77238449 -0.23422879]
Mean policy from posterior
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
>	^	^	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
Mean rewards
-0.16	-0.77	-0.77	-0.16	-0.77	-0.16	-0.06	-0.16	
-0.77	-0.06	-0.16	-0.77	-0.16	-0.24	-0.77	-0.25	
-0.16	-0.06	-0.24	-0.06	-0.24	-0.16	-0.06	-0.25	
-0.24	-0.06	-0.06	-0.24	-0.77	-0.77	-0.25	-0.06	
-0.16	-0.16	-0.77	-0.77	-0.06	-0.25	-0.06	-0.06	
-0.77	-0.16	-0.77	-0.25	-0.25	-0.77	-0.06	-0.16	
-0.06	-0.06	-0.16	-0.77	-0.16	-0.16	-0.16	-0.16	
-0.25	-0.06	-0.06	-0.77	-0.16	-0.77	-0.77	-0.23	
mean = 0.027244477154194757, map = 0.04163595724169644
CVaR policy
>	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	v	v	v	
^	^	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	>	>	.	
CVaR policy
>	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
>	^	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	^	>	^	^	>	.	
CVaR policy
>	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
>	^	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
>	^	^	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
>	^	^	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
cvar = , 0.11755753678241665, 0.03667085500192535, 0.0366708555695382, 0.027244482167264716, 0.027244481178268165
==========
iteration 89
==========
weights [-0.58568471 -0.0204329  -0.03012558 -0.5176621  -0.36880953  0.50165112]
expeced value MDP LP -1.1214171522536454
demonstration
[(32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.16998656  0.35135355  0.39326077  0.54972436 -0.52309032  0.34231691]
w_map [-0.53780469 -0.11529398 -0.34579903 -0.67764989 -0.31487021 -0.13979919] loglik -8.603798562489828e-09
accepted/total = 1473/3000 = 0.491
-------
true weights [-0.58568471 -0.0204329  -0.03012558 -0.5176621  -0.36880953  0.50165112]
features
4 	1 	4 	0 	1 	1 	3 	1 	
4 	1 	2 	3 	3 	4 	0 	2 	
1 	4 	2 	1 	3 	1 	1 	2 	
4 	0 	1 	4 	2 	0 	0 	3 	
2 	3 	3 	1 	2 	4 	0 	3 	
2 	4 	4 	1 	4 	2 	4 	3 	
1 	0 	0 	1 	4 	1 	0 	4 	
0 	1 	4 	0 	4 	4 	4 	5 	
optimal policy
>	v	v	<	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	<	<	
v	>	>	v	v	v	^	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	v	<	v	
^	>	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
optimal values
-1.46	-1.10	-1.44	-2.01	-1.59	-1.58	-1.78	-1.27	
-1.45	-1.09	-1.08	-1.55	-1.72	-1.58	-1.80	-1.26	
-1.42	-1.42	-1.06	-1.04	-1.21	-1.22	-1.23	-1.25	
-1.76	-1.62	-1.04	-1.03	-0.70	-1.23	-1.80	-1.41	
-1.41	-1.69	-1.18	-0.67	-0.68	-0.65	-1.23	-0.90	
-1.39	-1.38	-1.02	-0.65	-0.65	-0.29	-0.65	-0.39	
-1.40	-1.79	-1.22	-0.64	-0.63	-0.26	-0.46	0.13	
-1.97	-1.55	-1.55	-1.19	-0.61	-0.24	0.13	0.50	
map_weights [-0.53780469 -0.11529398 -0.34579903 -0.67764989 -0.31487021 -0.13979919]
MAP reward
-0.31	-0.12	-0.31	-0.54	-0.12	-0.12	-0.68	-0.12	
-0.31	-0.12	-0.35	-0.68	-0.68	-0.31	-0.54	-0.35	
-0.12	-0.31	-0.35	-0.12	-0.68	-0.12	-0.12	-0.35	
-0.31	-0.54	-0.12	-0.31	-0.35	-0.54	-0.54	-0.68	
-0.35	-0.68	-0.68	-0.12	-0.35	-0.31	-0.54	-0.68	
-0.35	-0.31	-0.31	-0.12	-0.31	-0.35	-0.31	-0.68	
-0.12	-0.54	-0.54	-0.12	-0.31	-0.12	-0.54	-0.31	
-0.54	-0.12	-0.31	-0.54	-0.31	-0.31	-0.31	-0.14	
Map policy
>	v	v	v	>	v	<	v	
v	v	v	v	>	v	v	v	
>	>	v	v	<	v	<	<	
>	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7307104108432116
mean w [-0.52959876 -0.10456495 -0.25214729 -0.51728412 -0.2227424  -0.25430429]
Mean policy from posterior
>	v	v	v	>	v	<	v	
v	v	v	v	>	v	v	v	
>	>	v	v	<	v	<	<	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.22	-0.10	-0.22	-0.53	-0.10	-0.10	-0.52	-0.10	
-0.22	-0.10	-0.25	-0.52	-0.52	-0.22	-0.53	-0.25	
-0.10	-0.22	-0.25	-0.10	-0.52	-0.10	-0.10	-0.25	
-0.22	-0.53	-0.10	-0.22	-0.25	-0.53	-0.53	-0.52	
-0.25	-0.52	-0.52	-0.10	-0.25	-0.22	-0.53	-0.52	
-0.25	-0.22	-0.22	-0.10	-0.22	-0.25	-0.22	-0.52	
-0.10	-0.53	-0.53	-0.10	-0.22	-0.10	-0.53	-0.22	
-0.53	-0.10	-0.22	-0.53	-0.22	-0.22	-0.22	-0.25	
mean = 0.06990888624350866, map = 0.07417425908198139
CVaR policy
>	v	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	>	v	v	v	<	v	
>	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	<	v	
v	v	v	v	>	v	v	v	
>	>	v	v	<	v	<	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	<	v	
v	v	v	v	>	v	v	v	
>	>	v	v	<	v	<	<	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.1849216896615069, 0.04648155982608704, 0.03670533487010719, 0.07505787127230401, 0.06750115786372612
==========
iteration 90
==========
weights [-0.27930672 -0.15031688 -0.42450686 -0.63538588 -0.23820077  0.50865676]
expeced value MDP LP -1.4310444342424207
demonstration
[(32, 3), (40, 1), (41, 1), (42, 3), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.20157089  0.45453917  0.62469203 -0.57634645 -0.11830521 -0.12787429]
w_map [-0.60182964 -0.13286924 -0.25252604 -0.62864148 -0.30709038 -0.25861716] loglik -0.6931471017082629
accepted/total = 1731/3000 = 0.577
-------
true weights [-0.27930672 -0.15031688 -0.42450686 -0.63538588 -0.23820077  0.50865676]
features
1 	1 	3 	3 	4 	2 	1 	1 	
3 	4 	3 	0 	3 	1 	0 	4 	
4 	1 	0 	0 	2 	3 	1 	1 	
1 	2 	1 	0 	2 	1 	4 	2 	
2 	3 	2 	1 	1 	4 	0 	3 	
4 	1 	1 	3 	0 	0 	2 	3 	
4 	4 	3 	1 	1 	2 	0 	3 	
2 	0 	2 	0 	3 	4 	0 	5 	
optimal policy
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	>	>	>	v	v	v	<	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.42	-2.29	-2.90	-2.55	-2.16	-1.94	-1.53	-1.63	
-2.77	-2.16	-2.43	-1.94	-2.15	-1.53	-1.39	-1.49	
-2.16	-1.94	-1.81	-1.67	-1.83	-1.71	-1.13	-1.27	
-2.09	-1.96	-1.55	-1.41	-1.42	-1.09	-0.99	-1.40	
-2.26	-2.17	-1.55	-1.14	-1.00	-0.95	-0.76	-1.38	
-1.85	-1.63	-1.50	-1.36	-0.86	-0.72	-0.48	-0.77	
-1.81	-1.58	-1.36	-0.73	-0.59	-0.44	-0.06	-0.13	
-2.01	-1.61	-1.34	-0.92	-0.65	-0.02	0.22	0.51	
map_weights [-0.60182964 -0.13286924 -0.25252604 -0.62864148 -0.30709038 -0.25861716]
MAP reward
-0.13	-0.13	-0.63	-0.63	-0.31	-0.25	-0.13	-0.13	
-0.63	-0.31	-0.63	-0.60	-0.63	-0.13	-0.60	-0.31	
-0.31	-0.13	-0.60	-0.60	-0.25	-0.63	-0.13	-0.13	
-0.13	-0.25	-0.13	-0.60	-0.25	-0.13	-0.31	-0.25	
-0.25	-0.63	-0.25	-0.13	-0.13	-0.31	-0.60	-0.63	
-0.31	-0.13	-0.13	-0.63	-0.60	-0.60	-0.25	-0.63	
-0.31	-0.31	-0.63	-0.13	-0.13	-0.25	-0.60	-0.63	
-0.25	-0.60	-0.25	-0.60	-0.63	-0.31	-0.60	-0.26	
Map policy
>	v	<	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	^	>	>	>	.	
expeced value MDP LP -2.343130377030796
mean w [-0.53169158 -0.12705955 -0.3429385  -0.57873211 -0.26001605 -0.08788128]
Mean policy from posterior
>	v	<	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
Mean rewards
-0.13	-0.13	-0.58	-0.58	-0.26	-0.34	-0.13	-0.13	
-0.58	-0.26	-0.58	-0.53	-0.58	-0.13	-0.53	-0.26	
-0.26	-0.13	-0.53	-0.53	-0.34	-0.58	-0.13	-0.13	
-0.13	-0.34	-0.13	-0.53	-0.34	-0.13	-0.26	-0.34	
-0.34	-0.58	-0.34	-0.13	-0.13	-0.26	-0.53	-0.58	
-0.26	-0.13	-0.13	-0.58	-0.53	-0.53	-0.34	-0.58	
-0.26	-0.26	-0.58	-0.13	-0.13	-0.34	-0.53	-0.58	
-0.34	-0.53	-0.34	-0.53	-0.58	-0.26	-0.53	-0.09	
mean = 0.16429919582471086, map = 0.1963778127376381
CVaR policy
>	v	v	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	<	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	<	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	v	<	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.19545058131756665, 0.18732295541117572, 0.1873229515105581, 0.19109911531585366, 0.19109906144784738
==========
iteration 91
==========
weights [-0.5155032  -0.0692184  -0.77005212 -0.15576571 -0.27468915  0.19174962]
expeced value MDP LP -1.6912166083940687
demonstration
[(32, 1), (33, 1), (34, 3), (42, 3), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.24012443 -0.11043782  0.60978885  0.35315453  0.56259211 -0.34215976]
w_map [-0.41335328 -0.18287206 -0.62123163 -0.3392758  -0.53839566  0.06921117] loglik -0.6931474060489862
accepted/total = 1669/3000 = 0.5563333333333333
-------
true weights [-0.5155032  -0.0692184  -0.77005212 -0.15576571 -0.27468915  0.19174962]
features
4 	3 	3 	4 	4 	2 	1 	1 	
2 	1 	1 	4 	0 	1 	0 	1 	
4 	3 	2 	2 	2 	1 	0 	1 	
0 	1 	2 	1 	0 	3 	2 	0 	
3 	2 	3 	0 	0 	0 	3 	2 	
2 	4 	3 	0 	3 	4 	2 	3 	
4 	3 	1 	0 	3 	1 	0 	2 	
4 	1 	3 	2 	1 	1 	0 	5 	
optimal policy
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	<	v	
>	v	v	v	>	v	<	<	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	<	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-2.72	-2.47	-2.42	-2.50	-2.24	-2.24	-2.04	-2.08	
-3.08	-2.34	-2.29	-2.24	-1.99	-1.49	-1.99	-2.03	
-2.65	-2.40	-2.97	-2.57	-2.19	-1.43	-1.94	-1.99	
-2.76	-2.26	-2.22	-1.82	-1.77	-1.38	-2.13	-1.99	
-2.35	-2.22	-1.46	-1.77	-1.27	-1.24	-1.38	-1.49	
-2.34	-1.58	-1.32	-1.27	-0.76	-0.73	-1.49	-0.73	
-1.58	-1.32	-1.18	-1.12	-0.61	-0.46	-0.84	-0.58	
-1.64	-1.38	-1.32	-1.22	-0.46	-0.39	-0.33	0.19	
map_weights [-0.41335328 -0.18287206 -0.62123163 -0.3392758  -0.53839566  0.06921117]
MAP reward
-0.54	-0.34	-0.34	-0.54	-0.54	-0.62	-0.18	-0.18	
-0.62	-0.18	-0.18	-0.54	-0.41	-0.18	-0.41	-0.18	
-0.54	-0.34	-0.62	-0.62	-0.62	-0.18	-0.41	-0.18	
-0.41	-0.18	-0.62	-0.18	-0.41	-0.34	-0.62	-0.41	
-0.34	-0.62	-0.34	-0.41	-0.41	-0.41	-0.34	-0.62	
-0.62	-0.54	-0.34	-0.41	-0.34	-0.54	-0.62	-0.34	
-0.54	-0.34	-0.18	-0.41	-0.34	-0.18	-0.41	-0.62	
-0.54	-0.18	-0.34	-0.62	-0.18	-0.18	-0.41	0.07	
Map policy
>	v	v	>	v	v	>	v	
>	v	>	>	>	v	>	v	
>	v	v	v	>	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7259868560631895
mean w [-0.35888336 -0.10577908 -0.57224391 -0.22388812 -0.48015564  0.05989811]
Mean policy from posterior
>	v	v	>	v	v	>	v	
>	v	<	>	>	v	<	v	
>	v	v	v	>	v	<	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.48	-0.22	-0.22	-0.48	-0.48	-0.57	-0.11	-0.11	
-0.57	-0.11	-0.11	-0.48	-0.36	-0.11	-0.36	-0.11	
-0.48	-0.22	-0.57	-0.57	-0.57	-0.11	-0.36	-0.11	
-0.36	-0.11	-0.57	-0.11	-0.36	-0.22	-0.57	-0.36	
-0.22	-0.57	-0.22	-0.36	-0.36	-0.36	-0.22	-0.57	
-0.57	-0.48	-0.22	-0.36	-0.22	-0.48	-0.57	-0.22	
-0.48	-0.22	-0.11	-0.36	-0.22	-0.11	-0.36	-0.57	
-0.48	-0.11	-0.22	-0.57	-0.11	-0.11	-0.36	0.06	
mean = 0.03272130194563028, map = 0.0453442892864917
CVaR policy
>	v	v	>	v	v	>	v	
>	v	>	>	>	v	>	v	
>	v	v	v	>	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
>	v	>	>	>	v	>	v	
>	v	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
>	v	>	>	>	v	<	v	
>	v	v	v	>	v	<	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
>	v	>	>	>	v	<	v	
>	v	v	v	>	v	<	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
>	v	>	>	>	v	<	v	
>	v	v	v	>	v	<	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.04534428910712118, 0.04534428911537214, 0.026608039581793363, 0.026608039528957184, 0.026608063792132874
==========
iteration 92
==========
weights [-0.3587393  -0.00138778 -0.51618354 -0.20340655 -0.72640482  0.18926319]
expeced value MDP LP -0.6299052228996818
demonstration
[(32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0), (32, 0)]
[-0.14089054  0.7545367  -0.33483996 -0.09884831 -0.09884567 -0.52836064]
w_map [-0.13987686  0.77856385 -0.35105172 -0.06096944 -0.00446374 -0.49729095] loglik 0.0
accepted/total = 2738/3000 = 0.9126666666666666
-------
true weights [-0.3587393  -0.00138778 -0.51618354 -0.20340655 -0.72640482  0.18926319]
features
2 	4 	4 	0 	1 	2 	3 	0 	
4 	2 	2 	3 	0 	3 	1 	2 	
1 	2 	2 	4 	4 	3 	1 	2 	
2 	1 	2 	4 	4 	3 	1 	3 	
1 	4 	2 	1 	4 	3 	1 	4 	
4 	1 	4 	1 	3 	2 	1 	3 	
1 	3 	3 	2 	4 	0 	2 	2 	
3 	3 	0 	4 	4 	0 	0 	5 	
optimal policy
v	v	>	>	^	<	v	<	
v	v	>	>	^	>	v	<	
<	<	<	^	>	>	v	<	
v	<	<	v	>	>	v	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	v	^	v	
^	<	<	^	>	>	>	.	
optimal values
-1.37	-1.88	-1.22	-0.50	-0.14	-0.65	-0.34	-0.70	
-0.86	-1.16	-1.20	-0.69	-0.50	-0.34	-0.14	-0.65	
-0.14	-0.65	-1.16	-1.41	-1.06	-0.34	-0.14	-0.65	
-0.65	-0.65	-1.16	-0.86	-1.06	-0.34	-0.14	-0.34	
-0.14	-0.86	-0.65	-0.14	-0.86	-0.34	-0.14	-0.86	
-0.86	-0.34	-0.86	-0.14	-0.34	-0.65	-0.14	-0.34	
-0.14	-0.34	-0.54	-0.65	-1.06	-0.88	-0.65	-0.33	
-0.34	-0.54	-0.89	-1.37	-1.25	-0.53	-0.17	0.19	
map_weights [-0.13987686  0.77856385 -0.35105172 -0.06096944 -0.00446374 -0.49729095]
MAP reward
-0.35	-0.00	-0.00	-0.14	0.78	-0.35	-0.06	-0.14	
-0.00	-0.35	-0.35	-0.06	-0.14	-0.06	0.78	-0.35	
0.78	-0.35	-0.35	-0.00	-0.00	-0.06	0.78	-0.35	
-0.35	0.78	-0.35	-0.00	-0.00	-0.06	0.78	-0.06	
0.78	-0.00	-0.35	0.78	-0.00	-0.06	0.78	-0.00	
-0.00	0.78	-0.00	0.78	-0.06	-0.35	0.78	-0.06	
0.78	-0.06	-0.06	-0.35	-0.00	-0.14	-0.35	-0.35	
-0.06	-0.06	-0.14	-0.00	-0.00	-0.14	-0.14	-0.50	
Map policy
v	>	>	>	^	<	v	<	
v	<	^	>	^	>	v	<	
<	<	<	v	>	>	v	<	
v	v	<	v	v	>	^	<	
<	<	>	v	<	>	^	<	
^	<	>	^	<	>	^	<	
<	<	^	^	^	^	^	^	
^	^	^	^	^	^	^	.	
expeced value MDP LP 45.27769291829388
mean w [-0.02965154  0.46755002 -0.28913219 -0.04133992 -0.18528176  0.17696748]
Mean policy from posterior
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	v	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	^	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
Mean rewards
-0.29	-0.19	-0.19	-0.03	0.47	-0.29	-0.04	-0.03	
-0.19	-0.29	-0.29	-0.04	-0.03	-0.04	0.47	-0.29	
0.47	-0.29	-0.29	-0.19	-0.19	-0.04	0.47	-0.29	
-0.29	0.47	-0.29	-0.19	-0.19	-0.04	0.47	-0.04	
0.47	-0.19	-0.29	0.47	-0.19	-0.04	0.47	-0.19	
-0.19	0.47	-0.19	0.47	-0.04	-0.29	0.47	-0.04	
0.47	-0.04	-0.04	-0.29	-0.19	-0.03	-0.29	-0.29	
-0.04	-0.04	-0.03	-0.19	-0.19	-0.03	-0.03	0.18	
mean = 0.05994785479900144, map = 0.09791384995364227
CVaR policy
v	>	>	>	^	<	v	v	
v	<	^	>	^	>	v	<	
<	<	<	v	^	>	v	<	
^	v	<	v	v	>	v	<	
<	<	>	v	<	>	^	<	
^	>	>	^	<	>	^	<	
<	<	^	^	<	^	^	<	
^	^	>	^	^	>	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	^	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	>	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
^	v	<	v	>	>	^	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	>	^	^	
^	<	<	^	^	^	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
cvar = , 0.12449306761825574, 0.06020040291763418, 0.059947049172350386, 0.05994761817709138, 0.06000179330147781
==========
iteration 93
==========
weights [-0.42813831 -0.01364042 -0.26249877 -0.12055687 -0.38381049  0.76535056]
expeced value MDP LP -0.6234165451809174
demonstration
[(32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.09177624 -0.4844469   0.39862407 -0.48991803 -0.10083834 -0.58974497]
w_map [-0.80077207 -0.21891161 -0.21163732 -0.28571323 -0.42298815 -0.07416478] loglik -0.693147049762679
accepted/total = 2006/3000 = 0.6686666666666666
-------
true weights [-0.42813831 -0.01364042 -0.26249877 -0.12055687 -0.38381049  0.76535056]
features
1 	2 	1 	0 	0 	1 	3 	4 	
1 	2 	4 	0 	4 	1 	1 	3 	
1 	2 	4 	4 	4 	0 	4 	3 	
2 	1 	2 	4 	2 	4 	2 	2 	
0 	2 	0 	2 	0 	0 	4 	3 	
0 	3 	1 	0 	3 	4 	0 	3 	
3 	1 	3 	0 	2 	2 	1 	2 	
1 	4 	0 	1 	4 	4 	4 	5 	
optimal policy
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	>	^	^	>	v	
>	v	<	v	v	>	>	v	
>	v	v	>	v	>	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
optimal values
-1.24	-1.39	-1.14	-1.14	-0.72	-0.29	-0.39	-0.64	
-1.24	-1.48	-1.46	-1.08	-0.66	-0.28	-0.27	-0.26	
-1.23	-1.23	-1.60	-1.41	-1.04	-0.71	-0.52	-0.14	
-1.23	-0.98	-1.23	-1.23	-0.86	-0.66	-0.28	-0.02	
-1.39	-0.98	-1.03	-0.86	-0.60	-0.57	-0.14	0.25	
-1.14	-0.72	-0.61	-0.60	-0.18	-0.18	0.04	0.37	
-0.72	-0.61	-0.60	-0.48	-0.06	0.21	0.48	0.50	
-0.73	-0.98	-0.83	-0.41	-0.40	-0.01	0.37	0.77	
map_weights [-0.80077207 -0.21891161 -0.21163732 -0.28571323 -0.42298815 -0.07416478]
MAP reward
-0.22	-0.21	-0.22	-0.80	-0.80	-0.22	-0.29	-0.42	
-0.22	-0.21	-0.42	-0.80	-0.42	-0.22	-0.22	-0.29	
-0.22	-0.21	-0.42	-0.42	-0.42	-0.80	-0.42	-0.29	
-0.21	-0.22	-0.21	-0.42	-0.21	-0.42	-0.21	-0.21	
-0.80	-0.21	-0.80	-0.21	-0.80	-0.80	-0.42	-0.29	
-0.80	-0.29	-0.22	-0.80	-0.29	-0.42	-0.80	-0.29	
-0.29	-0.22	-0.29	-0.80	-0.21	-0.21	-0.22	-0.21	
-0.22	-0.42	-0.80	-0.22	-0.42	-0.42	-0.42	-0.07	
Map policy
>	v	<	>	>	v	v	v	
>	v	v	>	>	>	>	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	v	>	v	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	^	>	>	.	
expeced value MDP LP -1.8314158362974706
mean w [-0.55453117 -0.20155898 -0.20646955 -0.22093919 -0.56222292 -0.02925868]
Mean policy from posterior
v	v	<	>	>	v	v	v	
v	v	<	>	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
Mean rewards
-0.20	-0.21	-0.20	-0.55	-0.55	-0.20	-0.22	-0.56	
-0.20	-0.21	-0.56	-0.55	-0.56	-0.20	-0.20	-0.22	
-0.20	-0.21	-0.56	-0.56	-0.56	-0.55	-0.56	-0.22	
-0.21	-0.20	-0.21	-0.56	-0.21	-0.56	-0.21	-0.21	
-0.55	-0.21	-0.55	-0.21	-0.55	-0.55	-0.56	-0.22	
-0.55	-0.22	-0.20	-0.55	-0.22	-0.56	-0.55	-0.22	
-0.22	-0.20	-0.22	-0.55	-0.21	-0.21	-0.20	-0.21	
-0.20	-0.56	-0.55	-0.20	-0.56	-0.56	-0.56	-0.03	
mean = 0.040092206633219996, map = 0.14984993445501638
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	>	v	>	>	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	>	v	>	>	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	^	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	^	>	>	.	
CVaR policy
v	v	<	>	>	v	v	v	
v	v	<	>	>	>	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
cvar = , 0.04000758209018074, 0.029013807892760313, 0.02203361597404352, 0.02338886135733842, 0.03932883836825074
==========
iteration 94
==========
weights [-0.2696454  -0.64597038 -0.38127187 -0.21671282 -0.12610708  0.5493432 ]
expeced value MDP LP -0.990944579120997
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.6313591   0.67887928 -0.01379869  0.1347346  -0.21824229  0.27301117]
w_map [-0.2713955  -0.5704804  -0.56766383 -0.39243723 -0.28629962 -0.2065912 ] loglik -4.293099209462525e-11
accepted/total = 1947/3000 = 0.649
-------
true weights [-0.2696454  -0.64597038 -0.38127187 -0.21671282 -0.12610708  0.5493432 ]
features
2 	2 	4 	0 	1 	1 	0 	3 	
2 	1 	0 	4 	3 	3 	1 	4 	
0 	0 	3 	1 	4 	4 	0 	4 	
4 	0 	1 	3 	3 	3 	1 	2 	
4 	0 	3 	3 	4 	4 	4 	2 	
2 	0 	4 	2 	0 	1 	0 	4 	
2 	2 	3 	1 	0 	3 	1 	3 	
3 	4 	4 	2 	0 	3 	1 	5 	
optimal policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	^	>	>	v	<	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	^	>	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.20	-1.84	-1.47	-1.36	-1.62	-1.50	-1.27	-1.01	
-1.98	-1.99	-1.36	-1.10	-0.99	-0.87	-1.44	-0.80	
-1.61	-1.64	-1.56	-1.42	-0.78	-0.66	-0.92	-0.69	
-1.36	-1.39	-1.51	-0.87	-0.66	-0.54	-0.84	-0.56	
-1.24	-1.13	-0.87	-0.66	-0.45	-0.32	-0.20	-0.19	
-1.61	-1.25	-0.99	-1.03	-0.71	-0.72	-0.07	0.20	
-1.76	-1.56	-1.19	-1.43	-0.80	-0.53	-0.32	0.33	
-1.40	-1.19	-1.08	-0.96	-0.58	-0.32	-0.10	0.55	
map_weights [-0.2713955  -0.5704804  -0.56766383 -0.39243723 -0.28629962 -0.2065912 ]
MAP reward
-0.57	-0.57	-0.29	-0.27	-0.57	-0.57	-0.27	-0.39	
-0.57	-0.57	-0.27	-0.29	-0.39	-0.39	-0.57	-0.29	
-0.27	-0.27	-0.39	-0.57	-0.29	-0.29	-0.27	-0.29	
-0.29	-0.27	-0.57	-0.39	-0.39	-0.39	-0.57	-0.57	
-0.29	-0.27	-0.39	-0.39	-0.29	-0.29	-0.29	-0.57	
-0.57	-0.27	-0.29	-0.57	-0.27	-0.57	-0.27	-0.29	
-0.57	-0.57	-0.39	-0.57	-0.27	-0.39	-0.57	-0.39	
-0.39	-0.29	-0.29	-0.57	-0.27	-0.39	-0.57	-0.21	
Map policy
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6229239271696683
mean w [-0.24777902 -0.5078457  -0.54050585 -0.24164466 -0.14615083 -0.01951978]
Mean policy from posterior
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	<	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	^	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.54	-0.54	-0.15	-0.25	-0.51	-0.51	-0.25	-0.24	
-0.54	-0.51	-0.25	-0.15	-0.24	-0.24	-0.51	-0.15	
-0.25	-0.25	-0.24	-0.51	-0.15	-0.15	-0.25	-0.15	
-0.15	-0.25	-0.51	-0.24	-0.24	-0.24	-0.51	-0.54	
-0.15	-0.25	-0.24	-0.24	-0.15	-0.15	-0.15	-0.54	
-0.54	-0.25	-0.15	-0.54	-0.25	-0.51	-0.25	-0.15	
-0.54	-0.54	-0.24	-0.51	-0.25	-0.24	-0.51	-0.24	
-0.24	-0.15	-0.15	-0.54	-0.25	-0.24	-0.51	-0.02	
mean = 0.004688843019541311, map = 0.09208623411424033
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	^	>	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.03823401227370282, 0.020593942952542132, 0.01219408247625986, 0.009745868433972404, 0.009745868316323847
==========
iteration 95
==========
weights [-0.49978479 -0.13993913 -0.09484483 -0.68582209 -0.12961334  0.48423663]
expeced value MDP LP -1.2930192472989095
demonstration
[(32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[ 0.42235738 -0.3516369  -0.07362267  0.56212915 -0.52362065  0.31996512]
w_map [-0.4897216  -0.24157972 -0.2133945  -0.77214958 -0.24417521 -0.02093496] loglik -1.1427303547861811e-10
accepted/total = 1746/3000 = 0.582
-------
true weights [-0.49978479 -0.13993913 -0.09484483 -0.68582209 -0.12961334  0.48423663]
features
1 	4 	1 	1 	0 	1 	0 	4 	
4 	4 	0 	1 	4 	0 	4 	4 	
1 	0 	1 	0 	4 	3 	2 	3 	
3 	0 	4 	2 	4 	0 	4 	3 	
0 	0 	1 	1 	2 	0 	3 	3 	
4 	3 	4 	1 	3 	0 	1 	0 	
2 	0 	0 	3 	1 	0 	4 	1 	
4 	2 	1 	0 	3 	0 	0 	5 	
optimal policy
>	>	>	v	v	v	v	v	
>	^	v	>	v	>	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	<	
>	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
optimal values
-1.98	-1.86	-1.75	-1.62	-1.86	-1.57	-1.44	-1.19	
-2.08	-1.97	-1.95	-1.50	-1.37	-1.44	-0.95	-1.07	
-2.07	-1.95	-1.46	-1.71	-1.25	-1.51	-0.83	-1.51	
-2.49	-1.82	-1.34	-1.22	-1.14	-1.24	-0.75	-1.42	
-2.24	-1.76	-1.28	-1.15	-1.02	-0.93	-0.62	-0.85	
-2.10	-2.03	-1.36	-1.24	-1.11	-0.44	0.06	-0.16	
-1.99	-2.09	-1.60	-1.11	-0.43	-0.30	0.21	0.34	
-1.92	-1.80	-1.73	-1.60	-1.11	-0.52	-0.02	0.48	
map_weights [-0.4897216  -0.24157972 -0.2133945  -0.77214958 -0.24417521 -0.02093496]
MAP reward
-0.24	-0.24	-0.24	-0.24	-0.49	-0.24	-0.49	-0.24	
-0.24	-0.24	-0.49	-0.24	-0.24	-0.49	-0.24	-0.24	
-0.24	-0.49	-0.24	-0.49	-0.24	-0.77	-0.21	-0.77	
-0.77	-0.49	-0.24	-0.21	-0.24	-0.49	-0.24	-0.77	
-0.49	-0.49	-0.24	-0.24	-0.21	-0.49	-0.77	-0.77	
-0.24	-0.77	-0.24	-0.24	-0.77	-0.49	-0.24	-0.49	
-0.21	-0.49	-0.49	-0.77	-0.24	-0.49	-0.24	-0.24	
-0.24	-0.21	-0.24	-0.49	-0.77	-0.49	-0.49	-0.02	
Map policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	v	v	>	v	<	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0478268948799836
mean w [-0.35838722 -0.17009451 -0.25161142 -0.70807357 -0.20161827 -0.22700519]
Mean policy from posterior
>	>	>	v	v	v	v	v	
v	>	v	>	v	>	v	<	
>	>	v	>	v	v	v	<	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.17	-0.20	-0.17	-0.17	-0.36	-0.17	-0.36	-0.20	
-0.20	-0.20	-0.36	-0.17	-0.20	-0.36	-0.20	-0.20	
-0.17	-0.36	-0.17	-0.36	-0.20	-0.71	-0.25	-0.71	
-0.71	-0.36	-0.20	-0.25	-0.20	-0.36	-0.20	-0.71	
-0.36	-0.36	-0.17	-0.17	-0.25	-0.36	-0.71	-0.71	
-0.20	-0.71	-0.20	-0.17	-0.71	-0.36	-0.17	-0.36	
-0.25	-0.36	-0.36	-0.71	-0.17	-0.36	-0.20	-0.17	
-0.20	-0.25	-0.17	-0.36	-0.71	-0.36	-0.36	-0.23	
mean = 0.03795403873666747, map = 0.018317206018822585
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	v	v	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	v	v	v	v	<	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	>	v	v	v	<	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	>	v	v	v	<	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.08552121270828561, 0.03518191726384545, 0.0362733313399477, 0.03665302678362159, 0.037954039426293384
==========
iteration 96
==========
weights [-0.12573145 -0.02573221 -0.16504818 -0.023658   -0.77151134  0.60041578]
expeced value MDP LP -0.11446347967932347
demonstration
[(32, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.04883762  0.50387799 -0.23338671  0.37918854 -0.56451041 -0.4762316 ]
w_map [-0.30907118 -0.18380492 -0.23261062 -0.2202074  -0.86858638 -0.11683087] loglik -1.363177091207035e-07
accepted/total = 1431/3000 = 0.477
-------
true weights [-0.12573145 -0.02573221 -0.16504818 -0.023658   -0.77151134  0.60041578]
features
3 	3 	3 	3 	3 	4 	1 	0 	
4 	4 	2 	0 	4 	2 	3 	1 	
4 	2 	0 	3 	4 	1 	2 	0 	
0 	2 	2 	3 	0 	1 	2 	1 	
2 	4 	0 	4 	4 	0 	4 	1 	
4 	2 	3 	0 	2 	0 	4 	1 	
3 	1 	2 	0 	2 	1 	2 	2 	
2 	1 	1 	3 	1 	4 	1 	5 	
optimal policy
>	>	>	v	<	>	v	v	
^	^	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	v	
^	v	v	^	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	^	>	>	.	
optimal values
-0.24	-0.22	-0.20	-0.18	-0.20	-0.64	0.13	0.06	
-1.01	-0.99	-0.32	-0.16	-0.78	-0.01	0.16	0.18	
-1.09	-0.32	-0.16	-0.03	-0.66	0.12	0.04	0.21	
-0.46	-0.34	-0.17	-0.01	0.02	0.14	0.17	0.34	
-0.62	-0.87	-0.20	-0.78	-0.66	0.11	-0.41	0.37	
-0.73	-0.10	-0.07	-0.05	0.07	0.24	-0.38	0.40	
0.04	0.07	-0.05	0.07	0.20	0.37	0.40	0.43	
-0.07	0.09	0.12	0.15	0.17	-0.21	0.57	0.60	
map_weights [-0.30907118 -0.18380492 -0.23261062 -0.2202074  -0.86858638 -0.11683087]
MAP reward
-0.22	-0.22	-0.22	-0.22	-0.22	-0.87	-0.18	-0.31	
-0.87	-0.87	-0.23	-0.31	-0.87	-0.23	-0.22	-0.18	
-0.87	-0.23	-0.31	-0.22	-0.87	-0.18	-0.23	-0.31	
-0.31	-0.23	-0.23	-0.22	-0.31	-0.18	-0.23	-0.18	
-0.23	-0.87	-0.31	-0.87	-0.87	-0.31	-0.87	-0.18	
-0.87	-0.23	-0.22	-0.31	-0.23	-0.31	-0.87	-0.18	
-0.22	-0.18	-0.23	-0.31	-0.23	-0.18	-0.23	-0.23	
-0.23	-0.18	-0.18	-0.22	-0.18	-0.87	-0.18	-0.12	
Map policy
>	>	>	v	<	v	v	v	
^	v	v	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
expeced value MDP LP -1.286740892364069
mean w [-0.25214399 -0.09749932 -0.16406812 -0.21588414 -0.82465413 -0.11509385]
Mean policy from posterior
>	>	v	v	v	v	v	v	
^	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
Mean rewards
-0.22	-0.22	-0.22	-0.22	-0.22	-0.82	-0.10	-0.25	
-0.82	-0.82	-0.16	-0.25	-0.82	-0.16	-0.22	-0.10	
-0.82	-0.16	-0.25	-0.22	-0.82	-0.10	-0.16	-0.25	
-0.25	-0.16	-0.16	-0.22	-0.25	-0.10	-0.16	-0.10	
-0.16	-0.82	-0.25	-0.82	-0.82	-0.25	-0.82	-0.10	
-0.82	-0.16	-0.22	-0.25	-0.16	-0.25	-0.82	-0.10	
-0.22	-0.10	-0.16	-0.25	-0.16	-0.10	-0.16	-0.16	
-0.16	-0.10	-0.10	-0.22	-0.10	-0.82	-0.10	-0.12	
mean = 0.07362406893696866, map = 0.040084281622942
CVaR policy
>	>	>	v	<	>	v	v	
^	>	v	v	>	v	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	v	<	v	v	v	
^	v	v	v	>	v	>	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	v	v	<	v	v	v	
^	v	v	v	>	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
cvar = , 0.023491942837242208, 0.03459759069523197, 0.058201906838049544, 0.07362406645247838, 0.07362406645492542
==========
iteration 97
==========
weights [-0.1466662  -0.34992279 -0.23201547 -0.79428403 -0.39089502  0.13610969]
expeced value MDP LP -1.9597827850887861
demonstration
[(32, 1), (33, 3), (41, 1), (42, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.55383531 -0.07542255  0.57471647 -0.25785711 -0.28045917  0.46057707]
w_map [-0.25603718 -0.43755419 -0.21261016 -0.72827276 -0.40896474 -0.01244348] loglik -6.066965596573937e-09
accepted/total = 1821/3000 = 0.607
-------
true weights [-0.1466662  -0.34992279 -0.23201547 -0.79428403 -0.39089502  0.13610969]
features
2 	0 	2 	4 	1 	2 	4 	0 	
1 	2 	4 	0 	2 	2 	1 	0 	
3 	3 	0 	2 	3 	3 	0 	4 	
2 	2 	2 	3 	3 	4 	4 	0 	
0 	2 	3 	4 	0 	4 	3 	3 	
3 	2 	2 	3 	2 	4 	1 	2 	
1 	4 	1 	0 	4 	1 	0 	2 	
3 	3 	3 	4 	4 	3 	1 	5 	
optimal policy
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
optimal values
-3.52	-3.32	-3.20	-3.00	-2.80	-2.47	-2.26	-1.89	
-3.52	-3.20	-3.00	-2.64	-2.52	-2.31	-2.10	-1.76	
-3.20	-3.08	-2.64	-2.69	-2.89	-2.51	-1.76	-1.63	
-2.43	-2.31	-2.52	-2.49	-2.11	-1.73	-1.63	-1.25	
-2.22	-2.10	-2.45	-1.71	-1.33	-1.36	-1.38	-1.12	
-2.66	-1.88	-1.67	-1.90	-1.20	-0.98	-0.59	-0.33	
-2.16	-1.83	-1.45	-1.11	-0.98	-0.59	-0.24	-0.10	
-2.93	-2.60	-2.23	-1.49	-1.36	-1.01	-0.22	0.14	
map_weights [-0.25603718 -0.43755419 -0.21261016 -0.72827276 -0.40896474 -0.01244348]
MAP reward
-0.21	-0.26	-0.21	-0.41	-0.44	-0.21	-0.41	-0.26	
-0.44	-0.21	-0.41	-0.26	-0.21	-0.21	-0.44	-0.26	
-0.73	-0.73	-0.26	-0.21	-0.73	-0.73	-0.26	-0.41	
-0.21	-0.21	-0.21	-0.73	-0.73	-0.41	-0.41	-0.26	
-0.26	-0.21	-0.73	-0.41	-0.26	-0.41	-0.73	-0.73	
-0.73	-0.21	-0.21	-0.73	-0.21	-0.41	-0.44	-0.21	
-0.44	-0.41	-0.44	-0.26	-0.41	-0.44	-0.26	-0.21	
-0.73	-0.73	-0.73	-0.41	-0.41	-0.73	-0.44	-0.01	
Map policy
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
>	v	<	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
expeced value MDP LP -1.7434460393374387
mean w [-0.15848822 -0.32416831 -0.17351728 -0.676761   -0.43800856  0.18901109]
Mean policy from posterior
>	v	v	v	>	v	>	v	
v	v	v	>	>	>	v	v	
v	v	v	<	v	>	v	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
Mean rewards
-0.17	-0.16	-0.17	-0.44	-0.32	-0.17	-0.44	-0.16	
-0.32	-0.17	-0.44	-0.16	-0.17	-0.17	-0.32	-0.16	
-0.68	-0.68	-0.16	-0.17	-0.68	-0.68	-0.16	-0.44	
-0.17	-0.17	-0.17	-0.68	-0.68	-0.44	-0.44	-0.16	
-0.16	-0.17	-0.68	-0.44	-0.16	-0.44	-0.68	-0.68	
-0.68	-0.17	-0.17	-0.68	-0.17	-0.44	-0.32	-0.17	
-0.32	-0.44	-0.32	-0.16	-0.44	-0.32	-0.16	-0.17	
-0.68	-0.68	-0.68	-0.44	-0.44	-0.68	-0.32	0.19	
mean = 0.00820499721172352, map = 0.025403551985675676
CVaR policy
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	>	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
v	v	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
v	v	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
v	v	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
cvar = , 0.018600282089644704, 0.010069138976797065, 0.005819607237770308, 0.005819596401004379, 0.005861697452706149
==========
iteration 98
==========
weights [-0.17623077 -0.53731002 -0.07272226 -0.1766744  -0.2995494   0.74431744]
expeced value MDP LP -0.31006315109263427
demonstration
[(32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.12909203  0.22780164  0.3961351  -0.65339484 -0.52254885  0.27301378]
w_map [-0.24068213 -0.37620763 -0.19067697 -0.60819794 -0.6167113   0.1180875 ] loglik -0.6931470917859839
accepted/total = 1859/3000 = 0.6196666666666667
-------
true weights [-0.17623077 -0.53731002 -0.07272226 -0.1766744  -0.2995494   0.74431744]
features
2 	3 	4 	3 	0 	1 	2 	0 	
0 	2 	0 	0 	2 	1 	2 	2 	
1 	0 	2 	1 	0 	1 	4 	3 	
3 	2 	2 	4 	4 	1 	0 	0 	
0 	1 	3 	3 	3 	3 	2 	2 	
2 	3 	3 	4 	4 	3 	0 	3 	
3 	2 	4 	2 	0 	3 	3 	2 	
0 	0 	4 	0 	4 	4 	4 	5 	
optimal policy
v	v	v	>	v	>	v	v	
>	v	v	>	v	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
optimal values
-0.99	-0.93	-0.98	-0.91	-0.74	-0.71	-0.17	-0.20	
-0.93	-0.76	-0.69	-0.74	-0.57	-0.64	-0.10	-0.03	
-1.22	-0.69	-0.52	-1.04	-0.50	-0.69	-0.15	0.04	
-0.69	-0.52	-0.45	-0.51	-0.33	-0.39	0.15	0.22	
-0.74	-0.92	-0.38	-0.21	-0.03	0.15	0.33	0.40	
-0.57	-0.50	-0.43	-0.25	-0.18	0.12	0.30	0.48	
-0.50	-0.32	-0.25	0.05	0.12	0.30	0.48	0.66	
-0.67	-0.50	-0.43	-0.13	-0.17	0.13	0.44	0.74	
map_weights [-0.24068213 -0.37620763 -0.19067697 -0.60819794 -0.6167113   0.1180875 ]
MAP reward
-0.19	-0.61	-0.62	-0.61	-0.24	-0.38	-0.19	-0.24	
-0.24	-0.19	-0.24	-0.24	-0.19	-0.38	-0.19	-0.19	
-0.38	-0.24	-0.19	-0.38	-0.24	-0.38	-0.62	-0.61	
-0.61	-0.19	-0.19	-0.62	-0.62	-0.38	-0.24	-0.24	
-0.24	-0.38	-0.61	-0.61	-0.61	-0.61	-0.19	-0.19	
-0.19	-0.61	-0.61	-0.62	-0.62	-0.61	-0.24	-0.61	
-0.61	-0.19	-0.62	-0.19	-0.24	-0.61	-0.61	-0.19	
-0.24	-0.24	-0.62	-0.24	-0.62	-0.62	-0.62	0.12	
Map policy
v	v	v	>	v	>	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
expeced value MDP LP -1.4731745072234164
mean w [-0.25699392 -0.50889777 -0.12655456 -0.37043665 -0.44861163  0.31438478]
Mean policy from posterior
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
Mean rewards
-0.13	-0.37	-0.45	-0.37	-0.26	-0.51	-0.13	-0.26	
-0.26	-0.13	-0.26	-0.26	-0.13	-0.51	-0.13	-0.13	
-0.51	-0.26	-0.13	-0.51	-0.26	-0.51	-0.45	-0.37	
-0.37	-0.13	-0.13	-0.45	-0.45	-0.51	-0.26	-0.26	
-0.26	-0.51	-0.37	-0.37	-0.37	-0.37	-0.13	-0.13	
-0.13	-0.37	-0.37	-0.45	-0.45	-0.37	-0.26	-0.37	
-0.37	-0.13	-0.45	-0.13	-0.26	-0.37	-0.37	-0.13	
-0.26	-0.26	-0.45	-0.26	-0.45	-0.45	-0.45	0.31	
mean = 0.04232852524752323, map = 0.2715826132925171
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	>	v	v	v	
>	>	v	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
cvar = , 0.10542524772257328, 0.06545517187491834, 0.05742561973170024, 0.04232852556568378, 0.04232852491675765
==========
iteration 99
==========
weights [-0.83363091 -0.26301518 -0.31569054 -0.05727325 -0.36344545  0.02914067]
expeced value MDP LP -1.533172126056173
demonstration
[(32, 3), (40, 3), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.22668968  0.40756528 -0.60087276 -0.42392161  0.338409    0.35668484]
w_map [-0.57589174 -0.17749038 -0.29932614 -0.29610863 -0.6585923  -0.16070344] loglik -3.82982534574694e-12
accepted/total = 1987/3000 = 0.6623333333333333
-------
true weights [-0.83363091 -0.26301518 -0.31569054 -0.05727325 -0.36344545  0.02914067]
features
2 	2 	2 	0 	1 	1 	3 	4 	
3 	2 	0 	1 	0 	1 	0 	4 	
1 	3 	2 	1 	1 	0 	3 	4 	
1 	2 	4 	2 	0 	1 	4 	3 	
4 	0 	0 	4 	3 	1 	4 	3 	
3 	0 	4 	0 	4 	3 	4 	2 	
2 	1 	2 	3 	2 	1 	1 	3 	
0 	3 	3 	0 	1 	2 	4 	5 	
optimal policy
v	v	<	>	>	>	>	v	
v	v	>	v	>	>	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	v	>	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
optimal values
-2.71	-2.73	-3.02	-2.86	-2.05	-1.81	-1.56	-1.52	
-2.42	-2.44	-2.87	-2.06	-2.74	-1.93	-1.68	-1.17	
-2.39	-2.15	-2.11	-1.81	-1.93	-1.68	-0.86	-0.81	
-2.39	-2.21	-1.91	-1.57	-1.73	-1.06	-0.81	-0.45	
-2.15	-2.90	-2.08	-1.26	-0.91	-0.86	-0.76	-0.40	
-1.81	-2.29	-1.57	-1.73	-0.96	-0.60	-0.65	-0.34	
-1.77	-1.47	-1.22	-0.91	-0.86	-0.55	-0.29	-0.03	
-2.13	-1.31	-1.26	-1.73	-0.90	-0.65	-0.33	0.03	
map_weights [-0.57589174 -0.17749038 -0.29932614 -0.29610863 -0.6585923  -0.16070344]
MAP reward
-0.30	-0.30	-0.30	-0.58	-0.18	-0.18	-0.30	-0.66	
-0.30	-0.30	-0.58	-0.18	-0.58	-0.18	-0.58	-0.66	
-0.18	-0.30	-0.30	-0.18	-0.18	-0.58	-0.30	-0.66	
-0.18	-0.30	-0.66	-0.30	-0.58	-0.18	-0.66	-0.30	
-0.66	-0.58	-0.58	-0.66	-0.30	-0.18	-0.66	-0.30	
-0.30	-0.58	-0.66	-0.58	-0.66	-0.30	-0.66	-0.30	
-0.30	-0.18	-0.30	-0.30	-0.30	-0.18	-0.18	-0.30	
-0.58	-0.30	-0.30	-0.58	-0.18	-0.30	-0.66	-0.16	
Map policy
v	v	>	>	>	v	<	<	
v	v	>	v	v	v	<	v	
>	>	>	>	v	v	v	v	
^	^	>	>	>	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	^	>	.	
expeced value MDP LP -1.634817867968587
mean w [-0.51451327 -0.19607478 -0.21883307 -0.18071284 -0.54737098 -0.01070253]
Mean policy from posterior
v	v	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	^	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.22	-0.22	-0.22	-0.51	-0.20	-0.20	-0.18	-0.55	
-0.18	-0.22	-0.51	-0.20	-0.51	-0.20	-0.51	-0.55	
-0.20	-0.18	-0.22	-0.20	-0.20	-0.51	-0.18	-0.55	
-0.20	-0.22	-0.55	-0.22	-0.51	-0.20	-0.55	-0.18	
-0.55	-0.51	-0.51	-0.55	-0.18	-0.20	-0.55	-0.18	
-0.18	-0.51	-0.55	-0.51	-0.55	-0.18	-0.55	-0.22	
-0.22	-0.20	-0.22	-0.18	-0.22	-0.20	-0.20	-0.18	
-0.51	-0.18	-0.18	-0.51	-0.20	-0.22	-0.55	-0.01	
mean = 0.13625495581020708, map = 0.27236634934406934
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	v	v	>	v	
v	v	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	v	v	>	v	
v	v	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
cvar = , 0.23393105924216195, 0.17820937026923422, 0.15218826393772855, 0.14475561815038573, 0.1447556216895065
