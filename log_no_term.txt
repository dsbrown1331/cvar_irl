24
==========
iteration 0
==========
weights [-0.00917575 -0.42474765  0.77745888 -0.46374879]
expeced value MDP LP 75.47870654972571
demonstration
[(24, 3), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1)]
[-0.18241148 -0.31922738 -0.19472169 -0.30363945]
w_map [-0.21692421 -0.33682493  0.03379477 -0.41245609] loglik 0.0
accepted/total = 2712/3000 = 0.904
-------
true weights [-0.00917575 -0.42474765  0.77745888 -0.46374879]
features
3 	0 	2 	1 	1 	0 	0 	
3 	0 	3 	2 	3 	2 	3 	
0 	2 	3 	3 	0 	1 	0 	
1 	3 	3 	1 	2 	3 	3 	
3 	3 	2 	2 	1 	0 	0 	
3 	3 	3 	3 	2 	1 	3 	
0 	1 	1 	3 	1 	1 	1 	
optimal policy
>	>	^	<	<	<	<	
>	^	^	^	<	<	<	
>	^	v	^	v	<	<	
^	>	v	v	<	<	<	
>	>	>	<	<	<	<	
>	^	^	^	^	<	<	
>	>	^	^	^	<	<	
optimal values
75.73	76.96	77.75	76.54	75.35	74.59	73.84	
74.95	76.18	76.50	76.56	75.33	75.35	74.13	
75.43	76.20	75.28	75.33	75.78	74.60	73.84	
74.25	75.28	76.50	76.54	76.56	75.33	74.11	
75.28	76.50	77.75	77.75	76.54	75.77	75.00	
74.06	75.28	76.50	76.50	76.56	75.37	74.15	
73.39	74.14	75.31	75.28	75.37	74.19	73.02	
map_weights [-0.21692421 -0.33682493  0.03379477 -0.41245609]
MAP reward
-0.41	-0.22	0.03	-0.34	-0.34	-0.22	-0.22	
-0.41	-0.22	-0.41	0.03	-0.41	0.03	-0.41	
-0.22	0.03	-0.41	-0.41	-0.22	-0.34	-0.22	
-0.34	-0.41	-0.41	-0.34	0.03	-0.41	-0.41	
-0.41	-0.41	0.03	0.03	-0.34	-0.22	-0.22	
-0.41	-0.41	-0.41	-0.41	0.03	-0.34	-0.41	
-0.22	-0.34	-0.34	-0.41	-0.34	-0.34	-0.34	
Map policy
>	>	^	<	<	<	<	
>	^	^	^	<	<	<	
>	^	v	^	v	<	<	
^	>	v	v	<	<	<	
>	>	>	<	<	<	<	
>	^	^	^	^	<	<	
>	>	^	^	^	<	<	
expeced value MDP LP 35.750659938013605
mean w [-0.08474135 -0.06645223  0.36710777 -0.17697287]
Mean policy from posterior
>	>	^	<	<	<	<	
>	^	^	^	<	<	<	
>	^	v	^	v	<	<	
>	>	v	v	<	<	<	
>	>	>	<	<	<	<	
>	>	^	^	^	<	<	
>	>	^	^	^	<	<	
Mean rewards
-0.18	-0.08	0.37	-0.07	-0.07	-0.08	-0.08	
-0.18	-0.08	-0.18	0.37	-0.18	0.37	-0.18	
-0.08	0.37	-0.18	-0.18	-0.08	-0.07	-0.08	
-0.07	-0.18	-0.18	-0.07	0.37	-0.18	-0.18	
-0.18	-0.18	0.37	0.37	-0.07	-0.08	-0.08	
-0.18	-0.18	-0.18	-0.18	0.37	-0.07	-0.18	
-0.08	-0.07	-0.07	-0.18	-0.07	-0.07	-0.07	
mean = 0.0030126347016192767, map = -1.0893283786117536e-07
CVaR policy
>	>	^	<	<	<	<	
>	^	^	^	<	v	<	
>	^	<	^	v	<	<	
^	^	v	v	<	<	v	
>	>	>	<	<	<	<	
>	^	^	^	^	<	<	
>	>	^	>	^	<	<	
CVaR policy
>	>	^	<	<	<	<	
>	^	^	^	<	<	<	
>	^	v	^	v	<	<	
^	>	v	v	<	<	<	
>	>	>	<	<	<	<	
>	^	^	^	^	<	<	
>	>	^	^	^	<	<	
CVaR policy
>	>	^	<	<	<	<	
>	^	^	^	<	<	<	
>	^	v	^	v	<	<	
>	>	v	v	<	<	<	
>	>	>	<	<	<	<	
>	^	^	^	^	<	<	
>	>	^	^	^	<	<	
CVaR policy
>	>	^	<	<	<	<	
>	^	^	^	<	<	<	
>	^	^	^	v	<	<	
>	>	v	v	<	<	<	
>	>	>	<	<	<	<	
>	^	^	^	^	<	<	
>	>	^	^	^	<	<	
CVaR policy
>	>	^	<	<	<	<	
>	^	^	^	<	<	<	
>	^	v	^	v	<	<	
>	>	v	v	<	<	<	
>	>	>	<	<	<	<	
>	^	^	^	^	<	<	
>	>	^	^	^	<	<	
cvar = , 0.07130516488692251, -1.2302159291266435e-07, 0.0030793551130443575, 0.003021190003863694, 0.0030134806578843154
==========
iteration 1
==========
weights [ 0.1953641  -0.61531708 -0.5486057  -0.53127163]
expeced value MDP LP 18.736983919213344
demonstration
[(24, 1), (25, 2), (18, 2), (11, 3), (18, 2), (11, 0), (10, 0), (9, 3), (16, 2), (9, 1), (10, 1), (11, 3), (18, 2), (11, 1), (12, 1), (13, 1), (13, 1), (13, 0), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 3), (18, 2), (11, 0), (10, 1), (11, 3), (18, 2), (11, 0), (10, 1), (11, 1), (12, 1), (13, 1), (13, 1), (13, 1), (13, 2), (6, 3), (13, 0), (12, 0), (11, 3), (18, 2), (11, 2), (4, 2), (4, 2), (4, 3), (11, 0), (10, 1), (11, 2)]
[-0.36423842 -0.11121701 -0.02803936 -0.49650522]
w_map [ 0.45660185  0.06134184 -0.38173421  0.1003221 ] loglik -42.583211119988846
accepted/total = 2568/3000 = 0.856
-------
true weights [ 0.1953641  -0.61531708 -0.5486057  -0.53127163]
features
1 	3 	0 	1 	0 	2 	0 	
2 	2 	0 	0 	0 	0 	0 	
1 	2 	0 	2 	0 	2 	2 	
1 	2 	2 	1 	3 	3 	0 	
0 	0 	3 	1 	3 	1 	0 	
1 	3 	3 	0 	2 	1 	0 	
2 	1 	2 	2 	2 	2 	1 	
optimal policy
>	>	v	>	^	>	>	
>	>	>	>	>	>	^	
>	>	^	>	^	^	^	
v	v	^	>	^	>	>	
<	<	<	<	^	>	>	
^	^	^	<	>	>	^	
^	^	^	^	>	>	^	
optimal values
18.01	18.81	19.54	18.73	19.54	18.79	19.54	
18.06	18.79	19.54	19.54	19.54	19.54	19.54	
17.99	18.79	19.54	18.79	19.54	18.79	18.79	
18.73	18.79	18.79	18.01	18.81	18.81	19.54	
19.54	19.54	18.81	18.01	18.09	18.73	19.54	
18.73	18.81	18.09	18.10	17.99	18.73	19.54	
17.99	18.01	17.36	17.38	17.26	17.99	18.73	
map_weights [ 0.45660185  0.06134184 -0.38173421  0.1003221 ]
MAP reward
0.06	0.10	0.46	0.06	0.46	-0.38	0.46	
-0.38	-0.38	0.46	0.46	0.46	0.46	0.46	
0.06	-0.38	0.46	-0.38	0.46	-0.38	-0.38	
0.06	-0.38	-0.38	0.06	0.10	0.10	0.46	
0.46	0.46	0.10	0.06	0.10	0.06	0.46	
0.06	0.10	0.10	0.46	-0.38	0.06	0.46	
-0.38	0.06	-0.38	-0.38	-0.38	-0.38	0.06	
Map policy
>	>	v	>	^	>	>	
^	>	>	>	>	>	^	
v	>	^	^	^	^	^	
v	v	^	>	^	>	>	
<	<	<	<	^	>	^	
^	^	^	<	>	>	^	
^	^	^	^	>	>	^	
expeced value MDP LP 37.508343673942385
mean w [ 0.37957848 -0.06240323 -0.19223229  0.09965599]
Mean policy from posterior
>	>	v	>	^	>	>	
>	>	>	>	>	>	^	
v	>	^	^	^	^	^	
v	v	^	>	^	>	>	
<	<	<	<	^	>	^	
^	^	^	<	>	>	^	
^	^	^	^	>	>	^	
Mean rewards
-0.06	0.10	0.38	-0.06	0.38	-0.19	0.38	
-0.19	-0.19	0.38	0.38	0.38	0.38	0.38	
-0.06	-0.19	0.38	-0.19	0.38	-0.19	-0.19	
-0.06	-0.19	-0.19	-0.06	0.10	0.10	0.38	
0.38	0.38	0.10	-0.06	0.10	-0.06	0.38	
-0.06	0.10	0.10	0.38	-0.19	-0.06	0.38	
-0.19	-0.06	-0.19	-0.19	-0.19	-0.19	-0.06	
mean = 0.0013478487828564312, map = 0.017229882056650325
CVaR policy
>	>	v	<	^	<	>	
>	>	>	>	>	>	^	
v	>	^	<	^	<	^	
v	v	^	>	^	>	v	
<	<	<	<	^	>	>	
^	^	<	<	<	>	^	
^	^	^	^	^	^	^	
CVaR policy
>	>	v	<	^	<	>	
>	>	>	>	>	>	^	
v	>	^	<	^	<	^	
v	v	^	>	^	>	>	
<	<	<	<	^	>	^	
^	^	<	<	>	>	^	
^	^	^	^	>	>	^	
CVaR policy
>	>	v	>	^	>	>	
>	>	>	>	>	>	^	
v	>	^	^	^	^	^	
v	v	^	>	^	>	>	
<	<	<	<	^	>	>	
^	^	<	<	>	>	^	
^	^	^	^	>	>	^	
CVaR policy
>	>	v	>	^	>	>	
>	>	>	>	>	>	^	
v	>	^	^	^	^	^	
v	v	^	>	^	>	>	
<	<	<	<	^	>	^	
^	^	^	<	>	>	^	
^	^	^	^	>	>	^	
CVaR policy
>	>	v	>	^	>	>	
>	>	>	>	>	>	^	
v	>	^	^	^	^	^	
v	v	^	>	^	>	>	
<	<	<	<	^	>	^	
^	^	^	<	>	>	^	
^	^	^	^	>	>	^	
cvar = , 0.026309304471574535, 0.0013480006149144685, 0.0013483491886461252, 0.001349675851461285, 0.0013906936831489247
==========
iteration 2
==========
weights [ 0.29777347 -0.46131039 -0.57747524  0.6041904 ]
expeced value MDP LP 59.771802504050875
demonstration
[(24, 2), (17, 1), (18, 0), (17, 3), (24, 2), (17, 2), (10, 3), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 1), (18, 0), (17, 1), (18, 0), (17, 3), (24, 2), (17, 1), (18, 0), (17, 3), (24, 2), (17, 1), (18, 0), (17, 1), (18, 0), (17, 3), (24, 2), (17, 2), (10, 2), (3, 3), (10, 3), (17, 1), (18, 0), (17, 3), (24, 2), (17, 1), (18, 0), (17, 1), (18, 0), (17, 3), (24, 2), (17, 1), (18, 0), (17, 3), (24, 2), (17, 2), (10, 2)]
[-0.30384125 -0.33153325 -0.35791273 -0.00671278]
w_map [-0.52132962 -0.00740224 -0.13996796  0.33130019] loglik -29.4269657227278
accepted/total = 2774/3000 = 0.9246666666666666
-------
true weights [ 0.29777347 -0.46131039 -0.57747524  0.6041904 ]
features
3 	2 	3 	3 	3 	0 	2 	
0 	2 	1 	3 	2 	2 	2 	
2 	0 	0 	3 	3 	2 	1 	
0 	0 	1 	3 	2 	3 	0 	
1 	3 	3 	1 	0 	3 	0 	
3 	2 	3 	3 	0 	1 	0 	
0 	2 	1 	1 	0 	0 	0 	
optimal policy
<	>	^	<	<	<	<	
^	<	>	^	<	^	v	
^	>	>	^	<	<	v	
>	v	>	^	^	v	<	
>	>	v	^	>	^	<	
<	>	^	<	<	^	^	
^	<	^	^	^	<	^	
optimal values
60.42	59.24	60.42	60.42	60.42	60.11	58.93	
60.11	58.93	59.35	60.42	59.24	58.93	57.88	
58.93	59.81	60.11	60.42	60.42	59.24	59.05	
59.81	60.11	59.35	60.42	59.24	60.42	60.11	
59.35	60.42	60.42	59.35	60.11	60.42	60.11	
60.42	59.24	60.42	60.42	60.11	59.35	59.81	
60.11	58.93	59.35	59.35	59.81	59.51	59.51	
map_weights [-0.52132962 -0.00740224 -0.13996796  0.33130019]
MAP reward
0.33	-0.14	0.33	0.33	0.33	-0.52	-0.14	
-0.52	-0.14	-0.01	0.33	-0.14	-0.14	-0.14	
-0.14	-0.52	-0.52	0.33	0.33	-0.14	-0.01	
-0.52	-0.52	-0.01	0.33	-0.14	0.33	-0.52	
-0.01	0.33	0.33	-0.01	-0.52	0.33	-0.52	
0.33	-0.14	0.33	0.33	-0.52	-0.01	-0.52	
-0.52	-0.14	-0.01	-0.01	-0.52	-0.52	-0.52	
Map policy
<	>	>	<	<	<	<	
^	>	^	^	^	v	v	
^	^	>	^	<	<	<	
v	v	>	^	<	v	<	
>	>	<	^	>	^	<	
<	^	^	<	<	^	<	
^	>	^	^	<	^	<	
expeced value MDP LP 34.46207692030658
mean w [-0.19891555 -0.05387331 -0.07468059  0.34925862]
Mean policy from posterior
<	>	>	^	<	<	<	
^	>	^	^	<	<	v	
^	>	>	^	<	<	<	
v	v	>	^	^	v	<	
>	>	v	^	>	^	<	
<	^	^	<	<	^	<	
^	>	^	^	<	^	^	
Mean rewards
0.35	-0.07	0.35	0.35	0.35	-0.20	-0.07	
-0.20	-0.07	-0.05	0.35	-0.07	-0.07	-0.07	
-0.07	-0.20	-0.20	0.35	0.35	-0.07	-0.05	
-0.20	-0.20	-0.05	0.35	-0.07	0.35	-0.20	
-0.05	0.35	0.35	-0.05	-0.20	0.35	-0.20	
0.35	-0.07	0.35	0.35	-0.20	-0.05	-0.20	
-0.20	-0.07	-0.05	-0.05	-0.20	-0.20	-0.20	
mean = 0.15394774001004663, map = 0.19294352026368244
CVaR policy
<	>	>	^	<	<	<	
^	^	>	^	v	<	<	
^	>	>	^	<	<	<	
v	v	>	^	<	v	<	
>	>	v	^	>	^	<	
<	^	^	<	<	^	<	
^	^	^	^	<	^	<	
CVaR policy
<	>	>	^	<	<	<	
^	^	>	^	<	<	<	
^	>	>	^	<	<	<	
v	v	>	^	>	v	<	
>	>	v	^	>	^	<	
<	>	^	<	<	^	<	
^	^	^	^	<	^	<	
CVaR policy
<	>	>	^	<	<	<	
^	^	^	^	<	<	<	
^	>	>	^	<	<	<	
v	v	>	^	^	v	<	
>	>	v	^	>	^	<	
<	^	^	<	<	^	<	
^	^	^	^	<	^	<	
CVaR policy
<	>	>	<	<	<	<	
^	>	^	^	<	<	v	
^	>	>	^	<	<	<	
v	v	>	^	^	v	<	
>	>	v	^	>	^	<	
<	^	^	<	<	^	<	
^	>	^	^	<	^	<	
CVaR policy
<	>	>	<	<	<	<	
^	>	^	^	<	<	v	
^	>	>	^	<	<	<	
v	v	>	^	^	v	<	
>	>	v	^	>	^	<	
<	^	^	<	<	^	<	
^	>	^	^	<	^	<	
cvar = , 0.16098977538569414, 0.16098876186390498, 0.16098887432688258, 0.15394776834325086, 0.15394789363962502
==========
iteration 3
==========
weights [-0.02192491  0.23971168 -0.96891096 -0.05717651]
expeced value MDP LP 23.374817467053397
demonstration
[(24, 2), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 0), (15, 2), (8, 3), (15, 1), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 0), (15, 1), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 0), (15, 2), (8, 3), (15, 2), (8, 3), (15, 1), (16, 0), (15, 2), (8, 3), (15, 1), (16, 1), (17, 0), (16, 0), (15, 1), (16, 1), (17, 0), (16, 0), (15, 2), (8, 3), (15, 2), (8, 3)]
[-0.30248343 -0.02755932 -0.43289295 -0.23706429]
w_map [ 0.03079099  0.33516429 -0.57788959  0.05615513] loglik -20.101268236221586
accepted/total = 2768/3000 = 0.9226666666666666
-------
true weights [-0.02192491  0.23971168 -0.96891096 -0.05717651]
features
1 	3 	2 	2 	0 	2 	0 	
2 	1 	0 	3 	0 	0 	1 	
0 	1 	1 	1 	0 	3 	0 	
3 	2 	0 	3 	2 	0 	0 	
3 	0 	2 	1 	0 	3 	0 	
3 	2 	0 	0 	3 	0 	2 	
3 	3 	0 	0 	1 	1 	1 	
optimal policy
<	<	v	v	v	>	v	
^	v	<	v	v	>	>	
>	^	<	<	<	<	^	
^	^	^	^	^	>	^	
^	<	^	^	<	v	^	
^	>	>	v	v	v	v	
>	>	>	>	>	>	>	
optimal values
23.97	23.67	22.50	22.47	23.19	22.50	23.71	
22.76	23.97	23.71	23.67	23.45	23.71	23.97	
23.71	23.97	23.97	23.97	23.71	23.42	23.71	
23.42	22.76	23.71	23.67	22.50	23.19	23.45	
23.12	22.87	22.50	23.68	23.42	23.42	23.19	
22.84	21.99	23.19	23.45	23.67	23.71	22.76	
22.87	23.16	23.45	23.71	23.97	23.97	23.97	
map_weights [ 0.03079099  0.33516429 -0.57788959  0.05615513]
MAP reward
0.34	0.06	-0.58	-0.58	0.03	-0.58	0.03	
-0.58	0.34	0.03	0.06	0.03	0.03	0.34	
0.03	0.34	0.34	0.34	0.03	0.06	0.03	
0.06	-0.58	0.03	0.06	-0.58	0.03	0.03	
0.06	0.03	-0.58	0.34	0.03	0.06	0.03	
0.06	-0.58	0.03	0.03	0.06	0.03	-0.58	
0.06	0.06	0.03	0.03	0.34	0.34	0.34	
Map policy
<	<	<	v	v	>	v	
^	v	<	v	<	>	>	
>	^	<	<	<	<	^	
^	^	^	^	<	v	^	
^	<	>	^	<	v	<	
^	>	>	^	v	v	v	
>	>	>	>	>	>	>	
expeced value MDP LP 31.27774426670721
mean w [-0.20159659  0.31993887 -0.1085445  -0.16099636]
Mean policy from posterior
<	<	<	v	<	>	v	
^	v	<	v	<	>	>	
>	^	<	<	<	<	^	
>	^	^	^	<	<	^	
^	^	>	^	<	v	v	
>	^	^	^	v	v	v	
>	>	>	>	>	>	>	
Mean rewards
0.32	-0.16	-0.11	-0.11	-0.20	-0.11	-0.20	
-0.11	0.32	-0.20	-0.16	-0.20	-0.20	0.32	
-0.20	0.32	0.32	0.32	-0.20	-0.16	-0.20	
-0.16	-0.11	-0.20	-0.16	-0.11	-0.20	-0.20	
-0.16	-0.20	-0.11	0.32	-0.20	-0.16	-0.20	
-0.16	-0.11	-0.20	-0.20	-0.16	-0.20	-0.11	
-0.16	-0.16	-0.20	-0.20	0.32	0.32	0.32	
mean = 0.17240018585839678, map = 0.006855431117756439
CVaR policy
<	<	v	v	v	>	v	
^	v	v	v	v	>	>	
>	^	<	<	<	<	^	
^	^	^	^	^	>	^	
^	^	^	^	<	v	v	
^	^	>	v	v	v	v	
>	>	>	>	>	>	>	
CVaR policy
<	<	<	v	<	>	v	
^	v	<	v	<	>	>	
>	^	<	<	<	<	^	
>	^	^	^	<	<	^	
^	^	>	^	<	v	v	
^	^	^	^	v	v	v	
>	>	>	>	>	>	>	
CVaR policy
<	<	<	v	<	>	v	
^	v	v	v	<	>	>	
>	^	<	<	<	<	^	
>	^	^	^	<	<	^	
^	^	>	^	<	v	v	
>	^	^	^	v	v	v	
>	>	>	>	>	>	>	
CVaR policy
<	<	<	v	<	>	v	
^	v	v	v	<	>	>	
>	^	<	<	<	<	^	
>	^	^	^	<	<	^	
^	^	>	^	<	v	v	
>	^	^	^	v	v	v	
>	>	>	>	>	>	>	
CVaR policy
<	<	<	v	<	>	v	
^	v	v	v	<	>	>	
>	^	<	<	<	<	^	
>	^	^	^	<	<	^	
^	^	>	^	<	v	v	
>	^	^	^	v	v	v	
>	>	>	>	>	>	>	
cvar = , 0.03496163398796526, 0.1546814272992556, 0.17240043783864678, 0.17240012516748848, 0.17239953379206696
==========
iteration 4
==========
weights [-0.48213527  0.26438736 -0.77117724  0.32082795]
expeced value MDP LP 31.43697425155206
demonstration
[(24, 2), (17, 1), (18, 0), (17, 1), (18, 1), (19, 3), (26, 2), (19, 0), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 1), (19, 0), (18, 1), (19, 0), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 1), (19, 0), (18, 0), (17, 1), (18, 1), (19, 3), (26, 2), (19, 0), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 3), (26, 2)]
[-0.11348209 -0.2693404  -0.18537647 -0.43180103]
w_map [-0.39487833 -0.28260106 -0.01477406  0.30774655] loglik -22.180709435018343
accepted/total = 2722/3000 = 0.9073333333333333
-------
true weights [-0.48213527  0.26438736 -0.77117724  0.32082795]
features
1 	2 	1 	1 	0 	0 	2 	
3 	0 	2 	1 	2 	0 	3 	
2 	1 	0 	3 	3 	3 	0 	
2 	1 	2 	0 	1 	3 	1 	
2 	0 	1 	1 	1 	2 	1 	
2 	0 	1 	2 	1 	0 	1 	
1 	3 	3 	2 	0 	2 	0 	
optimal policy
v	<	>	v	<	v	v	
<	<	>	v	v	>	>	
^	>	>	>	>	<	^	
>	^	v	^	>	^	<	
>	>	v	<	^	^	^	
v	v	v	<	^	<	^	
>	v	<	<	^	^	^	
optimal values
32.03	30.93	31.92	31.97	31.17	30.48	30.99	
32.08	31.28	30.93	32.03	30.99	31.28	32.08	
30.99	31.23	31.28	32.08	32.08	32.08	31.28	
30.10	31.18	30.88	31.28	32.03	32.08	32.03	
30.09	31.17	31.97	31.92	31.97	30.99	31.97	
30.93	31.28	32.03	30.93	31.92	31.11	31.92	
32.03	32.08	32.08	30.99	31.11	30.03	31.11	
map_weights [-0.39487833 -0.28260106 -0.01477406  0.30774655]
MAP reward
-0.28	-0.01	-0.28	-0.28	-0.39	-0.39	-0.01	
0.31	-0.39	-0.01	-0.28	-0.01	-0.39	0.31	
-0.01	-0.28	-0.39	0.31	0.31	0.31	-0.39	
-0.01	-0.28	-0.01	-0.39	-0.28	0.31	-0.28	
-0.01	-0.39	-0.28	-0.28	-0.28	-0.01	-0.28	
-0.01	-0.39	-0.28	-0.01	-0.28	-0.39	-0.28	
-0.28	0.31	0.31	-0.01	-0.39	-0.01	-0.39	
Map policy
v	<	v	v	v	>	v	
<	<	>	v	v	v	>	
^	<	>	>	>	<	<	
^	<	>	^	^	^	<	
^	v	v	v	>	^	<	
v	v	v	v	<	^	^	
>	v	<	<	<	^	<	
expeced value MDP LP 34.03473786218933
mean w [-0.05959458 -0.09996939 -0.15180011  0.34695332]
Mean policy from posterior
v	v	>	v	v	v	v	
<	<	v	v	v	>	>	
^	>	>	>	>	<	<	
^	v	>	^	^	^	<	
>	v	v	^	^	^	^	
>	v	v	<	^	^	^	
>	>	v	<	<	<	^	
Mean rewards
-0.10	-0.15	-0.10	-0.10	-0.06	-0.06	-0.15	
0.35	-0.06	-0.15	-0.10	-0.15	-0.06	0.35	
-0.15	-0.10	-0.06	0.35	0.35	0.35	-0.06	
-0.15	-0.10	-0.15	-0.06	-0.10	0.35	-0.10	
-0.15	-0.06	-0.10	-0.10	-0.10	-0.15	-0.10	
-0.15	-0.06	-0.10	-0.15	-0.10	-0.06	-0.10	
-0.10	0.35	0.35	-0.15	-0.06	-0.15	-0.06	
mean = 0.19553574204352486, map = 0.41529329510154867
CVaR policy
v	v	>	v	v	v	v	
<	<	v	v	v	>	>	
^	>	>	>	>	<	^	
^	v	>	^	^	^	<	
>	v	v	^	^	^	^	
>	v	v	<	^	^	^	
>	>	v	<	<	<	^	
CVaR policy
v	v	>	v	v	v	v	
<	<	v	v	v	>	>	
^	>	>	>	>	<	^	
^	v	>	^	^	^	<	
>	v	v	^	^	^	^	
>	v	v	<	^	^	^	
>	>	v	<	<	<	^	
CVaR policy
v	v	>	v	v	v	v	
<	<	v	v	v	>	>	
^	>	>	>	>	<	^	
^	v	^	^	^	^	<	
>	v	v	^	^	^	^	
>	v	v	<	^	^	^	
>	>	v	<	<	<	^	
CVaR policy
v	v	>	v	v	v	v	
<	<	v	v	v	>	>	
^	>	>	>	>	<	^	
^	v	^	^	^	^	<	
>	v	v	^	^	^	^	
>	v	v	<	^	^	^	
>	>	v	<	<	<	^	
CVaR policy
v	v	>	v	v	v	v	
<	<	v	v	v	>	>	
^	>	>	>	>	<	^	
^	v	^	^	^	^	<	
>	v	v	^	^	^	^	
>	v	v	<	^	^	^	
>	>	v	<	<	<	^	
cvar = , 0.19553574651537886, 0.19554212692424144, 0.19553579114744934, 0.19553851014778445, 0.19554099850787665
==========
iteration 5
==========
weights [-0.71653661  0.05383171  0.10267869  0.68784775]
expeced value MDP LP 67.69370480991465
demonstration
[(24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 0), (16, 1), (17, 2), (10, 3), (17, 2), (10, 3), (17, 3), (24, 2), (17, 0), (16, 2), (9, 1), (10, 3), (17, 3), (24, 2), (17, 3), (24, 2), (17, 0), (16, 1), (17, 2), (10, 3), (17, 2), (10, 0), (9, 3), (16, 2), (9, 0), (8, 1), (9, 0), (8, 1), (9, 1), (10, 1), (11, 0), (10, 3), (17, 3), (24, 2), (17, 2), (10, 1), (11, 0), (10, 1), (11, 0), (10, 3), (17, 2), (10, 3)]
[-0.10194816 -0.65714269 -0.00390663 -0.23700252]
w_map [ 0.15251825 -0.04937542  0.19932774  0.5987786 ] loglik -37.92818195961627
accepted/total = 2823/3000 = 0.941
-------
true weights [-0.71653661  0.05383171  0.10267869  0.68784775]
features
1 	2 	1 	1 	1 	0 	0 	
0 	3 	3 	3 	3 	2 	3 	
0 	2 	3 	3 	0 	2 	2 	
0 	3 	2 	3 	2 	3 	0 	
1 	0 	0 	2 	0 	2 	1 	
3 	1 	0 	1 	3 	0 	2 	
3 	2 	1 	1 	0 	1 	2 	
optimal policy
>	v	v	v	v	v	v	
>	>	>	<	<	<	>	
>	^	^	^	<	v	^	
>	^	^	^	<	<	<	
v	^	^	^	<	^	<	
v	<	<	^	<	^	^	
<	<	<	<	^	^	^	
optimal values
67.57	68.20	68.15	68.15	68.15	66.80	67.38	
67.38	68.78	68.78	68.78	68.78	68.20	68.78	
66.80	68.20	68.78	68.78	67.38	67.63	68.20	
66.81	68.21	68.20	68.78	68.20	68.21	66.81	
68.15	66.81	66.80	68.20	66.80	67.63	67.00	
68.78	68.15	66.75	67.57	67.58	66.23	66.44	
68.78	68.20	67.57	66.95	66.19	65.62	65.87	
map_weights [ 0.15251825 -0.04937542  0.19932774  0.5987786 ]
MAP reward
-0.05	0.20	-0.05	-0.05	-0.05	0.15	0.15	
0.15	0.60	0.60	0.60	0.60	0.20	0.60	
0.15	0.20	0.60	0.60	0.15	0.20	0.20	
0.15	0.60	0.20	0.60	0.20	0.60	0.15	
-0.05	0.15	0.15	0.20	0.15	0.20	-0.05	
0.60	-0.05	0.15	-0.05	0.60	0.15	0.20	
0.60	0.20	-0.05	-0.05	0.15	-0.05	0.20	
Map policy
>	v	v	v	v	v	v	
>	>	>	<	<	<	>	
>	>	^	^	<	v	^	
>	^	^	^	<	<	<	
v	^	^	^	<	^	<	
v	<	<	^	^	^	<	
<	<	<	<	^	^	^	
expeced value MDP LP 42.21929397956662
mean w [-0.10300217 -0.03296334 -0.06887045  0.42928261]
Mean policy from posterior
>	v	v	v	v	<	v	
>	>	>	<	<	<	>	
>	>	^	^	<	v	^	
v	^	^	^	<	<	<	
v	<	^	^	^	^	<	
v	<	<	^	<	<	^	
<	<	<	<	^	<	<	
Mean rewards
-0.03	-0.07	-0.03	-0.03	-0.03	-0.10	-0.10	
-0.10	0.43	0.43	0.43	0.43	-0.07	0.43	
-0.10	-0.07	0.43	0.43	-0.10	-0.07	-0.07	
-0.10	0.43	-0.07	0.43	-0.07	0.43	-0.10	
-0.03	-0.10	-0.10	-0.07	-0.10	-0.07	-0.03	
0.43	-0.03	-0.10	-0.03	0.43	-0.10	-0.07	
0.43	-0.07	-0.03	-0.03	-0.10	-0.03	-0.07	
mean = 0.022143787864621345, map = 0.06194704401821127
CVaR policy
>	v	v	v	v	v	v	
>	>	<	<	<	>	>	
>	>	^	^	<	v	^	
>	>	^	^	<	<	<	
v	^	^	^	<	^	<	
v	<	<	^	<	^	^	
<	<	<	<	^	^	^	
CVaR policy
>	v	v	v	v	<	v	
>	>	>	>	<	<	>	
>	^	^	^	<	v	^	
>	>	>	^	<	<	<	
v	^	^	^	^	^	<	
v	<	<	^	<	<	^	
<	<	<	<	^	^	^	
CVaR policy
>	v	v	v	v	<	v	
>	>	>	<	<	<	>	
>	>	^	^	<	v	^	
v	>	^	^	<	<	<	
v	v	^	^	^	^	<	
v	<	<	^	<	<	^	
<	<	<	<	^	<	^	
CVaR policy
>	v	v	v	v	<	v	
>	>	>	<	<	<	>	
>	>	^	^	<	v	^	
v	^	^	^	<	<	<	
v	<	^	^	^	^	<	
v	<	<	^	<	<	^	
<	<	<	<	^	<	^	
CVaR policy
>	v	v	v	v	<	v	
>	>	>	<	<	<	>	
>	>	^	^	<	v	^	
v	^	^	^	<	<	<	
v	<	^	^	^	^	<	
v	<	<	^	<	<	^	
<	<	<	<	^	<	^	
cvar = , 1.9525279526533268e-07, 0.0032225923190907224, 0.004906269789287876, 0.004910719997951674, 0.004906426893697358
==========
iteration 6
==========
weights [ 0.34196559 -0.50094678 -0.61396432  0.50513333]
expeced value MDP LP 49.409210213993305
demonstration
[(24, 1), (25, 2), (18, 2), (11, 0), (10, 2), (3, 0), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)]
[-0.46272652 -0.0657803  -0.0014717  -0.47002148]
w_map [ 0.21520137 -0.01999481 -0.31433723  0.45046659] loglik -0.6931471354628229
accepted/total = 2065/3000 = 0.6883333333333334
-------
true weights [ 0.34196559 -0.50094678 -0.61396432  0.50513333]
features
1 	2 	3 	0 	0 	0 	0 	
3 	3 	1 	0 	3 	0 	2 	
1 	1 	2 	2 	0 	2 	2 	
2 	3 	2 	3 	0 	0 	2 	
2 	2 	3 	2 	0 	1 	1 	
0 	1 	0 	2 	2 	1 	2 	
0 	3 	0 	1 	0 	0 	1 	
optimal policy
v	v	^	<	<	<	<	
<	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	^	<	
v	v	v	<	^	v	<	
>	v	<	<	<	<	<	
optimal values
49.51	49.39	50.51	50.35	50.19	50.03	49.87	
50.51	50.51	49.51	50.19	50.19	50.03	48.92	
49.51	49.51	48.40	49.07	50.03	48.92	47.81	
48.41	49.52	49.08	49.88	49.87	49.72	48.61	
49.07	49.08	50.19	49.08	49.72	48.72	47.73	
50.19	49.51	50.19	49.07	48.61	48.05	46.96	
50.35	50.51	50.35	49.35	49.19	49.04	48.05	
map_weights [ 0.21520137 -0.01999481 -0.31433723  0.45046659]
MAP reward
-0.02	-0.31	0.45	0.22	0.22	0.22	0.22	
0.45	0.45	-0.02	0.22	0.45	0.22	-0.31	
-0.02	-0.02	-0.31	-0.31	0.22	-0.31	-0.31	
-0.31	0.45	-0.31	0.45	0.22	0.22	-0.31	
-0.31	-0.31	0.45	-0.31	0.22	-0.02	-0.02	
0.22	-0.02	0.22	-0.31	-0.31	-0.02	-0.31	
0.22	0.45	0.22	-0.02	0.22	0.22	-0.02	
Map policy
v	>	^	<	<	<	<	
<	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	<	<	
v	v	v	<	v	v	v	
>	v	<	<	<	<	<	
expeced value MDP LP 29.872928086749624
mean w [ 0.15407899 -0.12795066 -0.29293039  0.30551543]
Mean policy from posterior
v	v	^	<	<	<	<	
>	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	<	<	
v	v	v	<	v	v	<	
>	v	<	<	<	<	<	
Mean rewards
-0.13	-0.29	0.31	0.15	0.15	0.15	0.15	
0.31	0.31	-0.13	0.15	0.31	0.15	-0.29	
-0.13	-0.13	-0.29	-0.29	0.15	-0.29	-0.29	
-0.29	0.31	-0.29	0.31	0.15	0.15	-0.29	
-0.29	-0.29	0.31	-0.29	0.15	-0.13	-0.13	
0.15	-0.13	0.15	-0.29	-0.29	-0.13	-0.29	
0.15	0.31	0.15	-0.13	0.15	0.15	-0.13	
mean = 0.010558131078383326, map = 0.01055840963452681
CVaR policy
v	v	^	<	<	<	<	
<	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	^	<	
v	v	v	<	^	v	v	
>	v	<	<	<	<	<	
CVaR policy
v	v	^	<	<	<	<	
>	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	<	<	
v	v	v	<	^	v	v	
>	v	<	<	<	<	<	
CVaR policy
v	v	^	<	<	<	<	
<	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	^	<	
v	v	v	<	v	v	v	
>	v	<	<	<	<	<	
CVaR policy
v	v	^	<	<	<	<	
<	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	<	<	
v	v	v	<	v	v	v	
>	v	<	<	<	<	<	
CVaR policy
v	v	^	<	<	<	<	
<	<	<	^	<	<	<	
^	^	^	^	^	^	<	
>	^	v	>	^	<	<	
v	>	v	<	^	<	<	
v	v	v	<	v	v	v	
>	v	<	<	<	<	<	
cvar = , 2.0678037060406496e-08, 1.694782897487812e-05, 0.010574049361416371, 0.010647027490399807, 0.010558659365329959
==========
iteration 7
==========
weights [ 0.90093951 -0.41617166 -0.12278139  0.00581983]
expeced value MDP LP 87.83789576388453
demonstration
[(24, 1), (25, 3), (32, 1), (33, 3), (40, 1), (41, 1), (41, 1), (41, 0), (40, 1), (41, 1), (41, 0), (40, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 0), (40, 1), (41, 0), (40, 1), (41, 0), (40, 1), (41, 0), (40, 1), (41, 1), (41, 1), (41, 0), (40, 1), (41, 0), (40, 1), (41, 0), (40, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 0), (40, 1), (41, 1), (41, 1), (41, 0), (40, 1), (41, 0), (40, 1), (41, 0)]
[-0.25604539 -0.45675448 -0.2795361  -0.00766402]
w_map [ 0.37324727 -0.51616509 -0.05203636 -0.05855128] loglik -22.180709525478505
accepted/total = 2553/3000 = 0.851
-------
true weights [ 0.90093951 -0.41617166 -0.12278139  0.00581983]
features
0 	3 	1 	1 	2 	0 	0 	
0 	3 	1 	0 	3 	1 	1 	
1 	2 	1 	1 	2 	0 	1 	
1 	3 	2 	2 	2 	1 	2 	
1 	2 	2 	1 	2 	3 	3 	
2 	1 	2 	2 	1 	0 	0 	
2 	1 	3 	3 	1 	1 	3 	
optimal policy
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	>	^	<	
^	^	<	>	v	v	v	
^	^	>	>	>	v	v	
^	>	>	>	>	>	>	
^	>	>	^	>	^	^	
optimal values
90.09	89.20	87.89	87.76	89.07	90.09	90.09	
90.09	89.20	87.89	88.20	88.19	88.78	88.78	
88.78	88.18	86.89	86.91	87.78	88.79	87.49	
87.47	87.31	86.31	86.18	87.18	87.89	88.18	
86.18	86.31	85.89	86.89	88.18	89.20	89.20	
85.20	85.48	86.77	87.77	88.78	90.09	90.09	
84.22	84.75	86.03	86.89	87.47	88.78	89.20	
map_weights [ 0.37324727 -0.51616509 -0.05203636 -0.05855128]
MAP reward
0.37	-0.06	-0.52	-0.52	-0.05	0.37	0.37	
0.37	-0.06	-0.52	0.37	-0.06	-0.52	-0.52	
-0.52	-0.05	-0.52	-0.52	-0.05	0.37	-0.52	
-0.52	-0.06	-0.05	-0.05	-0.05	-0.52	-0.05	
-0.52	-0.05	-0.05	-0.52	-0.05	-0.06	-0.06	
-0.05	-0.52	-0.05	-0.05	-0.52	0.37	0.37	
-0.05	-0.52	-0.06	-0.06	-0.52	-0.52	-0.06	
Map policy
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	^	^	v	
^	^	<	>	v	v	v	
>	^	^	>	>	v	v	
^	^	>	>	>	>	>	
^	>	^	^	>	^	^	
expeced value MDP LP 39.245053973545694
mean w [ 0.40209906 -0.28368379  0.05767197  0.06519771]
Mean policy from posterior
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	^	^	v	
^	^	<	>	v	v	v	
>	^	^	>	>	v	v	
^	^	>	>	>	>	>	
^	>	>	^	>	^	^	
Mean rewards
0.40	0.07	-0.28	-0.28	0.06	0.40	0.40	
0.40	0.07	-0.28	0.40	0.07	-0.28	-0.28	
-0.28	0.06	-0.28	-0.28	0.06	0.40	-0.28	
-0.28	0.07	0.06	0.06	0.06	-0.28	0.06	
-0.28	0.06	0.06	-0.28	0.06	0.07	0.07	
0.06	-0.28	0.06	0.06	-0.28	0.40	0.40	
0.06	-0.28	0.07	0.07	-0.28	-0.28	0.07	
mean = 0.11487136667632569, map = 0.1200424399911384
CVaR policy
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	^	^	v	
^	^	<	>	v	v	v	
>	^	^	>	>	v	v	
>	^	>	>	>	>	>	
^	>	^	^	>	^	^	
CVaR policy
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	^	^	v	
^	^	<	>	v	v	v	
>	^	^	>	>	v	v	
>	^	>	>	>	>	>	
^	>	>	^	>	^	^	
CVaR policy
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	^	^	v	
^	^	<	>	v	v	v	
>	^	^	>	>	v	v	
>	^	>	>	>	>	>	
^	>	>	^	>	^	^	
CVaR policy
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	^	^	v	
^	^	<	>	v	v	v	
>	^	^	>	>	v	v	
^	^	>	>	>	>	>	
^	>	>	^	>	^	^	
CVaR policy
<	<	<	>	>	>	>	
^	<	<	>	^	^	^	
^	^	<	^	^	^	v	
^	^	<	>	v	v	v	
>	^	^	>	>	v	v	
>	^	>	>	>	>	>	
^	>	>	^	>	^	^	
cvar = , 0.12004199251791192, 0.11487198538871723, 0.11487302568097846, 0.114893791151232, 0.11487068189789795
==========
iteration 8
==========
weights [ 0.62432117  0.41297858  0.64064346 -0.17101964]
expeced value MDP LP 63.63690272436194
demonstration
[(24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3)]
[-0.31039894 -0.13602222 -0.2011516  -0.35242724]
w_map [-0.48494016  0.01878839  0.32147135 -0.1748001 ] loglik 0.0
accepted/total = 2725/3000 = 0.9083333333333333
-------
true weights [ 0.62432117  0.41297858  0.64064346 -0.17101964]
features
2 	3 	2 	0 	0 	2 	2 	
0 	1 	1 	3 	2 	0 	1 	
2 	0 	0 	0 	1 	0 	2 	
3 	0 	0 	2 	1 	0 	3 	
1 	0 	3 	2 	3 	3 	1 	
1 	3 	2 	0 	3 	3 	1 	
3 	1 	1 	3 	3 	0 	1 	
optimal policy
<	>	^	<	>	>	>	
^	<	^	>	>	^	^	
<	<	>	v	^	>	>	
^	>	>	v	<	^	^	
>	^	>	^	<	^	^	
^	>	>	^	<	<	^	
>	>	^	^	<	>	^	
optimal values
64.06	63.25	64.06	64.05	64.05	64.06	64.06	
64.05	63.82	63.84	63.24	64.05	64.05	63.84	
64.06	64.05	64.03	64.05	63.82	64.05	64.06	
63.25	64.03	64.05	64.06	63.84	64.03	63.25	
63.79	64.02	63.25	64.06	63.25	63.22	63.03	
63.56	63.24	64.05	64.05	63.24	62.43	62.82	
62.79	63.60	63.82	63.24	62.43	62.60	62.60	
map_weights [-0.48494016  0.01878839  0.32147135 -0.1748001 ]
MAP reward
0.32	-0.17	0.32	-0.48	-0.48	0.32	0.32	
-0.48	0.02	0.02	-0.17	0.32	-0.48	0.02	
0.32	-0.48	-0.48	-0.48	0.02	-0.48	0.32	
-0.17	-0.48	-0.48	0.32	0.02	-0.48	-0.17	
0.02	-0.48	-0.17	0.32	-0.17	-0.17	0.02	
0.02	-0.17	0.32	-0.48	-0.17	-0.17	0.02	
-0.17	0.02	0.02	-0.17	-0.17	-0.48	0.02	
Map policy
<	<	^	<	>	>	>	
^	>	^	<	v	^	^	
<	<	^	v	v	>	>	
^	<	>	v	<	<	^	
^	>	>	^	<	<	^	
^	>	^	^	^	^	^	
>	>	^	<	^	>	^	
expeced value MDP LP 30.328091390949453
mean w [-0.14612647 -0.10656623  0.31023993 -0.20957891]
Mean policy from posterior
<	<	^	<	>	>	>	
^	>	^	<	^	^	^	
<	<	^	v	v	>	>	
^	^	>	v	<	<	^	
^	>	>	^	<	<	^	
^	>	>	^	<	<	^	
>	>	^	^	<	<	^	
Mean rewards
0.31	-0.21	0.31	-0.15	-0.15	0.31	0.31	
-0.15	-0.11	-0.11	-0.21	0.31	-0.15	-0.11	
0.31	-0.15	-0.15	-0.15	-0.11	-0.15	0.31	
-0.21	-0.15	-0.15	0.31	-0.11	-0.15	-0.21	
-0.11	-0.15	-0.21	0.31	-0.21	-0.21	-0.11	
-0.11	-0.21	0.31	-0.15	-0.21	-0.21	-0.11	
-0.21	-0.11	-0.11	-0.21	-0.21	-0.15	-0.11	
mean = 0.08690994431369603, map = 0.25587516390732645
CVaR policy
<	<	^	<	>	>	>	
^	>	^	<	^	^	^	
<	<	^	v	v	>	>	
^	<	>	v	<	<	^	
^	>	>	^	<	<	^	
^	>	^	^	^	^	^	
>	>	^	^	^	^	^	
CVaR policy
<	<	^	<	>	>	>	
^	>	^	<	>	^	^	
<	<	^	v	v	>	>	
^	<	>	v	<	<	^	
^	>	>	^	<	<	^	
^	>	^	^	^	<	^	
>	>	^	^	^	^	^	
CVaR policy
<	<	^	<	>	>	>	
^	>	^	<	^	^	^	
<	<	^	v	v	>	>	
^	^	>	v	<	<	^	
^	>	>	^	<	<	^	
^	>	>	^	<	<	^	
>	>	^	^	<	<	^	
CVaR policy
<	<	^	<	>	>	>	
^	>	^	<	^	^	^	
<	<	^	v	v	>	>	
^	^	>	v	<	<	^	
^	>	>	^	<	<	^	
^	>	>	^	<	<	^	
>	>	^	^	<	<	^	
CVaR policy
<	<	^	<	>	>	>	
^	>	^	<	>	^	^	
<	<	^	v	v	>	>	
^	^	>	v	<	<	^	
^	>	>	^	<	<	^	
^	>	>	^	<	<	^	
>	>	^	^	<	<	^	
cvar = , 0.24584174387142355, 0.24582513162656738, 0.08691004077017794, 0.08691449126281015, 0.08691153096342674
==========
iteration 9
==========
weights [-0.41180261 -0.364063    0.21330105  0.80770007]
expeced value MDP LP 79.52987210009503
demonstration
[(24, 1), (25, 1), (26, 1), (27, 3), (34, 1), (34, 2), (27, 3), (34, 1), (34, 2), (27, 0), (26, 1), (27, 3), (34, 2), (27, 1), (27, 0), (26, 2), (19, 3), (26, 1), (27, 0), (26, 1), (27, 0), (26, 1), (27, 1), (27, 3), (34, 1), (34, 1), (34, 2), (27, 3), (34, 1), (34, 2), (27, 0), (26, 1), (27, 1), (27, 0), (26, 2), (19, 3), (26, 1), (27, 3), (34, 1), (34, 1), (34, 2), (27, 0), (26, 1), (27, 3), (34, 1), (34, 2), (27, 0), (26, 2), (19, 3)]
[-0.22581948 -0.2638623  -0.03956783 -0.47075039]
w_map [ 0.02607912 -0.13164131  0.03208094  0.81019863] loglik -37.796845778973875
accepted/total = 2689/3000 = 0.8963333333333333
-------
true weights [-0.41180261 -0.364063    0.21330105  0.80770007]
features
2 	1 	3 	3 	2 	0 	2 	
0 	0 	2 	2 	1 	0 	1 	
1 	1 	2 	0 	2 	3 	2 	
1 	3 	1 	1 	2 	3 	3 	
0 	0 	2 	1 	1 	2 	3 	
1 	0 	3 	2 	3 	0 	2 	
1 	3 	2 	2 	3 	0 	2 	
optimal policy
>	>	>	<	<	<	v	
^	>	^	^	<	v	v	
>	>	^	^	>	v	v	
>	>	v	>	>	>	v	
v	>	v	v	v	>	>	
v	v	>	>	v	<	^	
>	v	<	>	v	<	^	
optimal values
79.02	79.60	80.77	80.77	80.18	78.96	78.43	
77.81	78.96	80.18	80.18	79.01	79.55	79.01	
77.28	78.43	79.59	78.96	80.18	80.77	80.18	
77.31	78.46	78.43	79.01	80.18	80.77	80.77	
77.24	78.39	79.59	79.01	79.60	80.18	80.77	
78.44	79.55	80.18	80.18	80.77	79.55	80.18	
79.60	80.77	80.18	80.18	80.77	79.55	79.59	
map_weights [ 0.02607912 -0.13164131  0.03208094  0.81019863]
MAP reward
0.03	-0.13	0.81	0.81	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	-0.13	0.03	-0.13	
-0.13	-0.13	0.03	0.03	0.03	0.81	0.03	
-0.13	0.81	-0.13	-0.13	0.03	0.81	0.81	
0.03	0.03	0.03	-0.13	-0.13	0.03	0.81	
-0.13	0.03	0.81	0.03	0.81	0.03	0.03	
-0.13	0.81	0.03	0.03	0.81	0.03	0.03	
Map policy
>	>	>	^	<	<	<	
>	>	^	^	v	v	v	
^	v	^	>	>	v	v	
>	v	v	>	>	>	>	
>	v	v	v	v	>	^	
>	v	>	>	v	<	^	
>	v	<	>	v	<	^	
expeced value MDP LP 33.70437060574232
mean w [-0.09556502 -0.25641865 -0.08084992  0.34351155]
Mean policy from posterior
>	>	>	^	<	<	<	
>	>	^	^	v	v	v	
^	>	^	>	>	v	v	
>	v	v	>	>	>	>	
>	v	v	v	v	>	>	
>	v	>	>	v	<	^	
>	v	<	>	v	<	^	
Mean rewards
-0.08	-0.26	0.34	0.34	-0.08	-0.10	-0.08	
-0.10	-0.10	-0.08	-0.08	-0.26	-0.10	-0.26	
-0.26	-0.26	-0.08	-0.10	-0.08	0.34	-0.08	
-0.26	0.34	-0.26	-0.26	-0.08	0.34	0.34	
-0.10	-0.10	-0.08	-0.26	-0.26	-0.08	0.34	
-0.26	-0.10	0.34	-0.08	0.34	-0.10	-0.08	
-0.26	0.34	-0.08	-0.08	0.34	-0.10	-0.08	
mean = 0.022869711669400772, map = 0.04751757804122292
CVaR policy
>	>	>	^	<	<	<	
>	>	^	^	v	v	v	
^	>	^	>	>	v	<	
>	v	v	>	>	>	v	
>	v	v	v	v	^	^	
>	v	>	>	v	<	^	
>	v	<	>	v	<	^	
CVaR policy
>	>	>	^	<	<	<	
>	>	^	^	v	v	v	
^	>	^	>	>	v	v	
>	v	v	>	>	>	v	
>	v	v	v	v	>	^	
>	v	>	>	v	<	^	
>	v	<	>	v	<	^	
CVaR policy
>	>	>	^	<	<	<	
>	>	^	^	v	v	v	
^	>	^	>	>	v	v	
>	v	v	>	>	>	v	
>	v	v	v	v	^	>	
>	v	>	>	v	<	^	
>	v	<	>	v	<	^	
CVaR policy
>	>	>	^	<	<	<	
>	>	^	^	v	v	v	
^	>	^	>	>	v	v	
>	v	v	>	>	>	>	
>	v	v	v	v	>	>	
>	v	>	>	v	<	^	
>	v	<	>	v	<	^	
CVaR policy
>	>	>	^	<	<	<	
>	>	^	^	v	v	v	
^	>	^	>	>	v	v	
>	v	v	>	>	>	>	
>	v	v	v	v	^	>	
>	v	>	>	v	<	^	
>	v	<	>	v	<	^	
cvar = , 0.02286912595964452, 0.02290343648026294, 0.02288375900336348, 0.02291284339598576, 0.023479398719601363
==========
iteration 10
==========
weights [-0.10322599 -0.16076929 -0.47255796 -0.86034098]
expeced value MDP LP -10.696544676574527
demonstration
[(24, 2), (17, 2), (10, 1), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0)]
[-0.31643796 -0.19272049 -0.44725912 -0.04358243]
w_map [ 0.37187415  0.06360305 -0.2880704  -0.2764524 ] loglik 0.0
accepted/total = 2355/3000 = 0.785
-------
true weights [-0.10322599 -0.16076929 -0.47255796 -0.86034098]
features
0 	3 	1 	3 	3 	1 	1 	
3 	2 	1 	1 	0 	0 	2 	
2 	1 	2 	0 	2 	1 	3 	
3 	2 	3 	2 	2 	0 	1 	
0 	0 	0 	3 	3 	2 	3 	
0 	1 	0 	3 	0 	1 	2 	
0 	1 	2 	3 	1 	3 	0 	
optimal policy
<	<	v	v	v	v	<	
^	>	>	>	>	<	<	
>	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	^	
v	<	^	<	>	>	v	
<	<	^	<	^	>	>	
optimal values
-10.32	-11.08	-10.49	-11.14	-11.08	-10.38	-10.44	
-11.08	-10.81	-10.44	-10.38	-10.32	-10.32	-10.69	
-11.11	-10.75	-10.75	-10.38	-10.69	-10.38	-11.14	
-11.08	-10.69	-11.08	-10.75	-10.75	-10.38	-10.44	
-10.32	-10.32	-10.32	-11.08	-11.49	-10.75	-11.19	
-10.32	-10.38	-10.32	-11.08	-10.74	-10.75	-10.69	
-10.32	-10.38	-10.69	-11.45	-10.79	-11.08	-10.32	
map_weights [ 0.37187415  0.06360305 -0.2880704  -0.2764524 ]
MAP reward
0.37	-0.28	0.06	-0.28	-0.28	0.06	0.06	
-0.28	-0.29	0.06	0.06	0.37	0.37	-0.29	
-0.29	0.06	-0.29	0.37	-0.29	0.06	-0.28	
-0.28	-0.29	-0.28	-0.29	-0.29	0.37	0.06	
0.37	0.37	0.37	-0.28	-0.28	-0.29	-0.28	
0.37	0.06	0.37	-0.28	0.37	0.06	-0.29	
0.37	0.06	-0.29	-0.28	0.06	-0.28	0.37	
Map policy
<	<	v	v	v	v	<	
^	>	>	>	>	<	<	
v	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	^	
v	<	^	<	<	<	v	
<	<	^	^	^	>	>	
expeced value MDP LP 30.336188495361068
mean w [ 0.30820829  0.0407738  -0.18876646 -0.23195989]
Mean policy from posterior
<	<	v	v	v	v	<	
^	>	>	>	>	<	<	
v	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	v	
v	<	^	<	<	>	v	
<	<	^	<	^	>	>	
Mean rewards
0.31	-0.23	0.04	-0.23	-0.23	0.04	0.04	
-0.23	-0.19	0.04	0.04	0.31	0.31	-0.19	
-0.19	0.04	-0.19	0.31	-0.19	0.04	-0.23	
-0.23	-0.19	-0.23	-0.19	-0.19	0.31	0.04	
0.31	0.31	0.31	-0.23	-0.23	-0.19	-0.23	
0.31	0.04	0.31	-0.23	0.31	0.04	-0.19	
0.31	0.04	-0.19	-0.23	0.04	-0.23	0.31	
mean = 0.03201222980130325, map = 0.04236893323784052
CVaR policy
<	<	<	v	v	v	<	
^	<	>	>	>	<	<	
v	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	^	
v	>	^	<	<	<	v	
<	<	^	^	^	>	>	
CVaR policy
<	<	v	v	v	v	<	
^	>	>	>	>	<	<	
v	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	v	
<	<	^	<	<	>	v	
<	<	^	<	^	>	>	
CVaR policy
<	<	v	v	v	v	<	
^	>	>	>	>	<	<	
v	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	v	
v	<	^	<	<	>	v	
<	<	^	<	^	>	>	
CVaR policy
<	<	v	v	v	v	<	
^	>	>	>	>	<	<	
v	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	v	
v	^	^	<	<	>	v	
<	<	^	<	^	>	>	
CVaR policy
<	<	v	v	v	v	<	
^	>	>	>	>	<	<	
^	v	>	^	^	^	<	
v	v	v	^	>	^	<	
v	<	<	<	v	^	v	
v	^	^	<	<	>	v	
<	<	^	<	^	>	>	
cvar = , 0.0683354526337947, 0.03201240971294261, 0.03205673199456349, 0.03201240150767326, 0.032101567083421045
==========
iteration 11
==========
weights [-0.96266846 -0.14166678  0.16441842  0.16176075]
expeced value MDP LP 15.988990004636914
demonstration
[(24, 0), (23, 0), (22, 2), (15, 2), (8, 3), (15, 0), (14, 0), (14, 0), (14, 0), (14, 1), (15, 2), (8, 3), (15, 0), (14, 0), (14, 1), (15, 0), (14, 1), (15, 2), (8, 3), (15, 0), (14, 1), (15, 0), (14, 1), (15, 0), (14, 0), (14, 1), (15, 2), (8, 3), (15, 0), (14, 0), (14, 1), (15, 0), (14, 1), (15, 2), (8, 3), (15, 0), (14, 0), (14, 1), (15, 0), (14, 1), (15, 0), (14, 1), (15, 0), (14, 0), (14, 1), (15, 2), (8, 3), (15, 0), (14, 1)]
[-0.12962965 -0.26331474 -0.47344715 -0.13360846]
w_map [-0.34461385 -0.2885133   0.28691654  0.07995632] loglik -28.41902169686182
accepted/total = 2050/3000 = 0.6833333333333333
-------
true weights [-0.96266846 -0.14166678  0.16441842  0.16176075]
features
2 	0 	3 	1 	2 	2 	3 	
1 	2 	3 	2 	1 	3 	0 	
2 	2 	3 	0 	2 	0 	2 	
1 	3 	3 	1 	3 	1 	1 	
0 	3 	0 	2 	1 	3 	0 	
1 	0 	2 	1 	2 	0 	1 	
1 	3 	0 	2 	2 	0 	0 	
optimal policy
<	<	v	>	>	<	<	
^	v	<	<	^	^	v	
<	<	<	^	v	>	>	
^	^	^	<	^	<	^	
>	^	^	>	v	<	<	
v	^	>	>	v	<	<	
>	v	>	v	<	<	<	
optimal values
16.44	15.31	16.44	16.14	16.44	16.44	16.44	
16.14	16.44	16.44	16.44	16.14	16.44	15.31	
16.44	16.44	16.44	15.31	16.31	15.31	16.44	
16.14	16.44	16.44	16.13	16.31	16.00	16.14	
15.31	16.44	15.31	16.14	16.14	16.14	15.01	
15.57	15.31	16.14	16.14	16.44	15.31	15.02	
15.87	16.18	15.31	16.44	16.44	15.31	14.20	
map_weights [-0.34461385 -0.2885133   0.28691654  0.07995632]
MAP reward
0.29	-0.34	0.08	-0.29	0.29	0.29	0.08	
-0.29	0.29	0.08	0.29	-0.29	0.08	-0.34	
0.29	0.29	0.08	-0.34	0.29	-0.34	0.29	
-0.29	0.08	0.08	-0.29	0.08	-0.29	-0.29	
-0.34	0.08	-0.34	0.29	-0.29	0.08	-0.34	
-0.29	-0.34	0.29	-0.29	0.29	-0.34	-0.29	
-0.29	0.08	-0.34	0.29	0.29	-0.34	-0.34	
Map policy
<	<	v	>	>	^	<	
^	v	<	<	^	^	v	
<	<	<	^	^	>	>	
^	^	^	<	^	>	^	
>	^	^	v	v	<	^	
>	^	>	v	v	<	<	
>	>	>	>	<	<	<	
expeced value MDP LP 19.908666636351214
mean w [-0.29062755 -0.34728375  0.20372197  0.04702003]
Mean policy from posterior
<	<	v	>	>	<	<	
^	v	<	<	^	^	v	
<	<	<	^	>	>	>	
^	^	^	<	^	^	^	
>	^	^	v	v	v	^	
>	^	v	v	v	<	<	
>	>	>	>	v	<	<	
Mean rewards
0.20	-0.29	0.05	-0.35	0.20	0.20	0.05	
-0.35	0.20	0.05	0.20	-0.35	0.05	-0.29	
0.20	0.20	0.05	-0.29	0.20	-0.29	0.20	
-0.35	0.05	0.05	-0.35	0.05	-0.35	-0.35	
-0.29	0.05	-0.29	0.20	-0.35	0.05	-0.29	
-0.35	-0.29	0.20	-0.35	0.20	-0.29	-0.35	
-0.35	0.05	-0.29	0.20	0.20	-0.29	-0.29	
mean = 0.13921060811455455, map = 0.05643960791045366
CVaR policy
<	<	v	>	>	^	<	
^	v	<	<	^	^	v	
<	<	<	^	^	>	>	
^	^	^	<	^	>	^	
>	^	^	v	v	<	^	
^	^	>	>	v	<	<	
>	>	>	>	v	<	<	
CVaR policy
<	<	v	>	^	^	<	
^	v	<	<	^	^	v	
<	<	<	^	>	>	>	
^	^	^	<	^	^	^	
>	^	^	v	v	v	^	
^	^	v	v	v	<	<	
>	>	>	>	v	<	<	
CVaR policy
<	<	v	>	^	^	<	
^	v	<	<	^	^	v	
<	<	<	^	>	>	>	
^	^	^	<	^	^	^	
>	^	^	v	v	v	^	
>	^	v	v	v	<	<	
>	>	>	>	v	<	<	
CVaR policy
<	<	v	>	^	^	<	
^	v	<	<	^	^	v	
<	<	<	^	>	>	>	
^	^	^	<	^	^	^	
>	^	^	v	v	v	^	
>	^	v	v	v	<	<	
>	>	>	>	v	<	<	
CVaR policy
<	<	v	>	^	^	<	
^	v	<	<	^	^	v	
<	<	<	^	>	>	>	
^	^	^	<	^	^	^	
>	^	^	v	v	v	^	
>	^	v	v	v	<	<	
>	>	>	v	v	<	<	
cvar = , 0.05652349616975094, 0.13921060809893504, 0.13921060961516574, 0.13921312361193827, 0.13921881908062517
==========
iteration 12
==========
weights [-0.53218419  0.26770812  0.68617759  0.41745979]
expeced value MDP LP 67.87451945377276
demonstration
[(24, 1), (25, 1), (26, 1), (27, 3), (34, 2), (27, 1), (27, 3), (34, 2), (27, 0), (26, 0), (25, 1), (26, 0), (25, 1), (26, 0), (25, 1), (26, 0), (25, 1), (26, 0), (25, 1), (26, 1), (27, 0), (26, 0), (25, 1), (26, 1), (27, 0), (26, 1), (27, 1), (27, 1), (27, 0), (26, 1), (27, 0), (26, 1), (27, 1), (27, 3), (34, 1), (34, 2), (27, 3), (34, 1), (34, 2), (27, 3), (34, 1), (34, 1), (34, 2), (27, 1), (27, 0), (26, 1), (27, 3), (34, 2), (27, 0)]
[-0.31700914 -0.20374262 -0.13019966 -0.34904859]
w_map [ 0.08557165 -0.12649467  0.6361267  -0.15180697] loglik -36.41054980654735
accepted/total = 2733/3000 = 0.911
-------
true weights [-0.53218419  0.26770812  0.68617759  0.41745979]
features
2 	1 	3 	1 	1 	3 	3 	
0 	0 	0 	3 	1 	3 	3 	
1 	3 	1 	0 	3 	3 	0 	
0 	2 	2 	0 	2 	2 	2 	
0 	2 	0 	1 	0 	0 	2 	
1 	0 	1 	0 	3 	2 	0 	
0 	3 	2 	1 	2 	3 	0 	
optimal policy
<	<	<	<	v	v	v	
^	v	v	>	v	v	<	
>	v	v	>	v	v	v	
>	>	<	>	>	>	v	
>	^	^	^	^	>	^	
^	^	v	>	v	<	^	
>	>	v	<	v	<	<	
optimal values
68.62	68.20	67.93	67.52	67.52	67.82	67.56	
67.40	67.13	66.99	67.67	67.93	68.08	67.82	
67.93	68.35	68.20	67.13	68.35	68.35	67.40	
67.40	68.62	68.62	67.40	68.62	68.62	68.62	
67.40	68.62	67.40	66.99	67.40	67.40	68.62	
66.99	67.40	68.20	67.13	68.35	68.35	67.40	
67.13	68.35	68.62	68.20	68.62	68.35	67.13	
map_weights [ 0.08557165 -0.12649467  0.6361267  -0.15180697]
MAP reward
0.64	-0.13	-0.15	-0.13	-0.13	-0.15	-0.15	
0.09	0.09	0.09	-0.15	-0.13	-0.15	-0.15	
-0.13	-0.15	-0.13	0.09	-0.15	-0.15	0.09	
0.09	0.64	0.64	0.09	0.64	0.64	0.64	
0.09	0.64	0.09	-0.13	0.09	0.09	0.64	
-0.13	0.09	-0.13	0.09	-0.15	0.64	0.09	
0.09	-0.15	0.64	-0.13	0.64	-0.15	0.09	
Map policy
<	<	<	<	v	v	v	
^	<	v	v	v	v	v	
^	v	v	v	v	v	v	
>	v	<	>	>	>	v	
>	^	^	>	^	^	^	
^	^	v	v	v	^	^	
>	>	v	>	v	<	^	
expeced value MDP LP 34.08525566425385
mean w [-0.15794919 -0.0669469   0.34662455 -0.10659308]
Mean policy from posterior
<	<	<	<	v	v	v	
^	^	v	>	v	v	v	
>	v	v	<	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	^	>	>	
^	^	v	v	v	<	^	
>	>	v	>	v	<	<	
Mean rewards
0.35	-0.07	-0.11	-0.07	-0.07	-0.11	-0.11	
-0.16	-0.16	-0.16	-0.11	-0.07	-0.11	-0.11	
-0.07	-0.11	-0.07	-0.16	-0.11	-0.11	-0.16	
-0.16	0.35	0.35	-0.16	0.35	0.35	0.35	
-0.16	0.35	-0.16	-0.07	-0.16	-0.16	0.35	
-0.07	-0.16	-0.07	-0.16	-0.11	0.35	-0.16	
-0.16	-0.11	0.35	-0.07	0.35	-0.11	-0.16	
mean = 0.036562322872299546, map = 0.16160069653500386
CVaR policy
<	<	<	<	v	<	v	
^	^	v	>	v	v	v	
>	v	v	<	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	^	>	>	
^	^	v	v	v	<	^	
>	>	v	<	v	<	<	
CVaR policy
<	<	<	<	v	v	v	
^	^	v	>	v	v	v	
>	v	v	<	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	^	>	>	
^	^	v	v	v	<	^	
>	>	v	<	v	<	<	
CVaR policy
<	<	<	<	v	v	v	
^	^	v	>	v	v	v	
>	v	v	<	v	v	v	
>	>	<	>	>	>	>	
>	^	^	^	^	^	>	
^	^	v	v	v	<	^	
>	>	v	<	v	<	<	
CVaR policy
<	<	<	<	v	v	v	
^	^	v	>	v	v	v	
>	v	v	<	v	v	v	
>	v	<	>	>	>	v	
>	^	^	^	^	^	>	
^	^	v	v	v	<	^	
>	>	v	<	v	<	<	
CVaR policy
<	<	<	<	v	v	v	
^	^	v	>	v	v	v	
>	v	v	<	v	v	v	
>	v	<	>	>	>	v	
>	^	^	^	^	>	>	
^	^	v	v	v	<	^	
>	>	v	<	v	<	<	
cvar = , 0.04790445576978186, 0.03658353409066706, 0.03656279429392839, 0.03662813723141767, 0.03656227624023245
==========
iteration 13
==========
weights [-0.30903133 -0.1665553  -0.91149449  0.21432862]
expeced value MDP LP 20.831202385617413
demonstration
[(24, 2), (17, 0), (16, 0), (15, 0), (14, 2), (7, 0), (7, 2), (0, 2), (0, 2), (0, 2), (0, 0), (0, 2), (0, 3), (7, 1), (8, 0), (7, 1), (8, 2), (1, 3), (8, 3), (15, 0), (14, 0), (14, 2), (7, 1), (8, 3), (15, 2), (8, 2), (1, 3), (8, 0), (7, 1), (8, 2), (1, 2), (1, 2), (1, 3), (8, 0), (7, 1), (8, 0), (7, 3), (14, 1), (15, 2), (8, 2), (1, 3), (8, 2), (1, 0), (0, 1), (1, 2), (1, 3), (8, 3), (15, 0), (14, 1)]
[-0.09865262 -0.27649735 -0.01053812 -0.61431191]
w_map [-0.2955312   0.03418254 -0.36008139  0.31020487] loglik -52.824070824980936
accepted/total = 2562/3000 = 0.854
-------
true weights [-0.30903133 -0.1665553  -0.91149449  0.21432862]
features
3 	3 	1 	1 	2 	2 	1 	
3 	3 	0 	0 	2 	1 	3 	
3 	3 	1 	3 	2 	3 	1 	
2 	2 	1 	1 	0 	3 	0 	
3 	3 	2 	0 	3 	2 	1 	
0 	3 	0 	2 	0 	2 	0 	
1 	0 	3 	1 	3 	0 	0 	
optimal policy
<	<	<	<	<	v	v	
^	<	<	v	>	v	>	
^	<	<	<	>	v	<	
^	^	^	^	>	^	<	
<	<	<	>	^	^	^	
^	^	<	v	v	<	^	
^	^	v	<	v	<	<	
optimal values
21.43	21.43	21.05	20.67	19.56	19.93	21.05	
21.43	21.43	20.91	20.54	19.93	21.05	21.43	
21.43	21.43	21.05	21.06	20.31	21.43	21.05	
20.31	20.31	20.67	20.68	20.91	21.43	20.91	
21.43	21.43	20.31	20.40	20.91	20.31	20.53	
20.91	21.43	20.91	19.93	20.91	19.79	20.02	
20.53	20.91	21.43	21.05	21.43	20.91	20.39	
map_weights [-0.2955312   0.03418254 -0.36008139  0.31020487]
MAP reward
0.31	0.31	0.03	0.03	-0.36	-0.36	0.03	
0.31	0.31	-0.30	-0.30	-0.36	0.03	0.31	
0.31	0.31	0.03	0.31	-0.36	0.31	0.03	
-0.36	-0.36	0.03	0.03	-0.30	0.31	-0.30	
0.31	0.31	-0.36	-0.30	0.31	-0.36	0.03	
-0.30	0.31	-0.30	-0.36	-0.30	-0.36	-0.30	
0.03	-0.30	0.31	0.03	0.31	-0.30	-0.30	
Map policy
<	<	<	<	<	v	v	
^	<	<	v	>	v	>	
^	<	<	<	>	v	<	
^	^	^	^	>	^	<	
<	<	<	^	^	^	^	
^	^	<	v	v	<	^	
^	^	v	<	v	<	<	
expeced value MDP LP 41.29583635707027
mean w [-0.17402597  0.10153585 -0.17229862  0.41785643]
Mean policy from posterior
<	<	<	<	<	v	v	
^	<	<	v	>	>	>	
^	^	<	<	>	v	<	
^	^	^	^	>	^	<	
<	<	<	>	>	^	<	
^	^	<	v	v	^	^	
^	^	v	<	v	<	<	
Mean rewards
0.42	0.42	0.10	0.10	-0.17	-0.17	0.10	
0.42	0.42	-0.17	-0.17	-0.17	0.10	0.42	
0.42	0.42	0.10	0.42	-0.17	0.42	0.10	
-0.17	-0.17	0.10	0.10	-0.17	0.42	-0.17	
0.42	0.42	-0.17	-0.17	0.42	-0.17	0.10	
-0.17	0.42	-0.17	-0.17	-0.17	-0.17	-0.17	
0.10	-0.17	0.42	0.10	0.42	-0.17	-0.17	
mean = 0.06061703678227204, map = 0.004769413218454588
CVaR policy
<	<	<	<	<	v	v	
^	<	<	v	>	>	>	
^	^	<	<	>	v	^	
^	^	^	^	>	^	<	
<	<	<	^	>	^	<	
^	^	<	v	v	^	^	
^	^	v	>	v	<	<	
CVaR policy
<	<	<	<	<	v	v	
^	<	<	v	>	>	>	
^	^	<	<	>	v	<	
^	^	^	^	>	^	<	
<	<	<	^	>	^	<	
^	^	<	v	v	^	^	
^	^	v	<	v	<	<	
CVaR policy
<	<	<	<	<	>	v	
^	<	<	v	>	>	>	
^	^	<	<	>	v	^	
^	^	^	^	>	^	<	
<	<	<	>	>	^	<	
^	^	<	v	v	^	^	
^	^	v	<	v	<	<	
CVaR policy
<	<	<	<	<	>	v	
^	<	<	v	>	>	>	
^	^	<	<	>	v	^	
^	^	^	^	>	^	<	
<	<	<	>	>	^	<	
^	^	<	v	v	^	^	
^	^	v	<	v	<	<	
CVaR policy
<	<	<	<	<	>	v	
^	<	<	v	>	>	>	
^	^	<	<	>	v	^	
^	^	^	^	>	^	<	
<	<	<	>	>	^	<	
^	^	<	v	v	^	^	
^	^	v	<	v	<	<	
cvar = , 0.053338564970953684, 0.053345770200483145, 0.060618545560299, 0.060608871712933166, 0.06061741920574448
==========
iteration 14
==========
weights [ 0.27707862  0.43714552 -0.47054204 -0.71464776]
expeced value MDP LP 43.05881829072253
demonstration
[(24, 3), (31, 0), (30, 3), (37, 2), (30, 1), (31, 2), (24, 2), (17, 0), (16, 1), (17, 3), (24, 2), (17, 0), (16, 1), (17, 0), (16, 1), (17, 3), (24, 3), (31, 2), (24, 3), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 2), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 0), (16, 1), (17, 3), (24, 3), (31, 2), (24, 3), (31, 0), (30, 1), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 2), (17, 3), (24, 3)]
[-0.0864926  -0.39440729 -0.20752402 -0.3115761 ]
w_map [-0.11685861  0.35523391 -0.36813685  0.15977063] loglik -31.19162276313864
accepted/total = 2779/3000 = 0.9263333333333333
-------
true weights [ 0.27707862  0.43714552 -0.47054204 -0.71464776]
features
0 	0 	1 	1 	2 	0 	2 	
3 	0 	3 	3 	2 	2 	3 	
1 	0 	1 	1 	0 	2 	1 	
1 	1 	3 	1 	3 	0 	1 	
3 	3 	1 	1 	0 	0 	2 	
2 	0 	1 	2 	3 	3 	3 	
3 	1 	1 	3 	1 	3 	3 	
optimal policy
>	>	^	^	<	<	<	
v	v	v	v	v	v	v	
v	v	>	v	<	>	>	
^	<	<	v	<	>	>	
^	^	v	<	<	<	^	
>	>	v	<	v	^	^	
>	>	v	<	v	<	<	
optimal values
43.40	43.55	43.71	43.71	42.81	42.66	41.76	
42.56	43.40	42.56	42.56	42.65	41.91	42.56	
43.71	43.55	43.71	43.71	43.55	42.81	43.71	
43.71	43.71	42.56	43.71	42.56	43.55	43.71	
42.56	42.56	43.71	43.71	43.55	43.40	42.81	
42.65	43.55	43.71	42.81	42.56	42.25	41.66	
42.56	43.71	43.71	42.56	43.71	42.56	41.42	
map_weights [-0.11685861  0.35523391 -0.36813685  0.15977063]
MAP reward
-0.12	-0.12	0.36	0.36	-0.37	-0.12	-0.37	
0.16	-0.12	0.16	0.16	-0.37	-0.37	0.16	
0.36	-0.12	0.36	0.36	-0.12	-0.37	0.36	
0.36	0.36	0.16	0.36	0.16	-0.12	0.36	
0.16	0.16	0.36	0.36	-0.12	-0.12	-0.37	
-0.37	-0.12	0.36	-0.37	0.16	0.16	0.16	
0.16	0.36	0.36	0.16	0.36	0.16	0.16	
Map policy
v	>	>	<	<	<	v	
v	>	v	v	<	>	v	
v	v	>	v	<	>	>	
<	<	<	v	<	>	>	
^	>	v	<	<	v	^	
v	v	v	<	v	<	<	
>	>	v	<	v	<	<	
expeced value MDP LP 37.62552156147757
mean w [-0.09076068  0.380648   -0.07349559 -0.14964077]
Mean policy from posterior
>	>	^	<	<	<	v	
v	v	v	v	^	v	v	
v	<	>	v	<	>	v	
<	<	v	v	<	>	^	
^	>	v	<	<	>	^	
>	v	v	<	v	<	^	
>	>	v	<	v	<	<	
Mean rewards
-0.09	-0.09	0.38	0.38	-0.07	-0.09	-0.07	
-0.15	-0.09	-0.15	-0.15	-0.07	-0.07	-0.15	
0.38	-0.09	0.38	0.38	-0.09	-0.07	0.38	
0.38	0.38	-0.15	0.38	-0.15	-0.09	0.38	
-0.15	-0.15	0.38	0.38	-0.09	-0.09	-0.07	
-0.07	-0.09	0.38	-0.07	-0.15	-0.15	-0.15	
-0.15	0.38	0.38	-0.15	0.38	-0.15	-0.15	
mean = 0.04892654946594632, map = 0.17484261190017492
CVaR policy
>	>	>	^	<	<	v	
v	v	v	v	v	>	v	
v	v	>	v	<	>	v	
<	<	<	v	<	>	>	
^	^	v	<	<	<	^	
>	v	v	^	v	^	^	
>	>	v	<	v	<	<	
CVaR policy
>	>	>	<	<	<	v	
v	^	v	v	v	v	v	
v	v	>	v	<	>	v	
^	<	<	v	<	>	>	
^	^	v	<	<	^	^	
>	v	v	^	v	<	^	
>	>	v	<	v	<	<	
CVaR policy
>	>	^	^	<	<	v	
v	v	v	v	v	v	v	
v	<	>	v	<	>	v	
<	<	<	v	<	>	>	
^	^	v	<	<	<	^	
>	>	v	^	v	<	^	
>	>	v	<	v	<	<	
CVaR policy
>	>	^	^	<	<	v	
v	v	v	v	v	v	v	
v	<	>	v	<	>	v	
<	<	<	v	<	>	>	
^	^	v	<	<	<	^	
>	>	v	<	v	<	^	
>	>	v	<	v	<	<	
CVaR policy
>	>	^	^	<	<	v	
v	v	v	v	^	v	v	
v	>	>	v	<	>	>	
^	<	^	v	<	>	>	
^	>	v	<	<	>	^	
>	v	v	<	v	<	^	
>	>	v	<	v	<	<	
cvar = , 0.006817941963831231, 0.01938929849885085, 0.01871657711427588, 0.018778663861610312, 0.04891842145333669
==========
iteration 15
==========
weights [ 0.21731786 -0.74960444 -0.62275738 -0.0551305 ]
expeced value MDP LP 20.72077432850832
demonstration
[(24, 3), (31, 3), (38, 3), (45, 0), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3), (44, 3)]
[-0.3057263  -0.25420367 -0.12819974 -0.31187028]
w_map [ 0.90515323 -0.09194245 -0.00168775 -0.00121658] loglik -1.7309357849135267
accepted/total = 2496/3000 = 0.832
-------
true weights [ 0.21731786 -0.74960444 -0.62275738 -0.0551305 ]
features
3 	1 	0 	2 	3 	3 	0 	
3 	1 	0 	1 	1 	2 	2 	
2 	3 	3 	1 	1 	0 	1 	
1 	3 	2 	1 	2 	1 	2 	
0 	2 	2 	2 	1 	1 	1 	
1 	2 	3 	0 	1 	1 	1 	
1 	1 	0 	3 	0 	3 	0 	
optimal policy
>	>	^	<	>	>	>	
>	>	^	<	^	^	^	
>	>	^	<	>	^	^	
v	^	^	v	v	^	^	
<	<	v	v	v	v	v	
^	>	v	v	v	v	v	
^	>	v	<	v	>	>	
optimal values
20.50	20.76	21.73	20.89	21.19	21.46	21.73	
20.50	20.76	21.73	20.76	20.23	20.62	20.89	
20.35	21.19	21.46	20.50	19.68	20.63	19.93	
20.76	20.92	20.62	19.67	18.99	19.68	19.11	
21.73	20.89	20.62	20.62	19.81	19.54	19.81	
20.76	20.62	21.46	21.46	20.76	20.50	20.76	
19.81	20.76	21.73	21.46	21.73	21.46	21.73	
map_weights [ 0.90515323 -0.09194245 -0.00168775 -0.00121658]
MAP reward
-0.00	-0.09	0.91	-0.00	-0.00	-0.00	0.91	
-0.00	-0.09	0.91	-0.09	-0.09	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.09	-0.09	0.91	-0.09	
-0.09	-0.00	-0.00	-0.09	-0.00	-0.09	-0.00	
0.91	-0.00	-0.00	-0.00	-0.09	-0.09	-0.09	
-0.09	-0.00	-0.00	0.91	-0.09	-0.09	-0.09	
-0.09	-0.09	0.91	-0.00	0.91	-0.00	0.91	
Map policy
>	>	^	<	>	>	>	
>	>	^	<	<	^	^	
v	>	^	<	>	^	^	
v	v	^	v	v	^	^	
<	<	v	v	v	v	v	
^	>	v	v	v	v	v	
^	>	v	<	v	>	>	
expeced value MDP LP 44.34051677732012
mean w [ 4.51327968e-01 -2.57389730e-01 -2.24372235e-05  6.50035081e-02]
Mean policy from posterior
>	>	^	<	>	>	>	
>	>	^	<	<	^	^	
v	>	^	<	>	^	^	
v	v	^	v	v	^	^	
<	<	v	v	v	v	v	
^	>	v	v	v	v	v	
^	>	v	<	v	>	>	
Mean rewards
0.07	-0.26	0.45	-0.00	0.07	0.07	0.45	
0.07	-0.26	0.45	-0.26	-0.26	-0.00	-0.00	
-0.00	0.07	0.07	-0.26	-0.26	0.45	-0.26	
-0.26	0.07	-0.00	-0.26	-0.00	-0.26	-0.00	
0.45	-0.00	-0.00	-0.00	-0.26	-0.26	-0.26	
-0.26	-0.00	0.07	0.45	-0.26	-0.26	-0.26	
-0.26	-0.26	0.45	0.07	0.45	0.07	0.45	
mean = 0.02318223159154087, map = 0.023182228807353766
CVaR policy
>	>	^	<	>	>	>	
>	>	^	<	^	^	^	
>	>	^	<	>	^	^	
v	v	^	v	>	^	^	
<	<	v	v	<	v	v	
^	>	v	v	v	v	v	
^	>	v	>	v	>	>	
CVaR policy
>	>	^	<	>	>	>	
>	>	^	<	^	^	^	
>	>	^	<	>	^	^	
v	v	^	v	^	^	^	
<	<	v	v	<	v	v	
^	>	v	v	v	v	v	
^	>	v	>	v	>	>	
CVaR policy
>	>	^	<	>	>	>	
>	>	^	<	<	^	^	
v	>	^	<	>	^	^	
v	v	^	v	v	^	^	
<	<	v	v	v	v	v	
^	>	v	v	v	v	v	
^	>	v	>	v	>	>	
CVaR policy
>	>	^	<	>	>	>	
>	>	^	<	<	^	^	
v	>	^	<	>	^	^	
v	v	^	v	v	^	^	
<	<	v	v	v	v	v	
^	>	v	v	v	v	v	
^	>	v	>	v	>	>	
CVaR policy
>	>	^	<	>	>	>	
>	>	^	<	<	^	^	
v	>	^	<	>	^	^	
v	v	^	v	v	^	^	
<	<	v	v	v	v	v	
^	>	v	v	v	v	v	
^	>	v	>	v	>	>	
cvar = , 0.01148689985408069, 0.011486918175613425, 0.023228701326228673, 0.023182244361358073, 0.023182308294931175
==========
iteration 16
==========
weights [-0.61133083 -0.64852601 -0.45294506  0.02300849]
expeced value MDP LP 1.7291994158597426
demonstration
[(24, 1), (25, 2), (18, 0), (17, 1), (18, 3), (25, 2), (18, 0), (17, 2), (10, 3), (17, 3), (24, 1), (25, 2), (18, 3), (25, 0), (24, 2), (17, 2), (10, 3), (17, 2), (10, 3), (17, 3), (24, 1), (25, 2), (18, 0), (17, 2), (10, 3), (17, 1), (18, 0), (17, 2), (10, 3), (17, 2), (10, 3), (17, 3), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 2), (18, 0), (17, 2), (10, 3), (17, 3), (24, 1), (25, 2), (18, 0), (17, 3), (24, 1), (25, 2), (18, 0)]
[-0.09294621 -0.49711708 -0.10955723 -0.30037948]
w_map [-0.19239303 -0.25589575 -0.10055659  0.45115462] loglik -34.788693097027135
accepted/total = 2720/3000 = 0.9066666666666666
-------
true weights [-0.61133083 -0.64852601 -0.45294506  0.02300849]
features
3 	0 	1 	1 	1 	2 	2 	
2 	2 	0 	3 	0 	1 	1 	
0 	3 	1 	3 	3 	2 	1 	
3 	0 	1 	3 	3 	1 	3 	
1 	2 	3 	0 	2 	2 	2 	
3 	1 	0 	1 	3 	3 	0 	
2 	2 	3 	1 	3 	1 	2 	
optimal policy
<	<	v	v	v	v	v	
^	<	>	v	v	v	v	
v	v	>	v	v	<	v	
<	<	>	>	<	<	>	
v	>	>	^	^	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	<	^	
optimal values
2.30	1.67	1.00	1.63	1.00	0.69	0.50	
1.82	1.35	1.67	2.30	1.67	1.16	0.96	
1.67	1.67	1.63	2.30	2.30	1.82	1.63	
2.30	1.67	1.63	2.30	2.30	1.63	2.30	
1.63	1.20	1.67	1.67	1.82	1.82	1.82	
2.30	1.63	1.67	1.63	2.30	2.30	1.67	
1.82	1.82	2.30	1.63	2.30	1.63	1.20	
map_weights [-0.19239303 -0.25589575 -0.10055659  0.45115462]
MAP reward
0.45	-0.19	-0.26	-0.26	-0.26	-0.10	-0.10	
-0.10	-0.10	-0.19	0.45	-0.19	-0.26	-0.26	
-0.19	0.45	-0.26	0.45	0.45	-0.10	-0.26	
0.45	-0.19	-0.26	0.45	0.45	-0.26	0.45	
-0.26	-0.10	0.45	-0.19	-0.10	-0.10	-0.10	
0.45	-0.26	-0.19	-0.26	0.45	0.45	-0.19	
-0.10	-0.10	0.45	-0.26	0.45	-0.26	-0.10	
Map policy
<	<	v	v	v	v	v	
^	<	>	v	v	v	v	
v	v	>	>	v	<	v	
<	<	>	>	<	<	>	
v	>	>	^	^	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	<	^	
expeced value MDP LP 26.20619908545136
mean w [-0.24944962 -0.16242885 -0.12782509  0.26619575]
Mean policy from posterior
<	<	>	v	<	v	v	
^	<	>	v	v	v	v	
v	>	>	>	v	<	v	
<	<	>	>	<	<	>	
v	>	^	^	v	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	<	<	
Mean rewards
0.27	-0.25	-0.16	-0.16	-0.16	-0.13	-0.13	
-0.13	-0.13	-0.25	0.27	-0.25	-0.16	-0.16	
-0.25	0.27	-0.16	0.27	0.27	-0.13	-0.16	
0.27	-0.25	-0.16	0.27	0.27	-0.16	0.27	
-0.16	-0.13	0.27	-0.25	-0.13	-0.13	-0.13	
0.27	-0.16	-0.25	-0.16	0.27	0.27	-0.25	
-0.13	-0.13	0.27	-0.16	0.27	-0.16	-0.13	
mean = 0.004501920734651943, map = -2.3906725488131997e-09
CVaR policy
<	<	>	v	<	v	<	
^	<	>	v	<	v	v	
v	^	>	v	v	<	v	
<	<	>	>	<	<	>	
^	>	^	^	v	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	^	<	
CVaR policy
<	<	>	v	<	v	v	
^	<	>	v	<	v	v	
v	>	>	>	v	<	v	
<	<	>	>	^	<	>	
^	>	^	^	v	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	^	<	
CVaR policy
<	<	>	v	<	v	v	
^	<	>	v	<	v	v	
v	>	>	>	v	<	v	
<	<	>	>	^	<	>	
^	>	^	^	v	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	<	<	
CVaR policy
<	<	>	v	<	v	v	
^	<	>	v	<	v	v	
v	>	>	>	<	<	v	
<	<	>	>	^	<	>	
^	>	^	^	v	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	<	<	
CVaR policy
<	<	>	v	<	v	v	
^	<	>	v	v	v	v	
v	>	>	>	v	<	v	
<	<	>	>	^	<	>	
^	>	^	^	v	v	^	
<	<	v	>	v	<	<	
^	>	v	>	v	<	<	
cvar = , 0.015544077758288921, 0.004501686697023111, 0.004508801963551967, 0.004504155577524038, 0.004638042544887977
==========
iteration 17
==========
weights [0.08455494 0.78410106 0.34726712 0.50738697]
expeced value MDP LP 77.73433795998835
demonstration
[(24, 2), (17, 2), (10, 1), (11, 2), (4, 3), (11, 0), (10, 1), (11, 0), (10, 1), (11, 2), (4, 2), (4, 3), (11, 2), (4, 2), (4, 3), (11, 2), (4, 3), (11, 2), (4, 3), (11, 0), (10, 1), (11, 2), (4, 2), (4, 3), (11, 0), (10, 1), (11, 2), (4, 3), (11, 0), (10, 1), (11, 0), (10, 1), (11, 2), (4, 3), (11, 2), (4, 3), (11, 0), (10, 1), (11, 2), (4, 3), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 2), (4, 3), (11, 2)]
[-0.19152538 -0.01430547 -0.73703018 -0.05713897]
w_map [-0.31720446  0.47414036 -0.0256877   0.18296748] loglik -24.953298499252014
accepted/total = 2554/3000 = 0.8513333333333334
-------
true weights [0.08455494 0.78410106 0.34726712 0.50738697]
features
2 	0 	2 	2 	1 	0 	3 	
0 	2 	0 	1 	1 	2 	3 	
3 	0 	2 	3 	0 	1 	3 	
0 	3 	1 	2 	3 	3 	3 	
0 	0 	3 	2 	1 	2 	3 	
0 	0 	3 	1 	3 	1 	2 	
1 	2 	0 	1 	2 	3 	1 	
optimal policy
>	>	>	>	^	<	<	
>	>	>	>	^	<	<	
>	>	>	^	^	^	<	
>	>	v	^	v	^	<	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
optimal values
76.43	76.85	77.54	77.97	78.41	77.71	77.44	
76.59	77.28	77.71	78.41	78.41	77.97	77.70	
76.74	77.01	77.70	78.13	77.71	77.98	77.71	
76.90	77.59	77.86	77.70	77.86	77.71	77.44	
77.02	77.17	77.86	77.97	78.14	77.70	77.70	
77.71	77.44	78.13	78.41	78.13	78.14	77.97	
78.41	77.97	77.71	78.41	77.97	78.13	78.41	
map_weights [-0.31720446  0.47414036 -0.0256877   0.18296748]
MAP reward
-0.03	-0.32	-0.03	-0.03	0.47	-0.32	0.18	
-0.32	-0.03	-0.32	0.47	0.47	-0.03	0.18	
0.18	-0.32	-0.03	0.18	-0.32	0.47	0.18	
-0.32	0.18	0.47	-0.03	0.18	0.18	0.18	
-0.32	-0.32	0.18	-0.03	0.47	-0.03	0.18	
-0.32	-0.32	0.18	0.47	0.18	0.47	-0.03	
0.47	-0.03	-0.32	0.47	-0.03	0.18	0.47	
Map policy
>	>	>	>	^	<	v	
>	>	>	>	^	<	<	
>	>	>	^	^	^	<	
>	>	v	^	v	^	<	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
expeced value MDP LP 36.754569815571
mean w [-0.0867932   0.37472819 -0.25000667  0.01379608]
Mean policy from posterior
>	>	v	>	^	<	<	
>	>	>	>	^	<	<	
v	>	>	^	^	<	<	
v	>	v	^	v	^	<	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
Mean rewards
-0.25	-0.09	-0.25	-0.25	0.37	-0.09	0.01	
-0.09	-0.25	-0.09	0.37	0.37	-0.25	0.01	
0.01	-0.09	-0.25	0.01	-0.09	0.37	0.01	
-0.09	0.01	0.37	-0.25	0.01	0.01	0.01	
-0.09	-0.09	0.01	-0.25	0.37	-0.25	0.01	
-0.09	-0.09	0.01	0.37	0.01	0.37	-0.25	
0.37	-0.25	-0.09	0.37	-0.25	0.01	0.37	
mean = 0.06204329917187579, map = 0.00019466224524933295
CVaR policy
>	>	v	v	^	<	<	
>	>	>	>	^	<	<	
>	v	>	^	^	<	<	
>	>	v	^	v	^	<	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
CVaR policy
>	>	v	>	^	<	<	
>	>	>	>	^	<	<	
v	v	>	^	^	<	<	
v	>	v	^	v	^	^	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
CVaR policy
>	>	v	>	^	<	<	
>	>	>	>	^	<	<	
v	>	>	^	^	<	<	
v	>	v	^	v	^	<	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
CVaR policy
>	>	v	>	^	<	<	
>	>	>	>	^	<	<	
v	>	>	^	^	<	<	
v	>	v	^	v	^	<	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
CVaR policy
>	>	v	>	^	<	<	
>	>	>	>	^	<	<	
v	>	>	^	^	<	<	
v	>	v	^	v	^	<	
v	>	v	v	v	v	v	
v	>	>	v	<	v	v	
<	<	>	v	<	>	>	
cvar = , 0.04103223696736791, 0.06415337398138377, 0.06204486190129899, 0.06204335991952803, 0.06204338700658241
==========
iteration 18
==========
weights [ 0.57608567 -0.51532916 -0.01949013  0.63417765]
expeced value MDP LP 63.027148776202345
demonstration
[(24, 3), (31, 3), (38, 0), (37, 1), (38, 0), (37, 1), (38, 0), (37, 2), (30, 1), (31, 3), (38, 0), (37, 2), (30, 1), (31, 0), (30, 1), (31, 0), (30, 3), (37, 2), (30, 1), (31, 0), (30, 3), (37, 2), (30, 3), (37, 1), (38, 0), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 1), (31, 3), (38, 0), (37, 1), (38, 2), (31, 0), (30, 1), (31, 3), (38, 0), (37, 1), (38, 2), (31, 3), (38, 2), (31, 3), (38, 0), (37, 2), (30, 1), (31, 3), (38, 2)]
[-0.24644815 -0.23977185 -0.25511082 -0.25866917]
w_map [-0.08973114 -0.13109657 -0.60229183  0.17688046] loglik -33.271064638197686
accepted/total = 2746/3000 = 0.9153333333333333
-------
true weights [ 0.57608567 -0.51532916 -0.01949013  0.63417765]
features
3 	1 	1 	1 	3 	1 	3 	
3 	0 	3 	3 	0 	0 	0 	
0 	2 	3 	1 	1 	1 	3 	
1 	1 	2 	2 	0 	3 	0 	
0 	1 	3 	3 	0 	1 	3 	
0 	0 	3 	3 	2 	3 	2 	
3 	2 	0 	2 	3 	0 	0 	
optimal policy
v	<	v	v	^	>	>	
^	<	>	<	<	<	^	
^	>	^	^	^	>	>	
^	>	^	v	>	>	^	
v	>	v	<	<	>	>	
v	>	^	^	<	v	^	
<	<	^	^	v	<	<	
optimal values
63.42	62.27	62.27	62.27	63.42	62.27	63.42	
63.42	63.36	63.42	63.42	63.36	63.30	63.36	
63.36	62.76	63.42	62.27	62.21	62.27	63.42	
62.21	61.62	62.76	62.76	63.30	63.36	63.36	
63.30	62.27	63.42	63.42	63.36	62.27	63.42	
63.36	63.36	63.42	63.42	62.76	63.36	62.76	
63.42	62.76	63.36	62.76	63.42	63.36	63.30	
map_weights [-0.08973114 -0.13109657 -0.60229183  0.17688046]
MAP reward
0.18	-0.13	-0.13	-0.13	0.18	-0.13	0.18	
0.18	-0.09	0.18	0.18	-0.09	-0.09	-0.09	
-0.09	-0.60	0.18	-0.13	-0.13	-0.13	0.18	
-0.13	-0.13	-0.60	-0.60	-0.09	0.18	-0.09	
-0.09	-0.13	0.18	0.18	-0.09	-0.13	0.18	
-0.09	-0.09	0.18	0.18	-0.60	0.18	-0.60	
0.18	-0.60	-0.09	-0.60	0.18	-0.09	-0.09	
Map policy
<	<	v	v	^	>	>	
^	<	>	<	<	>	^	
^	>	^	^	^	>	>	
^	v	v	v	>	>	^	
v	>	v	v	<	>	>	
v	>	>	<	<	v	^	
<	<	^	^	v	<	<	
expeced value MDP LP 30.339812967238817
mean w [-0.2297293  -0.0490414  -0.0999689   0.30700584]
Mean policy from posterior
<	<	v	v	^	>	>	
^	<	>	<	<	^	^	
^	>	^	^	<	>	>	
^	v	v	v	>	^	^	
>	>	v	v	<	>	>	
v	>	>	^	<	^	^	
<	<	^	^	v	<	^	
Mean rewards
0.31	-0.05	-0.05	-0.05	0.31	-0.05	0.31	
0.31	-0.23	0.31	0.31	-0.23	-0.23	-0.23	
-0.23	-0.10	0.31	-0.05	-0.05	-0.05	0.31	
-0.05	-0.05	-0.10	-0.10	-0.23	0.31	-0.23	
-0.23	-0.05	0.31	0.31	-0.23	-0.05	0.31	
-0.23	-0.23	0.31	0.31	-0.10	0.31	-0.10	
0.31	-0.10	-0.23	-0.10	0.31	-0.23	-0.23	
mean = 0.1541367300075862, map = 0.01001865238447408
CVaR policy
<	<	v	>	^	>	>	
^	<	>	<	<	^	^	
^	>	^	^	<	>	>	
^	>	v	v	<	^	^	
>	>	v	v	<	>	>	
v	>	>	<	<	<	^	
<	<	^	^	v	<	^	
CVaR policy
<	<	v	>	^	>	>	
^	<	>	<	<	^	^	
^	>	^	^	<	>	>	
^	v	v	v	>	^	^	
>	>	v	v	<	>	>	
v	>	>	^	<	^	^	
<	<	^	^	v	<	^	
CVaR policy
<	<	v	v	^	>	>	
^	<	>	<	<	^	^	
^	>	^	^	<	>	>	
^	v	v	v	>	^	^	
>	>	>	<	<	>	>	
v	>	>	^	<	^	^	
<	<	^	^	v	<	^	
CVaR policy
<	<	v	v	^	>	>	
^	<	>	<	<	^	^	
^	>	^	^	<	>	>	
^	v	v	v	>	^	^	
>	>	>	v	<	>	>	
v	>	>	<	<	^	^	
<	<	^	^	v	<	^	
CVaR policy
<	<	v	v	^	>	>	
^	<	>	<	<	^	^	
^	>	^	^	<	>	>	
^	v	v	v	>	^	^	
>	>	>	v	<	>	>	
v	>	>	<	<	^	^	
<	<	^	^	v	<	^	
cvar = , 0.12431510342334207, 0.15422820541686377, 0.1541378787689638, 0.15413714517844568, 0.1541400840217122
==========
iteration 19
==========
weights [-0.97993807  0.08216199  0.02445666 -0.17992402]
expeced value MDP LP 7.783013936417164
demonstration
[(24, 2), (17, 2), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1), (11, 0), (10, 1)]
[-0.45279669 -0.08296544 -0.00154305 -0.46269482]
w_map [-0.49833281  0.12341891 -0.18640551 -0.19184277] loglik 0.0
accepted/total = 2691/3000 = 0.897
-------
true weights [-0.97993807  0.08216199  0.02445666 -0.17992402]
features
3 	3 	1 	0 	3 	2 	0 	
1 	0 	2 	1 	1 	3 	0 	
2 	1 	2 	2 	0 	2 	3 	
1 	2 	2 	0 	0 	0 	0 	
3 	2 	1 	0 	3 	3 	2 	
0 	3 	2 	3 	0 	1 	3 	
1 	1 	3 	1 	1 	1 	0 	
optimal policy
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
^	<	<	^	^	^	<	
<	<	<	^	v	v	v	
^	^	<	<	>	v	v	
v	v	^	v	v	v	<	
<	<	<	v	v	<	<	
optimal values
7.95	7.95	8.22	7.15	7.95	7.90	6.84	
8.22	7.15	8.16	8.22	8.22	7.95	6.89	
8.16	8.16	8.10	8.16	7.15	7.90	7.64	
8.22	8.16	8.10	7.10	6.64	6.89	6.84	
7.95	8.10	8.10	7.04	7.69	7.95	7.90	
7.15	7.95	8.05	7.95	7.15	8.22	7.95	
8.22	8.22	7.95	8.22	8.22	8.22	7.15	
map_weights [-0.49833281  0.12341891 -0.18640551 -0.19184277]
MAP reward
-0.19	-0.19	0.12	-0.50	-0.19	-0.19	-0.50	
0.12	-0.50	-0.19	0.12	0.12	-0.19	-0.50	
-0.19	0.12	-0.19	-0.19	-0.50	-0.19	-0.19	
0.12	-0.19	-0.19	-0.50	-0.50	-0.50	-0.50	
-0.19	-0.19	0.12	-0.50	-0.19	-0.19	-0.19	
-0.50	-0.19	-0.19	-0.19	-0.50	0.12	-0.19	
0.12	0.12	-0.19	0.12	0.12	0.12	-0.50	
Map policy
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
^	<	<	^	^	^	<	
<	<	<	^	^	v	v	
^	^	^	v	>	v	v	
v	v	v	v	v	v	<	
<	<	<	>	v	<	<	
expeced value MDP LP 30.73130818901229
mean w [-0.26117056  0.31293999 -0.15625698 -0.1166736 ]
Mean policy from posterior
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
v	<	<	^	^	^	<	
<	<	<	^	^	v	v	
^	v	v	v	>	v	v	
v	v	v	v	v	v	<	
<	<	<	>	v	<	<	
Mean rewards
-0.12	-0.12	0.31	-0.26	-0.12	-0.16	-0.26	
0.31	-0.26	-0.16	0.31	0.31	-0.12	-0.26	
-0.16	0.31	-0.16	-0.16	-0.26	-0.16	-0.12	
0.31	-0.16	-0.16	-0.26	-0.26	-0.26	-0.26	
-0.12	-0.16	0.31	-0.26	-0.12	-0.12	-0.16	
-0.26	-0.12	-0.16	-0.12	-0.26	0.31	-0.12	
0.31	0.31	-0.12	0.31	0.31	0.31	-0.26	
mean = 0.02513527071255428, map = 0.016917891905907645
CVaR policy
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
^	<	<	^	^	^	<	
<	<	<	^	^	v	v	
^	v	v	v	>	v	v	
v	v	v	v	v	v	<	
<	<	<	>	v	<	<	
CVaR policy
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
^	<	<	^	^	^	<	
<	<	<	^	^	v	v	
^	v	v	v	>	v	v	
v	v	v	v	>	v	<	
<	<	<	>	v	<	<	
CVaR policy
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
^	<	<	^	^	^	<	
<	<	<	^	^	v	v	
^	v	v	v	>	v	v	
v	v	v	v	v	v	<	
<	<	<	>	v	<	<	
CVaR policy
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
^	<	<	^	^	^	<	
<	<	<	^	^	v	v	
^	v	v	v	>	v	v	
v	v	v	v	v	v	<	
<	<	<	v	v	<	<	
CVaR policy
v	>	^	<	v	<	<	
<	<	^	>	<	<	<	
^	<	<	^	^	^	<	
<	<	<	^	^	v	v	
^	v	v	v	>	v	v	
v	v	v	v	v	v	<	
<	<	<	v	v	<	<	
cvar = , 0.025135348930354517, 0.025135925097830913, 0.025195445535799443, 0.02513556627481428, 0.025268146258709656
==========
iteration 20
==========
weights [-0.16368012  0.79691946 -0.18642331  0.55079446]
expeced value MDP LP 77.65928850536528
demonstration
[(24, 3), (31, 1), (32, 3), (39, 3), (46, 0), (45, 0), (44, 0), (43, 0), (42, 3), (42, 0), (42, 0), (42, 0), (42, 0), (42, 0), (42, 3), (42, 3), (42, 0), (42, 0), (42, 0), (42, 0), (42, 3), (42, 0), (42, 0), (42, 3), (42, 0), (42, 0), (42, 0), (42, 0), (42, 0), (42, 3), (42, 3), (42, 3), (42, 3), (42, 3), (42, 0), (42, 3), (42, 0), (42, 3), (42, 3), (42, 0), (42, 3), (42, 3), (42, 0), (42, 0), (42, 3), (42, 3), (42, 3), (42, 0), (42, 3)]
[-0.35578426 -0.28854928 -0.09018256 -0.2654839 ]
w_map [-0.54764952  0.23944281 -0.18298962  0.02991805] loglik -28.419034432748504
accepted/total = 1905/3000 = 0.635
-------
true weights [-0.16368012  0.79691946 -0.18642331  0.55079446]
features
3 	0 	3 	2 	0 	1 	2 	
2 	3 	2 	2 	2 	1 	0 	
2 	2 	0 	0 	0 	2 	1 	
2 	2 	3 	3 	0 	2 	0 	
3 	2 	2 	3 	1 	0 	0 	
2 	3 	0 	0 	3 	2 	2 	
1 	2 	3 	3 	3 	2 	3 	
optimal policy
>	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	v	>	>	>	>	
v	v	>	v	v	>	^	
v	v	>	>	v	<	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
optimal values
76.38	76.59	77.53	77.76	78.73	79.69	78.71	
75.60	76.55	76.77	77.74	78.71	79.69	78.73	
76.54	75.60	76.19	76.82	77.76	78.71	79.69	
77.50	76.54	77.12	77.34	76.86	77.76	78.73	
78.47	77.50	76.61	77.57	77.80	76.86	77.78	
78.71	78.47	77.52	77.29	77.78	76.81	76.82	
79.69	78.71	78.47	78.24	78.01	77.04	76.82	
map_weights [-0.54764952  0.23944281 -0.18298962  0.02991805]
MAP reward
0.03	-0.55	0.03	-0.18	-0.55	0.24	-0.18	
-0.18	0.03	-0.18	-0.18	-0.18	0.24	-0.55	
-0.18	-0.18	-0.55	-0.55	-0.55	-0.18	0.24	
-0.18	-0.18	0.03	0.03	-0.55	-0.18	-0.55	
0.03	-0.18	-0.18	0.03	0.24	-0.55	-0.55	
-0.18	0.03	-0.55	-0.55	0.03	-0.18	-0.18	
0.24	-0.18	0.03	0.03	0.03	-0.18	0.03	
Map policy
v	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	^	^	>	>	>	
v	v	<	v	>	^	^	
v	v	<	>	v	^	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
expeced value MDP LP 25.227624404117492
mean w [-0.31355312  0.26221063 -0.18790818  0.14480333]
Mean policy from posterior
>	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	v	^	>	>	>	
v	v	>	v	v	^	^	
v	v	>	>	v	<	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
Mean rewards
0.14	-0.31	0.14	-0.19	-0.31	0.26	-0.19	
-0.19	0.14	-0.19	-0.19	-0.19	0.26	-0.31	
-0.19	-0.19	-0.31	-0.31	-0.31	-0.19	0.26	
-0.19	-0.19	0.14	0.14	-0.31	-0.19	-0.31	
0.14	-0.19	-0.19	0.14	0.26	-0.31	-0.31	
-0.19	0.14	-0.31	-0.31	0.14	-0.19	-0.19	
0.26	-0.19	0.14	0.14	0.14	-0.19	0.14	
mean = 0.0009190288907916511, map = 0.04812294916240489
CVaR policy
>	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	v	^	>	>	>	
v	v	>	v	>	^	^	
v	v	<	>	v	^	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
CVaR policy
>	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	v	^	>	>	>	
v	v	>	v	>	^	^	
v	v	<	>	v	^	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
CVaR policy
>	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	v	^	>	>	>	
v	v	>	v	v	^	^	
v	v	>	>	v	<	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
CVaR policy
>	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	v	^	>	>	>	
v	v	>	v	v	^	^	
v	v	>	>	v	<	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
CVaR policy
>	>	>	>	>	^	<	
>	>	>	>	>	^	v	
v	^	v	^	>	>	>	
v	v	>	v	v	^	^	
v	v	>	>	v	<	^	
v	<	<	v	v	<	^	
<	<	<	<	<	<	<	
cvar = , 0.00478745427183469, 0.004789055937806097, 0.0009191288151839672, 0.0009190847750488729, 0.0009196254102903367
==========
iteration 21
==========
weights [ 0.7800773   0.10833555 -0.49633758  0.36522844]
expeced value MDP LP 77.43118510753811
demonstration
[(24, 2), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3)]
[-0.06913291 -0.4594355  -0.18781518 -0.28361641]
w_map [ 0.25616489 -0.29653232 -0.1677113  -0.27959149] loglik 0.0
accepted/total = 2742/3000 = 0.914
-------
true weights [ 0.7800773   0.10833555 -0.49633758  0.36522844]
features
0 	0 	1 	2 	0 	3 	3 	
2 	1 	3 	0 	3 	2 	3 	
3 	2 	2 	0 	3 	2 	2 	
0 	1 	1 	2 	3 	0 	1 	
0 	3 	0 	3 	0 	1 	0 	
0 	3 	3 	3 	0 	3 	0 	
0 	1 	1 	3 	0 	0 	2 	
optimal policy
<	<	<	v	^	<	<	
^	^	>	v	<	<	^	
v	<	>	^	<	v	v	
v	<	v	^	v	<	v	
v	<	<	>	v	>	v	
v	<	^	>	v	v	>	
<	<	>	>	>	v	^	
optimal values
78.01	78.01	77.34	76.73	78.01	77.59	77.18	
76.73	77.34	77.59	78.01	77.59	76.32	76.78	
77.59	76.32	76.73	78.01	77.59	76.32	76.07	
78.01	77.34	76.93	76.73	77.59	77.60	77.34	
78.01	77.59	77.60	77.59	78.01	77.34	78.01	
78.01	77.59	77.19	77.59	78.01	77.59	78.01	
78.01	77.34	76.93	77.59	78.01	78.01	76.73	
map_weights [ 0.25616489 -0.29653232 -0.1677113  -0.27959149]
MAP reward
0.26	0.26	-0.30	-0.17	0.26	-0.28	-0.28	
-0.17	-0.30	-0.28	0.26	-0.28	-0.17	-0.28	
-0.28	-0.17	-0.17	0.26	-0.28	-0.17	-0.17	
0.26	-0.30	-0.30	-0.17	-0.28	0.26	-0.30	
0.26	-0.28	0.26	-0.28	0.26	-0.30	0.26	
0.26	-0.28	-0.28	-0.28	0.26	-0.28	0.26	
0.26	-0.30	-0.30	-0.28	0.26	0.26	-0.17	
Map policy
<	<	<	v	^	<	<	
^	^	>	v	<	<	<	
v	>	>	^	<	v	v	
v	<	>	^	v	<	v	
v	<	<	>	v	>	v	
v	<	^	>	v	v	>	
<	<	>	>	>	v	<	
expeced value MDP LP 37.44590660241749
mean w [ 0.37846434 -0.07633379 -0.01659908 -0.0973287 ]
Mean policy from posterior
<	<	<	v	^	<	<	
^	^	>	v	<	<	v	
v	>	>	^	<	v	v	
v	<	>	^	v	v	v	
v	<	<	>	v	<	v	
v	<	^	>	v	v	>	
<	<	<	>	>	v	<	
Mean rewards
0.38	0.38	-0.08	-0.02	0.38	-0.10	-0.10	
-0.02	-0.08	-0.10	0.38	-0.10	-0.02	-0.10	
-0.10	-0.02	-0.02	0.38	-0.10	-0.02	-0.02	
0.38	-0.08	-0.08	-0.02	-0.10	0.38	-0.08	
0.38	-0.10	0.38	-0.10	0.38	-0.08	0.38	
0.38	-0.10	-0.10	-0.10	0.38	-0.10	0.38	
0.38	-0.08	-0.08	-0.10	0.38	0.38	-0.02	
mean = 0.07296260355292361, map = 0.05230527238117588
CVaR policy
<	<	<	>	^	<	<	
^	^	>	v	^	<	<	
v	>	>	^	<	v	v	
v	<	>	^	v	<	v	
v	<	<	>	v	>	v	
v	<	^	>	v	>	>	
<	<	>	>	>	v	^	
CVaR policy
<	<	<	>	^	<	<	
^	^	>	v	^	<	v	
v	>	>	^	<	v	v	
v	<	>	^	v	v	v	
v	<	<	>	v	<	v	
v	<	^	>	v	<	>	
<	<	<	>	>	v	<	
CVaR policy
<	<	<	>	^	<	<	
^	^	>	v	<	<	v	
v	>	>	^	<	v	v	
v	<	>	^	v	v	v	
v	<	<	>	v	<	v	
v	<	^	>	v	v	>	
<	<	<	>	>	v	<	
CVaR policy
<	<	<	>	^	<	<	
^	^	>	v	^	<	v	
v	>	>	^	<	v	v	
v	<	>	^	v	v	v	
v	<	<	>	v	<	v	
v	<	^	>	v	<	>	
<	<	<	>	>	v	<	
CVaR policy
<	<	<	>	^	<	<	
^	^	>	v	^	<	v	
v	>	>	^	<	v	v	
v	<	>	^	v	v	v	
v	<	<	>	v	<	v	
v	<	^	>	v	v	>	
<	<	<	>	>	v	<	
cvar = , 0.05230526474962005, 0.07295148001011853, 0.07296261030737128, 0.07296259998112475, 0.07296542091307856
==========
iteration 22
==========
weights [ 0.33698107 -0.90306837 -0.21486569 -0.15730229]
expeced value MDP LP 32.6518805274133
demonstration
[(24, 1), (25, 1), (26, 0), (25, 1), (26, 2), (19, 3), (26, 2), (19, 1), (20, 1), (20, 1), (20, 1), (20, 0), (19, 1), (20, 0), (19, 3), (26, 0), (25, 1), (26, 0), (25, 1), (26, 0), (25, 1), (26, 0), (25, 1), (26, 0), (25, 1), (26, 2), (19, 3), (26, 2), (19, 1), (20, 1), (20, 0), (19, 3), (26, 2), (19, 3), (26, 2), (19, 3), (26, 0), (25, 1), (26, 2), (19, 3), (26, 0), (25, 1), (26, 0), (25, 1), (26, 2), (19, 1), (20, 0), (19, 3), (26, 0)]
[-0.17280438 -0.40000055 -0.26046272 -0.16673234]
w_map [ 0.40272984 -0.39118847 -0.10833858  0.09774311] loglik -26.33959286127083
accepted/total = 2758/3000 = 0.9193333333333333
-------
true weights [ 0.33698107 -0.90306837 -0.21486569 -0.15730229]
features
0 	2 	2 	3 	0 	1 	1 	
3 	3 	1 	1 	3 	1 	1 	
1 	1 	1 	1 	2 	0 	0 	
2 	2 	2 	3 	0 	0 	1 	
2 	3 	1 	3 	1 	1 	1 	
3 	2 	0 	0 	0 	2 	1 	
1 	1 	2 	2 	0 	1 	1 	
optimal policy
<	<	>	>	^	<	v	
^	<	<	>	^	v	v	
^	^	v	v	>	>	>	
v	v	>	>	>	^	<	
>	v	v	v	^	^	^	
>	>	>	>	v	<	<	
^	>	^	>	v	<	<	
optimal values
33.70	33.15	32.66	33.20	33.70	32.46	31.23	
33.20	32.71	31.48	31.97	33.20	32.46	32.46	
31.97	31.48	31.43	31.97	33.15	33.70	33.70	
31.58	32.12	32.66	33.20	33.70	33.70	32.46	
32.12	32.66	32.46	33.20	32.46	32.46	31.23	
32.66	33.15	33.70	33.70	33.70	33.15	31.91	
31.43	31.91	33.15	33.15	33.70	32.46	31.23	
map_weights [ 0.40272984 -0.39118847 -0.10833858  0.09774311]
MAP reward
0.40	-0.11	-0.11	0.10	0.40	-0.39	-0.39	
0.10	0.10	-0.39	-0.39	0.10	-0.39	-0.39	
-0.39	-0.39	-0.39	-0.39	-0.11	0.40	0.40	
-0.11	-0.11	-0.11	0.10	0.40	0.40	-0.39	
-0.11	0.10	-0.39	0.10	-0.39	-0.39	-0.39	
0.10	-0.11	0.40	0.40	0.40	-0.11	-0.39	
-0.39	-0.39	-0.11	-0.11	0.40	-0.39	-0.39	
Map policy
<	<	>	>	^	<	v	
^	<	<	>	^	v	v	
^	^	v	v	>	>	>	
^	v	>	>	>	^	^	
>	v	v	v	v	^	^	
>	>	>	>	v	<	<	
^	>	^	>	v	<	<	
expeced value MDP LP 36.85005001519642
mean w [ 0.3748061  -0.14348347 -0.05545851 -0.08726423]
Mean policy from posterior
<	<	<	>	^	<	v	
^	^	^	>	^	v	v	
^	^	v	>	>	>	>	
^	>	>	>	>	^	^	
>	v	v	v	v	^	^	
>	>	>	>	v	<	<	
^	>	^	>	v	<	<	
Mean rewards
0.37	-0.06	-0.06	-0.09	0.37	-0.14	-0.14	
-0.09	-0.09	-0.14	-0.14	-0.09	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.06	0.37	0.37	
-0.06	-0.06	-0.06	-0.09	0.37	0.37	-0.14	
-0.06	-0.09	-0.14	-0.09	-0.14	-0.14	-0.14	
-0.09	-0.06	0.37	0.37	0.37	-0.06	-0.14	
-0.14	-0.14	-0.06	-0.06	0.37	-0.14	-0.14	
mean = 0.009943266084476932, map = 0.002976803234496117
CVaR policy
<	<	<	>	^	<	v	
^	^	^	>	^	v	v	
^	<	>	>	>	>	>	
^	>	v	>	>	^	<	
>	>	v	v	^	^	^	
>	>	>	>	v	<	<	
>	>	^	>	v	<	<	
CVaR policy
<	<	<	>	^	<	v	
^	^	^	>	^	v	v	
^	^	v	>	>	v	>	
^	>	v	>	>	^	<	
v	v	v	v	^	^	^	
>	>	>	>	v	<	<	
>	>	^	>	v	<	<	
CVaR policy
<	<	<	>	^	<	v	
^	^	^	>	^	v	v	
^	^	v	>	v	>	>	
^	>	>	>	>	^	<	
>	v	v	v	^	^	^	
>	>	>	>	v	<	<	
^	>	^	^	v	<	<	
CVaR policy
<	<	<	>	^	<	v	
^	^	^	>	^	v	v	
^	^	v	>	>	>	>	
^	>	>	>	>	^	<	
>	v	v	v	^	^	^	
>	>	>	>	v	<	<	
^	>	^	>	v	<	<	
CVaR policy
<	<	<	>	^	<	v	
^	^	^	>	^	v	v	
^	^	v	>	>	>	>	
^	>	>	>	>	^	<	
>	v	v	v	^	^	^	
>	>	>	>	v	<	<	
^	>	^	>	v	<	<	
cvar = , 0.11165447808989626, 0.06997934115743476, 0.010113879745787813, 0.009945892377359655, 0.009960316526012036
==========
iteration 23
==========
weights [ 0.74596622 -0.61694464 -0.23770219 -0.08007103]
expeced value MDP LP 73.50638877958932
demonstration
[(24, 2), (17, 2), (10, 3), (17, 2), (10, 0), (9, 1), (10, 1), (11, 0), (10, 1), (11, 2), (4, 3), (11, 0), (10, 1), (11, 0), (10, 0), (9, 1), (10, 0), (9, 1), (10, 0), (9, 1), (10, 0), (9, 1), (10, 0), (9, 1), (10, 1), (11, 2), (4, 2), (4, 3), (11, 2), (4, 2), (4, 1), (5, 2), (5, 0), (4, 1), (5, 0), (4, 3), (11, 2), (4, 1), (5, 1), (6, 2), (6, 0), (5, 1), (6, 1), (6, 1), (6, 2), (6, 2), (6, 0), (5, 1), (6, 0)]
[-0.54395282 -0.14374951 -0.27136673 -0.04093094]
w_map [ 0.72704772  0.15018826  0.04496421 -0.07779982] loglik -41.10623356496126
accepted/total = 2778/3000 = 0.926
-------
true weights [ 0.74596622 -0.61694464 -0.23770219 -0.08007103]
features
1 	2 	2 	1 	0 	0 	0 	
3 	1 	0 	0 	0 	2 	1 	
1 	0 	2 	0 	1 	3 	1 	
0 	3 	2 	3 	1 	1 	1 	
1 	2 	0 	3 	1 	0 	0 	
3 	1 	0 	1 	1 	3 	1 	
3 	2 	0 	2 	3 	2 	3 	
optimal policy
>	>	v	>	>	>	>	
>	>	>	>	^	^	^	
v	v	>	^	^	^	^	
<	<	v	^	<	v	v	
^	>	v	<	>	>	>	
>	>	v	<	>	^	^	
>	>	^	<	<	^	^	
optimal values
71.30	72.64	73.61	73.23	74.60	74.60	74.60	
72.42	73.23	74.60	74.60	74.60	73.61	73.23	
73.23	73.78	73.61	74.60	73.23	72.80	71.88	
74.60	73.77	73.61	73.77	72.42	73.23	73.23	
73.23	73.61	74.60	73.77	73.23	74.60	74.60	
72.42	73.23	74.60	73.23	72.42	73.77	73.23	
72.80	73.61	74.60	73.61	72.80	72.80	72.42	
map_weights [ 0.72704772  0.15018826  0.04496421 -0.07779982]
MAP reward
0.15	0.04	0.04	0.15	0.73	0.73	0.73	
-0.08	0.15	0.73	0.73	0.73	0.04	0.15	
0.15	0.73	0.04	0.73	0.15	-0.08	0.15	
0.73	-0.08	0.04	-0.08	0.15	0.15	0.15	
0.15	0.04	0.73	-0.08	0.15	0.73	0.73	
-0.08	0.15	0.73	0.15	0.15	-0.08	0.15	
-0.08	0.04	0.73	0.04	-0.08	0.04	-0.08	
Map policy
>	v	v	>	>	>	>	
>	>	>	>	^	^	^	
v	^	>	^	^	<	^	
<	<	v	^	^	v	v	
^	>	v	<	>	>	>	
>	>	v	<	<	^	^	
>	>	v	<	<	^	^	
expeced value MDP LP 38.5824179325397
mean w [ 0.39065827 -0.16141564 -0.02410202 -0.07693048]
Mean policy from posterior
>	>	v	>	>	>	>	
>	>	>	>	^	^	^	
v	>	>	^	^	^	^	
<	<	v	^	<	v	v	
^	>	v	<	>	>	>	
>	>	v	<	>	^	^	
>	>	v	<	<	^	^	
Mean rewards
-0.16	-0.02	-0.02	-0.16	0.39	0.39	0.39	
-0.08	-0.16	0.39	0.39	0.39	-0.02	-0.16	
-0.16	0.39	-0.02	0.39	-0.16	-0.08	-0.16	
0.39	-0.08	-0.02	-0.08	-0.16	-0.16	-0.16	
-0.16	-0.02	0.39	-0.08	-0.16	0.39	0.39	
-0.08	-0.16	0.39	-0.16	-0.16	-0.08	-0.16	
-0.08	-0.02	0.39	-0.02	-0.08	-0.02	-0.08	
mean = 0.00318479367190605, map = 0.05545208378190125
CVaR policy
>	>	v	v	>	>	>	
>	>	>	>	^	<	^	
v	v	^	^	<	^	^	
<	<	v	^	<	v	v	
^	>	v	<	>	>	>	
>	>	v	<	>	^	^	
>	>	^	<	<	^	^	
CVaR policy
>	>	v	v	>	>	>	
>	>	>	>	^	^	^	
v	v	>	^	<	^	^	
<	<	v	^	<	v	v	
^	>	v	<	>	>	>	
>	>	^	<	>	^	^	
>	>	v	<	<	^	^	
CVaR policy
>	>	v	>	>	>	>	
>	>	>	>	^	^	^	
v	>	>	^	^	^	^	
<	<	v	^	<	v	v	
^	>	v	<	>	>	>	
>	>	v	<	>	^	^	
>	>	v	<	<	^	^	
CVaR policy
>	>	v	>	>	>	>	
>	>	>	>	^	^	^	
v	>	>	^	^	^	^	
<	<	v	^	<	v	v	
^	>	v	<	>	>	>	
>	>	v	<	>	^	^	
>	>	^	<	<	^	^	
CVaR policy
>	>	v	>	>	>	>	
>	>	>	>	^	^	^	
v	>	>	^	^	^	^	
<	<	v	^	<	v	v	
^	>	v	<	>	>	>	
>	>	v	<	>	^	^	
>	>	^	<	<	^	^	
cvar = , 4.2377189402031945e-07, 6.74760528340812e-06, 0.0031848442288691103, 0.003184955024721603, 0.0031848005735923834
==========
iteration 24
==========
weights [-0.21828339  0.10337873  0.95071939  0.19441665]
expeced value MDP LP 94.069173730126
demonstration
[(24, 2), (17, 1), (18, 1), (19, 1), (20, 2), (13, 1), (13, 1), (13, 3), (20, 1), (20, 2), (13, 3), (20, 1), (20, 2), (13, 3), (20, 1), (20, 2), (13, 3), (20, 0), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 1), (20, 0), (19, 1), (20, 0), (19, 0), (18, 1), (19, 1), (20, 2), (13, 1), (13, 1), (13, 1), (13, 1), (13, 1), (13, 3), (20, 0), (19, 1), (20, 0), (19, 1), (20, 0), (19, 0), (18, 1), (19, 1), (20, 2), (13, 1), (13, 3)]
[-0.36901667 -0.39338624 -0.1607825  -0.07681459]
w_map [-0.03759817 -0.34216692  0.49223745 -0.12799746] loglik -35.194157919091595
accepted/total = 2598/3000 = 0.866
-------
true weights [-0.21828339  0.10337873  0.95071939  0.19441665]
features
1 	1 	2 	2 	2 	2 	3 	
2 	1 	3 	2 	3 	0 	2 	
0 	1 	1 	3 	2 	2 	2 	
0 	0 	1 	2 	1 	0 	0 	
1 	0 	0 	0 	3 	3 	1 	
0 	0 	2 	3 	0 	1 	0 	
0 	0 	2 	0 	2 	2 	3 	
optimal policy
v	>	>	>	>	^	<	
<	<	^	^	^	^	>	
^	^	^	^	>	>	^	
^	>	>	^	^	^	^	
>	>	v	^	^	v	^	
>	>	v	<	v	v	v	
>	>	v	>	v	<	<	
optimal values
94.22	94.22	95.07	95.07	95.07	95.07	94.32	
95.07	94.22	94.32	95.07	94.32	93.90	95.07	
93.90	93.39	93.48	94.32	95.07	95.07	95.07	
92.75	92.33	93.48	94.32	94.22	93.90	93.90	
91.92	92.75	93.90	93.16	93.48	93.48	93.07	
92.75	93.90	95.07	94.32	93.90	94.22	93.15	
92.75	93.90	95.07	93.90	95.07	95.07	94.32	
map_weights [-0.03759817 -0.34216692  0.49223745 -0.12799746]
MAP reward
-0.34	-0.34	0.49	0.49	0.49	0.49	-0.13	
0.49	-0.34	-0.13	0.49	-0.13	-0.04	0.49	
-0.04	-0.34	-0.34	-0.13	0.49	0.49	0.49	
-0.04	-0.04	-0.34	0.49	-0.34	-0.04	-0.04	
-0.34	-0.04	-0.04	-0.04	-0.13	-0.13	-0.34	
-0.04	-0.04	0.49	-0.13	-0.04	-0.34	-0.04	
-0.04	-0.04	0.49	-0.04	0.49	0.49	-0.13	
Map policy
v	>	>	>	^	<	<	
<	<	^	^	^	^	>	
^	<	^	^	>	>	^	
^	v	v	^	^	^	^	
>	v	v	<	v	^	^	
>	>	v	<	v	v	v	
>	>	v	>	>	<	<	
expeced value MDP LP 38.32385267091556
mean w [-0.13334901 -0.20828308  0.3882203   0.07516425]
Mean policy from posterior
v	>	>	>	>	^	<	
<	<	^	^	^	^	>	
^	<	^	^	>	>	^	
^	>	>	^	^	^	^	
>	>	v	^	v	^	^	
>	>	v	<	v	v	v	
>	>	v	>	v	v	<	
Mean rewards
-0.21	-0.21	0.39	0.39	0.39	0.39	0.08	
0.39	-0.21	0.08	0.39	0.08	-0.13	0.39	
-0.13	-0.21	-0.21	0.08	0.39	0.39	0.39	
-0.13	-0.13	-0.21	0.39	-0.21	-0.13	-0.13	
-0.21	-0.13	-0.13	-0.13	0.08	0.08	-0.21	
-0.13	-0.13	0.39	0.08	-0.13	-0.21	-0.13	
-0.13	-0.13	0.39	-0.13	0.39	0.39	0.08	
mean = 0.01949666780548398, map = 0.05138406813050267
CVaR policy
v	>	>	>	>	^	<	
<	<	>	^	^	^	>	
^	<	^	^	>	>	^	
^	>	>	^	^	^	^	
>	>	v	^	v	^	^	
>	>	v	<	v	v	v	
>	>	v	>	v	<	<	
CVaR policy
v	>	>	>	>	^	<	
<	<	^	^	<	^	>	
^	<	^	^	>	>	^	
^	>	>	^	^	^	^	
>	>	v	^	v	^	^	
>	>	v	<	v	v	v	
>	>	v	>	v	<	<	
CVaR policy
v	>	^	>	^	^	<	
<	<	>	^	<	^	>	
^	<	^	^	>	>	^	
^	>	>	^	^	^	^	
>	>	v	^	v	^	^	
>	>	v	<	v	v	v	
>	>	v	>	v	v	<	
CVaR policy
v	>	^	>	^	^	<	
<	<	>	^	^	^	>	
^	<	^	^	>	>	^	
^	>	>	^	^	^	^	
>	>	v	^	v	^	^	
>	>	v	<	v	v	v	
>	>	v	>	v	v	<	
CVaR policy
v	>	^	>	>	^	<	
<	<	>	^	^	^	>	
^	<	^	^	>	>	^	
^	>	>	^	^	^	^	
>	>	v	^	v	^	^	
>	>	v	<	v	v	v	
>	>	v	>	v	v	<	
cvar = , 0.019496664678840148, 0.019510149100355534, 0.019496667365999087, 0.019551458659890386, 0.019545667530479705
==========
iteration 25
==========
weights [ 0.33544029 -0.39991624  0.82190943  0.22806073]
expeced value MDP LP 81.22335740925467
demonstration
[(24, 3), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2)]
[-0.0184706  -0.63703904 -0.22465886 -0.1198315 ]
w_map [-0.00082201 -0.10211012  0.28055567 -0.6165122 ] loglik -0.6931415298622596
accepted/total = 2730/3000 = 0.91
-------
true weights [ 0.33544029 -0.39991624  0.82190943  0.22806073]
features
0 	3 	2 	1 	2 	1 	2 	
3 	0 	3 	1 	3 	1 	0 	
3 	1 	0 	3 	2 	0 	0 	
3 	3 	1 	3 	2 	1 	0 	
0 	1 	1 	2 	1 	2 	1 	
0 	0 	3 	2 	1 	2 	3 	
0 	0 	2 	0 	0 	1 	0 	
optimal policy
>	>	^	>	^	<	>	
>	>	^	>	^	v	^	
^	>	>	>	v	<	<	
v	>	>	v	^	v	^	
v	v	>	v	>	v	<	
>	v	>	^	>	^	<	
>	>	v	^	<	^	^	
optimal values
81.12	81.60	82.19	80.97	82.19	80.97	82.19	
80.53	81.12	81.60	80.38	81.60	80.49	81.70	
79.96	79.91	81.12	81.60	82.19	81.70	81.22	
79.70	79.81	80.38	81.60	82.19	80.97	80.75	
80.27	80.01	80.97	82.19	80.97	82.19	80.97	
80.75	81.22	81.60	82.19	80.97	82.19	81.60	
81.22	81.70	82.19	81.70	81.22	80.97	81.12	
map_weights [-0.00082201 -0.10211012  0.28055567 -0.6165122 ]
MAP reward
-0.00	-0.62	0.28	-0.10	0.28	-0.10	0.28	
-0.62	-0.00	-0.62	-0.10	-0.62	-0.10	-0.00	
-0.62	-0.10	-0.00	-0.62	0.28	-0.00	-0.00	
-0.62	-0.62	-0.10	-0.62	0.28	-0.10	-0.00	
-0.00	-0.10	-0.10	0.28	-0.10	0.28	-0.10	
-0.00	-0.00	-0.62	0.28	-0.10	0.28	-0.62	
-0.00	-0.00	0.28	-0.00	-0.00	-0.10	-0.00	
Map policy
>	>	^	>	^	>	>	
>	>	^	^	^	>	^	
>	>	v	>	v	<	^	
v	>	v	>	^	<	<	
>	>	>	v	^	v	<	
>	v	>	^	>	^	<	
>	>	v	^	<	^	<	
expeced value MDP LP 31.538228948112305
mean w [-0.14509542 -0.15327434  0.32167597 -0.11258238]
Mean policy from posterior
>	>	^	<	^	<	>	
>	^	^	v	v	<	^	
^	>	>	>	v	<	<	
>	>	>	v	^	v	<	
>	>	>	v	<	v	<	
>	>	v	^	<	^	<	
>	>	v	<	<	^	^	
Mean rewards
-0.15	-0.11	0.32	-0.15	0.32	-0.15	0.32	
-0.11	-0.15	-0.11	-0.15	-0.11	-0.15	-0.15	
-0.11	-0.15	-0.15	-0.11	0.32	-0.15	-0.15	
-0.11	-0.11	-0.15	-0.11	0.32	-0.15	-0.15	
-0.15	-0.15	-0.15	0.32	-0.15	0.32	-0.15	
-0.15	-0.15	-0.11	0.32	-0.15	0.32	-0.11	
-0.15	-0.15	0.32	-0.15	-0.15	-0.15	-0.15	
mean = 0.04614095212824054, map = 0.22350201821413407
CVaR policy
>	>	^	>	^	>	>	
>	^	^	^	^	^	^	
>	>	>	>	v	<	^	
>	>	v	>	^	<	<	
>	>	>	v	^	v	<	
>	>	v	^	>	^	<	
>	>	v	<	^	^	<	
CVaR policy
>	>	^	>	^	>	>	
>	>	^	>	^	<	^	
^	>	>	>	v	<	^	
>	>	>	v	^	v	<	
>	>	>	v	>	v	<	
>	>	v	^	>	^	<	
>	>	v	<	^	^	^	
CVaR policy
>	>	^	>	^	>	>	
>	>	^	>	^	<	^	
^	>	>	>	v	<	^	
>	>	>	>	^	v	<	
>	>	>	v	>	v	<	
>	>	>	^	>	^	<	
>	>	v	<	^	^	^	
CVaR policy
>	>	^	>	^	>	>	
>	>	^	>	^	<	^	
^	>	>	>	v	<	^	
>	>	>	v	^	v	<	
>	>	>	v	>	v	<	
>	>	v	^	>	^	<	
>	>	v	<	^	^	^	
CVaR policy
>	>	^	>	^	>	>	
>	>	^	>	^	<	^	
^	>	>	>	v	<	^	
>	>	>	v	^	v	<	
>	>	>	v	>	v	<	
>	>	v	^	>	^	<	
>	>	v	<	<	^	^	
cvar = , 0.14943462079125425, 0.060998168069318126, 0.06099830186869326, 0.06100415830235306, 0.04622867648380691
==========
iteration 26
==========
weights [-0.34533818  0.57686709  0.41964744  0.60980483]
expeced value MDP LP 60.542656141147994
demonstration
[(24, 0), (23, 3), (30, 0), (29, 3), (36, 2), (29, 3), (36, 2), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 3), (36, 2), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 3), (36, 2), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 3), (36, 2), (29, 3), (36, 2), (29, 1), (30, 0), (29, 3), (36, 2), (29, 3), (36, 2), (29, 3), (36, 2)]
[-0.01716561 -0.62824029 -0.31451334 -0.04008077]
w_map [-0.3576336   0.06349205 -0.26108689  0.31778746] loglik -15.942385073034984
accepted/total = 2456/3000 = 0.8186666666666667
-------
true weights [-0.34533818  0.57686709  0.41964744  0.60980483]
features
0 	1 	2 	2 	1 	1 	1 	
2 	0 	1 	0 	3 	3 	3 	
2 	1 	3 	0 	2 	1 	2 	
0 	2 	1 	1 	0 	0 	1 	
2 	3 	3 	2 	3 	1 	3 	
1 	3 	0 	0 	0 	0 	0 	
2 	0 	3 	0 	1 	2 	0 	
optimal policy
>	>	v	>	v	v	v	
v	>	v	>	>	>	>	
>	>	v	<	^	^	^	
>	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	^	^	^	^	
^	>	v	<	<	<	^	
optimal values
59.74	60.70	60.73	60.76	60.95	60.95	60.95	
60.54	59.96	60.92	60.03	60.98	60.98	60.98	
60.73	60.92	60.95	59.99	60.79	60.95	60.79	
59.84	60.79	60.95	60.91	59.99	59.99	60.95	
60.79	60.98	60.98	60.79	60.95	60.95	60.98	
60.95	60.98	60.03	59.84	59.99	59.99	60.03	
60.76	60.03	60.98	60.03	60.00	59.82	59.08	
map_weights [-0.3576336   0.06349205 -0.26108689  0.31778746]
MAP reward
-0.36	0.06	-0.26	-0.26	0.06	0.06	0.06	
-0.26	-0.36	0.06	-0.36	0.32	0.32	0.32	
-0.26	0.06	0.32	-0.36	-0.26	0.06	-0.26	
-0.36	-0.26	0.06	0.06	-0.36	-0.36	0.06	
-0.26	0.32	0.32	-0.26	0.32	0.06	0.32	
0.06	0.32	-0.36	-0.36	-0.36	-0.36	-0.36	
-0.26	-0.36	0.32	-0.36	0.06	-0.26	-0.36	
Map policy
>	>	v	>	v	v	v	
v	v	v	>	>	>	>	
>	>	v	<	^	^	^	
v	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	^	^	^	^	
^	>	v	<	<	<	^	
expeced value MDP LP 35.082201303030814
mean w [-0.14701447  0.06058759 -0.19495075  0.35675121]
Mean policy from posterior
>	v	v	>	v	v	v	
>	>	v	>	>	>	>	
>	>	v	<	^	^	^	
v	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	<	^	^	^	
^	>	v	<	<	<	^	
Mean rewards
-0.15	0.06	-0.19	-0.19	0.06	0.06	0.06	
-0.19	-0.15	0.06	-0.15	0.36	0.36	0.36	
-0.19	0.06	0.36	-0.15	-0.19	0.06	-0.19	
-0.15	-0.19	0.06	0.06	-0.15	-0.15	0.06	
-0.19	0.36	0.36	-0.19	0.36	0.06	0.36	
0.06	0.36	-0.15	-0.15	-0.15	-0.15	-0.15	
-0.19	-0.15	0.36	-0.15	0.06	-0.19	-0.15	
mean = 0.06166876540186905, map = 2.813196360307302e-07
CVaR policy
>	v	v	>	v	v	v	
>	v	v	>	>	>	>	
>	>	v	<	^	^	^	
>	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	<	^	^	^	
^	>	v	<	<	<	^	
CVaR policy
>	v	v	>	v	v	v	
>	v	v	>	>	>	>	
>	>	v	<	^	^	^	
>	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	<	^	^	^	
^	>	v	<	<	<	^	
CVaR policy
>	v	v	>	v	v	v	
>	v	v	>	>	>	>	
>	>	v	<	^	^	^	
>	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	<	^	^	^	
^	>	v	<	<	<	^	
CVaR policy
>	v	v	>	v	v	v	
>	v	v	>	>	>	>	
>	>	v	<	^	^	^	
v	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	<	^	^	^	
^	>	v	<	<	<	^	
CVaR policy
>	v	v	>	v	v	v	
>	>	v	>	>	>	>	
>	>	v	<	^	^	^	
v	v	v	<	v	^	v	
>	v	<	<	>	>	>	
>	^	v	<	^	^	^	
^	>	v	<	<	<	^	
cvar = , 0.06166854694633628, 0.06167347396571188, 0.06173834879606943, 0.0616691535358882, 0.06167359645744597
==========
iteration 27
==========
weights [ 0.01525307 -0.37994099  0.89617975 -0.22863516]
expeced value MDP LP 88.37010343390334
demonstration
[(24, 0), (23, 0), (22, 3), (29, 0), (28, 0), (28, 1), (29, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 1), (29, 0), (28, 0), (28, 0), (28, 0), (28, 1), (29, 0), (28, 1), (29, 0), (28, 1), (29, 0), (28, 1), (29, 0), (28, 0), (28, 0), (28, 1), (29, 0), (28, 0), (28, 1), (29, 0), (28, 0), (28, 0), (28, 1), (29, 0), (28, 0), (28, 1), (29, 0), (28, 0), (28, 1), (29, 0), (28, 1), (29, 0), (28, 1), (29, 0), (28, 1), (29, 0), (28, 0), (28, 0)]
[-0.23404565 -0.12739734 -0.53338833 -0.10516868]
w_map [ 0.23254356 -0.19284947  0.54641039  0.02819659] loglik -22.180709777981974
accepted/total = 2512/3000 = 0.8373333333333334
-------
true weights [ 0.01525307 -0.37994099  0.89617975 -0.22863516]
features
0 	1 	2 	3 	1 	3 	3 	
0 	3 	0 	3 	1 	0 	2 	
2 	2 	1 	3 	2 	3 	3 	
0 	0 	0 	0 	1 	2 	3 	
2 	2 	1 	3 	0 	1 	1 	
1 	0 	1 	1 	0 	2 	0 	
2 	3 	2 	3 	2 	1 	2 	
optimal policy
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	<	>	^	^	
v	^	<	<	v	^	^	
<	<	<	<	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
optimal values
87.86	88.34	89.62	88.49	87.23	87.62	88.49	
88.74	88.49	88.74	87.62	87.47	88.74	89.62	
89.62	89.62	88.34	87.23	87.64	87.62	88.49	
88.74	88.74	87.86	87.00	86.61	87.64	87.38	
89.62	89.62	88.34	87.23	87.86	87.48	87.47	
88.34	88.74	88.34	87.47	88.74	88.75	88.74	
89.62	88.49	89.62	88.49	89.62	88.34	89.62	
map_weights [ 0.23254356 -0.19284947  0.54641039  0.02819659]
MAP reward
0.23	-0.19	0.55	0.03	-0.19	0.03	0.03	
0.23	0.03	0.23	0.03	-0.19	0.23	0.55	
0.55	0.55	-0.19	0.03	0.55	0.03	0.03	
0.23	0.23	0.23	0.23	-0.19	0.55	0.03	
0.55	0.55	-0.19	0.03	0.23	-0.19	-0.19	
-0.19	0.23	-0.19	-0.19	0.23	0.55	0.23	
0.55	0.03	0.55	0.03	0.55	-0.19	0.55	
Map policy
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	<	>	^	^	
v	v	<	<	v	^	^	
<	<	<	>	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
expeced value MDP LP 29.69758976664172
mean w [ 0.0473532  -0.29438089  0.30164726 -0.15747401]
Mean policy from posterior
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	<	>	^	^	
v	v	<	<	v	^	^	
<	<	<	>	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
Mean rewards
0.05	-0.29	0.30	-0.16	-0.29	-0.16	-0.16	
0.05	-0.16	0.05	-0.16	-0.29	0.05	0.30	
0.30	0.30	-0.29	-0.16	0.30	-0.16	-0.16	
0.05	0.05	0.05	0.05	-0.29	0.30	-0.16	
0.30	0.30	-0.29	-0.16	0.05	-0.29	-0.29	
-0.29	0.05	-0.29	-0.29	0.05	0.30	0.05	
0.30	-0.16	0.30	-0.16	0.30	-0.29	0.30	
mean = 0.009635800662081806, map = 0.009635798050325661
CVaR policy
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	v	>	^	^	
v	v	<	<	v	^	^	
<	<	<	>	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
CVaR policy
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	<	>	^	^	
v	v	<	<	v	^	^	
<	<	<	>	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
CVaR policy
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	<	>	^	^	
v	v	<	<	v	^	^	
<	<	<	>	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
CVaR policy
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	<	>	^	^	
v	v	<	<	v	^	^	
<	<	<	>	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
CVaR policy
v	>	^	<	<	v	v	
v	v	^	<	>	>	>	
<	<	<	<	>	^	^	
v	v	<	<	v	^	^	
<	<	<	>	v	v	v	
v	^	v	>	v	>	v	
<	<	v	<	v	>	>	
cvar = , 0.0367163640549677, 0.00979548423869403, 0.009743887179098465, 0.009695697780870205, 0.009675793921573472
==========
iteration 28
==========
weights [0.19339818 0.40393523 0.89224565 0.05771634]
expeced value MDP LP 88.31846683953032
demonstration
[(24, 0), (23, 2), (16, 2), (9, 2), (2, 0), (1, 1), (2, 3), (9, 2), (2, 0), (1, 0), (0, 1), (1, 1), (2, 0), (1, 2), (1, 2), (1, 2), (1, 0), (0, 1), (1, 1), (2, 2), (2, 3), (9, 2), (2, 0), (1, 0), (0, 3), (7, 0), (7, 2), (0, 2), (0, 1), (1, 2), (1, 0), (0, 2), (0, 3), (7, 0), (7, 2), (0, 2), (0, 2), (0, 0), (0, 3), (7, 0), (7, 0), (7, 2), (0, 0), (0, 0), (0, 0), (0, 1), (1, 2), (1, 0), (0, 3)]
[-0.04735227 -0.02993783 -0.20884348 -0.71386641]
w_map [-0.18161079  0.00460327  0.80956951  0.00421643] loglik -49.00498342834999
accepted/total = 2527/3000 = 0.8423333333333334
-------
true weights [0.19339818 0.40393523 0.89224565 0.05771634]
features
2 	2 	2 	3 	3 	3 	1 	
2 	1 	2 	0 	2 	0 	1 	
1 	1 	0 	0 	0 	0 	2 	
1 	0 	1 	1 	1 	3 	1 	
0 	0 	3 	3 	3 	0 	3 	
2 	3 	0 	0 	0 	2 	0 	
3 	1 	3 	1 	0 	2 	2 	
optimal policy
<	<	<	<	v	>	v	
^	^	^	<	<	>	v	
^	^	^	<	^	>	>	
^	^	^	<	>	>	^	
v	<	^	v	>	v	^	
<	<	<	>	>	v	v	
^	<	>	>	>	>	>	
optimal values
89.22	89.22	89.22	88.39	87.71	87.43	88.25	
89.22	88.74	89.22	88.53	88.53	88.04	88.74	
88.74	88.25	88.53	87.83	87.84	88.53	89.22	
88.25	87.56	88.04	87.57	87.43	87.91	88.74	
88.53	87.83	87.22	87.01	87.70	88.53	87.91	
89.22	88.39	87.70	87.83	88.53	89.22	88.53	
88.39	87.91	87.22	88.04	88.53	89.22	89.22	
map_weights [-0.18161079  0.00460327  0.80956951  0.00421643]
MAP reward
0.81	0.81	0.81	0.00	0.00	0.00	0.00	
0.81	0.00	0.81	-0.18	0.81	-0.18	0.00	
0.00	0.00	-0.18	-0.18	-0.18	-0.18	0.81	
0.00	-0.18	0.00	0.00	0.00	0.00	0.00	
-0.18	-0.18	0.00	0.00	0.00	-0.18	0.00	
0.81	0.00	-0.18	-0.18	-0.18	0.81	-0.18	
0.00	0.00	0.00	0.00	-0.18	0.81	0.81	
Map policy
<	<	<	<	<	>	v	
^	<	^	<	<	>	v	
^	^	^	^	^	>	>	
^	^	^	<	>	>	^	
v	v	^	>	>	v	^	
<	<	<	>	>	v	v	
^	<	<	>	>	>	>	
expeced value MDP LP 38.430033229543305
mean w [-0.19524602  0.02990569  0.39129712 -0.18725564]
Mean policy from posterior
<	<	<	<	<	>	v	
^	^	^	<	<	>	v	
^	^	^	<	^	>	>	
^	^	^	<	>	>	^	
v	v	^	>	>	v	^	
<	<	<	>	>	v	v	
^	<	<	>	>	>	>	
Mean rewards
0.39	0.39	0.39	-0.19	-0.19	-0.19	0.03	
0.39	0.03	0.39	-0.20	0.39	-0.20	0.03	
0.03	0.03	-0.20	-0.20	-0.20	-0.20	0.39	
0.03	-0.20	0.03	0.03	0.03	-0.19	0.03	
-0.20	-0.20	-0.19	-0.19	-0.19	-0.20	-0.19	
0.39	-0.19	-0.20	-0.20	-0.20	0.39	-0.20	
-0.19	0.03	-0.19	0.03	-0.20	0.39	0.39	
mean = 0.011079090026100857, map = 0.011142804822981134
CVaR policy
<	<	<	<	v	>	v	
^	>	^	<	<	>	v	
^	<	^	^	^	>	>	
^	^	^	<	>	>	^	
v	<	^	v	>	v	^	
<	<	<	>	>	v	<	
^	<	>	>	>	>	>	
CVaR policy
v	<	<	<	v	>	v	
^	^	^	<	<	>	v	
^	^	^	<	^	>	>	
^	^	^	<	>	>	^	
v	<	^	v	>	v	^	
<	<	<	>	>	v	v	
^	<	>	>	>	>	>	
CVaR policy
v	<	<	<	v	>	v	
^	<	^	<	<	>	v	
^	^	^	<	^	>	>	
^	^	^	<	>	>	^	
v	<	^	v	>	v	^	
<	<	<	>	>	v	v	
^	<	>	>	>	>	>	
CVaR policy
v	<	<	<	v	>	v	
^	^	^	<	<	>	v	
^	^	^	<	^	>	>	
^	^	^	<	>	>	^	
v	<	^	v	>	v	^	
<	<	<	>	>	v	v	
^	<	>	>	>	>	>	
CVaR policy
v	<	<	<	v	>	v	
^	^	^	<	<	>	v	
^	^	^	<	^	>	>	
^	^	^	<	>	>	^	
v	v	^	>	>	v	^	
<	<	<	>	>	v	v	
^	<	<	>	>	>	>	
cvar = , 1.4540814845531713e-06, 4.8441892886330606e-05, 3.528098829974624e-05, 0.0012174110706268948, 0.00821431628089897
==========
iteration 29
==========
weights [ 0.46301256  0.8207644  -0.18861222  0.27638849]
expeced value MDP LP 81.2802726241544
demonstration
[(24, 1), (25, 3), (32, 2), (25, 3), (32, 2), (25, 3), (32, 1), (33, 0), (32, 1), (33, 0), (32, 2), (25, 3), (32, 2), (25, 3), (32, 2), (25, 3), (32, 1), (33, 0), (32, 2), (25, 3), (32, 2), (25, 3), (32, 2), (25, 3), (32, 2), (25, 3), (32, 2), (25, 3), (32, 1), (33, 0), (32, 1), (33, 0), (32, 1), (33, 0), (32, 2), (25, 3), (32, 1), (33, 0), (32, 2), (25, 3), (32, 1), (33, 0), (32, 2), (25, 3), (32, 1), (33, 0), (32, 1), (33, 0), (32, 1)]
[-0.31774657 -0.18098767 -0.29887298 -0.20239278]
w_map [-0.21343219  0.35395901 -0.03157744 -0.40103136] loglik -16.63553227457578
accepted/total = 2750/3000 = 0.9166666666666666
-------
true weights [ 0.46301256  0.8207644  -0.18861222  0.27638849]
features
3 	2 	2 	2 	3 	0 	2 	
0 	1 	3 	1 	3 	1 	1 	
3 	1 	3 	2 	2 	0 	1 	
0 	0 	1 	2 	1 	3 	1 	
1 	0 	2 	0 	1 	1 	3 	
2 	2 	2 	2 	2 	3 	0 	
0 	2 	3 	0 	0 	2 	3 	
optimal policy
v	v	v	v	>	v	v	
>	v	<	>	>	>	v	
>	^	<	^	v	>	>	
v	^	<	>	v	>	>	
<	<	^	>	^	<	^	
^	^	^	^	^	^	^	
^	<	>	>	^	^	^	
optimal values
81.18	81.07	80.53	80.53	81.18	81.72	81.07	
81.72	82.08	81.53	81.54	81.53	82.08	82.08	
81.53	82.08	81.53	80.53	81.07	81.72	82.08	
81.72	81.72	81.72	81.07	82.08	81.53	82.08	
82.08	81.72	80.72	81.72	82.08	82.08	81.53	
81.07	80.71	79.72	80.71	81.07	81.53	81.18	
80.72	79.72	79.85	80.38	80.72	80.53	80.64	
map_weights [-0.21343219  0.35395901 -0.03157744 -0.40103136]
MAP reward
-0.40	-0.03	-0.03	-0.03	-0.40	-0.21	-0.03	
-0.21	0.35	-0.40	0.35	-0.40	0.35	0.35	
-0.40	0.35	-0.40	-0.03	-0.03	-0.21	0.35	
-0.21	-0.21	0.35	-0.03	0.35	-0.40	0.35	
0.35	-0.21	-0.03	-0.21	0.35	0.35	-0.40	
-0.03	-0.03	-0.03	-0.03	-0.03	-0.40	-0.21	
-0.21	-0.03	-0.40	-0.21	-0.21	-0.03	-0.40	
Map policy
>	v	<	v	>	v	v	
>	v	<	>	>	>	v	
>	^	<	>	v	>	>	
v	^	>	>	v	>	>	
<	<	^	>	>	<	^	
^	<	^	>	^	^	^	
^	^	^	^	^	^	<	
expeced value MDP LP 32.70200314795882
mean w [-0.18336001  0.33269769 -0.14891899 -0.05058671]
Mean policy from posterior
>	v	v	v	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	v	>	>	
v	^	^	>	v	>	>	
<	<	^	>	^	<	^	
^	<	^	>	^	^	^	
^	^	^	^	^	^	<	
Mean rewards
-0.05	-0.15	-0.15	-0.15	-0.05	-0.18	-0.15	
-0.18	0.33	-0.05	0.33	-0.05	0.33	0.33	
-0.05	0.33	-0.05	-0.15	-0.15	-0.18	0.33	
-0.18	-0.18	0.33	-0.15	0.33	-0.05	0.33	
0.33	-0.18	-0.15	-0.18	0.33	0.33	-0.05	
-0.15	-0.15	-0.15	-0.15	-0.15	-0.05	-0.18	
-0.18	-0.15	-0.05	-0.18	-0.18	-0.15	-0.05	
mean = 0.1108459601343128, map = 0.16299673908795853
CVaR policy
>	v	<	v	v	v	v	
>	v	<	>	>	>	v	
>	^	<	v	v	^	>	
v	^	>	>	v	>	^	
<	<	^	>	>	<	^	
^	<	^	>	^	^	^	
^	^	^	^	^	^	<	
CVaR policy
>	v	v	v	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	v	^	v	
v	^	^	>	v	>	>	
<	<	^	>	^	<	^	
^	<	^	>	^	^	^	
^	^	^	^	^	^	<	
CVaR policy
>	v	v	v	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	v	>	v	
v	^	^	>	v	>	^	
<	<	^	>	>	<	^	
^	<	^	>	^	^	^	
^	^	^	^	^	^	<	
CVaR policy
>	v	v	v	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	v	>	v	
v	^	^	>	v	>	^	
<	<	^	>	>	<	^	
^	<	^	>	^	^	^	
^	^	^	^	^	^	<	
CVaR policy
>	v	v	v	v	v	v	
>	v	<	>	>	>	v	
>	^	<	^	v	>	v	
v	^	^	>	v	>	^	
<	<	^	>	>	<	^	
^	<	^	>	^	^	^	
^	^	^	^	^	^	<	
cvar = , 0.1667657142614445, 0.11085537984222071, 0.11087297682520614, 0.11087124237636203, 0.11089229095939857
==========
iteration 30
==========
weights [0.56770596 0.00953044 0.77065829 0.28931802]
expeced value MDP LP 76.52518374445043
demonstration
[(24, 1), (25, 2), (18, 3), (25, 2), (18, 0), (17, 1), (18, 3), (25, 2), (18, 0), (17, 1), (18, 0), (17, 1), (18, 3), (25, 2), (18, 3), (25, 2), (18, 0), (17, 1), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 0), (17, 1), (18, 3), (25, 2), (18, 0), (17, 1), (18, 3), (25, 2), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 3), (25, 2), (18, 0), (17, 1), (18, 3), (25, 2), (18, 0), (17, 1), (18, 0)]
[-0.31115143 -0.15825462 -0.38941702 -0.14117693]
w_map [ 0.12581212 -0.31367557  0.40301981  0.15749249] loglik -17.328679469731924
accepted/total = 2711/3000 = 0.9036666666666666
-------
true weights [0.56770596 0.00953044 0.77065829 0.28931802]
features
3 	1 	0 	3 	3 	3 	2 	
3 	3 	2 	0 	1 	1 	3 	
2 	1 	3 	2 	2 	3 	2 	
3 	0 	1 	0 	2 	1 	3 	
3 	1 	3 	1 	1 	2 	1 	
2 	3 	2 	1 	2 	1 	0 	
0 	1 	2 	1 	3 	1 	2 	
optimal policy
v	>	v	v	>	>	>	
v	>	>	v	v	>	^	
<	<	>	>	v	<	>	
^	<	>	>	^	<	^	
v	v	v	^	^	^	v	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
optimal values
76.11	75.91	76.66	76.38	76.11	76.58	77.07	
76.58	76.39	76.86	76.86	76.30	75.83	76.58	
77.07	76.30	76.58	77.07	77.07	76.58	77.07	
76.58	76.39	76.10	76.86	77.07	76.30	76.58	
76.58	75.83	76.58	76.10	76.30	76.31	76.10	
77.07	76.58	77.07	76.30	76.31	76.10	76.86	
76.86	76.30	77.07	76.30	75.84	76.30	77.07	
map_weights [ 0.12581212 -0.31367557  0.40301981  0.15749249]
MAP reward
0.16	-0.31	0.13	0.16	0.16	0.16	0.40	
0.16	0.16	0.40	0.13	-0.31	-0.31	0.16	
0.40	-0.31	0.16	0.40	0.40	0.16	0.40	
0.16	0.13	-0.31	0.13	0.40	-0.31	0.16	
0.16	-0.31	0.16	-0.31	-0.31	0.40	-0.31	
0.40	0.16	0.40	-0.31	0.40	-0.31	0.13	
0.13	-0.31	0.40	-0.31	0.16	-0.31	0.40	
Map policy
v	v	v	v	>	>	>	
v	>	v	v	v	>	^	
<	<	>	>	<	<	>	
^	<	v	^	^	<	^	
v	v	v	<	^	^	^	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
expeced value MDP LP 31.113960740005425
mean w [-0.13852059 -0.16837863  0.31615063 -0.12306266]
Mean policy from posterior
v	v	v	v	>	>	>	
v	>	v	v	v	>	^	
<	<	>	>	v	>	>	
^	<	v	>	^	<	^	
v	v	v	<	^	^	^	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
Mean rewards
-0.12	-0.17	-0.14	-0.12	-0.12	-0.12	0.32	
-0.12	-0.12	0.32	-0.14	-0.17	-0.17	-0.12	
0.32	-0.17	-0.12	0.32	0.32	-0.12	0.32	
-0.12	-0.14	-0.17	-0.14	0.32	-0.17	-0.12	
-0.12	-0.17	-0.12	-0.17	-0.17	0.32	-0.17	
0.32	-0.12	0.32	-0.17	0.32	-0.17	-0.14	
-0.14	-0.17	0.32	-0.17	-0.12	-0.17	0.32	
mean = 0.0447721550043525, map = 0.04477210540014198
CVaR policy
v	v	v	v	>	>	>	
v	>	v	v	v	>	^	
<	<	>	>	v	>	>	
^	<	v	>	^	<	^	
v	v	v	<	^	^	^	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
CVaR policy
v	v	v	v	>	>	>	
v	>	v	v	v	>	^	
<	<	>	>	v	>	>	
^	<	v	>	^	<	^	
v	v	v	<	^	^	^	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
CVaR policy
v	v	v	v	>	>	>	
v	>	v	v	v	>	^	
<	<	>	>	v	>	>	
^	<	v	^	^	<	^	
v	v	v	<	^	^	^	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
CVaR policy
v	v	v	v	>	>	>	
v	>	v	v	v	>	^	
<	<	>	>	<	>	>	
^	<	v	>	^	<	^	
v	v	v	<	^	^	^	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
CVaR policy
v	v	v	v	>	>	>	
v	>	v	v	v	>	^	
<	<	>	>	v	>	>	
^	<	v	>	^	<	^	
v	v	v	<	^	^	^	
<	>	v	<	<	>	v	
^	>	v	<	^	>	>	
cvar = , 0.04478089654155326, 0.04477926657418152, 0.04490362377094925, 0.044778928901621384, 0.04477499796551854
==========
iteration 31
==========
weights [ 0.36403671  0.75149071 -0.00533718 -0.55019133]
expeced value MDP LP 74.2717114426998
demonstration
[(24, 3), (31, 3), (38, 0), (37, 3), (44, 0), (43, 3), (43, 3), (43, 3), (43, 3), (43, 3), (43, 1), (44, 2), (37, 3), (44, 0), (43, 3), (43, 1), (44, 0), (43, 1), (44, 3), (44, 2), (37, 3), (44, 0), (43, 1), (44, 0), (43, 1), (44, 3), (44, 3), (44, 2), (37, 3), (44, 0), (43, 1), (44, 2), (37, 3), (44, 3), (44, 0), (43, 1), (44, 3), (44, 0), (43, 3), (43, 3), (43, 3), (43, 1), (44, 2), (37, 3), (44, 0), (43, 1), (44, 3), (44, 3), (44, 0)]
[-0.03071565 -0.72059999 -0.18978785 -0.05889651]
w_map [-0.00887541  0.20206126 -0.33755867 -0.45150465] loglik -37.33925382336133
accepted/total = 2628/3000 = 0.876
-------
true weights [ 0.36403671  0.75149071 -0.00533718 -0.55019133]
features
2 	2 	0 	1 	2 	0 	2 	
2 	1 	0 	0 	0 	0 	2 	
2 	1 	0 	2 	2 	0 	0 	
0 	0 	0 	2 	0 	3 	3 	
3 	3 	2 	0 	0 	3 	2 	
0 	2 	1 	0 	1 	2 	1 	
0 	1 	1 	3 	3 	1 	1 	
optimal policy
v	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	<	
>	^	^	v	v	<	v	
v	^	v	v	v	v	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	>	
optimal values
73.64	74.39	74.76	75.15	74.39	74.01	73.27	
74.39	75.15	74.76	74.76	74.38	74.00	73.25	
74.39	75.15	74.76	74.01	73.63	73.62	73.25	
74.38	74.76	74.38	73.63	74.00	72.71	73.10	
73.08	73.46	74.39	74.38	74.38	73.10	74.39	
74.38	74.39	75.15	74.76	74.77	74.39	75.15	
74.76	75.15	75.15	73.85	73.85	75.15	75.15	
map_weights [-0.00887541  0.20206126 -0.33755867 -0.45150465]
MAP reward
-0.34	-0.34	-0.01	0.20	-0.34	-0.01	-0.34	
-0.34	0.20	-0.01	-0.01	-0.01	-0.01	-0.34	
-0.34	0.20	-0.01	-0.34	-0.34	-0.01	-0.01	
-0.01	-0.01	-0.01	-0.34	-0.01	-0.45	-0.45	
-0.45	-0.45	-0.34	-0.01	-0.01	-0.45	-0.34	
-0.01	-0.34	0.20	-0.01	0.20	-0.34	0.20	
-0.01	0.20	0.20	-0.45	-0.45	0.20	0.20	
Map policy
v	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	<	
>	^	<	v	v	<	v	
v	^	v	v	v	<	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	>	
expeced value MDP LP 31.416555848254916
mean w [-0.016785    0.32062175 -0.28409773 -0.16567422]
Mean policy from posterior
v	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	v	
>	^	^	v	v	<	v	
v	^	v	v	v	v	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	>	
Mean rewards
-0.28	-0.28	-0.02	0.32	-0.28	-0.02	-0.28	
-0.28	0.32	-0.02	-0.02	-0.02	-0.02	-0.28	
-0.28	0.32	-0.02	-0.28	-0.28	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.28	-0.02	-0.17	-0.17	
-0.17	-0.17	-0.28	-0.02	-0.02	-0.17	-0.28	
-0.02	-0.28	0.32	-0.02	0.32	-0.28	0.32	
-0.02	0.32	0.32	-0.17	-0.17	0.32	0.32	
mean = 0.01059140161413552, map = 0.00021030483306105907
CVaR policy
>	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	<	
>	^	^	<	v	<	v	
^	^	v	v	v	<	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	<	
CVaR policy
v	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	v	
>	^	^	<	v	<	v	
^	^	v	v	v	<	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	<	
CVaR policy
v	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	v	
>	^	^	v	v	<	v	
v	^	v	v	v	v	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	>	
CVaR policy
v	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	v	
>	^	^	v	v	<	v	
v	^	v	v	v	v	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	>	
CVaR policy
v	v	>	^	<	<	<	
>	v	<	^	<	<	<	
>	^	<	<	^	^	v	
>	^	^	v	v	<	v	
v	^	v	v	v	v	v	
v	v	v	<	<	v	v	
>	v	<	<	>	>	>	
cvar = , 0.00020963619604685846, 0.010836132536525156, 0.010593180432749705, 0.010591488316407549, 0.010591956460373808
==========
iteration 32
==========
weights [ 0.42572692 -0.36150784  0.74237917  0.37005654]
expeced value MDP LP 73.50375938614044
demonstration
[(24, 2), (17, 1), (18, 2), (11, 3), (18, 2), (11, 3), (18, 2), (11, 2), (4, 1), (5, 0), (4, 1), (5, 2), (5, 2), (5, 0), (4, 3), (11, 3), (18, 2), (11, 3), (18, 2), (11, 2), (4, 3), (11, 3), (18, 2), (11, 2), (4, 3), (11, 0), (10, 1), (11, 2), (4, 1), (5, 0), (4, 1), (5, 0), (4, 1), (5, 2), (5, 0), (4, 1), (5, 2), (5, 2), (5, 0), (4, 2), (4, 1), (5, 0), (4, 3), (11, 3), (18, 2), (11, 0), (10, 1), (11, 3), (18, 2)]
[-0.17860031 -0.36573898 -0.24451934 -0.21114137]
w_map [-0.33309409 -0.17814227  0.47083219  0.01793145] loglik -36.476220500631825
accepted/total = 2534/3000 = 0.8446666666666667
-------
true weights [ 0.42572692 -0.36150784  0.74237917  0.37005654]
features
3 	1 	3 	0 	2 	2 	1 	
0 	2 	3 	2 	2 	3 	0 	
1 	2 	0 	3 	2 	1 	2 	
3 	3 	1 	3 	1 	1 	1 	
1 	2 	1 	0 	2 	0 	0 	
3 	1 	1 	3 	3 	1 	0 	
0 	0 	1 	2 	1 	1 	2 	
optimal policy
v	v	>	>	^	^	<	
>	v	>	>	^	^	v	
>	^	<	>	^	<	>	
>	^	^	^	^	v	^	
>	^	<	v	<	>	v	
^	^	>	v	<	>	v	
>	>	>	v	<	>	>	
optimal values
73.55	73.13	73.55	73.92	74.24	74.24	73.13	
73.92	74.24	73.87	74.24	74.24	73.87	73.92	
73.13	74.24	73.92	73.87	74.24	73.13	74.24	
73.50	73.87	72.82	73.50	73.13	72.20	73.13	
72.77	73.87	72.77	73.55	73.56	73.30	73.61	
72.41	72.77	72.77	73.87	73.50	72.82	73.92	
72.53	72.83	73.13	74.24	73.13	73.13	74.24	
map_weights [-0.33309409 -0.17814227  0.47083219  0.01793145]
MAP reward
0.02	-0.18	0.02	-0.33	0.47	0.47	-0.18	
-0.33	0.47	0.02	0.47	0.47	0.02	-0.33	
-0.18	0.47	-0.33	0.02	0.47	-0.18	0.47	
0.02	0.02	-0.18	0.02	-0.18	-0.18	-0.18	
-0.18	0.47	-0.18	-0.33	0.47	-0.33	-0.33	
0.02	-0.18	-0.18	0.02	0.02	-0.18	-0.33	
-0.33	-0.33	-0.18	0.47	-0.18	-0.18	0.47	
Map policy
>	v	v	>	^	^	<	
>	v	>	>	^	^	v	
>	^	<	>	^	<	>	
>	^	<	^	^	^	^	
>	^	<	v	^	<	^	
^	^	>	v	<	v	v	
>	>	>	v	<	>	>	
expeced value MDP LP 39.939660840394
mean w [-0.08637152 -0.17284717  0.40488328  0.08238775]
Mean policy from posterior
v	v	v	>	>	^	<	
>	v	>	>	^	^	v	
>	^	<	^	^	<	>	
>	^	<	^	^	^	^	
>	^	<	v	^	<	v	
^	^	>	v	<	>	v	
>	>	>	v	<	>	>	
Mean rewards
0.08	-0.17	0.08	-0.09	0.40	0.40	-0.17	
-0.09	0.40	0.08	0.40	0.40	0.08	-0.09	
-0.17	0.40	-0.09	0.08	0.40	-0.17	0.40	
0.08	0.08	-0.17	0.08	-0.17	-0.17	-0.17	
-0.17	0.40	-0.17	-0.09	0.40	-0.09	-0.09	
0.08	-0.17	-0.17	0.08	0.08	-0.17	-0.09	
-0.09	-0.09	-0.17	0.40	-0.17	-0.17	0.40	
mean = 0.023358016328245412, map = 0.07107393745729951
CVaR policy
v	v	v	v	>	^	<	
>	v	>	>	^	^	v	
>	^	<	^	^	<	>	
>	^	<	^	^	^	^	
>	^	<	v	v	<	v	
>	^	>	v	<	<	v	
^	>	>	v	<	>	>	
CVaR policy
v	v	v	>	^	^	<	
>	v	>	>	^	^	v	
>	^	<	>	^	<	>	
>	^	<	^	^	^	^	
>	^	<	v	v	<	v	
>	^	>	v	<	>	v	
>	>	>	v	<	>	>	
CVaR policy
v	v	v	>	>	^	<	
>	v	>	>	^	<	v	
>	^	<	>	^	<	>	
>	^	<	^	^	^	^	
>	^	<	v	^	<	v	
^	^	>	v	<	>	v	
>	>	>	v	<	>	>	
CVaR policy
v	v	v	>	>	^	<	
>	v	>	>	^	^	v	
>	^	<	^	^	<	>	
>	^	<	^	^	^	^	
>	^	<	v	^	<	v	
^	^	>	v	<	>	v	
>	>	>	v	<	>	>	
CVaR policy
v	v	v	>	>	^	<	
>	v	>	>	^	^	v	
>	^	<	^	^	<	>	
>	^	<	^	^	^	^	
>	^	<	v	^	<	v	
>	^	>	v	<	>	v	
>	>	>	v	<	>	>	
cvar = , 0.025760137027717178, 0.008779290027760567, 0.023363556304559552, 0.023357853011404472, 0.023357718969236885
==========
iteration 33
==========
weights [-0.71386043 -0.62993296 -0.29873799  0.06590414]
expeced value MDP LP 5.815888446198784
demonstration
[(24, 2), (17, 1), (18, 2), (11, 2), (4, 1), (5, 0), (4, 1), (5, 2), (5, 0), (4, 2), (4, 1), (5, 2), (5, 0), (4, 1), (5, 0), (4, 1), (5, 2), (5, 0), (4, 1), (5, 0), (4, 1), (5, 0), (4, 1), (5, 2), (5, 2), (5, 0), (4, 1), (5, 2), (5, 2), (5, 2), (5, 2), (5, 2), (5, 2), (5, 2), (5, 2), (5, 2), (5, 2), (5, 2), (5, 0), (4, 2), (4, 1), (5, 2), (5, 0), (4, 2), (4, 2), (4, 1), (5, 2), (5, 0), (4, 2)]
[-0.42527398 -0.42617038 -0.12235087 -0.02620478]
w_map [-0.11204435 -0.10139328  0.1319737   0.65458868] loglik -31.19162287162635
accepted/total = 2379/3000 = 0.793
-------
true weights [-0.71386043 -0.62993296 -0.29873799  0.06590414]
features
3 	2 	1 	0 	3 	3 	1 	
2 	2 	3 	0 	1 	2 	0 	
0 	0 	0 	3 	2 	1 	0 	
1 	3 	0 	1 	0 	2 	0 	
3 	0 	2 	1 	0 	2 	1 	
1 	1 	1 	1 	3 	3 	1 	
0 	3 	1 	2 	3 	1 	3 	
optimal policy
<	<	<	>	>	^	<	
^	<	<	>	^	^	<	
^	v	^	>	^	^	<	
v	<	<	^	>	v	<	
<	<	<	v	v	v	<	
^	v	v	>	v	<	v	
>	v	<	>	v	>	>	
optimal values
6.59	6.23	5.53	5.81	6.59	6.59	5.89	
6.23	5.86	5.87	5.12	5.89	6.23	5.45	
5.45	5.13	5.10	5.55	5.54	5.53	4.76	
5.89	5.90	5.13	4.86	5.09	5.86	5.09	
6.59	5.81	5.45	5.21	5.81	6.23	5.53	
5.89	5.89	5.21	5.89	6.59	6.59	5.89	
5.81	6.59	5.89	6.23	6.59	5.89	6.59	
map_weights [-0.11204435 -0.10139328  0.1319737   0.65458868]
MAP reward
0.65	0.13	-0.10	-0.11	0.65	0.65	-0.10	
0.13	0.13	0.65	-0.11	-0.10	0.13	-0.11	
-0.11	-0.11	-0.11	0.65	0.13	-0.10	-0.11	
-0.10	0.65	-0.11	-0.10	-0.11	0.13	-0.11	
0.65	-0.11	0.13	-0.10	-0.11	0.13	-0.10	
-0.10	-0.10	-0.10	-0.10	0.65	0.65	-0.10	
-0.11	0.65	-0.10	0.13	0.65	-0.10	0.65	
Map policy
<	<	<	>	>	^	<	
^	<	<	>	^	^	<	
^	v	^	>	^	^	<	
v	<	<	^	v	v	<	
<	<	<	v	v	v	<	
^	v	v	>	v	<	v	
>	v	<	>	v	<	>	
expeced value MDP LP 38.84130086702528
mean w [-0.23420128 -0.09293439  0.09916     0.39428901]
Mean policy from posterior
<	<	<	>	>	^	<	
^	<	<	>	^	^	<	
^	v	^	>	^	^	<	
v	<	<	^	>	v	<	
<	<	<	v	v	v	<	
^	v	<	>	v	<	<	
>	v	<	>	v	<	>	
Mean rewards
0.39	0.10	-0.09	-0.23	0.39	0.39	-0.09	
0.10	0.10	0.39	-0.23	-0.09	0.10	-0.23	
-0.23	-0.23	-0.23	0.39	0.10	-0.09	-0.23	
-0.09	0.39	-0.23	-0.09	-0.23	0.10	-0.23	
0.39	-0.23	0.10	-0.09	-0.23	0.10	-0.09	
-0.09	-0.09	-0.09	-0.09	0.39	0.39	-0.09	
-0.23	0.39	-0.09	0.10	0.39	-0.09	0.39	
mean = -3.171933826706663e-10, map = 0.0010946167686656239
CVaR policy
<	<	<	>	>	<	<	
^	<	<	<	^	^	<	
^	^	^	>	^	^	<	
v	<	<	^	>	v	<	
<	<	<	v	v	v	<	
^	v	v	>	v	<	v	
>	v	<	>	v	>	>	
CVaR policy
<	<	<	>	>	^	<	
^	^	<	>	^	^	<	
^	v	^	>	^	^	<	
v	<	<	^	>	v	<	
<	<	<	v	v	v	<	
^	v	v	>	v	<	v	
>	v	<	>	v	>	>	
CVaR policy
<	<	<	>	>	^	<	
^	^	<	>	^	^	<	
^	v	^	>	^	^	<	
v	<	<	^	>	v	<	
<	<	<	v	v	v	<	
^	v	v	>	v	<	v	
>	v	<	>	v	>	>	
CVaR policy
<	<	<	>	>	^	<	
^	^	<	>	^	^	<	
^	v	^	>	^	^	<	
v	<	<	^	>	v	<	
<	<	<	v	v	v	<	
^	v	v	>	v	<	v	
>	v	<	>	v	>	>	
CVaR policy
<	<	<	>	>	^	<	
^	^	<	>	^	^	<	
^	v	^	>	^	^	<	
v	<	<	^	>	v	<	
<	<	<	v	v	v	<	
^	v	>	>	v	<	v	
>	v	<	>	v	>	>	
cvar = , 0.0011981725797554077, 9.407369110370212e-06, 1.9331115082010797e-06, 1.7620140031482379e-06, 1.8338153484620534e-07
==========
iteration 34
==========
weights [0.17811168 0.90560321 0.32405976 0.20771215]
expeced value MDP LP 89.69118406735934
demonstration
[(24, 1), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 2), (11, 3), (18, 2), (11, 3), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 2), (11, 3), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 2), (11, 3), (18, 3), (25, 2), (18, 2), (11, 3), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 2), (11, 3), (18, 2), (11, 3), (18, 2), (11, 3), (18, 2)]
[-0.05387468 -0.54981084 -0.24494167 -0.1513728 ]
w_map [-0.72204142  0.2456112   0.00253664  0.02981074] loglik -16.635532049792346
accepted/total = 2665/3000 = 0.8883333333333333
-------
true weights [0.17811168 0.90560321 0.32405976 0.20771215]
features
3 	3 	0 	2 	2 	0 	0 	
0 	1 	0 	0 	1 	3 	1 	
2 	0 	0 	0 	1 	0 	1 	
1 	2 	0 	2 	1 	3 	3 	
3 	0 	0 	2 	2 	2 	2 	
3 	0 	3 	1 	2 	3 	1 	
1 	1 	3 	2 	0 	3 	2 	
optimal policy
v	v	>	>	v	<	v	
v	v	>	>	v	>	v	
v	v	>	>	v	>	>	
<	<	>	>	^	<	^	
^	^	>	^	^	>	v	
v	v	v	^	^	>	>	
<	<	<	<	^	>	^	
optimal values
88.57	88.59	88.69	89.40	89.98	89.26	89.83	
89.26	89.27	89.11	89.83	90.56	89.86	90.56	
89.98	89.26	89.11	89.83	90.56	89.83	90.56	
90.56	89.98	89.26	89.98	90.56	89.86	89.86	
89.86	89.26	88.69	89.40	89.98	89.40	89.98	
89.86	89.83	89.17	89.41	89.40	89.86	90.56	
90.56	90.56	89.86	89.29	88.69	89.29	89.98	
map_weights [-0.72204142  0.2456112   0.00253664  0.02981074]
MAP reward
0.03	0.03	-0.72	0.00	0.00	-0.72	-0.72	
-0.72	0.25	-0.72	-0.72	0.25	0.03	0.25	
0.00	-0.72	-0.72	-0.72	0.25	-0.72	0.25	
0.25	0.00	-0.72	0.00	0.25	0.03	0.03	
0.03	-0.72	-0.72	0.00	0.00	0.00	0.00	
0.03	-0.72	0.03	0.25	0.00	0.03	0.25	
0.25	0.25	0.03	0.00	-0.72	0.03	0.00	
Map policy
v	v	>	>	v	v	v	
v	v	>	>	v	>	v	
v	v	>	>	^	>	>	
<	<	>	>	^	<	^	
^	<	v	>	^	^	v	
v	v	v	<	>	>	>	
<	<	<	<	>	^	^	
expeced value MDP LP 27.922163382561145
mean w [-0.15890747  0.28504824 -0.23484815 -0.08032915]
Mean policy from posterior
v	v	v	v	v	v	v	
v	>	>	>	v	>	v	
v	v	>	>	v	>	>	
<	<	>	>	^	<	^	
^	<	v	>	^	^	v	
v	v	v	<	>	>	>	
<	<	<	<	>	^	^	
Mean rewards
-0.08	-0.08	-0.16	-0.23	-0.23	-0.16	-0.16	
-0.16	0.29	-0.16	-0.16	0.29	-0.08	0.29	
-0.23	-0.16	-0.16	-0.16	0.29	-0.16	0.29	
0.29	-0.23	-0.16	-0.23	0.29	-0.08	-0.08	
-0.08	-0.16	-0.16	-0.23	-0.23	-0.23	-0.23	
-0.08	-0.16	-0.08	0.29	-0.23	-0.08	0.29	
0.29	0.29	-0.08	-0.23	-0.16	-0.08	-0.23	
mean = 0.040413221484939754, map = 0.02578770046910961
CVaR policy
v	v	v	v	v	>	v	
v	>	>	>	v	>	v	
v	<	>	>	v	>	>	
<	<	>	>	^	<	^	
^	v	<	^	^	v	v	
v	v	<	<	>	>	>	
<	<	<	<	>	^	^	
CVaR policy
v	v	v	v	v	v	v	
v	>	>	>	v	>	v	
v	v	>	>	v	>	>	
<	<	>	>	^	<	^	
^	<	v	>	^	^	v	
v	v	v	<	>	>	>	
<	<	<	<	>	^	^	
CVaR policy
v	v	v	v	v	v	v	
v	>	>	>	v	>	v	
v	v	>	>	^	>	>	
<	<	>	>	^	<	^	
^	<	v	>	^	^	v	
v	v	v	<	>	>	>	
<	<	<	<	>	^	^	
CVaR policy
v	v	v	v	v	v	v	
v	>	>	>	v	>	v	
v	v	>	>	v	>	>	
<	<	>	>	^	<	^	
^	<	v	>	^	^	v	
v	v	v	<	>	>	>	
<	<	<	<	>	^	^	
CVaR policy
v	v	v	v	v	v	v	
v	>	>	>	v	>	v	
v	v	>	>	v	>	>	
<	<	>	>	^	<	^	
^	<	v	>	^	^	v	
v	v	v	<	>	>	>	
<	<	<	<	>	^	^	
cvar = , 0.04398962045024746, 0.04041323000576824, 0.040451601709506235, 0.040414165166680505, 0.040418250640954057
==========
iteration 35
==========
weights [ 0.7043407  -0.48856625  0.02500647 -0.51437523]
expeced value MDP LP 69.39336832051598
demonstration
[(24, 0), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1)]
[-0.2294341  -0.23210698 -0.44041311 -0.09804581]
w_map [ 0.12426098 -0.22135845 -0.26849281 -0.38588777] loglik 0.0
accepted/total = 2781/3000 = 0.927
-------
true weights [ 0.7043407  -0.48856625  0.02500647 -0.51437523]
features
2 	3 	2 	3 	1 	1 	2 	
2 	2 	2 	2 	0 	0 	1 	
3 	1 	1 	0 	3 	0 	3 	
3 	0 	0 	3 	1 	1 	2 	
0 	1 	1 	3 	2 	0 	0 	
3 	0 	1 	0 	2 	1 	2 	
3 	0 	0 	0 	2 	3 	3 	
optimal policy
v	v	v	v	v	v	v	
>	v	>	>	>	v	<	
>	v	v	^	>	^	<	
v	>	<	<	v	v	v	
<	v	^	v	>	>	>	
>	v	v	v	<	^	^	
>	>	v	<	<	<	^	
optimal values
67.26	67.37	68.42	68.54	69.24	69.24	68.57	
67.91	68.57	69.08	69.75	70.43	70.43	69.24	
68.03	69.24	69.24	69.76	69.22	70.43	69.22	
69.22	70.43	70.43	69.22	68.57	69.24	69.75	
70.43	69.24	69.24	69.22	69.75	70.43	70.43	
69.22	70.43	69.24	70.43	69.75	69.24	69.75	
69.22	70.43	70.43	70.43	69.75	68.54	68.54	
map_weights [ 0.12426098 -0.22135845 -0.26849281 -0.38588777]
MAP reward
-0.27	-0.39	-0.27	-0.39	-0.22	-0.22	-0.27	
-0.27	-0.27	-0.27	-0.27	0.12	0.12	-0.22	
-0.39	-0.22	-0.22	0.12	-0.39	0.12	-0.39	
-0.39	0.12	0.12	-0.39	-0.22	-0.22	-0.27	
0.12	-0.22	-0.22	-0.39	-0.27	0.12	0.12	
-0.39	0.12	-0.22	0.12	-0.27	-0.22	-0.27	
-0.39	0.12	0.12	0.12	-0.27	-0.39	-0.39	
Map policy
v	v	v	>	v	v	v	
>	v	v	>	>	v	<	
>	v	v	<	>	^	<	
v	>	<	<	>	v	v	
<	v	^	v	>	>	>	
>	v	v	v	<	^	^	
>	>	v	<	<	^	^	
expeced value MDP LP 36.25005467430779
mean w [ 0.36752551 -0.10226843 -0.08335199 -0.1261879 ]
Mean policy from posterior
v	v	v	v	v	v	v	
>	v	>	>	>	v	<	
>	v	v	^	>	^	<	
v	>	<	<	v	v	v	
<	v	^	v	>	>	>	
>	v	v	v	<	^	^	
>	>	v	<	<	<	^	
Mean rewards
-0.08	-0.13	-0.08	-0.13	-0.10	-0.10	-0.08	
-0.08	-0.08	-0.08	-0.08	0.37	0.37	-0.10	
-0.13	-0.10	-0.10	0.37	-0.13	0.37	-0.13	
-0.13	0.37	0.37	-0.13	-0.10	-0.10	-0.08	
0.37	-0.10	-0.10	-0.13	-0.08	0.37	0.37	
-0.13	0.37	-0.10	0.37	-0.08	-0.10	-0.08	
-0.13	0.37	0.37	0.37	-0.08	-0.13	-0.13	
mean = 3.156586103614245e-09, map = 0.062153818877604294
CVaR policy
v	v	v	>	v	v	<	
>	v	v	>	>	v	<	
>	v	v	<	^	^	<	
v	>	<	<	>	^	v	
<	v	^	v	>	>	>	
>	v	<	v	<	^	^	
>	>	v	<	<	^	^	
CVaR policy
v	v	v	>	v	v	<	
>	v	v	>	>	v	<	
>	v	v	<	>	^	<	
v	>	<	<	>	v	v	
<	v	^	v	>	>	>	
>	v	v	v	<	^	^	
>	>	v	<	<	^	^	
CVaR policy
v	v	v	v	v	v	v	
>	v	>	>	>	v	<	
>	v	v	^	>	^	<	
v	>	<	<	v	v	v	
<	v	^	v	>	>	>	
>	v	v	v	<	^	^	
>	>	v	<	<	<	^	
CVaR policy
v	v	v	v	v	v	v	
>	v	>	>	>	v	<	
>	v	v	^	>	^	<	
v	>	<	<	v	v	v	
<	v	^	v	>	>	>	
>	v	v	v	<	^	^	
>	>	v	<	<	<	^	
CVaR policy
v	v	v	v	v	v	v	
>	v	>	>	>	v	<	
>	v	v	^	>	^	<	
v	>	<	<	v	v	v	
<	v	^	v	>	>	>	
>	v	v	v	<	^	^	
>	>	v	<	<	<	^	
cvar = , 0.0621555285949853, 0.06212911254945652, 0.00014010701168842843, 4.070736054018198e-06, 1.0014502436206385e-06
==========
iteration 36
==========
weights [-0.57889991  0.77659866  0.07837441  0.2358535 ]
expeced value MDP LP 76.58113910796095
demonstration
[(24, 2), (17, 1), (18, 0), (17, 1), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 0), (17, 1), (18, 0), (17, 1), (18, 1), (19, 0), (18, 1), (19, 0), (18, 0), (17, 1), (18, 0), (17, 1), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 1), (19, 0), (18, 0), (17, 1), (18, 1), (19, 0), (18, 1), (19, 0), (18, 0), (17, 1), (18, 1), (19, 0), (18, 1)]
[-0.18137319 -0.38878753 -0.21442086 -0.21541842]
w_map [ 0.01319824  0.56269284 -0.17700616 -0.24710276] loglik -16.635532157340094
accepted/total = 2755/3000 = 0.9183333333333333
-------
true weights [-0.57889991  0.77659866  0.07837441  0.2358535 ]
features
0 	2 	2 	0 	1 	2 	0 	
0 	3 	0 	0 	3 	0 	0 	
2 	3 	3 	1 	1 	1 	2 	
2 	1 	2 	3 	0 	3 	3 	
3 	2 	0 	2 	2 	2 	2 	
3 	3 	0 	3 	3 	1 	0 	
1 	2 	1 	1 	0 	1 	1 	
optimal policy
>	v	>	>	^	<	<	
>	v	v	v	v	v	v	
>	>	>	>	>	<	<	
>	^	>	^	^	^	<	
v	^	>	v	v	v	<	
v	<	v	v	>	v	v	
<	>	v	<	>	>	>	
optimal values
74.04	75.37	75.62	76.30	77.66	76.96	75.61	
74.71	76.05	75.77	76.30	77.12	76.30	75.61	
75.90	76.58	77.12	77.66	77.66	77.66	76.96	
75.91	76.59	76.43	77.12	76.30	77.12	76.58	
76.58	75.91	75.08	76.43	76.43	76.96	76.27	
77.12	76.58	76.30	77.12	77.12	77.66	76.30	
77.66	76.96	77.66	77.66	76.30	77.66	77.66	
map_weights [ 0.01319824  0.56269284 -0.17700616 -0.24710276]
MAP reward
0.01	-0.18	-0.18	0.01	0.56	-0.18	0.01	
0.01	-0.25	0.01	0.01	-0.25	0.01	0.01	
-0.18	-0.25	-0.25	0.56	0.56	0.56	-0.18	
-0.18	0.56	-0.18	-0.25	0.01	-0.25	-0.25	
-0.25	-0.18	0.01	-0.18	-0.18	-0.18	-0.18	
-0.25	-0.25	0.01	-0.25	-0.25	0.56	0.01	
0.56	-0.18	0.56	0.56	0.01	0.56	0.56	
Map policy
>	>	>	>	^	<	<	
>	>	>	v	v	v	<	
>	>	>	>	>	<	<	
>	>	>	^	^	^	^	
v	>	v	v	^	v	v	
v	>	v	v	>	v	v	
<	<	>	v	>	>	>	
expeced value MDP LP 35.968923963309095
mean w [-0.19001376  0.36610539 -0.07279512 -0.05369166]
Mean policy from posterior
>	>	>	>	^	<	<	
>	v	v	v	v	v	v	
>	>	>	>	>	<	<	
>	^	>	^	^	^	<	
v	^	v	v	v	v	<	
v	<	v	v	>	v	v	
<	>	>	v	>	>	>	
Mean rewards
-0.19	-0.07	-0.07	-0.19	0.37	-0.07	-0.19	
-0.19	-0.05	-0.19	-0.19	-0.05	-0.19	-0.19	
-0.07	-0.05	-0.05	0.37	0.37	0.37	-0.07	
-0.07	0.37	-0.07	-0.05	-0.19	-0.05	-0.05	
-0.05	-0.07	-0.19	-0.07	-0.07	-0.07	-0.07	
-0.05	-0.05	-0.19	-0.05	-0.05	0.37	-0.19	
0.37	-0.07	0.37	0.37	-0.19	0.37	0.37	
mean = 0.01991720506605077, map = 0.20353802477009708
CVaR policy
>	>	>	>	^	<	<	
v	v	v	v	^	v	v	
>	>	>	>	>	<	<	
>	>	>	^	^	^	^	
v	^	v	^	>	v	<	
v	v	v	v	>	v	v	
<	<	>	v	>	>	>	
CVaR policy
>	>	>	>	^	<	<	
v	v	v	v	v	v	v	
>	>	>	>	>	<	<	
>	>	>	^	^	^	^	
v	^	v	v	>	v	<	
v	v	v	v	>	v	<	
<	>	>	v	>	>	>	
CVaR policy
>	>	>	>	^	<	<	
v	v	v	v	^	v	v	
>	>	>	>	>	<	<	
>	>	>	^	^	^	^	
v	^	v	v	>	v	<	
v	v	v	v	>	v	<	
<	>	v	v	>	>	>	
CVaR policy
>	>	>	>	^	<	<	
>	v	v	v	v	v	v	
>	>	>	>	>	<	<	
>	^	>	^	^	^	<	
v	^	v	v	v	v	<	
v	<	v	v	>	v	v	
<	>	>	v	>	>	>	
CVaR policy
>	>	>	>	^	<	<	
>	v	v	v	v	v	v	
>	>	>	>	>	<	<	
>	^	>	^	^	^	<	
v	^	v	v	v	v	<	
v	<	v	v	>	v	v	
<	>	>	v	>	>	>	
cvar = , 0.04212559230865054, 0.04212732341282788, 0.04212027032986043, 0.019917687254007888, 0.0199302173596152
==========
iteration 37
==========
weights [-0.61256252 -0.70172905 -0.00119718  0.36378848]
expeced value MDP LP 35.54103626659017
demonstration
[(24, 1), (25, 2), (18, 1), (19, 2), (12, 3), (19, 2), (12, 3), (19, 0), (18, 1), (19, 2), (12, 1), (13, 0), (12, 3), (19, 2), (12, 3), (19, 2), (12, 1), (13, 0), (12, 1), (13, 1), (13, 0), (12, 3), (19, 2), (12, 3), (19, 0), (18, 1), (19, 2), (12, 1), (13, 2), (6, 2), (6, 3), (13, 0), (12, 3), (19, 2), (12, 1), (13, 1), (13, 1), (13, 1), (13, 2), (6, 3), (13, 2), (6, 2), (6, 2), (6, 2), (6, 3), (13, 2), (6, 3), (13, 2), (6, 1)]
[-0.41195664 -0.00186184 -0.16176111 -0.42442041]
w_map [-0.04658476 -0.00695679  0.01857175  0.92788669] loglik -40.1118321299582
accepted/total = 2678/3000 = 0.8926666666666667
-------
true weights [-0.61256252 -0.70172905 -0.00119718  0.36378848]
features
3 	3 	2 	1 	3 	0 	3 	
0 	1 	3 	1 	1 	3 	3 	
0 	0 	1 	2 	3 	3 	2 	
0 	0 	1 	0 	2 	1 	1 	
3 	0 	3 	1 	1 	0 	2 	
3 	1 	0 	3 	1 	3 	3 	
3 	3 	1 	0 	2 	1 	1 	
optimal policy
<	<	<	>	^	>	>	
^	^	^	<	>	>	^	
v	^	^	>	>	^	^	
v	<	v	^	^	^	^	
v	<	<	<	^	v	v	
v	v	^	>	>	>	>	
>	<	<	^	>	^	^	
optimal values
36.38	36.38	36.01	35.31	36.38	35.40	36.38	
35.40	35.31	36.02	34.96	35.31	36.38	36.38	
34.44	34.35	34.96	36.01	36.38	36.38	36.01	
35.40	34.44	34.36	35.04	36.01	35.31	34.95	
36.38	35.40	35.41	34.36	34.95	35.40	36.01	
36.38	35.31	34.45	35.32	35.31	36.38	36.38	
36.38	36.38	35.31	34.36	34.96	35.31	35.31	
map_weights [-0.04658476 -0.00695679  0.01857175  0.92788669]
MAP reward
0.93	0.93	0.02	-0.01	0.93	-0.05	0.93	
-0.05	-0.01	0.93	-0.01	-0.01	0.93	0.93	
-0.05	-0.05	-0.01	0.02	0.93	0.93	0.02	
-0.05	-0.05	-0.01	-0.05	0.02	-0.01	-0.01	
0.93	-0.05	0.93	-0.01	-0.01	-0.05	0.02	
0.93	-0.01	-0.05	0.93	-0.01	0.93	0.93	
0.93	0.93	-0.01	-0.05	0.02	-0.01	-0.01	
Map policy
<	<	<	>	^	>	>	
^	^	^	<	>	>	^	
v	^	^	>	>	^	^	
v	<	v	^	^	^	^	
v	<	<	v	^	v	v	
v	v	>	>	>	>	>	
<	<	<	^	>	^	^	
expeced value MDP LP 38.82193904685707
mean w [-0.0873385  -0.18334871 -0.00812095  0.39308264]
Mean policy from posterior
<	<	<	>	^	>	>	
^	^	^	<	>	>	^	
v	^	^	>	>	^	^	
v	<	v	^	^	^	^	
v	<	<	<	^	v	v	
v	<	^	>	>	>	>	
<	v	<	^	>	^	^	
Mean rewards
0.39	0.39	-0.01	-0.18	0.39	-0.09	0.39	
-0.09	-0.18	0.39	-0.18	-0.18	0.39	0.39	
-0.09	-0.09	-0.18	-0.01	0.39	0.39	-0.01	
-0.09	-0.09	-0.18	-0.09	-0.01	-0.18	-0.18	
0.39	-0.09	0.39	-0.18	-0.18	-0.09	-0.01	
0.39	-0.18	-0.09	0.39	-0.18	0.39	0.39	
0.39	0.39	-0.18	-0.09	-0.01	-0.18	-0.18	
mean = 1.2429950402292889e-09, map = 0.003569076418287409
CVaR policy
<	<	<	>	^	v	>	
^	^	^	<	>	>	^	
v	^	^	>	>	^	<	
v	<	v	^	^	^	^	
v	<	<	<	^	v	v	
v	v	^	>	>	>	>	
<	<	<	^	>	^	^	
CVaR policy
<	<	<	>	^	>	>	
^	^	^	<	>	>	^	
v	^	^	>	>	^	^	
v	<	v	^	^	^	^	
v	<	<	<	^	v	v	
v	v	^	>	>	>	>	
<	<	<	^	>	^	^	
CVaR policy
<	<	<	>	^	>	>	
^	^	^	<	v	>	^	
v	^	^	>	>	^	<	
v	<	v	^	^	^	^	
v	<	<	<	^	v	v	
v	v	^	>	>	>	>	
<	<	<	^	>	^	^	
CVaR policy
<	<	<	>	^	>	>	
^	^	^	<	>	>	^	
v	^	^	>	>	^	^	
v	<	v	^	^	^	^	
v	<	<	<	^	v	v	
v	<	^	>	>	>	>	
<	v	<	^	>	^	^	
CVaR policy
<	<	<	>	^	>	>	
^	^	^	<	>	>	^	
v	^	^	>	>	^	^	
v	<	v	^	^	^	^	
v	<	<	<	^	v	v	
v	<	^	>	>	>	>	
<	v	<	^	>	^	^	
cvar = , 2.7331005725272917e-07, 5.247818791076497e-06, 3.4608419241521915e-08, 1.8166256893437094e-05, 6.424518730341333e-07
==========
iteration 38
==========
weights [0.42626709 0.38762558 0.66997117 0.46816815]
expeced value MDP LP 66.73235196726995
demonstration
[(24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0)]
[-0.23556573 -0.50937893 -0.02447388 -0.23058147]
w_map [-0.3504703  -0.3172144   0.13575674 -0.19655856] loglik 0.0
accepted/total = 2777/3000 = 0.9256666666666666
-------
true weights [0.42626709 0.38762558 0.66997117 0.46816815]
features
1 	2 	2 	0 	3 	3 	1 	
1 	3 	2 	3 	2 	1 	1 	
2 	2 	0 	3 	2 	0 	1 	
3 	1 	2 	2 	1 	3 	0 	
0 	3 	1 	0 	1 	0 	1 	
0 	3 	0 	0 	1 	2 	3 	
2 	1 	0 	1 	1 	2 	2 	
optimal policy
>	>	^	<	v	<	<	
v	^	^	<	v	<	<	
<	<	^	>	^	<	<	
^	^	>	<	<	v	<	
^	^	^	^	>	v	v	
v	<	^	^	>	v	v	
<	<	<	>	>	>	>	
optimal values
66.71	67.00	67.00	66.75	66.80	66.60	66.32	
66.71	66.80	67.00	66.80	67.00	66.71	66.44	
67.00	67.00	66.75	66.80	67.00	66.75	66.47	
66.80	66.71	67.00	67.00	66.71	66.55	66.31	
66.55	66.52	66.71	66.75	66.47	66.75	66.51	
66.75	66.55	66.47	66.51	66.71	67.00	66.80	
67.00	66.71	66.47	66.44	66.71	67.00	67.00	
map_weights [-0.3504703  -0.3172144   0.13575674 -0.19655856]
MAP reward
-0.32	0.14	0.14	-0.35	-0.20	-0.20	-0.32	
-0.32	-0.20	0.14	-0.20	0.14	-0.32	-0.32	
0.14	0.14	-0.35	-0.20	0.14	-0.35	-0.32	
-0.20	-0.32	0.14	0.14	-0.32	-0.20	-0.35	
-0.35	-0.20	-0.32	-0.35	-0.32	-0.35	-0.32	
-0.35	-0.20	-0.35	-0.35	-0.32	0.14	-0.20	
0.14	-0.32	-0.35	-0.32	-0.32	0.14	0.14	
Map policy
>	>	^	<	v	<	<	
v	^	^	<	v	<	<	
<	<	^	v	^	<	<	
^	^	>	<	<	<	<	
^	^	^	^	v	v	v	
v	v	^	>	>	v	v	
<	<	<	>	>	>	>	
expeced value MDP LP 32.84888479109761
mean w [-0.16458318 -0.1162698   0.33362153 -0.18317135]
Mean policy from posterior
>	>	^	<	v	v	v	
v	^	^	<	v	<	<	
<	<	^	v	^	<	<	
^	^	>	<	<	<	v	
v	^	^	^	v	v	<	
v	v	^	>	>	v	v	
<	<	<	>	>	>	>	
Mean rewards
-0.12	0.33	0.33	-0.16	-0.18	-0.18	-0.12	
-0.12	-0.18	0.33	-0.18	0.33	-0.12	-0.12	
0.33	0.33	-0.16	-0.18	0.33	-0.16	-0.12	
-0.18	-0.12	0.33	0.33	-0.12	-0.18	-0.16	
-0.16	-0.18	-0.12	-0.16	-0.12	-0.16	-0.12	
-0.16	-0.18	-0.16	-0.16	-0.12	0.33	-0.18	
0.33	-0.12	-0.16	-0.12	-0.12	0.33	0.33	
mean = 0.011308882787290031, map = 0.0038957649984610043
CVaR policy
>	>	^	<	v	v	v	
v	^	^	<	v	<	<	
<	<	<	v	^	<	<	
^	^	>	<	<	<	v	
v	^	^	^	v	v	<	
v	v	^	>	>	v	v	
<	<	<	>	>	>	>	
CVaR policy
>	^	^	<	v	v	v	
v	>	^	<	v	<	<	
<	<	^	v	^	<	<	
^	^	>	<	<	<	v	
v	^	^	^	v	v	<	
v	v	^	>	>	v	<	
<	<	<	>	>	>	>	
CVaR policy
>	^	^	<	v	v	v	
v	^	^	<	v	<	<	
<	<	^	v	^	<	<	
^	^	>	<	<	<	v	
v	^	^	^	v	v	<	
v	v	^	>	>	v	v	
<	<	<	>	>	>	>	
CVaR policy
>	^	^	<	v	v	v	
v	^	^	<	v	<	<	
<	<	^	v	^	<	<	
^	^	>	<	<	<	v	
v	^	^	^	v	v	<	
v	v	^	>	>	v	v	
<	<	<	>	>	>	>	
CVaR policy
>	^	^	<	v	v	v	
v	^	^	<	v	<	<	
<	<	^	v	^	<	<	
^	^	>	<	<	<	v	
v	^	^	^	v	v	<	
v	v	^	>	>	v	v	
<	<	<	>	>	>	>	
cvar = , 0.011312461358201631, 0.011308891531641052, 0.011320310093722696, 0.011311321269147356, 0.011312823506855807
==========
iteration 39
==========
weights [-0.67762181  0.32713917  0.45054317  0.48043679]
expeced value MDP LP 47.5030026269918
demonstration
[(24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 1), (24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 1), (24, 0), (23, 1), (24, 0), (23, 2), (16, 3), (23, 1), (24, 0), (23, 1), (24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3)]
[-0.27246666 -0.43527047 -0.07919354 -0.21306933]
w_map [ 0.09134624 -0.02466881 -0.02667955  0.85730539] loglik -16.635532333420997
accepted/total = 2710/3000 = 0.9033333333333333
-------
true weights [-0.67762181  0.32713917  0.45054317  0.48043679]
features
2 	0 	3 	1 	1 	0 	1 	
1 	0 	0 	3 	2 	3 	2 	
2 	0 	3 	2 	2 	0 	1 	
0 	0 	3 	3 	2 	1 	0 	
0 	3 	2 	2 	3 	2 	3 	
3 	3 	0 	0 	0 	1 	1 	
0 	1 	2 	2 	2 	2 	0 	
optimal policy
>	>	^	<	v	v	v	
v	>	v	v	<	<	<	
>	>	v	v	v	^	^	
v	v	>	<	<	<	v	
>	v	<	^	^	>	>	
<	<	<	^	^	^	^	
^	^	<	<	<	^	^	
optimal values
46.87	46.89	48.04	47.89	47.83	46.83	47.80	
46.73	45.74	46.89	48.01	47.98	47.99	47.96	
46.87	46.89	48.04	48.01	47.98	46.83	47.80	
45.74	46.89	48.04	48.04	48.01	47.86	46.89	
46.89	48.04	48.01	48.01	48.01	48.01	48.04	
48.04	48.04	46.89	46.86	46.86	47.86	47.89	
46.89	47.89	47.86	47.83	47.81	47.83	46.73	
map_weights [ 0.09134624 -0.02466881 -0.02667955  0.85730539]
MAP reward
-0.03	0.09	0.86	-0.02	-0.02	0.09	-0.02	
-0.02	0.09	0.09	0.86	-0.03	0.86	-0.03	
-0.03	0.09	0.86	-0.03	-0.03	0.09	-0.02	
0.09	0.09	0.86	0.86	-0.03	-0.02	0.09	
0.09	0.86	-0.03	-0.03	0.86	-0.03	0.86	
0.86	0.86	0.09	0.09	0.09	-0.02	-0.02	
0.09	-0.02	-0.03	-0.03	-0.03	-0.03	0.09	
Map policy
>	>	^	<	<	v	<	
>	v	v	<	<	<	<	
>	>	v	<	<	^	v	
v	v	>	<	<	>	v	
v	v	<	^	<	>	>	
<	<	<	<	^	>	^	
^	^	^	^	^	>	^	
expeced value MDP LP 31.532235427350404
mean w [-0.17322109 -0.15789254 -0.0832152   0.32192228]
Mean policy from posterior
>	>	^	<	<	v	v	
v	>	v	v	<	<	<	
>	>	v	<	<	^	v	
v	v	^	<	<	<	v	
v	v	<	^	<	>	>	
<	<	<	^	^	^	^	
^	^	<	<	^	^	^	
Mean rewards
-0.08	-0.17	0.32	-0.16	-0.16	-0.17	-0.16	
-0.16	-0.17	-0.17	0.32	-0.08	0.32	-0.08	
-0.08	-0.17	0.32	-0.08	-0.08	-0.17	-0.16	
-0.17	-0.17	0.32	0.32	-0.08	-0.16	-0.17	
-0.17	0.32	-0.08	-0.08	0.32	-0.08	0.32	
0.32	0.32	-0.17	-0.17	-0.17	-0.16	-0.16	
-0.17	-0.16	-0.08	-0.08	-0.08	-0.08	-0.17	
mean = 0.04327472340335703, map = 0.37884587380197843
CVaR policy
>	>	^	<	<	v	<	
>	v	v	<	<	<	<	
>	>	v	<	<	^	v	
v	v	>	<	<	>	v	
v	v	<	^	<	>	>	
<	<	<	<	^	^	^	
^	^	^	^	^	>	^	
CVaR policy
>	>	^	<	<	v	v	
v	^	^	v	<	<	<	
>	>	v	<	v	^	v	
>	v	^	<	<	<	v	
>	v	<	^	^	>	>	
<	<	<	^	^	^	^	
^	^	^	<	^	^	^	
CVaR policy
>	>	^	<	<	v	v	
v	>	v	v	<	<	<	
>	>	v	<	v	^	v	
v	v	^	<	<	<	v	
v	v	<	^	<	>	>	
<	<	<	^	^	^	^	
^	^	^	<	^	^	^	
CVaR policy
>	>	^	<	<	v	v	
v	>	v	v	<	<	<	
>	>	v	<	<	^	v	
v	v	^	<	<	<	v	
v	v	<	^	<	>	>	
<	<	<	^	^	^	^	
^	^	<	<	^	^	^	
CVaR policy
>	>	^	<	<	v	v	
v	>	v	v	<	<	<	
>	>	v	<	<	^	v	
v	v	^	<	<	<	v	
v	v	<	^	^	>	>	
<	<	<	^	^	^	^	
^	^	<	<	^	^	^	
cvar = , 0.3763611879304136, 0.08400050063853826, 0.08367374559303187, 0.04364016069227006, 0.043351210484075864
==========
iteration 40
==========
weights [0.3430663  0.70316279 0.4439771  0.43675159]
expeced value MDP LP 70.01457630273849
demonstration
[(24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 3), (30, 2), (23, 1), (24, 0), (23, 3), (30, 2), (23, 1), (24, 0), (23, 1), (24, 0), (23, 3), (30, 2), (23, 3), (30, 2), (23, 3), (30, 2), (23, 3), (30, 2), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 3), (30, 2), (23, 3), (30, 2), (23, 3), (30, 2), (23, 1), (24, 0), (23, 1), (24, 0), (23, 3), (30, 2), (23, 1), (24, 0), (23, 3), (30, 2), (23, 1), (24, 0), (23, 1), (24, 0)]
[-0.30556727 -0.37708817 -0.12111731 -0.19622725]
w_map [0.05610707 0.80442888 0.05127867 0.08818537] loglik -16.63553225837859
accepted/total = 2763/3000 = 0.921
-------
true weights [0.3430663  0.70316279 0.4439771  0.43675159]
features
2 	0 	1 	0 	1 	3 	1 	
1 	0 	1 	0 	0 	2 	1 	
0 	1 	2 	3 	2 	3 	3 	
0 	3 	1 	1 	2 	0 	3 	
3 	3 	1 	2 	0 	2 	3 	
1 	0 	2 	1 	2 	3 	3 	
3 	0 	3 	3 	2 	1 	0 	
optimal policy
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	>	^	v	v	^	^	
>	>	v	<	<	<	^	
v	>	^	^	<	v	<	
<	<	^	^	<	v	<	
^	<	^	^	>	v	<	
optimal values
70.06	69.96	70.32	69.96	70.32	70.05	70.32	
70.32	69.96	70.32	69.96	69.96	70.06	70.32	
69.96	70.06	70.06	70.05	69.80	69.79	70.05	
69.69	70.05	70.32	70.32	70.06	69.70	69.79	
70.05	70.05	70.32	70.06	69.70	69.79	69.53	
70.32	69.96	70.06	70.06	69.80	70.05	69.79	
70.05	69.69	69.79	69.80	70.06	70.32	69.96	
map_weights [0.05610707 0.80442888 0.05127867 0.08818537]
MAP reward
0.05	0.06	0.80	0.06	0.80	0.09	0.80	
0.80	0.06	0.80	0.06	0.06	0.05	0.80	
0.06	0.80	0.05	0.09	0.05	0.09	0.09	
0.06	0.09	0.80	0.80	0.05	0.06	0.09	
0.09	0.09	0.80	0.05	0.06	0.05	0.09	
0.80	0.06	0.05	0.80	0.05	0.09	0.09	
0.09	0.06	0.09	0.09	0.05	0.80	0.06	
Map policy
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	v	^	v	<	>	^	
>	>	>	<	<	<	^	
v	>	^	^	<	v	^	
<	<	^	^	>	v	<	
^	<	^	^	>	v	<	
expeced value MDP LP 40.13327608767244
mean w [-0.07979058  0.40610783 -0.06510128 -0.01700786]
Mean policy from posterior
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	v	^	v	<	>	^	
>	>	>	<	<	<	^	
v	>	^	^	<	v	^	
<	<	^	^	>	v	<	
^	<	^	^	>	v	<	
Mean rewards
-0.07	-0.08	0.41	-0.08	0.41	-0.02	0.41	
0.41	-0.08	0.41	-0.08	-0.08	-0.07	0.41	
-0.08	0.41	-0.07	-0.02	-0.07	-0.02	-0.02	
-0.08	-0.02	0.41	0.41	-0.07	-0.08	-0.02	
-0.02	-0.02	0.41	-0.07	-0.08	-0.07	-0.02	
0.41	-0.08	-0.07	0.41	-0.07	-0.02	-0.02	
-0.02	-0.08	-0.02	-0.02	-0.07	0.41	-0.08	
mean = 0.0007822389065808011, map = 0.0007821434083012946
CVaR policy
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	v	^	v	<	>	^	
v	>	>	<	<	<	^	
v	>	^	<	<	v	^	
<	<	^	^	>	v	<	
^	<	^	^	>	v	<	
CVaR policy
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	v	^	v	<	>	^	
v	>	v	<	<	<	^	
v	>	^	<	<	v	^	
<	<	^	^	>	v	<	
^	<	^	^	>	v	<	
CVaR policy
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	v	^	v	<	>	^	
>	>	v	<	<	<	^	
v	>	^	<	<	v	^	
<	<	^	^	>	v	<	
^	<	^	^	>	v	<	
CVaR policy
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	v	^	v	<	>	^	
>	>	v	<	<	<	^	
v	>	^	^	<	v	^	
<	<	^	^	>	v	<	
^	<	^	^	>	v	<	
CVaR policy
v	>	^	<	^	>	>	
<	>	^	<	^	>	^	
^	v	^	v	<	>	^	
>	>	v	<	<	<	^	
v	>	^	^	<	v	^	
<	<	^	^	>	v	<	
^	<	^	^	>	v	<	
cvar = , 0.0007822163826034512, 0.0008440984926636474, 0.0007822390491725173, 0.0007822680814655314, 0.0007957638189850513
==========
iteration 41
==========
weights [-0.95664418  0.21139223 -0.16559599 -0.11279718]
expeced value MDP LP 20.621705316184116
demonstration
[(24, 1), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3)]
[-0.33317114 -0.14134514 -0.31839883 -0.2070849 ]
w_map [-0.39452713  0.22438604  0.00654307 -0.37454376] loglik -0.6931449086500834
accepted/total = 2691/3000 = 0.897
-------
true weights [-0.95664418  0.21139223 -0.16559599 -0.11279718]
features
0 	2 	0 	3 	1 	2 	3 	
1 	3 	3 	3 	0 	3 	1 	
3 	3 	0 	2 	1 	0 	1 	
0 	1 	1 	0 	1 	0 	1 	
1 	0 	0 	0 	2 	3 	2 	
0 	3 	3 	2 	3 	0 	1 	
3 	1 	3 	2 	1 	1 	3 	
optimal policy
v	v	>	>	^	<	v	
<	<	<	^	v	>	v	
^	v	v	>	v	>	^	
v	>	<	>	^	>	^	
<	<	^	>	^	>	^	
^	v	v	>	v	v	>	
>	v	<	>	>	v	<	
optimal values
19.97	20.44	19.65	20.82	21.14	20.76	20.82	
21.14	20.82	20.49	20.49	19.97	20.82	21.14	
20.82	20.82	19.97	20.76	21.14	19.97	21.14	
19.97	21.14	21.14	19.97	21.14	19.97	21.14	
21.14	19.97	19.97	19.60	20.76	20.44	20.76	
19.97	20.82	20.49	20.44	20.82	19.97	21.14	
20.82	21.14	20.82	20.76	21.14	21.14	20.82	
map_weights [-0.39452713  0.22438604  0.00654307 -0.37454376]
MAP reward
-0.39	0.01	-0.39	-0.37	0.22	0.01	-0.37	
0.22	-0.37	-0.37	-0.37	-0.39	-0.37	0.22	
-0.37	-0.37	-0.39	0.01	0.22	-0.39	0.22	
-0.39	0.22	0.22	-0.39	0.22	-0.39	0.22	
0.22	-0.39	-0.39	-0.39	0.01	-0.37	0.01	
-0.39	-0.37	-0.37	0.01	-0.37	-0.39	0.22	
-0.37	0.22	-0.37	0.01	0.22	0.22	-0.37	
Map policy
v	v	>	>	^	<	v	
<	<	<	v	v	>	v	
^	v	v	>	v	>	>	
v	>	<	<	^	>	^	
<	<	^	>	^	>	^	
^	v	>	v	v	v	>	
>	v	<	>	>	v	<	
expeced value MDP LP 29.589715425782632
mean w [-0.12669651  0.29945521 -0.19213065 -0.06394677]
Mean policy from posterior
v	v	>	>	^	<	v	
<	<	<	^	v	>	v	
^	v	v	>	v	>	>	
v	>	<	>	^	>	^	
<	<	^	^	^	^	^	
^	v	<	>	v	v	>	
>	v	<	>	v	v	<	
Mean rewards
-0.13	-0.19	-0.13	-0.06	0.30	-0.19	-0.06	
0.30	-0.06	-0.06	-0.06	-0.13	-0.06	0.30	
-0.06	-0.06	-0.13	-0.19	0.30	-0.13	0.30	
-0.13	0.30	0.30	-0.13	0.30	-0.13	0.30	
0.30	-0.13	-0.13	-0.13	-0.19	-0.06	-0.19	
-0.13	-0.06	-0.06	-0.19	-0.06	-0.13	0.30	
-0.06	0.30	-0.06	-0.19	0.30	0.30	-0.06	
mean = 0.031964487526973784, map = 0.010741228763070154
CVaR policy
v	v	>	>	^	<	v	
<	<	<	^	v	>	v	
^	v	v	>	v	>	>	
v	>	<	<	^	>	^	
<	<	^	>	^	>	^	
^	v	<	>	v	v	>	
>	v	<	>	>	v	<	
CVaR policy
v	v	>	>	^	<	v	
<	<	<	^	v	>	v	
^	v	v	>	v	>	>	
v	>	<	<	^	>	^	
<	<	^	>	^	>	^	
^	v	<	>	v	v	>	
>	v	<	>	>	v	<	
CVaR policy
v	v	>	>	^	<	v	
<	<	<	^	v	>	v	
^	v	v	>	v	>	>	
v	>	<	>	^	>	^	
<	<	^	^	^	^	^	
^	v	<	>	v	v	>	
>	v	<	>	>	v	<	
CVaR policy
v	v	>	>	^	<	v	
<	<	<	^	v	>	v	
^	v	v	>	v	>	>	
v	>	<	>	^	>	^	
<	<	^	^	^	^	^	
^	v	<	>	v	v	>	
>	v	<	>	>	v	<	
CVaR policy
v	v	>	>	^	<	v	
<	<	<	^	v	>	v	
^	v	v	>	v	>	>	
v	>	<	>	^	>	^	
<	<	^	^	^	^	^	
^	v	<	>	v	v	>	
>	v	<	>	>	v	<	
cvar = , -2.016507103519416e-07, 0.0012834229664129282, 0.031996217394489435, 0.03196844537395904, 0.03196601210258976
==========
iteration 42
==========
weights [ 0.493916   -0.10202596 -0.36313973  0.78343297]
expeced value MDP LP 77.09714568203854
demonstration
[(24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3)]
[-0.03854192 -0.32016251 -0.36985155 -0.27144402]
w_map [-0.22968655 -0.41539255 -0.26577357  0.08914733] loglik 0.0
accepted/total = 2762/3000 = 0.9206666666666666
-------
true weights [ 0.493916   -0.10202596 -0.36313973  0.78343297]
features
0 	2 	3 	3 	1 	3 	0 	
2 	2 	0 	2 	0 	3 	1 	
2 	1 	3 	1 	2 	1 	2 	
1 	2 	3 	1 	3 	2 	2 	
1 	2 	1 	3 	1 	1 	1 	
3 	1 	2 	2 	3 	2 	1 	
2 	2 	2 	2 	1 	2 	3 	
optimal policy
>	>	>	^	<	^	<	
^	>	^	^	>	^	<	
>	>	v	<	^	^	^	
v	>	^	<	<	<	v	
v	<	^	<	<	<	v	
<	<	<	^	^	>	v	
^	^	^	^	>	>	>	
optimal values
76.92	77.20	78.34	78.34	77.46	78.34	78.05	
75.79	76.91	78.05	77.20	78.05	78.34	77.46	
76.32	77.46	78.34	77.46	76.91	77.46	76.32	
76.58	77.20	78.34	77.46	77.47	76.33	75.45	
77.46	76.32	77.46	77.47	76.59	75.72	76.58	
78.34	77.46	76.32	76.33	76.61	76.32	77.46	
77.20	76.32	75.19	75.20	76.32	77.20	78.34	
map_weights [-0.22968655 -0.41539255 -0.26577357  0.08914733]
MAP reward
-0.23	-0.27	0.09	0.09	-0.42	0.09	-0.23	
-0.27	-0.27	-0.23	-0.27	-0.23	0.09	-0.42	
-0.27	-0.42	0.09	-0.42	-0.27	-0.42	-0.27	
-0.42	-0.27	0.09	-0.42	0.09	-0.27	-0.27	
-0.42	-0.27	-0.42	0.09	-0.42	-0.42	-0.42	
0.09	-0.42	-0.27	-0.27	0.09	-0.27	-0.42	
-0.27	-0.27	-0.27	-0.27	-0.42	-0.27	0.09	
Map policy
>	>	>	^	<	^	<	
^	>	^	^	>	^	<	
>	>	v	<	^	^	^	
>	>	^	<	<	<	<	
v	^	^	<	^	v	v	
<	<	<	^	>	v	v	
^	<	<	^	>	>	>	
expeced value MDP LP 28.725842457183028
mean w [-0.19091452 -0.16747568 -0.17458114  0.29357711]
Mean policy from posterior
>	>	>	^	<	^	<	
>	v	^	^	>	^	<	
>	>	v	<	v	^	^	
v	>	^	<	<	<	v	
v	<	^	^	<	<	v	
<	<	<	^	^	>	v	
^	^	^	^	>	>	>	
Mean rewards
-0.19	-0.17	0.29	0.29	-0.17	0.29	-0.19	
-0.17	-0.17	-0.19	-0.17	-0.19	0.29	-0.17	
-0.17	-0.17	0.29	-0.17	-0.17	-0.17	-0.17	
-0.17	-0.17	0.29	-0.17	0.29	-0.17	-0.17	
-0.17	-0.17	-0.17	0.29	-0.17	-0.17	-0.17	
0.29	-0.17	-0.17	-0.17	0.29	-0.17	-0.17	
-0.17	-0.17	-0.17	-0.17	-0.17	-0.17	0.29	
mean = 0.03599484172296741, map = 0.052774086362816774
CVaR policy
>	>	^	^	<	^	<	
^	>	^	^	>	^	<	
>	>	v	<	^	^	<	
v	>	^	<	<	<	v	
v	<	^	^	<	<	v	
<	<	<	^	^	>	v	
^	^	^	^	>	>	>	
CVaR policy
>	>	>	^	<	^	<	
^	>	^	^	>	^	<	
>	>	v	<	v	^	<	
v	>	^	<	<	<	v	
v	<	^	<	<	<	v	
<	<	<	^	^	>	v	
^	^	^	^	>	>	>	
CVaR policy
>	>	^	^	<	^	<	
>	v	^	^	>	^	<	
>	>	v	<	v	^	<	
v	>	^	<	<	<	v	
v	<	^	<	<	<	v	
<	<	<	^	^	>	v	
^	^	^	^	>	>	>	
CVaR policy
>	>	>	^	<	^	<	
>	v	^	^	>	^	<	
>	>	v	<	v	^	^	
v	>	^	<	<	<	v	
v	<	^	<	<	<	v	
<	<	<	^	^	>	v	
^	^	^	^	>	>	>	
CVaR policy
>	>	>	^	<	^	<	
>	v	^	^	>	^	<	
>	>	v	<	v	^	^	
v	>	^	<	<	<	v	
v	<	^	<	<	<	v	
<	<	<	^	^	>	v	
^	^	^	^	>	>	>	
cvar = , -3.098381426980268e-07, 0.012046699147205686, 0.036002166279985204, 0.03599802214317549, 0.035997485810327134
==========
iteration 43
==========
weights [-0.74702125 -0.08376351  0.65244248  0.09623788]
expeced value MDP LP 64.34216441458284
demonstration
[(24, 0), (23, 3), (30, 2), (23, 2), (16, 2), (9, 3), (16, 3), (23, 3), (30, 3), (37, 2), (30, 2), (23, 2), (16, 3), (23, 2), (16, 2), (9, 3), (16, 2), (9, 2), (2, 3), (9, 2), (2, 2), (2, 3), (9, 2), (2, 3), (9, 3), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 2), (9, 2), (2, 3), (9, 3), (16, 2), (9, 2), (2, 2), (2, 3), (9, 2), (2, 2), (2, 2), (2, 2), (2, 3), (9, 3), (16, 3), (23, 3), (30, 3), (37, 2)]
[-0.10581396 -0.18671283 -0.43878147 -0.26869175]
w_map [-0.06207908 -0.05350482  0.58094043 -0.30347567] loglik -31.88476959934087
accepted/total = 2725/3000 = 0.9083333333333333
-------
true weights [-0.74702125 -0.08376351  0.65244248  0.09623788]
features
1 	0 	2 	3 	3 	1 	3 	
2 	0 	2 	3 	2 	3 	3 	
3 	0 	2 	3 	1 	0 	0 	
2 	0 	2 	1 	1 	2 	0 	
0 	3 	2 	1 	0 	2 	0 	
0 	3 	2 	0 	1 	3 	2 	
3 	3 	3 	3 	1 	0 	2 	
optimal policy
v	>	^	<	v	<	v	
<	>	^	<	<	<	<	
^	>	v	<	^	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
>	>	^	<	>	>	v	
>	^	^	<	<	>	>	
optimal values
64.51	63.84	65.24	64.69	64.14	63.42	63.06	
65.24	63.84	65.24	64.69	64.69	64.14	63.60	
64.69	63.84	65.24	64.69	63.96	63.84	62.46	
65.24	63.84	65.24	64.51	64.51	65.24	63.84	
63.84	64.69	65.24	64.51	63.84	65.24	63.84	
63.29	64.69	65.24	63.84	63.96	64.69	65.24	
63.59	64.14	64.69	64.14	63.41	63.84	65.24	
map_weights [-0.06207908 -0.05350482  0.58094043 -0.30347567]
MAP reward
-0.05	-0.06	0.58	-0.30	-0.30	-0.05	-0.30	
0.58	-0.06	0.58	-0.30	0.58	-0.30	-0.30	
-0.30	-0.06	0.58	-0.30	-0.05	-0.06	-0.06	
0.58	-0.06	0.58	-0.05	-0.05	0.58	-0.06	
-0.06	-0.30	0.58	-0.05	-0.06	0.58	-0.06	
-0.06	-0.30	0.58	-0.06	-0.05	-0.30	0.58	
-0.30	-0.30	-0.30	-0.30	-0.05	-0.06	0.58	
Map policy
v	>	^	<	v	v	<	
<	>	^	<	<	v	v	
^	>	^	<	v	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
^	>	^	<	<	>	v	
^	>	^	^	>	>	>	
expeced value MDP LP 34.65965950636163
mean w [-0.10449006 -0.01960415  0.35161118 -0.12713216]
Mean policy from posterior
v	>	^	<	v	v	<	
<	>	^	<	<	v	v	
^	>	^	<	v	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
^	>	^	<	<	>	v	
^	>	^	^	>	>	>	
Mean rewards
-0.02	-0.10	0.35	-0.13	-0.13	-0.02	-0.13	
0.35	-0.10	0.35	-0.13	0.35	-0.13	-0.13	
-0.13	-0.10	0.35	-0.13	-0.02	-0.10	-0.10	
0.35	-0.10	0.35	-0.02	-0.02	0.35	-0.10	
-0.10	-0.13	0.35	-0.02	-0.10	0.35	-0.10	
-0.10	-0.13	0.35	-0.10	-0.02	-0.13	0.35	
-0.13	-0.13	-0.13	-0.13	-0.02	-0.10	0.35	
mean = 0.18326557531793242, map = 0.18326717285017935
CVaR policy
v	>	^	<	v	v	<	
<	>	^	<	<	v	v	
^	>	^	<	v	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
^	>	^	<	<	>	v	
^	^	^	^	>	>	^	
CVaR policy
v	>	^	<	v	v	<	
<	>	^	<	<	v	v	
^	>	^	<	v	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
^	>	^	<	<	>	v	
^	^	^	^	>	>	>	
CVaR policy
v	>	^	<	v	v	<	
<	>	^	<	<	v	v	
^	>	^	<	v	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
^	>	^	<	<	>	v	
^	^	^	^	>	>	>	
CVaR policy
v	>	^	<	v	v	<	
<	>	^	<	<	v	v	
^	>	^	<	v	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
^	>	^	<	<	>	v	
^	^	^	^	>	>	>	
CVaR policy
v	>	^	<	v	v	<	
<	>	^	<	<	v	v	
^	>	^	<	v	v	v	
<	>	^	<	>	v	<	
^	>	^	<	>	^	v	
^	>	^	<	<	>	v	
^	^	^	^	>	>	>	
cvar = , 0.18328422639314113, 0.18326558916984936, 0.18327230868061406, 0.18326557804306276, 0.18326580949283766
==========
iteration 44
==========
weights [0.48753185 0.15187393 0.04148168 0.8587935 ]
expeced value MDP LP 85.16560592576074
demonstration
[(24, 0), (23, 3), (30, 2), (23, 3), (30, 3), (37, 2), (30, 3), (37, 0), (36, 2), (29, 3), (36, 3), (43, 2), (36, 2), (29, 0), (28, 1), (29, 0), (28, 1), (29, 1), (30, 0), (29, 3), (36, 2), (29, 0), (28, 1), (29, 3), (36, 2), (29, 1), (30, 3), (37, 0), (36, 1), (37, 0), (36, 2), (29, 1), (30, 3), (37, 2), (30, 0), (29, 3), (36, 2), (29, 1), (30, 2), (23, 3), (30, 2), (23, 3), (30, 0), (29, 1), (30, 0), (29, 0), (28, 0), (28, 1), (29, 1)]
[-0.46408647 -0.18558516 -0.26938526 -0.08094312]
w_map [-0.07847129 -0.29282624 -0.15579267  0.4729098 ] loglik -46.31161516085285
accepted/total = 2784/3000 = 0.928
-------
true weights [0.48753185 0.15187393 0.04148168 0.8587935 ]
features
1 	0 	2 	2 	3 	1 	1 	
0 	0 	3 	3 	2 	3 	3 	
0 	0 	0 	2 	3 	0 	1 	
0 	2 	3 	0 	1 	1 	1 	
3 	3 	3 	0 	1 	0 	1 	
1 	3 	3 	2 	0 	1 	0 	
0 	3 	3 	2 	0 	1 	1 	
optimal policy
v	v	v	>	^	<	v	
>	>	>	<	<	>	>	
v	>	v	^	>	^	^	
v	v	v	<	^	^	^	
>	v	v	<	<	<	^	
>	v	<	<	<	<	<	
>	v	<	<	<	<	<	
optimal values
84.44	85.14	85.06	85.06	85.88	85.17	85.17	
85.14	85.51	85.88	85.88	85.06	85.88	85.88	
85.14	85.14	85.51	85.06	85.51	85.51	85.17	
85.51	85.06	85.88	85.51	84.81	84.80	84.47	
85.88	85.88	85.88	85.51	84.80	84.44	83.78	
85.17	85.88	85.88	85.06	84.70	84.00	83.65	
85.51	85.88	85.88	85.06	84.70	84.00	83.32	
map_weights [-0.07847129 -0.29282624 -0.15579267  0.4729098 ]
MAP reward
-0.29	-0.08	-0.16	-0.16	0.47	-0.29	-0.29	
-0.08	-0.08	0.47	0.47	-0.16	0.47	0.47	
-0.08	-0.08	-0.08	-0.16	0.47	-0.08	-0.29	
-0.08	-0.16	0.47	-0.08	-0.29	-0.29	-0.29	
0.47	0.47	0.47	-0.08	-0.29	-0.08	-0.29	
-0.29	0.47	0.47	-0.16	-0.08	-0.29	-0.08	
-0.08	0.47	0.47	-0.16	-0.08	-0.29	-0.29	
Map policy
>	v	v	>	^	v	v	
>	>	>	<	^	>	>	
v	>	v	^	>	^	^	
v	v	v	<	^	^	^	
>	v	<	<	<	<	^	
>	v	<	<	<	<	<	
>	v	<	<	<	<	<	
expeced value MDP LP 32.62445418816585
mean w [-0.12097514 -0.15031371 -0.1126407   0.33202348]
Mean policy from posterior
>	>	v	>	^	v	v	
>	>	>	<	>	>	>	
v	v	v	^	^	^	^	
v	v	v	<	^	^	^	
>	v	<	<	<	<	^	
>	v	<	<	<	<	<	
>	v	<	<	<	<	<	
Mean rewards
-0.15	-0.12	-0.11	-0.11	0.33	-0.15	-0.15	
-0.12	-0.12	0.33	0.33	-0.11	0.33	0.33	
-0.12	-0.12	-0.12	-0.11	0.33	-0.12	-0.15	
-0.12	-0.11	0.33	-0.12	-0.15	-0.15	-0.15	
0.33	0.33	0.33	-0.12	-0.15	-0.12	-0.15	
-0.15	0.33	0.33	-0.11	-0.12	-0.15	-0.12	
-0.12	0.33	0.33	-0.11	-0.12	-0.15	-0.15	
mean = 0.044879925712294266, map = -3.2852227604962536e-09
CVaR policy
>	>	v	>	^	<	v	
>	>	>	<	^	>	>	
v	v	v	^	^	^	^	
v	>	v	<	^	^	^	
>	v	<	<	<	<	^	
>	v	<	<	<	<	^	
>	v	<	<	<	<	<	
CVaR policy
>	>	v	v	^	<	v	
>	>	>	<	^	>	>	
v	v	v	^	^	^	^	
v	>	v	<	^	^	^	
>	v	<	<	<	<	^	
^	v	<	<	<	<	^	
>	v	<	<	<	<	<	
CVaR policy
>	>	v	>	^	<	v	
>	>	>	<	^	>	>	
v	v	v	^	^	^	^	
v	v	v	<	^	^	^	
>	v	<	<	<	<	^	
>	v	<	<	<	<	<	
>	v	<	<	<	<	<	
CVaR policy
>	>	v	>	^	v	v	
>	>	>	<	^	>	>	
v	v	v	^	^	^	^	
v	v	v	<	^	^	^	
>	v	<	<	<	<	^	
>	v	<	<	<	<	<	
>	^	<	<	<	<	<	
CVaR policy
>	>	v	>	^	v	v	
>	>	>	<	>	>	>	
v	v	v	^	^	^	^	
v	v	v	<	^	^	^	
>	v	<	<	<	<	^	
>	v	<	<	<	<	<	
>	^	<	<	<	<	<	
cvar = , 0.04940904844538352, 0.04940900850006358, 0.04489510801511187, 0.044880017432248565, 0.044879422169302075
==========
iteration 45
==========
weights [-0.44509712 -0.7035789   0.13026281 -0.53842073]
expeced value MDP LP 12.26476180615011
demonstration
[(24, 1), (25, 3), (32, 3), (39, 2), (32, 3), (39, 0), (38, 0), (37, 3), (44, 2), (37, 0), (36, 1), (37, 0), (36, 1), (37, 3), (44, 3), (44, 2), (37, 0), (36, 2), (29, 3), (36, 2), (29, 3), (36, 2), (29, 3), (36, 2), (29, 3), (36, 2), (29, 3), (36, 1), (37, 1), (38, 0), (37, 3), (44, 3), (44, 3), (44, 3), (44, 2), (37, 0), (36, 1), (37, 3), (44, 3), (44, 3), (44, 3), (44, 2), (37, 0), (36, 1), (37, 1), (38, 1), (39, 0), (38, 1), (39, 2)]
[-0.24190462 -0.16782427 -0.25577147 -0.33449964]
w_map [ 0.12939747 -0.07974514  0.43022924 -0.36062816] loglik -34.95859213382755
accepted/total = 2637/3000 = 0.879
-------
true weights [-0.44509712 -0.7035789   0.13026281 -0.53842073]
features
3 	3 	1 	0 	1 	3 	1 	
1 	2 	2 	1 	1 	2 	3 	
2 	3 	0 	1 	3 	1 	0 	
1 	1 	1 	0 	0 	3 	2 	
0 	2 	0 	3 	2 	3 	1 	
1 	2 	2 	2 	2 	2 	0 	
1 	3 	2 	1 	2 	1 	3 	
optimal policy
>	v	v	<	<	v	v	
v	>	<	<	<	>	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	v	v	v	v	^	
>	>	v	<	v	<	<	
>	>	v	<	v	<	^	
optimal values
11.70	12.36	12.19	11.63	10.81	11.14	10.97	
12.19	13.03	13.03	12.19	11.37	11.80	11.79	
13.03	12.36	12.45	11.62	11.79	11.62	12.45	
12.19	12.19	11.62	11.88	12.45	12.36	13.03	
12.45	13.03	12.45	12.36	13.03	12.36	12.19	
12.19	13.03	13.03	13.03	13.03	13.03	12.45	
11.53	12.36	13.03	12.19	13.03	12.19	11.79	
map_weights [ 0.12939747 -0.07974514  0.43022924 -0.36062816]
MAP reward
-0.36	-0.36	-0.08	0.13	-0.08	-0.36	-0.08	
-0.08	0.43	0.43	-0.08	-0.08	0.43	-0.36	
0.43	-0.36	0.13	-0.08	-0.36	-0.08	0.13	
-0.08	-0.08	-0.08	0.13	0.13	-0.36	0.43	
0.13	0.43	0.13	-0.36	0.43	-0.36	-0.08	
-0.08	0.43	0.43	0.43	0.43	0.43	0.13	
-0.08	-0.36	0.43	-0.08	0.43	-0.08	-0.36	
Map policy
v	v	v	<	<	v	v	
v	>	<	<	<	v	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	v	v	v	v	^	
>	>	v	<	v	<	<	
^	>	v	<	v	<	^	
expeced value MDP LP 39.356529061801396
mean w [ 0.06918836 -0.10570882  0.39875199 -0.23886885]
Mean policy from posterior
v	v	v	<	<	v	v	
v	>	<	<	<	v	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	v	v	v	v	^	
>	>	v	<	v	<	<	
^	>	v	<	v	<	^	
Mean rewards
-0.24	-0.24	-0.11	0.07	-0.11	-0.24	-0.11	
-0.11	0.40	0.40	-0.11	-0.11	0.40	-0.24	
0.40	-0.24	0.07	-0.11	-0.24	-0.11	0.07	
-0.11	-0.11	-0.11	0.07	0.07	-0.24	0.40	
0.07	0.40	0.07	-0.24	0.40	-0.24	-0.11	
-0.11	0.40	0.40	0.40	0.40	0.40	0.07	
-0.11	-0.24	0.40	-0.11	0.40	-0.11	-0.24	
mean = 0.013314119829798798, map = 0.013314108009746306
CVaR policy
v	v	v	<	<	v	v	
v	>	<	<	<	v	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	v	>	v	<	^	
>	>	v	<	v	<	<	
^	^	v	<	v	^	^	
CVaR policy
v	v	v	<	<	v	v	
v	>	<	<	<	v	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	<	v	v	v	^	
>	>	v	<	<	<	<	
^	>	v	>	v	<	^	
CVaR policy
v	v	v	<	<	v	v	
v	>	<	<	<	v	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	<	v	v	v	^	
>	>	v	<	v	<	<	
^	>	v	<	v	<	^	
CVaR policy
v	v	v	<	<	v	v	
v	>	<	<	<	v	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	v	v	v	v	^	
>	>	v	<	v	<	<	
^	>	v	<	v	<	^	
CVaR policy
v	v	v	<	<	v	v	
v	>	<	<	<	v	v	
<	<	^	<	v	>	v	
^	v	v	>	v	>	>	
>	v	v	v	v	v	^	
>	>	v	<	v	<	<	
^	>	v	<	v	<	^	
cvar = , 0.013314106533698578, 0.013315255066489229, 0.013314441559986179, 0.013314294224274548, 0.01331431492571511
==========
iteration 46
==========
weights [0.78571871 0.24904857 0.44507    0.35004802]
expeced value MDP LP 78.16855090842071
demonstration
[(24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 0), (15, 0), (14, 1), (15, 1), (16, 3), (23, 2), (16, 0), (15, 1), (16, 0), (15, 0), (14, 0), (14, 1), (15, 0), (14, 0), (14, 0), (14, 1), (15, 0), (14, 1), (15, 1), (16, 3), (23, 2), (16, 0), (15, 0), (14, 0), (14, 1), (15, 0), (14, 0), (14, 1), (15, 1), (16, 3), (23, 2)]
[-0.24635654 -0.10704907 -0.31788536 -0.32870904]
w_map [ 0.42339834 -0.44060426 -0.05389607  0.08210133] loglik -24.953298500149685
accepted/total = 2746/3000 = 0.9153333333333333
-------
true weights [0.78571871 0.24904857 0.44507    0.35004802]
features
3 	1 	3 	1 	3 	3 	2 	
2 	1 	2 	2 	0 	0 	0 	
0 	0 	0 	2 	0 	1 	3 	
1 	3 	0 	2 	3 	0 	2 	
0 	0 	3 	2 	0 	3 	1 	
2 	3 	2 	2 	3 	3 	2 	
3 	2 	1 	0 	1 	0 	0 	
optimal policy
v	v	v	v	v	v	v	
v	v	v	>	>	>	>	
<	<	<	<	^	^	^	
v	^	^	<	^	<	<	
<	<	^	^	^	^	v	
^	^	>	v	<	v	v	
^	^	>	v	>	>	>	
optimal values
77.80	77.50	77.80	77.70	78.14	78.14	78.23	
78.23	78.04	78.23	78.23	78.57	78.57	78.57	
78.57	78.57	78.57	78.23	78.57	78.04	78.14	
78.04	78.14	78.57	78.23	78.14	78.14	77.80	
78.57	78.57	78.14	77.89	78.14	77.71	77.70	
78.23	78.14	77.89	78.23	77.80	78.14	78.23	
77.80	77.80	78.04	78.57	78.04	78.57	78.57	
map_weights [ 0.42339834 -0.44060426 -0.05389607  0.08210133]
MAP reward
0.08	-0.44	0.08	-0.44	0.08	0.08	-0.05	
-0.05	-0.44	-0.05	-0.05	0.42	0.42	0.42	
0.42	0.42	0.42	-0.05	0.42	-0.44	0.08	
-0.44	0.08	0.42	-0.05	0.08	0.42	-0.05	
0.42	0.42	0.08	-0.05	0.42	0.08	-0.44	
-0.05	0.08	-0.05	-0.05	0.08	0.08	-0.05	
0.08	-0.05	-0.44	0.42	-0.44	0.42	0.42	
Map policy
v	>	v	>	v	v	v	
v	v	v	>	>	>	>	
<	>	<	<	^	^	^	
^	^	^	<	^	<	<	
<	<	^	>	^	^	v	
^	^	^	v	^	v	v	
^	^	>	v	>	>	>	
expeced value MDP LP 34.46135747328998
mean w [ 0.34927009 -0.10806052 -0.12728992 -0.14908532]
Mean policy from posterior
v	v	v	v	v	v	v	
v	v	v	>	>	>	>	
<	<	<	<	^	^	^	
^	^	^	<	^	^	<	
<	<	^	^	^	^	v	
^	^	v	v	v	v	v	
^	>	>	v	>	>	>	
Mean rewards
-0.15	-0.11	-0.15	-0.11	-0.15	-0.15	-0.13	
-0.13	-0.11	-0.13	-0.13	0.35	0.35	0.35	
0.35	0.35	0.35	-0.13	0.35	-0.11	-0.15	
-0.11	-0.15	0.35	-0.13	-0.15	0.35	-0.13	
0.35	0.35	-0.15	-0.13	0.35	-0.15	-0.11	
-0.13	-0.15	-0.13	-0.13	-0.15	-0.15	-0.13	
-0.15	-0.13	-0.11	0.35	-0.11	0.35	0.35	
mean = 0.016042452448033373, map = 0.01227634924855181
CVaR policy
v	v	v	>	v	v	v	
v	v	v	>	>	>	>	
<	<	<	<	^	^	^	
^	>	^	<	^	<	<	
<	<	^	>	^	^	v	
^	^	^	v	^	v	v	
^	^	>	v	>	>	>	
CVaR policy
v	v	v	>	v	v	v	
v	v	v	>	>	>	>	
<	<	<	<	^	^	^	
v	>	^	<	^	^	<	
<	<	^	>	^	^	v	
^	^	v	v	v	v	v	
^	>	>	v	>	>	>	
CVaR policy
v	v	v	>	v	v	v	
v	v	v	>	>	>	>	
<	<	<	<	^	<	^	
^	^	^	<	^	^	<	
<	<	^	>	^	^	v	
^	^	v	v	v	v	v	
^	>	>	v	>	>	>	
CVaR policy
v	v	v	v	v	v	v	
v	v	v	>	>	>	>	
<	<	<	<	^	^	^	
v	^	^	<	^	^	<	
<	<	^	^	^	^	v	
^	^	v	v	v	v	v	
^	>	>	v	>	>	>	
CVaR policy
v	v	v	v	v	v	v	
v	v	v	>	>	>	>	
<	<	<	<	^	^	^	
v	^	^	<	^	^	<	
<	<	^	^	^	^	v	
^	^	v	v	v	v	v	
^	>	>	v	>	>	>	
cvar = , 0.007503348380211605, 0.019788895039795307, 0.0197941027795423, 0.016043509554720004, 0.016044766283869194
==========
iteration 47
==========
weights [-0.15235257 -0.87199826  0.41367441  0.21279381]
expeced value MDP LP 40.60046301528885
demonstration
[(24, 1), (25, 1), (26, 2), (19, 3), (26, 2), (19, 3), (26, 2), (19, 3), (26, 0), (25, 1), (26, 3), (33, 2), (26, 0), (25, 1), (26, 3), (33, 2), (26, 3), (33, 2), (26, 0), (25, 1), (26, 2), (19, 3), (26, 2), (19, 3), (26, 2), (19, 3), (26, 2), (19, 3), (26, 2), (19, 3), (26, 0), (25, 1), (26, 3), (33, 2), (26, 2), (19, 3), (26, 3), (33, 2), (26, 0), (25, 1), (26, 2), (19, 3), (26, 2), (19, 3), (26, 2), (19, 3), (26, 2), (19, 3), (26, 2)]
[-0.18567455 -0.04523125 -0.02168101 -0.74741319]
w_map [ 0.07293058 -0.2708165   0.46080396 -0.19544897] loglik -26.366694637357796
accepted/total = 2837/3000 = 0.9456666666666667
-------
true weights [-0.15235257 -0.87199826  0.41367441  0.21279381]
features
1 	3 	1 	2 	0 	3 	1 	
2 	2 	0 	0 	2 	3 	1 	
0 	3 	0 	1 	1 	2 	1 	
1 	1 	0 	1 	2 	2 	1 	
1 	1 	3 	1 	1 	2 	3 	
2 	0 	0 	3 	3 	3 	0 	
1 	0 	1 	0 	2 	3 	3 	
optimal policy
v	v	>	^	<	v	<	
<	<	<	^	>	v	<	
^	^	<	^	>	v	<	
^	^	^	>	>	^	<	
v	v	v	v	>	^	<	
<	<	>	>	v	^	^	
^	^	>	>	v	<	<	
optimal values
40.08	41.17	40.08	41.37	40.80	40.97	39.69	
41.37	41.37	40.80	40.80	41.17	41.17	39.88	
40.80	41.17	40.60	39.52	40.08	41.37	40.08	
39.52	39.88	40.04	40.08	41.37	41.37	40.08	
40.08	39.52	40.21	39.69	40.08	41.37	41.17	
41.37	40.80	40.41	40.97	41.17	41.17	40.60	
40.08	40.24	39.52	40.80	41.37	41.17	40.97	
map_weights [ 0.07293058 -0.2708165   0.46080396 -0.19544897]
MAP reward
-0.27	-0.20	-0.27	0.46	0.07	-0.20	-0.27	
0.46	0.46	0.07	0.07	0.46	-0.20	-0.27	
0.07	-0.20	0.07	-0.27	-0.27	0.46	-0.27	
-0.27	-0.27	0.07	-0.27	0.46	0.46	-0.27	
-0.27	-0.27	-0.20	-0.27	-0.27	0.46	-0.20	
0.46	0.07	0.07	-0.20	-0.20	-0.20	0.07	
-0.27	0.07	-0.27	0.07	0.46	-0.20	-0.20	
Map policy
v	v	>	^	<	<	<	
<	<	<	^	^	v	<	
^	^	^	^	>	v	<	
^	^	>	>	>	<	<	
v	v	v	>	^	^	<	
<	<	<	v	v	^	^	
^	^	>	>	v	<	<	
expeced value MDP LP 38.96144300432081
mean w [-0.07969215 -0.07550694  0.39543238 -0.12985869]
Mean policy from posterior
v	v	>	^	<	<	v	
<	<	<	^	v	v	v	
^	^	^	>	>	v	<	
v	^	>	>	>	<	<	
v	<	>	>	^	^	<	
<	<	<	v	v	^	^	
^	<	>	>	v	<	<	
Mean rewards
-0.08	-0.13	-0.08	0.40	-0.08	-0.13	-0.08	
0.40	0.40	-0.08	-0.08	0.40	-0.13	-0.08	
-0.08	-0.13	-0.08	-0.08	-0.08	0.40	-0.08	
-0.08	-0.08	-0.08	-0.08	0.40	0.40	-0.08	
-0.08	-0.08	-0.13	-0.08	-0.08	0.40	-0.13	
0.40	-0.08	-0.08	-0.13	-0.13	-0.13	-0.08	
-0.08	-0.08	-0.08	-0.08	0.40	-0.13	-0.13	
mean = 0.2317815443635851, map = 0.07192053961922085
CVaR policy
v	v	>	^	<	<	<	
<	<	<	^	<	v	<	
^	^	^	^	>	v	<	
^	^	>	>	>	v	<	
v	v	v	>	>	^	<	
<	<	<	v	v	^	<	
^	^	>	>	v	<	<	
CVaR policy
v	v	>	^	<	<	<	
<	<	<	^	<	v	<	
^	^	^	^	>	v	<	
^	^	>	>	>	v	<	
v	v	v	>	>	^	<	
<	<	<	v	v	^	<	
^	^	>	>	v	<	<	
CVaR policy
v	v	>	^	<	<	<	
<	<	<	^	<	v	v	
^	^	^	^	v	v	<	
^	^	>	>	>	<	<	
v	v	v	>	>	^	<	
<	<	<	v	v	^	<	
^	^	>	>	v	<	<	
CVaR policy
v	v	>	^	<	<	v	
<	<	<	^	^	v	v	
^	^	^	^	>	v	<	
^	^	>	>	>	v	<	
v	v	v	>	>	^	<	
<	<	<	v	v	^	^	
^	^	>	>	v	<	<	
CVaR policy
v	v	>	^	<	<	v	
<	<	<	^	^	v	v	
^	^	^	^	>	v	<	
^	^	>	>	>	v	<	
v	v	v	>	^	^	<	
<	<	<	v	v	^	^	
^	^	>	>	v	<	<	
cvar = , 0.0719196506973816, 0.08162124151947836, 0.09421898037074072, 0.13014944461126277, 0.1301531083072831
==========
iteration 48
==========
weights [ 0.48340733 -0.51317902  0.53945729  0.46038079]
expeced value MDP LP 53.56567033493869
demonstration
[(24, 3), (31, 1), (32, 0), (31, 1), (32, 3), (39, 2), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 3), (39, 1), (40, 0), (39, 2), (32, 0), (31, 1), (32, 3), (39, 2), (32, 3), (39, 1), (40, 0), (39, 1), (40, 0), (39, 2), (32, 0), (31, 1), (32, 0), (31, 1), (32, 3), (39, 2), (32, 3), (39, 1), (40, 0), (39, 2), (32, 3), (39, 2), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 3)]
[-0.11828993 -0.36116616 -0.40795816 -0.11258575]
w_map [ 0.03299947 -0.00574778  0.84878007 -0.11247268] loglik -22.180707514302412
accepted/total = 2785/3000 = 0.9283333333333333
-------
true weights [ 0.48340733 -0.51317902  0.53945729  0.46038079]
features
3 	3 	0 	3 	0 	0 	0 	
3 	2 	1 	1 	0 	1 	1 	
0 	3 	2 	1 	2 	3 	3 	
1 	3 	2 	0 	1 	1 	0 	
2 	0 	1 	2 	2 	3 	0 	
0 	1 	3 	3 	2 	2 	3 	
0 	0 	2 	1 	0 	1 	2 	
optimal policy
v	v	<	<	<	<	<	
>	v	v	^	v	v	v	
>	>	v	<	>	>	v	
v	>	^	v	v	v	v	
<	<	>	>	v	v	v	
^	v	v	>	^	<	v	
>	>	v	<	^	>	>	
optimal values
53.71	53.79	53.73	53.66	53.60	53.55	53.50	
53.79	53.87	52.89	52.61	53.55	52.55	52.63	
53.81	53.87	53.95	52.89	53.61	53.60	53.68	
52.89	53.87	53.95	53.89	52.89	52.81	53.76	
53.95	53.89	52.89	53.95	53.95	53.87	53.81	
53.89	52.84	53.87	53.87	53.95	53.95	53.87	
53.83	53.89	53.95	52.89	53.89	52.89	53.95	
map_weights [ 0.03299947 -0.00574778  0.84878007 -0.11247268]
MAP reward
-0.11	-0.11	0.03	-0.11	0.03	0.03	0.03	
-0.11	0.85	-0.01	-0.01	0.03	-0.01	-0.01	
0.03	-0.11	0.85	-0.01	0.85	-0.11	-0.11	
-0.01	-0.11	0.85	0.03	-0.01	-0.01	0.03	
0.85	0.03	-0.01	0.85	0.85	-0.11	0.03	
0.03	-0.01	-0.11	-0.11	0.85	0.85	-0.11	
0.03	0.03	0.85	-0.01	0.03	-0.01	0.85	
Map policy
v	v	v	<	v	<	<	
>	>	v	v	v	<	<	
v	>	v	<	v	<	<	
v	>	^	v	v	<	<	
<	<	>	>	v	v	v	
^	v	v	>	>	<	v	
>	>	v	<	^	>	>	
expeced value MDP LP 28.97803167687112
mean w [-0.22092504 -0.19956987  0.29658115 -0.09790547]
Mean policy from posterior
v	v	v	v	v	v	v	
>	v	v	v	v	v	v	
>	>	v	<	v	<	<	
v	>	^	v	v	v	<	
<	<	>	>	v	v	v	
^	>	v	>	>	<	v	
^	>	v	<	^	>	>	
Mean rewards
-0.10	-0.10	-0.22	-0.10	-0.22	-0.22	-0.22	
-0.10	0.30	-0.20	-0.20	-0.22	-0.20	-0.20	
-0.22	-0.10	0.30	-0.20	0.30	-0.10	-0.10	
-0.20	-0.10	0.30	-0.22	-0.20	-0.20	-0.22	
0.30	-0.22	-0.20	0.30	0.30	-0.10	-0.22	
-0.22	-0.20	-0.10	-0.10	0.30	0.30	-0.10	
-0.22	-0.22	0.30	-0.20	-0.22	-0.20	0.30	
mean = 0.2786472716146804, map = 0.37185053443646154
CVaR policy
>	v	v	v	v	v	v	
>	v	v	<	v	v	v	
>	>	v	<	v	<	<	
v	>	^	v	v	v	<	
<	<	>	>	v	<	v	
^	>	v	>	^	<	v	
^	>	v	<	^	>	>	
CVaR policy
>	v	v	v	v	v	v	
>	v	v	<	v	v	v	
>	>	v	<	v	<	<	
v	>	^	v	v	v	<	
<	<	>	>	v	<	v	
^	>	v	>	^	<	v	
^	>	v	<	^	>	>	
CVaR policy
v	v	v	v	v	v	v	
>	v	v	<	v	v	v	
>	>	v	<	v	<	<	
v	>	^	v	v	v	<	
<	<	>	>	v	v	v	
^	>	v	>	>	<	v	
^	>	v	<	^	>	>	
CVaR policy
v	v	v	v	v	v	v	
>	v	v	v	v	v	v	
>	>	v	<	v	<	<	
v	>	^	v	v	v	<	
<	<	>	>	v	v	v	
^	>	v	>	>	<	v	
^	>	v	<	^	>	>	
CVaR policy
v	v	v	v	v	v	v	
>	v	v	v	v	v	v	
>	>	v	<	v	<	<	
v	>	^	v	v	v	<	
<	<	>	>	v	v	v	
^	>	v	>	>	<	v	
^	>	v	<	^	>	>	
cvar = , 0.27864722279173293, 0.27864723505855693, 0.27864723009533066, 0.27864842716182636, 0.2786474378069812
==========
iteration 49
==========
weights [ 0.61433965 -0.26268014  0.212684    0.71298769]
expeced value MDP LP 70.78701045969692
demonstration
[(24, 0), (23, 0), (22, 0), (21, 3), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0), (28, 0)]
[-0.15023052 -0.25330928 -0.09986979 -0.49659041]
w_map [ 0.07441746 -0.16341383 -0.48360948  0.27855923] loglik -2.079425753553096
accepted/total = 2195/3000 = 0.7316666666666667
-------
true weights [ 0.61433965 -0.26268014  0.212684    0.71298769]
features
3 	1 	3 	1 	0 	0 	3 	
0 	1 	1 	1 	3 	0 	2 	
0 	3 	2 	0 	2 	3 	3 	
0 	0 	0 	1 	1 	2 	1 	
3 	0 	0 	0 	0 	1 	0 	
2 	2 	2 	0 	1 	1 	0 	
3 	2 	2 	1 	3 	3 	1 	
optimal policy
<	>	^	<	v	>	>	
^	<	^	>	>	v	^	
^	<	<	>	>	>	>	
v	<	<	<	v	^	^	
<	<	<	<	<	<	^	
v	^	^	^	v	v	<	
<	<	<	>	v	<	<	
optimal values
71.30	70.32	71.30	70.32	71.10	71.20	71.30	
71.20	70.23	70.32	70.23	71.20	71.20	70.80	
71.10	71.10	70.61	70.70	70.80	71.30	71.30	
71.20	71.10	71.01	70.03	69.94	70.80	70.32	
71.30	71.20	71.10	71.01	70.91	69.94	70.23	
70.80	70.70	70.60	70.91	70.32	70.32	70.23	
71.30	70.80	70.30	70.32	71.30	71.30	70.32	
map_weights [ 0.07441746 -0.16341383 -0.48360948  0.27855923]
MAP reward
0.28	-0.16	0.28	-0.16	0.07	0.07	0.28	
0.07	-0.16	-0.16	-0.16	0.28	0.07	-0.48	
0.07	0.28	-0.48	0.07	-0.48	0.28	0.28	
0.07	0.07	0.07	-0.16	-0.16	-0.48	-0.16	
0.28	0.07	0.07	0.07	0.07	-0.16	0.07	
-0.48	-0.48	-0.48	0.07	-0.16	-0.16	0.07	
0.28	-0.48	-0.48	-0.16	0.28	0.28	-0.16	
Map policy
<	<	^	<	v	>	>	
^	<	^	>	>	v	^	
^	<	<	^	>	>	>	
v	<	<	<	v	^	^	
<	<	<	<	v	v	^	
v	^	^	v	v	v	v	
<	<	>	>	>	v	<	
expeced value MDP LP 31.19471030381908
mean w [ 0.13827395 -0.17653607 -0.26570696  0.31649117]
Mean policy from posterior
<	>	^	<	v	>	>	
^	<	^	>	>	v	v	
^	<	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	<	<	v	v	^	
v	^	^	v	v	v	v	
<	<	>	>	>	v	<	
Mean rewards
0.32	-0.18	0.32	-0.18	0.14	0.14	0.32	
0.14	-0.18	-0.18	-0.18	0.32	0.14	-0.27	
0.14	0.32	-0.27	0.14	-0.27	0.32	0.32	
0.14	0.14	0.14	-0.18	-0.18	-0.27	-0.18	
0.32	0.14	0.14	0.14	0.14	-0.18	0.14	
-0.27	-0.27	-0.27	0.14	-0.18	-0.18	0.14	
0.32	-0.27	-0.27	-0.18	0.32	0.32	-0.18	
mean = 0.051303337925645565, map = 0.07426158236089009
CVaR policy
<	<	^	<	v	>	>	
^	<	^	>	>	v	^	
^	<	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	<	<	v	v	^	
v	^	^	>	v	v	v	
<	<	>	>	>	v	<	
CVaR policy
<	<	^	<	v	>	>	
^	<	^	>	>	v	^	
^	<	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	<	<	v	v	^	
v	^	^	>	v	v	v	
<	<	>	>	v	v	<	
CVaR policy
<	>	^	<	v	>	>	
^	<	^	>	>	v	^	
^	<	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	<	<	v	v	^	
v	^	^	v	v	v	v	
<	<	>	>	v	v	<	
CVaR policy
<	>	^	<	v	>	>	
^	<	^	>	>	v	^	
^	<	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	<	<	v	v	^	
v	^	^	>	v	v	v	
<	<	>	>	v	v	<	
CVaR policy
<	>	^	<	v	>	>	
^	<	^	>	>	v	v	
^	<	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	<	<	v	v	^	
v	^	^	v	v	v	v	
<	<	>	>	v	v	<	
cvar = , 0.05130335141454623, 0.05130431192745277, 0.05130502884603061, 0.05130950787278721, 0.051303385093120824
==========
iteration 50
==========
weights [ 0.35967907  0.2144835  -0.88700309 -0.19455929]
expeced value MDP LP 35.18057675111298
demonstration
[(24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2)]
[-0.26386066 -0.03939233 -0.4613143  -0.23543271]
w_map [ 0.31374455 -0.05398621 -0.38383468 -0.24843456] loglik 0.0
accepted/total = 2743/3000 = 0.9143333333333333
-------
true weights [ 0.35967907  0.2144835  -0.88700309 -0.19455929]
features
1 	2 	1 	3 	2 	3 	1 	
2 	3 	3 	3 	3 	2 	0 	
2 	0 	1 	0 	1 	3 	2 	
1 	2 	3 	0 	1 	0 	2 	
3 	2 	3 	1 	2 	2 	3 	
2 	0 	3 	3 	3 	1 	2 	
0 	0 	2 	0 	3 	1 	0 	
optimal policy
>	v	v	v	>	>	v	
>	v	v	v	v	>	>	
>	>	>	v	<	v	^	
>	^	>	^	<	<	<	
v	v	>	^	<	^	v	
v	v	<	v	>	v	v	
<	<	<	v	<	>	>	
optimal values
33.91	34.03	35.13	34.86	34.03	35.27	35.82	
34.03	35.27	35.27	35.41	35.27	34.72	35.97	
34.58	35.82	35.82	35.97	35.82	35.27	34.72	
34.45	34.58	35.41	35.97	35.82	35.82	34.58	
34.18	34.72	35.27	35.82	34.58	34.58	34.18	
34.72	35.97	35.41	35.41	35.13	35.68	34.72	
35.97	35.97	34.72	35.97	35.41	35.82	35.97	
map_weights [ 0.31374455 -0.05398621 -0.38383468 -0.24843456]
MAP reward
-0.05	-0.38	-0.05	-0.25	-0.38	-0.25	-0.05	
-0.38	-0.25	-0.25	-0.25	-0.25	-0.38	0.31	
-0.38	0.31	-0.05	0.31	-0.05	-0.25	-0.38	
-0.05	-0.38	-0.25	0.31	-0.05	0.31	-0.38	
-0.25	-0.38	-0.25	-0.05	-0.38	-0.38	-0.25	
-0.38	0.31	-0.25	-0.25	-0.25	-0.05	-0.38	
0.31	0.31	-0.38	0.31	-0.25	-0.05	0.31	
Map policy
v	v	v	v	>	>	v	
>	v	v	v	v	>	>	
>	>	>	v	<	v	^	
>	^	>	^	<	<	<	
v	v	>	^	<	^	v	
v	v	<	v	v	v	v	
<	<	<	v	<	>	>	
expeced value MDP LP 32.02744887165548
mean w [ 0.3265956  -0.08066189 -0.07627184 -0.21973074]
Mean policy from posterior
v	v	v	v	v	v	v	
v	v	v	v	>	>	>	
>	>	>	v	<	>	^	
>	v	>	^	<	<	^	
v	v	<	^	<	^	v	
v	v	<	v	v	>	v	
<	<	<	v	<	>	>	
Mean rewards
-0.08	-0.08	-0.08	-0.22	-0.08	-0.22	-0.08	
-0.08	-0.22	-0.22	-0.22	-0.22	-0.08	0.33	
-0.08	0.33	-0.08	0.33	-0.08	-0.22	-0.08	
-0.08	-0.08	-0.22	0.33	-0.08	0.33	-0.08	
-0.22	-0.08	-0.22	-0.08	-0.08	-0.08	-0.22	
-0.08	0.33	-0.22	-0.22	-0.22	-0.08	-0.08	
0.33	0.33	-0.08	0.33	-0.22	-0.08	0.33	
mean = 0.23316325110669567, map = 0.005360127646532931
CVaR policy
v	<	v	v	>	v	v	
v	v	v	v	>	>	>	
>	>	>	v	<	>	^	
>	v	>	^	<	<	^	
v	v	<	^	<	^	v	
v	v	<	v	^	>	v	
<	<	<	v	<	>	>	
CVaR policy
v	v	v	v	v	v	v	
v	v	v	v	>	>	>	
>	>	>	v	<	>	^	
>	v	>	^	<	<	^	
v	v	<	^	<	^	v	
>	v	<	v	^	>	v	
<	<	<	v	<	>	>	
CVaR policy
v	v	v	v	v	v	v	
v	v	v	v	>	>	>	
>	>	>	v	<	>	^	
>	v	>	^	<	<	^	
v	v	<	^	<	^	v	
v	v	<	v	v	>	v	
<	<	<	v	<	>	>	
CVaR policy
v	v	v	v	v	v	v	
v	v	v	v	>	>	>	
>	>	>	v	<	^	^	
>	v	>	^	<	<	^	
v	v	<	^	<	^	v	
v	v	<	v	<	>	v	
<	<	<	v	<	>	>	
CVaR policy
v	v	v	v	v	v	v	
v	v	v	v	>	>	>	
>	>	>	v	<	>	^	
>	v	>	^	<	<	^	
v	v	<	^	<	^	v	
v	v	<	v	v	>	v	
<	<	<	v	<	>	>	
cvar = , 0.2913591477439752, 0.2500576059112163, 0.23305696613672922, 0.23309087180085442, 0.23297135697978177
==========
iteration 51
==========
weights [ 0.20177152  0.68639443  0.2657777  -0.64615257]
expeced value MDP LP 67.77938578133188
demonstration
[(24, 3), (31, 0), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0)]
[-0.25087597 -0.10091693 -0.11678443 -0.53142267]
w_map [-0.32068219  0.36528136  0.02513779 -0.28889867] loglik 0.0
accepted/total = 2405/3000 = 0.8016666666666666
-------
true weights [ 0.20177152  0.68639443  0.2657777  -0.64615257]
features
2 	3 	2 	3 	0 	2 	0 	
0 	2 	3 	0 	2 	3 	3 	
3 	1 	1 	0 	0 	0 	1 	
3 	0 	3 	0 	2 	3 	1 	
2 	1 	1 	2 	3 	1 	1 	
0 	0 	2 	2 	1 	2 	1 	
3 	3 	1 	3 	1 	0 	0 	
optimal policy
v	v	v	v	v	>	v	
>	v	v	v	v	v	v	
>	>	<	<	>	>	v	
v	v	v	v	<	v	v	
>	>	<	<	>	>	v	
^	^	v	>	v	^	>	
^	>	v	>	v	<	^	
optimal values
67.33	66.89	66.90	66.35	66.79	66.43	66.84	
67.74	68.22	67.31	67.68	67.26	66.83	67.31	
67.31	68.64	68.64	68.15	67.68	68.15	68.64	
66.89	68.15	67.31	67.74	67.33	67.31	68.64	
68.22	68.64	68.64	68.22	67.31	68.64	68.64	
67.74	68.15	68.22	68.22	68.64	68.22	68.64	
66.41	67.31	68.64	67.31	68.64	68.15	68.15	
map_weights [-0.32068219  0.36528136  0.02513779 -0.28889867]
MAP reward
0.03	-0.29	0.03	-0.29	-0.32	0.03	-0.32	
-0.32	0.03	-0.29	-0.32	0.03	-0.29	-0.29	
-0.29	0.37	0.37	-0.32	-0.32	-0.32	0.37	
-0.29	-0.32	-0.29	-0.32	0.03	-0.29	0.37	
0.03	0.37	0.37	0.03	-0.29	0.37	0.37	
-0.32	-0.32	0.03	0.03	0.37	0.03	0.37	
-0.29	-0.29	0.37	-0.29	0.37	-0.32	-0.32	
Map policy
>	v	v	<	v	v	v	
>	v	v	<	>	>	v	
>	>	<	<	>	>	v	
v	v	v	v	>	v	v	
>	>	<	<	>	>	v	
^	^	v	>	v	>	>	
>	>	v	>	v	<	^	
expeced value MDP LP 31.92905786978354
mean w [-0.19400231  0.32487183  0.06613497 -0.26219191]
Mean policy from posterior
v	v	v	<	v	>	v	
>	v	v	v	v	v	v	
>	>	<	<	>	>	v	
v	v	v	v	>	v	v	
>	>	<	<	>	>	v	
^	^	v	>	v	>	>	
>	>	v	>	v	<	^	
Mean rewards
0.07	-0.26	0.07	-0.26	-0.19	0.07	-0.19	
-0.19	0.07	-0.26	-0.19	0.07	-0.26	-0.26	
-0.26	0.32	0.32	-0.19	-0.19	-0.19	0.32	
-0.26	-0.19	-0.26	-0.19	0.07	-0.26	0.32	
0.07	0.32	0.32	0.07	-0.26	0.32	0.32	
-0.19	-0.19	0.07	0.07	0.32	0.07	0.32	
-0.26	-0.26	0.32	-0.26	0.32	-0.19	-0.19	
mean = 0.03310368221183069, map = 0.1694723451852269
CVaR policy
v	v	v	<	v	>	v	
>	v	v	v	v	v	v	
>	>	<	<	>	>	v	
v	^	^	v	>	v	v	
>	>	<	<	>	>	v	
^	^	v	>	v	>	>	
>	>	v	>	v	<	^	
CVaR policy
v	v	v	<	v	>	v	
>	v	v	v	v	v	v	
>	>	<	<	>	>	v	
v	v	v	v	>	v	v	
>	>	<	<	>	>	v	
^	^	v	>	v	^	>	
>	>	v	>	v	<	^	
CVaR policy
v	v	v	<	v	>	v	
>	v	v	v	v	v	v	
>	>	<	<	>	>	v	
v	v	v	v	>	v	v	
>	>	<	<	>	>	<	
^	^	v	>	v	^	>	
>	>	v	>	v	<	^	
CVaR policy
v	v	v	<	v	>	v	
>	v	v	v	v	v	v	
>	>	<	<	>	>	v	
v	v	v	v	>	v	v	
>	>	<	<	>	>	>	
^	^	v	>	v	>	>	
>	>	v	>	v	<	^	
CVaR policy
v	v	v	<	v	>	v	
>	v	v	v	v	v	v	
>	>	<	<	>	>	v	
v	v	v	v	>	v	v	
>	>	<	<	>	>	>	
^	^	v	>	v	>	>	
>	>	v	>	v	<	^	
cvar = , 0.0331354636819583, 0.033151983260580664, 0.03317247285150415, 0.03312682792105193, 0.03349221903567923
==========
iteration 52
==========
weights [-0.65125821 -0.40001646 -0.29382502  0.57403522]
expeced value MDP LP 55.79507480977595
demonstration
[(24, 1), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3)]
[-0.56172161 -0.02818462 -0.33187273 -0.07822104]
w_map [-0.36440635 -0.17058005 -0.30036467  0.16464893] loglik 0.0
accepted/total = 2809/3000 = 0.9363333333333334
-------
true weights [-0.65125821 -0.40001646 -0.29382502  0.57403522]
features
3 	1 	3 	1 	3 	0 	1 	
1 	1 	3 	3 	0 	1 	1 	
0 	2 	0 	2 	3 	1 	1 	
1 	0 	1 	1 	3 	1 	2 	
3 	2 	0 	3 	1 	1 	2 	
2 	1 	1 	2 	2 	0 	2 	
2 	1 	1 	2 	0 	1 	0 	
optimal policy
<	<	^	<	^	<	<	
^	>	^	<	<	v	<	
^	^	^	^	v	<	<	
v	v	>	>	^	<	<	
<	<	<	^	^	^	^	
^	<	>	^	^	<	^	
^	<	>	^	^	^	^	
optimal values
57.40	56.43	57.40	56.43	57.40	56.18	55.22	
56.43	56.43	57.40	57.40	56.18	55.47	54.51	
55.21	55.57	56.18	56.54	57.40	56.43	55.47	
56.43	55.32	55.47	56.43	57.40	56.43	55.57	
57.40	56.54	55.32	56.44	56.43	55.47	54.72	
56.54	55.57	54.63	55.58	55.57	54.36	53.88	
55.68	54.72	53.78	54.73	54.36	53.42	52.69	
map_weights [-0.36440635 -0.17058005 -0.30036467  0.16464893]
MAP reward
0.16	-0.17	0.16	-0.17	0.16	-0.36	-0.17	
-0.17	-0.17	0.16	0.16	-0.36	-0.17	-0.17	
-0.36	-0.30	-0.36	-0.30	0.16	-0.17	-0.17	
-0.17	-0.36	-0.17	-0.17	0.16	-0.17	-0.30	
0.16	-0.30	-0.36	0.16	-0.17	-0.17	-0.30	
-0.30	-0.17	-0.17	-0.30	-0.30	-0.36	-0.30	
-0.30	-0.17	-0.17	-0.30	-0.36	-0.17	-0.36	
Map policy
<	>	^	<	^	<	<	
^	>	^	<	<	v	<	
^	^	^	^	v	<	<	
v	<	>	>	^	<	<	
<	<	>	^	^	^	<	
^	^	>	^	^	^	^	
^	^	^	^	^	^	<	
expeced value MDP LP 28.745190256996153
mean w [-0.20832669 -0.09886879 -0.22642856  0.29473182]
Mean policy from posterior
<	>	^	<	^	<	<	
^	>	^	<	<	v	<	
^	^	^	^	v	<	<	
v	<	>	>	^	<	<	
<	<	>	^	^	^	<	
^	^	^	^	^	^	<	
^	^	^	^	^	^	<	
Mean rewards
0.29	-0.10	0.29	-0.10	0.29	-0.21	-0.10	
-0.10	-0.10	0.29	0.29	-0.21	-0.10	-0.10	
-0.21	-0.23	-0.21	-0.23	0.29	-0.10	-0.10	
-0.10	-0.21	-0.10	-0.10	0.29	-0.10	-0.23	
0.29	-0.23	-0.21	0.29	-0.10	-0.10	-0.23	
-0.23	-0.10	-0.10	-0.23	-0.23	-0.21	-0.23	
-0.23	-0.10	-0.10	-0.23	-0.21	-0.10	-0.21	
mean = 0.04991450933265895, map = 0.028322138063941793
CVaR policy
<	>	v	<	^	<	<	
^	>	^	<	<	v	v	
^	^	^	^	v	<	<	
v	<	>	>	^	<	<	
<	<	>	^	^	^	<	
^	^	^	^	^	^	<	
^	^	^	^	^	^	<	
CVaR policy
<	<	^	v	^	<	<	
^	>	^	<	<	v	<	
^	^	^	^	v	<	<	
v	<	>	>	^	<	<	
<	<	>	^	^	^	<	
^	^	^	^	^	^	<	
^	^	^	^	^	^	<	
CVaR policy
<	>	^	<	^	<	<	
^	>	^	<	v	v	v	
^	^	^	^	v	<	<	
v	<	>	>	^	<	<	
<	<	>	^	^	^	<	
^	^	^	^	^	^	<	
^	^	^	^	^	^	<	
CVaR policy
<	<	^	v	^	<	<	
^	>	^	<	<	v	<	
^	^	^	^	v	<	<	
v	<	>	>	^	<	<	
<	<	>	^	^	^	<	
^	^	^	^	^	^	<	
^	^	^	^	^	^	<	
CVaR policy
<	<	v	<	^	<	<	
^	>	^	<	<	v	<	
^	^	^	^	v	<	<	
v	<	>	>	^	<	<	
<	<	>	^	^	^	<	
^	^	^	^	^	^	<	
^	^	^	^	^	^	<	
cvar = , 0.049914953781531324, 0.0499164812487507, 0.049914540244117234, 0.04991454568395426, 0.04991945760840366
==========
iteration 53
==========
weights [ 0.55414713 -0.39945189  0.38885406  0.61818417]
expeced value MDP LP 61.41087827769615
demonstration
[(24, 2), (17, 1), (18, 0), (17, 0), (16, 1), (17, 1), (18, 0), (17, 0), (16, 1), (17, 1), (18, 0), (17, 1), (18, 0), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 0), (16, 1), (17, 0), (16, 1)]
[-0.4729666  -0.3655662  -0.15519563 -0.00627158]
w_map [ 0.10828796 -0.00676112 -0.0081361   0.87681482] loglik -16.635532333420997
accepted/total = 2746/3000 = 0.9153333333333333
-------
true weights [ 0.55414713 -0.39945189  0.38885406  0.61818417]
features
0 	0 	3 	2 	3 	3 	1 	
3 	3 	0 	1 	0 	0 	0 	
2 	1 	3 	3 	3 	2 	2 	
3 	2 	1 	0 	0 	0 	0 	
2 	0 	1 	1 	3 	1 	2 	
1 	2 	1 	1 	2 	2 	2 	
2 	3 	1 	0 	2 	2 	0 	
optimal policy
v	v	^	>	^	<	<	
<	<	<	v	^	^	<	
^	^	>	<	<	<	^	
<	<	^	^	^	<	<	
^	^	<	>	^	<	^	
v	v	<	>	^	<	^	
>	v	<	>	^	^	^	
optimal values
61.75	61.75	61.82	61.59	61.82	61.82	60.80	
61.82	61.82	61.75	60.80	61.75	61.75	61.69	
61.59	60.80	61.82	61.82	61.82	61.59	61.46	
61.82	61.59	60.80	61.75	61.75	61.69	61.63	
61.59	61.53	60.51	60.74	61.76	60.74	61.40	
60.57	61.59	60.57	60.51	61.53	61.30	61.18	
61.59	61.82	60.80	61.24	61.30	61.08	61.12	
map_weights [ 0.10828796 -0.00676112 -0.0081361   0.87681482]
MAP reward
0.11	0.11	0.88	-0.01	0.88	0.88	-0.01	
0.88	0.88	0.11	-0.01	0.11	0.11	0.11	
-0.01	-0.01	0.88	0.88	0.88	-0.01	-0.01	
0.88	-0.01	-0.01	0.11	0.11	0.11	0.11	
-0.01	0.11	-0.01	-0.01	0.88	-0.01	-0.01	
-0.01	-0.01	-0.01	-0.01	-0.01	-0.01	-0.01	
-0.01	0.88	-0.01	0.11	-0.01	-0.01	0.11	
Map policy
v	v	^	>	^	<	<	
<	<	<	v	^	^	<	
^	^	>	<	<	<	<	
<	<	^	^	^	<	<	
^	^	^	>	^	<	<	
^	v	v	^	^	^	^	
>	v	<	<	^	^	^	
expeced value MDP LP 27.735822637414792
mean w [-0.15752048 -0.22696523 -0.14484319  0.28372741]
Mean policy from posterior
v	v	^	<	^	<	<	
<	<	<	v	^	^	<	
^	^	>	<	<	<	<	
<	<	^	^	^	^	^	
^	^	^	>	^	<	<	
^	v	<	>	^	<	<	
>	v	<	<	^	<	<	
Mean rewards
-0.16	-0.16	0.28	-0.14	0.28	0.28	-0.23	
0.28	0.28	-0.16	-0.23	-0.16	-0.16	-0.16	
-0.14	-0.23	0.28	0.28	0.28	-0.14	-0.14	
0.28	-0.14	-0.23	-0.16	-0.16	-0.16	-0.16	
-0.14	-0.16	-0.23	-0.23	0.28	-0.23	-0.14	
-0.23	-0.14	-0.23	-0.23	-0.14	-0.14	-0.14	
-0.14	0.28	-0.23	-0.16	-0.14	-0.14	-0.16	
mean = 0.05884986156597449, map = 0.14379016186295956
CVaR policy
v	v	^	>	^	<	<	
<	<	<	v	^	^	^	
^	^	>	<	<	<	<	
<	<	^	^	^	^	^	
^	v	^	>	^	<	<	
v	v	<	<	^	<	<	
>	v	<	<	^	<	<	
CVaR policy
v	v	^	>	^	<	<	
<	<	<	v	^	^	<	
^	^	>	<	<	<	<	
<	<	^	^	^	^	^	
^	v	^	>	^	<	<	
^	v	<	>	^	<	<	
>	v	<	<	^	^	<	
CVaR policy
v	v	^	>	^	<	<	
<	<	<	v	^	^	<	
^	^	>	<	<	<	<	
<	<	^	^	^	^	^	
^	v	^	>	^	<	<	
v	v	<	>	^	<	<	
>	v	<	<	^	<	<	
CVaR policy
v	v	^	>	^	<	<	
<	<	<	v	^	^	<	
^	^	>	<	<	<	<	
<	<	^	^	^	^	^	
^	^	^	>	^	<	<	
^	v	<	>	^	<	<	
>	v	<	<	^	<	<	
CVaR policy
v	v	^	>	^	<	<	
<	<	<	v	^	^	<	
^	^	>	<	<	<	<	
<	<	^	^	^	^	^	
^	^	^	>	^	<	<	
v	v	<	>	^	<	<	
>	v	<	<	^	<	<	
cvar = , 0.09736544272914927, 0.0588498743973318, 0.05886257178389087, 0.05886170958233805, 0.0588731612431701
==========
iteration 54
==========
weights [ 0.54622993  0.60522549 -0.03222478  0.57818383]
expeced value MDP LP 60.302337350438194
demonstration
[(24, 3), (31, 3), (38, 2), (31, 3), (38, 3), (45, 2), (38, 3), (45, 2), (38, 2), (31, 3), (38, 3), (45, 3), (45, 2), (38, 3), (45, 3), (45, 3), (45, 3), (45, 2), (38, 3), (45, 2), (38, 3), (45, 2), (38, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 2), (38, 2), (31, 3), (38, 2), (31, 3), (38, 2), (31, 3), (38, 3), (45, 3), (45, 2), (38, 2), (31, 3), (38, 3), (45, 2), (38, 3), (45, 2), (38, 2), (31, 3), (38, 2), (31, 3), (38, 3), (45, 3)]
[-0.00374963 -0.20209531 -0.13125032 -0.66290475]
w_map [-0.60892323  0.20727369 -0.0172999  -0.16650319] loglik -27.03273939992323
accepted/total = 2757/3000 = 0.919
-------
true weights [ 0.54622993  0.60522549 -0.03222478  0.57818383]
features
3 	1 	1 	0 	2 	3 	1 	
3 	2 	3 	2 	1 	1 	1 	
2 	3 	2 	2 	3 	3 	2 	
0 	3 	2 	0 	2 	1 	2 	
2 	1 	0 	1 	0 	2 	0 	
3 	1 	3 	1 	3 	1 	0 	
2 	3 	2 	1 	0 	2 	1 	
optimal policy
>	^	^	<	v	>	>	
^	^	^	>	>	>	^	
^	v	^	>	^	^	^	
>	v	<	v	>	^	<	
>	v	>	v	<	^	v	
>	^	>	v	<	<	v	
>	^	>	v	<	>	>	
optimal values
60.50	60.52	60.52	60.46	59.89	60.50	60.52	
60.47	59.89	60.50	59.89	60.52	60.52	60.52	
59.83	60.47	59.86	59.86	60.50	60.50	59.89	
60.44	60.50	59.86	60.46	59.86	60.50	59.86	
59.89	60.52	60.46	60.52	60.46	59.86	60.41	
60.50	60.52	60.50	60.52	60.50	60.50	60.46	
59.86	60.50	59.89	60.52	60.46	59.89	60.52	
map_weights [-0.60892323  0.20727369 -0.0172999  -0.16650319]
MAP reward
-0.17	0.21	0.21	-0.61	-0.02	-0.17	0.21	
-0.17	-0.02	-0.17	-0.02	0.21	0.21	0.21	
-0.02	-0.17	-0.02	-0.02	-0.17	-0.17	-0.02	
-0.61	-0.17	-0.02	-0.61	-0.02	0.21	-0.02	
-0.02	0.21	-0.61	0.21	-0.61	-0.02	-0.61	
-0.17	0.21	-0.17	0.21	-0.17	0.21	-0.61	
-0.02	-0.17	-0.02	0.21	-0.61	-0.02	0.21	
Map policy
>	>	^	<	v	>	>	
>	^	^	>	>	>	^	
^	^	^	^	^	^	^	
v	v	<	v	>	^	^	
>	v	>	v	<	v	<	
>	^	>	v	<	v	v	
>	^	>	v	<	>	>	
expeced value MDP LP 30.07068825389102
mean w [-0.15088538  0.30532145 -0.11967659 -0.19051177]
Mean policy from posterior
>	>	^	<	v	>	>	
>	^	^	>	>	>	^	
v	^	^	^	^	^	^	
v	v	v	v	v	^	^	
>	v	>	v	<	v	v	
>	^	>	v	<	v	v	
>	^	>	v	<	>	>	
Mean rewards
-0.19	0.31	0.31	-0.15	-0.12	-0.19	0.31	
-0.19	-0.12	-0.19	-0.12	0.31	0.31	0.31	
-0.12	-0.19	-0.12	-0.12	-0.19	-0.19	-0.12	
-0.15	-0.19	-0.12	-0.15	-0.12	0.31	-0.12	
-0.12	0.31	-0.15	0.31	-0.15	-0.12	-0.15	
-0.19	0.31	-0.19	0.31	-0.19	0.31	-0.15	
-0.12	-0.19	-0.12	0.31	-0.15	-0.12	0.31	
mean = 0.10036300364663475, map = 0.12273098270685523
CVaR policy
>	>	^	<	v	v	>	
>	^	^	>	>	>	^	
>	^	^	^	^	^	^	
v	v	<	v	>	^	^	
>	v	>	v	<	v	v	
>	^	>	v	<	v	v	
^	^	>	v	<	>	>	
CVaR policy
>	^	^	<	v	v	>	
>	^	^	>	>	>	^	
v	^	^	^	^	^	^	
v	v	v	v	>	^	^	
>	v	>	v	<	v	v	
>	^	>	v	<	v	v	
^	^	>	v	<	>	>	
CVaR policy
>	^	^	<	v	>	>	
>	^	^	>	>	>	^	
v	^	^	^	^	^	^	
v	v	v	v	v	^	^	
>	v	>	v	<	v	v	
>	^	>	v	<	v	v	
>	^	>	v	<	>	>	
CVaR policy
>	^	^	<	v	>	>	
>	^	^	>	>	>	^	
v	^	^	^	^	^	^	
v	v	v	v	v	^	^	
>	v	>	v	<	v	v	
>	^	>	v	<	v	v	
>	^	>	v	<	>	>	
CVaR policy
>	^	^	<	v	>	>	
>	^	^	>	>	>	^	
v	^	^	^	^	^	^	
v	v	v	v	v	^	^	
>	v	>	v	<	v	v	
>	^	>	v	<	v	v	
>	^	>	v	<	>	>	
cvar = , 0.09842232677308971, 0.09971223042144572, 0.10036721426936168, 0.10036303767824961, 0.10036310940984094
==========
iteration 55
==========
weights [0.82095841 0.13248638 0.39005919 0.39538397]
expeced value MDP LP 81.73568198578738
demonstration
[(24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1), (25, 0), (24, 1)]
[-0.11816289 -0.17256631 -0.47890299 -0.23036781]
w_map [ 0.21436903 -0.09950404 -0.29019048 -0.39593645] loglik 0.0
accepted/total = 2800/3000 = 0.9333333333333333
-------
true weights [0.82095841 0.13248638 0.39005919 0.39538397]
features
0 	3 	2 	0 	1 	0 	0 	
3 	2 	2 	3 	2 	3 	0 	
2 	0 	0 	3 	2 	2 	2 	
2 	1 	1 	0 	0 	3 	2 	
3 	0 	2 	1 	3 	0 	0 	
2 	0 	3 	2 	0 	3 	1 	
3 	2 	0 	0 	0 	1 	2 	
optimal policy
<	<	>	^	>	>	>	
^	v	v	^	>	>	^	
>	>	<	v	v	^	^	
v	v	>	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	v	v	<	^	
>	>	>	v	<	<	<	
optimal values
82.10	81.67	81.66	82.10	81.41	82.10	82.10	
81.67	81.66	81.66	81.67	81.24	81.67	82.10	
81.66	82.10	82.10	81.67	81.66	81.24	81.66	
81.24	81.41	81.41	82.10	82.10	81.67	81.66	
81.67	82.10	81.66	81.41	81.67	82.10	82.10	
81.66	82.10	81.67	81.66	82.10	81.67	81.41	
81.24	81.66	82.10	82.10	82.10	81.41	80.98	
map_weights [ 0.21436903 -0.09950404 -0.29019048 -0.39593645]
MAP reward
0.21	-0.40	-0.29	0.21	-0.10	0.21	0.21	
-0.40	-0.29	-0.29	-0.40	-0.29	-0.40	0.21	
-0.29	0.21	0.21	-0.40	-0.29	-0.29	-0.29	
-0.29	-0.10	-0.10	0.21	0.21	-0.40	-0.29	
-0.40	0.21	-0.29	-0.10	-0.40	0.21	0.21	
-0.29	0.21	-0.40	-0.29	0.21	-0.40	-0.10	
-0.40	-0.29	0.21	0.21	0.21	-0.10	-0.29	
Map policy
<	<	>	^	>	>	>	
^	v	v	^	^	>	^	
>	>	<	v	v	>	^	
>	v	>	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	v	v	<	^	
>	>	>	v	<	<	<	
expeced value MDP LP 35.6887474223135
mean w [ 0.36039204 -0.13377197 -0.07043448 -0.14412668]
Mean policy from posterior
<	<	>	^	>	>	>	
^	v	v	^	v	>	^	
>	>	<	v	v	>	^	
^	v	>	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	v	v	<	^	
>	>	>	v	<	<	<	
Mean rewards
0.36	-0.14	-0.07	0.36	-0.13	0.36	0.36	
-0.14	-0.07	-0.07	-0.14	-0.07	-0.14	0.36	
-0.07	0.36	0.36	-0.14	-0.07	-0.07	-0.07	
-0.07	-0.13	-0.13	0.36	0.36	-0.14	-0.07	
-0.14	0.36	-0.07	-0.13	-0.14	0.36	0.36	
-0.07	0.36	-0.14	-0.07	0.36	-0.14	-0.13	
-0.14	-0.07	0.36	0.36	0.36	-0.13	-0.07	
mean = 0.000322746447722011, map = 0.01073079120874354
CVaR policy
<	<	>	^	>	>	>	
^	v	v	^	v	>	^	
>	>	<	v	v	>	^	
^	^	^	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	>	v	<	^	
>	>	>	v	<	<	<	
CVaR policy
<	<	>	^	>	>	<	
^	v	v	^	v	>	^	
>	>	<	v	v	>	^	
^	v	^	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	>	v	<	^	
>	>	>	v	<	<	<	
CVaR policy
<	<	>	^	>	>	>	
^	v	v	^	v	>	^	
>	>	<	v	v	>	^	
^	v	^	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	v	v	<	^	
>	>	>	v	<	<	<	
CVaR policy
<	<	>	^	>	>	>	
^	v	v	^	v	>	^	
>	>	<	v	v	>	^	
^	v	>	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	v	v	<	^	
>	>	>	v	<	<	<	
CVaR policy
<	<	>	^	>	>	>	
^	v	v	^	v	>	^	
>	>	<	v	v	>	^	
^	v	>	>	<	v	v	
>	v	<	^	v	>	>	
>	^	v	v	v	<	^	
>	>	>	v	<	<	<	
cvar = , 0.0003227838489294754, 0.000322750624661694, 0.00032482795620580873, 0.0003230717903193181, 0.0003227442913384948
==========
iteration 56
==========
weights [-0.13733696 -0.86065135  0.43927932  0.21783365]
expeced value MDP LP 43.238161390120844
demonstration
[(24, 2), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1)]
[-0.23191382 -0.15109933 -0.40580844 -0.21117841]
w_map [-0.61501941 -0.00707282  0.34403125  0.03387652] loglik 0.0
accepted/total = 2788/3000 = 0.9293333333333333
-------
true weights [-0.13733696 -0.86065135  0.43927932  0.21783365]
features
2 	0 	2 	0 	2 	1 	3 	
0 	1 	3 	3 	0 	3 	0 	
1 	0 	2 	2 	0 	0 	1 	
3 	1 	0 	1 	0 	3 	1 	
3 	3 	2 	3 	3 	3 	1 	
3 	2 	0 	2 	2 	1 	1 	
2 	2 	0 	2 	2 	3 	1 	
optimal policy
<	<	^	<	^	<	<	
^	>	^	v	^	<	<	
^	>	>	<	<	<	<	
v	v	^	^	v	v	<	
v	v	<	v	v	<	<	
v	v	<	v	v	<	<	
<	<	<	>	<	<	<	
optimal values
43.93	43.35	43.93	43.35	43.93	42.63	42.42	
43.35	42.41	43.71	43.71	43.35	43.14	42.57	
42.06	43.35	43.93	43.93	43.35	42.78	41.49	
43.27	42.41	43.35	42.63	43.13	43.27	41.98	
43.49	43.71	43.71	43.71	43.71	43.49	42.19	
43.71	43.93	43.35	43.93	43.93	42.63	41.34	
43.93	43.93	43.35	43.93	43.93	43.71	42.41	
map_weights [-0.61501941 -0.00707282  0.34403125  0.03387652]
MAP reward
0.34	-0.62	0.34	-0.62	0.34	-0.01	0.03	
-0.62	-0.01	0.03	0.03	-0.62	0.03	-0.62	
-0.01	-0.62	0.34	0.34	-0.62	-0.62	-0.01	
0.03	-0.01	-0.62	-0.01	-0.62	0.03	-0.01	
0.03	0.03	0.34	0.03	0.03	0.03	-0.01	
0.03	0.34	-0.62	0.34	0.34	-0.01	-0.01	
0.34	0.34	-0.62	0.34	0.34	0.03	-0.01	
Map policy
<	<	^	<	^	<	<	
^	>	^	v	^	^	^	
v	>	>	<	<	^	v	
v	v	^	^	v	v	<	
v	v	<	v	v	<	<	
v	v	<	v	v	<	<	
<	<	<	v	<	<	<	
expeced value MDP LP 34.88398033758311
mean w [-0.20800862 -0.1237443   0.35459191 -0.05052504]
Mean policy from posterior
<	<	^	<	^	<	<	
^	>	^	v	^	^	<	
^	>	>	<	<	<	<	
v	v	^	^	v	v	<	
v	v	>	v	v	<	<	
v	v	<	v	v	<	<	
<	<	<	v	<	<	<	
Mean rewards
0.35	-0.21	0.35	-0.21	0.35	-0.12	-0.05	
-0.21	-0.12	-0.05	-0.05	-0.21	-0.05	-0.21	
-0.12	-0.21	0.35	0.35	-0.21	-0.21	-0.12	
-0.05	-0.12	-0.21	-0.12	-0.21	-0.05	-0.12	
-0.05	-0.05	0.35	-0.05	-0.05	-0.05	-0.12	
-0.05	0.35	-0.21	0.35	0.35	-0.12	-0.12	
0.35	0.35	-0.21	0.35	0.35	-0.05	-0.12	
mean = 0.029081647997834636, map = 0.06578171786671305
CVaR policy
<	>	^	>	^	<	<	
^	>	^	v	^	<	<	
^	>	>	<	<	<	<	
v	v	^	^	v	v	<	
>	v	<	v	v	<	<	
v	v	>	v	v	<	<	
<	<	>	v	<	<	<	
CVaR policy
<	<	^	>	^	<	<	
^	>	^	v	^	<	<	
^	>	>	<	<	<	<	
v	v	^	^	v	v	<	
v	v	<	v	v	<	<	
>	v	<	v	v	<	<	
<	<	<	v	<	<	<	
CVaR policy
<	<	^	<	^	<	<	
^	>	^	v	^	^	<	
^	>	>	<	<	<	<	
v	v	^	^	v	v	<	
v	v	<	v	v	<	<	
v	v	<	v	v	<	<	
<	<	<	v	<	<	<	
CVaR policy
<	<	^	<	^	<	<	
^	>	^	v	^	^	<	
^	>	>	<	<	<	<	
v	v	^	^	v	v	<	
v	v	<	v	v	<	<	
v	v	<	v	v	<	<	
<	<	<	v	<	<	<	
CVaR policy
<	<	^	<	^	<	<	
^	>	^	v	^	^	<	
^	>	>	<	<	<	<	
v	v	^	^	v	v	<	
v	v	<	v	v	<	<	
v	v	<	v	v	<	<	
<	<	<	v	<	<	<	
cvar = , -1.3513378860352532e-08, 3.7471669145361375e-07, 0.02908169117120707, 0.029098468806417088, 0.029081890532765442
==========
iteration 57
==========
weights [ 0.17508597  0.34774987  0.25301923 -0.88566145]
expeced value MDP LP 34.28778493502573
demonstration
[(24, 2), (17, 2), (10, 1), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0)]
[-0.3538062  -0.28506553 -0.01096065 -0.35016763]
w_map [-0.02699503  0.47761398  0.28463593 -0.21075506] loglik -0.6931461332842446
accepted/total = 2112/3000 = 0.704
-------
true weights [ 0.17508597  0.34774987  0.25301923 -0.88566145]
features
3 	3 	1 	0 	2 	3 	2 	
0 	1 	0 	2 	1 	1 	3 	
0 	0 	0 	2 	2 	0 	3 	
3 	1 	0 	3 	3 	1 	1 	
2 	0 	1 	0 	0 	0 	3 	
2 	2 	3 	2 	1 	1 	3 	
1 	1 	3 	0 	3 	0 	2 	
optimal policy
v	>	^	<	v	v	<	
>	>	^	>	>	<	<	
>	^	^	>	^	v	v	
v	v	v	^	>	>	>	
v	v	<	v	v	^	^	
v	v	<	>	>	<	<	
<	<	<	^	^	^	<	
optimal values
33.20	33.54	34.77	34.60	34.68	33.54	33.46	
34.43	34.60	34.60	34.68	34.77	34.77	33.54	
34.26	34.43	34.43	34.59	34.68	34.60	33.54	
33.35	34.51	34.34	33.35	33.54	34.77	34.77	
34.59	34.51	34.51	34.51	34.60	34.60	33.54	
34.68	34.68	33.45	34.68	34.77	34.77	33.54	
34.77	34.77	33.54	34.51	33.54	34.60	34.51	
map_weights [-0.02699503  0.47761398  0.28463593 -0.21075506]
MAP reward
-0.21	-0.21	0.48	-0.03	0.28	-0.21	0.28	
-0.03	0.48	-0.03	0.28	0.48	0.48	-0.21	
-0.03	-0.03	-0.03	0.28	0.28	-0.03	-0.21	
-0.21	0.48	-0.03	-0.21	-0.21	0.48	0.48	
0.28	-0.03	0.48	-0.03	-0.03	-0.03	-0.21	
0.28	0.28	-0.21	0.28	0.48	0.48	-0.21	
0.48	0.48	-0.21	-0.03	-0.21	-0.03	0.28	
Map policy
>	>	^	<	v	v	<	
>	>	^	>	>	<	<	
^	^	>	^	^	^	v	
v	v	v	^	>	>	>	
v	v	<	v	v	^	^	
v	v	<	>	>	<	<	
>	<	<	^	^	^	<	
expeced value MDP LP 31.910766312414477
mean w [-0.14900633  0.323908    0.1572322  -0.25105445]
Mean policy from posterior
>	>	^	<	v	v	<	
>	>	^	>	>	<	<	
v	^	>	^	^	v	v	
v	v	v	^	>	>	>	
v	v	>	v	v	^	^	
v	v	<	>	>	<	<	
<	v	<	^	^	^	<	
Mean rewards
-0.25	-0.25	0.32	-0.15	0.16	-0.25	0.16	
-0.15	0.32	-0.15	0.16	0.32	0.32	-0.25	
-0.15	-0.15	-0.15	0.16	0.16	-0.15	-0.25	
-0.25	0.32	-0.15	-0.25	-0.25	0.32	0.32	
0.16	-0.15	0.32	-0.15	-0.15	-0.15	-0.25	
0.16	0.16	-0.25	0.16	0.32	0.32	-0.25	
0.32	0.32	-0.25	-0.15	-0.25	-0.15	0.16	
mean = 0.04011557435570268, map = 0.018333304109724224
CVaR policy
>	>	^	<	v	v	v	
>	>	^	>	>	<	<	
v	^	>	>	^	v	v	
v	v	v	^	>	>	>	
v	v	<	v	v	^	^	
v	v	<	>	>	<	<	
<	<	<	^	^	^	<	
CVaR policy
>	>	^	<	v	v	v	
>	>	^	>	>	<	<	
v	^	>	>	^	v	v	
v	v	v	^	>	>	>	
v	v	<	v	v	^	^	
v	v	<	>	>	<	<	
<	<	<	^	^	^	<	
CVaR policy
>	>	^	<	v	v	v	
>	>	^	>	>	<	<	
v	^	>	>	^	v	v	
v	v	v	^	>	>	>	
v	v	<	v	v	^	^	
v	v	<	>	>	<	<	
<	<	<	^	^	^	<	
CVaR policy
>	>	^	<	v	v	v	
>	>	^	>	>	<	<	
v	^	>	>	^	v	v	
v	v	v	^	>	>	>	
v	v	<	v	v	^	^	
v	v	<	>	>	<	<	
<	<	<	^	^	^	<	
CVaR policy
>	>	^	<	v	v	v	
>	>	^	>	>	<	<	
v	^	>	>	^	v	v	
v	v	v	^	>	>	>	
v	v	<	v	v	^	^	
v	v	<	>	>	<	<	
<	<	<	^	^	^	<	
cvar = , 0.04011683503726715, 0.04014180118873867, 0.040117717213043136, 0.040115630965821936, 0.040115629688983745
==========
iteration 58
==========
weights [ 0.83897602 -0.08792649 -0.33556454 -0.41926675]
expeced value MDP LP 82.35589784453605
demonstration
[(24, 0), (23, 0), (22, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 0), (21, 3), (28, 2), (21, 3), (28, 0), (28, 0), (28, 2), (21, 0), (21, 3), (28, 0), (28, 2), (21, 1), (22, 0), (21, 1), (22, 0), (21, 3), (28, 2), (21, 1), (22, 0), (21, 1), (22, 0), (21, 1), (22, 0), (21, 3), (28, 2), (21, 0), (21, 0), (21, 3), (28, 0), (28, 0), (28, 0), (28, 2), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 3), (28, 2), (21, 3)]
[-0.40232495 -0.24250763 -0.20108172 -0.1540857 ]
w_map [ 0.82514901 -0.02284506  0.08007445 -0.07193147] loglik -37.574832326155956
accepted/total = 2772/3000 = 0.924
-------
true weights [ 0.83897602 -0.08792649 -0.33556454 -0.41926675]
features
1 	0 	3 	1 	1 	3 	0 	
3 	1 	2 	1 	2 	2 	2 	
1 	3 	0 	2 	2 	3 	0 	
0 	0 	2 	0 	1 	2 	1 	
0 	1 	0 	1 	3 	0 	3 	
3 	3 	3 	2 	1 	2 	2 	
0 	1 	2 	0 	3 	3 	2 	
optimal policy
>	^	<	<	>	>	>	
v	^	<	<	^	>	^	
v	v	v	<	>	>	>	
<	<	<	<	<	>	^	
<	<	<	<	<	^	^	
v	v	^	v	<	^	^	
<	<	>	v	<	<	<	
optimal values
82.97	83.90	82.64	81.73	81.73	82.64	83.90	
81.72	82.97	81.81	80.90	80.57	81.56	82.72	
82.97	82.64	82.73	81.57	81.48	82.64	83.90	
83.90	83.90	82.72	82.73	81.82	81.81	82.97	
83.90	82.97	82.98	82.06	80.82	81.83	81.72	
82.64	81.72	81.73	82.72	81.81	80.67	80.57	
83.90	82.97	82.72	83.90	82.64	81.39	80.24	
map_weights [ 0.82514901 -0.02284506  0.08007445 -0.07193147]
MAP reward
-0.02	0.83	-0.07	-0.02	-0.02	-0.07	0.83	
-0.07	-0.02	0.08	-0.02	0.08	0.08	0.08	
-0.02	-0.07	0.83	0.08	0.08	-0.07	0.83	
0.83	0.83	0.08	0.83	-0.02	0.08	-0.02	
0.83	-0.02	0.83	-0.02	-0.07	0.83	-0.07	
-0.07	-0.07	-0.07	0.08	-0.02	0.08	0.08	
0.83	-0.02	0.08	0.83	-0.07	-0.07	0.08	
Map policy
>	^	<	<	>	>	>	
v	^	v	v	>	>	^	
v	v	v	<	>	>	>	
v	<	<	<	<	>	^	
<	<	^	<	>	^	^	
v	v	^	v	<	^	^	
<	<	>	v	<	<	<	
expeced value MDP LP 41.591401578297464
mean w [ 0.42299852 -0.1633147  -0.03022291 -0.10040923]
Mean policy from posterior
>	^	<	<	>	>	>	
v	^	v	v	>	>	^	
v	v	v	<	>	>	>	
v	<	<	<	<	^	^	
<	<	^	<	>	^	^	
v	<	^	v	<	^	^	
<	<	>	v	<	<	<	
Mean rewards
-0.16	0.42	-0.10	-0.16	-0.16	-0.10	0.42	
-0.10	-0.16	-0.03	-0.16	-0.03	-0.03	-0.03	
-0.16	-0.10	0.42	-0.03	-0.03	-0.10	0.42	
0.42	0.42	-0.03	0.42	-0.16	-0.03	-0.16	
0.42	-0.16	0.42	-0.16	-0.10	0.42	-0.10	
-0.10	-0.10	-0.10	-0.03	-0.16	-0.03	-0.03	
0.42	-0.16	-0.03	0.42	-0.10	-0.10	-0.03	
mean = 0.06562806593346693, map = 0.03249021789495998
CVaR policy
>	^	<	<	>	>	>	
^	^	v	v	>	>	^	
v	v	v	v	>	>	>	
v	<	<	<	<	^	^	
<	^	^	^	>	^	^	
v	<	^	v	<	^	^	
<	<	>	v	<	<	<	
CVaR policy
>	^	<	<	>	>	>	
v	^	v	v	>	>	^	
v	v	v	v	>	>	>	
v	<	<	<	<	^	^	
<	<	^	<	>	^	^	
v	<	^	v	<	^	^	
<	<	>	v	<	<	<	
CVaR policy
>	^	<	<	>	>	>	
>	^	v	v	>	>	^	
v	v	v	<	>	>	>	
v	<	<	<	<	^	^	
<	<	^	<	>	^	^	
v	<	^	v	<	^	^	
<	<	>	v	<	<	<	
CVaR policy
>	^	<	<	>	>	>	
v	^	v	v	>	>	^	
v	v	v	<	>	>	>	
v	<	<	<	<	^	^	
<	<	^	<	>	^	^	
v	<	^	v	<	^	^	
<	<	>	v	<	<	<	
CVaR policy
>	^	<	<	>	>	>	
v	^	v	v	>	>	^	
v	v	v	<	>	>	>	
v	<	<	<	<	^	^	
<	<	^	<	>	^	^	
v	<	^	v	<	^	^	
<	<	>	v	<	<	<	
cvar = , 0.06562796570038643, 0.06562803287692986, 0.06563056186575977, 0.06562796947773109, 0.0656629858894604
==========
iteration 59
==========
weights [ 0.17739597  0.00902185  0.89325236 -0.41297638]
expeced value MDP LP 87.88873760448625
demonstration
[(24, 0), (23, 0), (22, 2), (15, 3), (22, 3), (29, 2), (22, 2), (15, 3), (22, 2), (15, 3), (22, 2), (15, 3), (22, 2), (15, 3), (22, 3), (29, 2), (22, 2), (15, 3), (22, 2), (15, 3), (22, 3), (29, 2), (22, 2), (15, 3), (22, 3), (29, 2), (22, 3), (29, 2), (22, 2), (15, 3), (22, 2), (15, 3), (22, 3), (29, 2), (22, 3), (29, 2), (22, 3), (29, 2), (22, 3), (29, 2), (22, 2), (15, 3), (22, 3), (29, 2), (22, 2), (15, 3), (22, 3), (29, 2), (22, 3)]
[-0.26358439 -0.22204229 -0.26001012 -0.25436319]
w_map [-0.08577898 -0.43007596  0.41917256 -0.06497249] loglik -16.63553233343191
accepted/total = 2664/3000 = 0.888
-------
true weights [ 0.17739597  0.00902185  0.89325236 -0.41297638]
features
0 	2 	1 	2 	3 	2 	3 	
1 	0 	1 	0 	0 	2 	2 	
1 	2 	1 	1 	3 	1 	2 	
3 	2 	0 	1 	2 	3 	3 	
3 	2 	3 	3 	3 	3 	1 	
3 	0 	0 	2 	0 	2 	1 	
0 	3 	0 	0 	1 	3 	3 	
optimal policy
>	^	>	^	>	^	<	
>	^	<	^	>	>	>	
>	v	<	^	^	>	^	
>	^	<	<	<	^	^	
>	^	<	<	^	v	^	
>	^	<	<	<	<	<	
^	^	^	^	<	^	^	
optimal values
88.61	89.33	88.44	89.33	88.02	89.33	88.02	
87.73	88.61	87.73	88.61	88.61	89.33	89.33	
88.44	89.33	88.44	87.73	87.31	88.44	89.33	
88.02	89.33	88.61	87.73	87.75	87.14	88.02	
88.02	89.33	88.02	86.73	86.46	85.95	87.15	
87.31	88.61	87.90	87.91	87.21	87.23	86.37	
86.61	87.31	87.20	87.21	86.35	85.95	85.09	
map_weights [-0.08577898 -0.43007596  0.41917256 -0.06497249]
MAP reward
-0.09	0.42	-0.43	0.42	-0.06	0.42	-0.06	
-0.43	-0.09	-0.43	-0.09	-0.09	0.42	0.42	
-0.43	0.42	-0.43	-0.43	-0.06	-0.43	0.42	
-0.06	0.42	-0.09	-0.43	0.42	-0.06	-0.06	
-0.06	0.42	-0.06	-0.06	-0.06	-0.06	-0.43	
-0.06	-0.09	-0.09	0.42	-0.09	0.42	-0.43	
-0.09	-0.06	-0.09	-0.09	-0.43	-0.06	-0.06	
Map policy
>	^	<	^	>	^	<	
>	^	<	^	>	^	<	
>	v	<	^	^	^	^	
>	^	<	<	>	>	^	
>	^	<	<	^	^	^	
^	^	^	^	<	^	^	
^	^	<	^	<	^	<	
expeced value MDP LP 23.280313712481
mean w [-0.12051674 -0.26476387  0.23961516 -0.26337721]
Mean policy from posterior
>	^	<	^	>	^	<	
>	^	<	^	>	^	>	
>	v	<	^	^	^	^	
>	^	<	<	^	>	^	
>	^	<	<	^	^	^	
>	^	<	<	<	<	^	
^	^	^	^	<	^	^	
Mean rewards
-0.12	0.24	-0.26	0.24	-0.26	0.24	-0.26	
-0.26	-0.12	-0.26	-0.12	-0.12	0.24	0.24	
-0.26	0.24	-0.26	-0.26	-0.26	-0.26	0.24	
-0.26	0.24	-0.12	-0.26	0.24	-0.26	-0.26	
-0.26	0.24	-0.26	-0.26	-0.26	-0.26	-0.26	
-0.26	-0.12	-0.12	0.24	-0.12	0.24	-0.26	
-0.12	-0.26	-0.12	-0.12	-0.26	-0.26	-0.26	
mean = 0.03924145724052153, map = 0.31719775726988075
CVaR policy
>	^	<	^	>	^	<	
>	^	<	^	>	^	>	
>	v	<	^	^	>	^	
>	^	<	<	^	>	^	
>	^	<	<	^	^	^	
>	^	<	<	<	<	^	
>	^	^	^	<	^	^	
CVaR policy
>	^	>	^	>	^	<	
>	^	<	^	>	^	>	
>	v	<	^	^	^	^	
>	^	<	<	^	>	^	
>	^	<	<	^	^	^	
>	^	<	<	<	<	^	
^	^	^	^	<	^	^	
CVaR policy
>	^	<	^	>	^	v	
>	^	<	^	>	^	>	
>	v	<	^	^	^	^	
>	^	<	<	^	>	^	
>	^	<	<	^	^	^	
>	^	<	<	<	<	^	
>	^	^	^	<	^	^	
CVaR policy
>	^	>	^	>	^	v	
>	^	<	^	>	^	<	
>	v	<	^	^	^	^	
>	^	<	<	^	>	^	
>	^	<	<	^	^	^	
>	^	<	<	<	<	^	
^	^	^	^	<	^	^	
CVaR policy
>	^	<	^	>	^	v	
>	^	<	^	>	^	<	
>	v	<	^	^	^	^	
>	^	<	<	^	>	^	
>	^	<	<	^	^	^	
>	^	<	<	<	<	^	
^	^	^	^	<	^	^	
cvar = , 0.03933052161194439, 0.03924328868642135, 0.0392415809379969, 0.03924189247817367, 0.03923819852461463
==========
iteration 60
==========
weights [-0.30710551 -0.74042524  0.58638991  0.11663418]
expeced value MDP LP 57.46451293287256
demonstration
[(24, 0), (23, 0), (22, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 0)]
[-0.27102351 -0.14215515 -0.32889329 -0.25792805]
w_map [ 0.02868418 -0.304309    0.34250235 -0.32450447] loglik 0.0
accepted/total = 2527/3000 = 0.8423333333333334
-------
true weights [-0.30710551 -0.74042524  0.58638991  0.11663418]
features
1 	3 	2 	3 	0 	2 	3 	
3 	3 	3 	2 	3 	0 	3 	
1 	3 	1 	1 	3 	2 	1 	
2 	0 	0 	2 	1 	1 	2 	
1 	3 	2 	1 	1 	1 	0 	
1 	1 	1 	1 	2 	1 	0 	
3 	2 	2 	1 	0 	0 	2 	
optimal policy
>	>	^	<	>	^	<	
>	^	^	^	<	^	^	
v	v	^	^	>	^	v	
<	<	<	<	>	>	>	
^	^	v	<	v	>	^	
v	v	v	v	v	>	v	
>	v	<	<	>	>	>	
optimal values
56.85	58.17	58.64	58.17	57.75	58.64	58.17	
57.24	57.70	58.17	58.17	57.71	57.75	57.70	
57.31	57.28	56.85	56.85	57.29	57.75	57.31	
58.64	57.75	56.86	56.88	56.00	57.31	58.64	
57.31	57.28	57.33	56.01	55.57	56.43	57.75	
56.85	57.31	57.31	56.00	56.88	56.43	57.75	
58.17	58.64	58.64	57.31	56.86	57.75	58.64	
map_weights [ 0.02868418 -0.304309    0.34250235 -0.32450447]
MAP reward
-0.30	-0.32	0.34	-0.32	0.03	0.34	-0.32	
-0.32	-0.32	-0.32	0.34	-0.32	0.03	-0.32	
-0.30	-0.32	-0.30	-0.30	-0.32	0.34	-0.30	
0.34	0.03	0.03	0.34	-0.30	-0.30	0.34	
-0.30	-0.32	0.34	-0.30	-0.30	-0.30	0.03	
-0.30	-0.30	-0.30	-0.30	0.34	-0.30	0.03	
-0.32	0.34	0.34	-0.30	0.03	0.03	0.34	
Map policy
>	>	^	<	>	^	<	
v	>	^	^	^	^	<	
v	v	v	v	>	^	v	
<	<	<	<	<	>	>	
^	^	^	<	v	>	^	
>	v	v	>	v	>	v	
>	>	<	<	>	>	>	
expeced value MDP LP 42.03782817071157
mean w [ 0.12613303 -0.18963074  0.4263485  -0.05804348]
Mean policy from posterior
>	>	^	<	>	^	<	
v	^	^	^	^	^	<	
v	v	^	^	>	^	v	
<	<	<	<	<	>	>	
^	^	^	<	v	>	^	
v	v	v	>	v	>	v	
>	>	<	<	>	>	>	
Mean rewards
-0.19	-0.06	0.43	-0.06	0.13	0.43	-0.06	
-0.06	-0.06	-0.06	0.43	-0.06	0.13	-0.06	
-0.19	-0.06	-0.19	-0.19	-0.06	0.43	-0.19	
0.43	0.13	0.13	0.43	-0.19	-0.19	0.43	
-0.19	-0.06	0.43	-0.19	-0.19	-0.19	0.13	
-0.19	-0.19	-0.19	-0.19	0.43	-0.19	0.13	
-0.06	0.43	0.43	-0.19	0.13	0.13	0.43	
mean = 0.060795435013453414, map = 0.13071323234257193
CVaR policy
>	>	^	<	>	^	<	
>	^	^	^	^	^	<	
v	v	^	^	>	^	v	
<	<	<	<	<	>	>	
^	^	^	<	v	>	^	
v	v	v	>	v	>	v	
>	v	v	<	>	>	>	
CVaR policy
>	>	^	<	>	^	<	
v	^	^	^	^	^	<	
v	v	^	^	>	^	v	
<	<	<	<	<	>	>	
^	^	^	<	v	>	^	
v	v	v	>	v	>	v	
>	>	v	<	>	>	>	
CVaR policy
>	>	^	<	>	^	<	
v	^	^	^	^	^	<	
v	v	^	^	>	^	v	
<	<	<	<	<	>	>	
^	^	^	^	v	>	^	
v	v	v	>	v	>	v	
>	>	v	<	>	>	>	
CVaR policy
>	>	^	<	>	^	<	
v	^	^	^	^	^	<	
v	v	^	^	>	^	v	
<	<	<	<	<	>	>	
^	^	^	^	v	>	^	
v	v	v	>	v	>	v	
>	>	v	<	>	>	>	
CVaR policy
>	>	^	<	>	^	<	
v	^	^	^	^	^	<	
v	v	^	^	>	^	v	
<	<	<	<	<	>	>	
^	^	^	^	v	>	^	
v	v	v	>	v	>	v	
>	>	v	<	>	>	>	
cvar = , 0.05287558414817539, 0.060996721395206066, 0.06079631449142653, 0.06081147635313755, 0.06080830602857645
==========
iteration 61
==========
weights [-0.25037154 -0.42170236  0.374418   -0.78695131]
expeced value MDP LP 36.41469840369976
demonstration
[(24, 3), (31, 0), (30, 3), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 1), (31, 0), (30, 1), (31, 0), (30, 3), (37, 2), (30, 3), (37, 2), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 3), (37, 2), (30, 3), (37, 2), (30, 1), (31, 0), (30, 3), (37, 2), (30, 1), (31, 0), (30, 3), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 3), (37, 2), (30, 1)]
[-0.38458414 -0.39052477 -0.02632463 -0.19856647]
w_map [ 0.2076068  -0.0253069   0.75535784  0.01172847] loglik -16.635532086686
accepted/total = 2744/3000 = 0.9146666666666666
-------
true weights [-0.25037154 -0.42170236  0.374418   -0.78695131]
features
0 	1 	0 	2 	0 	1 	3 	
1 	3 	3 	0 	2 	1 	2 	
2 	0 	1 	3 	2 	2 	1 	
2 	0 	1 	0 	0 	1 	3 	
3 	3 	2 	2 	3 	1 	3 	
3 	3 	2 	0 	3 	2 	1 	
3 	2 	0 	0 	1 	3 	1 	
optimal policy
v	>	>	^	v	<	v	
v	v	>	>	v	<	>	
v	<	<	>	^	<	^	
<	<	v	v	^	^	^	
^	>	>	<	<	^	<	
^	>	^	<	<	^	<	
>	v	^	<	<	^	^	
optimal values
36.03	36.03	36.82	37.44	36.82	36.03	36.28	
36.65	35.66	35.66	36.82	37.44	36.65	37.44	
37.44	36.82	36.03	36.28	37.44	37.44	36.65	
37.44	36.82	36.65	36.82	36.82	36.65	35.49	
36.28	36.28	37.44	37.44	36.28	35.86	34.71	
35.13	36.28	37.44	36.82	35.66	35.87	35.09	
36.28	37.44	36.82	36.20	35.41	34.73	34.32	
map_weights [ 0.2076068  -0.0253069   0.75535784  0.01172847]
MAP reward
0.21	-0.03	0.21	0.76	0.21	-0.03	0.01	
-0.03	0.01	0.01	0.21	0.76	-0.03	0.76	
0.76	0.21	-0.03	0.01	0.76	0.76	-0.03	
0.76	0.21	-0.03	0.21	0.21	-0.03	0.01	
0.01	0.01	0.76	0.76	0.01	-0.03	0.01	
0.01	0.01	0.76	0.21	0.01	0.76	-0.03	
0.01	0.76	0.21	0.21	-0.03	0.01	-0.03	
Map policy
v	>	>	^	v	<	v	
v	v	>	>	v	<	>	
v	<	<	>	^	<	<	
<	<	v	v	^	^	^	
^	>	v	<	<	<	^	
>	>	^	<	<	<	<	
>	v	^	<	<	^	<	
expeced value MDP LP 35.887296282466075
mean w [-0.1065552  -0.08755092  0.36429335 -0.05180994]
Mean policy from posterior
v	>	>	^	v	>	v	
v	<	>	>	v	<	>	
v	<	>	>	^	<	<	
<	<	v	v	^	^	^	
^	>	v	<	<	<	^	
^	v	^	<	^	<	<	
>	v	<	<	^	^	<	
Mean rewards
-0.11	-0.09	-0.11	0.36	-0.11	-0.09	-0.05	
-0.09	-0.05	-0.05	-0.11	0.36	-0.09	0.36	
0.36	-0.11	-0.09	-0.05	0.36	0.36	-0.09	
0.36	-0.11	-0.09	-0.11	-0.11	-0.09	-0.05	
-0.05	-0.05	0.36	0.36	-0.05	-0.09	-0.05	
-0.05	-0.05	0.36	-0.11	-0.05	0.36	-0.09	
-0.05	0.36	-0.11	-0.11	-0.09	-0.05	-0.09	
mean = 0.13785213119621176, map = 0.03779181359188044
CVaR policy
v	>	>	^	v	>	v	
v	v	>	>	v	<	>	
v	<	>	>	^	<	<	
<	<	v	v	^	^	^	
^	>	v	<	<	<	^	
>	v	^	<	^	<	<	
>	v	<	<	^	^	<	
CVaR policy
v	>	>	^	v	>	v	
v	v	>	>	v	<	>	
v	<	>	>	^	<	<	
<	<	v	v	^	^	^	
^	>	v	<	<	<	^	
^	v	^	^	^	<	<	
>	v	<	<	^	^	<	
CVaR policy
v	>	>	^	v	>	v	
v	v	>	>	v	<	>	
v	<	>	>	^	<	<	
<	<	v	v	^	^	^	
^	>	v	<	<	<	^	
^	v	^	^	^	<	<	
>	v	<	<	^	^	<	
CVaR policy
v	>	>	^	v	>	v	
v	<	>	>	v	v	>	
v	<	>	>	^	<	<	
<	<	v	v	^	^	^	
^	>	v	<	<	<	^	
^	v	^	^	^	<	<	
>	v	<	<	^	^	<	
CVaR policy
v	>	>	^	v	>	v	
v	<	>	>	v	v	>	
v	<	>	>	^	<	<	
<	<	v	v	^	^	^	
^	>	v	<	<	<	^	
^	v	^	^	^	<	<	
>	v	<	<	^	^	<	
cvar = , 0.12259752541410052, 0.1343908264317406, 0.1343944802269803, 0.13785242054159141, 0.13787745183140743
==========
iteration 62
==========
weights [-0.83093613  0.10487124  0.17161183 -0.51874517]
expeced value MDP LP 16.365081689446562
demonstration
[(24, 2), (17, 0), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0)]
[-0.21224595 -0.13569921 -0.18174099 -0.47031385]
w_map [-0.33845868 -0.2984798   0.3393001  -0.02376142] loglik 0.0
accepted/total = 2637/3000 = 0.879
-------
true weights [-0.83093613  0.10487124  0.17161183 -0.51874517]
features
0 	2 	1 	2 	0 	1 	0 	
0 	1 	3 	3 	1 	1 	0 	
3 	2 	2 	3 	1 	2 	0 	
3 	0 	0 	0 	1 	1 	1 	
0 	3 	1 	0 	0 	3 	3 	
1 	3 	3 	0 	3 	3 	3 	
2 	2 	1 	2 	1 	2 	0 	
optimal policy
>	^	<	^	<	v	<	
>	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	^	<	
v	v	v	<	v	v	^	
v	v	v	v	v	v	<	
>	<	>	v	>	v	<	
optimal values
16.16	17.16	17.09	17.16	16.16	16.30	15.31	
16.09	17.09	16.47	16.47	16.41	16.36	15.36	
16.47	17.16	17.16	16.47	16.41	16.42	15.42	
15.79	16.16	16.16	15.48	16.35	16.36	16.30	
16.09	15.79	16.35	15.35	15.41	15.79	15.62	
17.09	16.47	16.40	16.16	16.40	16.47	15.79	
17.16	17.16	17.09	17.16	17.09	17.16	16.16	
map_weights [-0.33845868 -0.2984798   0.3393001  -0.02376142]
MAP reward
-0.34	0.34	-0.30	0.34	-0.34	-0.30	-0.34	
-0.34	-0.30	-0.02	-0.02	-0.30	-0.30	-0.34	
-0.02	0.34	0.34	-0.02	-0.30	0.34	-0.34	
-0.02	-0.34	-0.34	-0.34	-0.30	-0.30	-0.30	
-0.34	-0.02	-0.30	-0.34	-0.34	-0.02	-0.02	
-0.30	-0.02	-0.02	-0.34	-0.02	-0.02	-0.02	
0.34	0.34	-0.30	0.34	-0.30	0.34	-0.34	
Map policy
>	^	<	^	<	<	<	
v	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	v	v	
v	v	^	v	v	v	v	
v	v	<	v	>	v	<	
<	<	<	v	<	v	<	
expeced value MDP LP 35.635055734909685
mean w [-0.2781476  -0.09584629  0.36331558  0.0075104 ]
Mean policy from posterior
>	^	<	^	<	<	<	
v	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	v	v	
v	v	^	v	v	v	v	
v	v	<	v	>	v	<	
<	<	<	v	<	v	<	
Mean rewards
-0.28	0.36	-0.10	0.36	-0.28	-0.10	-0.28	
-0.28	-0.10	0.01	0.01	-0.10	-0.10	-0.28	
0.01	0.36	0.36	0.01	-0.10	0.36	-0.28	
0.01	-0.28	-0.28	-0.28	-0.10	-0.10	-0.10	
-0.28	0.01	-0.10	-0.28	-0.28	0.01	0.01	
-0.10	0.01	0.01	-0.28	0.01	0.01	0.01	
0.36	0.36	-0.10	0.36	-0.10	0.36	-0.28	
mean = 0.1154257593654151, map = 0.11542575150918566
CVaR policy
>	^	<	^	<	<	<	
v	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	v	v	
v	v	v	v	v	v	v	
v	v	<	v	>	v	<	
<	<	<	v	<	v	<	
CVaR policy
>	^	<	^	<	<	<	
v	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	v	v	
v	v	<	v	v	v	<	
v	v	<	v	>	v	<	
<	<	<	v	<	v	<	
CVaR policy
>	^	<	^	<	<	<	
v	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	v	v	
v	v	^	v	v	v	v	
v	v	<	v	>	v	<	
<	<	<	v	<	v	<	
CVaR policy
>	^	<	^	<	<	<	
v	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	v	v	
v	v	^	v	v	v	v	
v	v	<	v	>	v	<	
<	<	<	v	<	v	<	
CVaR policy
>	^	<	^	<	<	<	
v	^	v	^	<	v	<	
>	>	<	<	<	<	<	
^	^	^	^	^	v	v	
v	v	^	v	v	v	v	
v	v	<	v	>	v	<	
<	<	<	v	<	v	<	
cvar = , 0.12292768660218911, 0.1229904068044334, 0.11545950268506999, 0.11542580334543828, 0.11542836025224545
==========
iteration 63
==========
weights [-0.14918332 -0.9046088  -0.26868157 -0.29535989]
expeced value MDP LP -15.270914058806735
demonstration
[(24, 1), (25, 1), (26, 1), (27, 1), (27, 1), (27, 3), (34, 2), (27, 1), (27, 1), (27, 1), (27, 3), (34, 2), (27, 1), (27, 3), (34, 1), (34, 1), (34, 1), (34, 2), (27, 1), (27, 3), (34, 1), (34, 1), (34, 1), (34, 1), (34, 1), (34, 2), (27, 3), (34, 2), (27, 1), (27, 3), (34, 1), (34, 1), (34, 1), (34, 2), (27, 1), (27, 3), (34, 2), (27, 3), (34, 1), (34, 2), (27, 3), (34, 2), (27, 1), (27, 3), (34, 1), (34, 1), (34, 2), (27, 1), (27, 1)]
[-0.18722695 -0.32216181 -0.31813688 -0.17247436]
w_map [ 0.36368425 -0.15241741  0.09227289 -0.39162544] loglik -31.884770294041573
accepted/total = 2700/3000 = 0.9
-------
true weights [-0.14918332 -0.9046088  -0.26868157 -0.29535989]
features
1 	0 	3 	3 	0 	2 	2 	
3 	1 	2 	2 	3 	3 	2 	
0 	1 	1 	1 	2 	2 	1 	
0 	3 	1 	2 	0 	3 	0 	
2 	3 	0 	3 	3 	2 	0 	
1 	3 	3 	2 	0 	1 	2 	
2 	3 	3 	2 	1 	2 	0 	
optimal policy
>	^	<	>	^	<	<	
v	^	^	^	^	^	^	
<	<	^	>	v	v	v	
<	<	<	>	>	>	v	
^	<	<	<	>	>	>	
^	^	^	>	^	>	v	
>	^	^	^	>	>	>	
optimal values
-15.67	-14.92	-15.06	-15.06	-14.92	-15.04	-15.16	
-15.06	-15.67	-15.18	-15.18	-15.06	-15.18	-15.27	
-14.92	-15.67	-15.94	-15.93	-15.18	-15.18	-15.67	
-14.92	-15.06	-15.82	-15.18	-15.06	-15.06	-14.92	
-15.04	-15.18	-15.18	-15.32	-15.18	-15.04	-14.92	
-15.79	-15.33	-15.32	-15.30	-15.18	-15.79	-15.04	
-15.58	-15.47	-15.47	-15.41	-15.79	-15.04	-14.92	
map_weights [ 0.36368425 -0.15241741  0.09227289 -0.39162544]
MAP reward
-0.15	0.36	-0.39	-0.39	0.36	0.09	0.09	
-0.39	-0.15	0.09	0.09	-0.39	-0.39	0.09	
0.36	-0.15	-0.15	-0.15	0.09	0.09	-0.15	
0.36	-0.39	-0.15	0.09	0.36	-0.39	0.36	
0.09	-0.39	0.36	-0.39	-0.39	0.09	0.36	
-0.15	-0.39	-0.39	0.09	0.36	-0.15	0.09	
0.09	-0.39	-0.39	0.09	-0.15	0.09	0.36	
Map policy
>	^	<	>	^	<	<	
v	^	<	^	^	^	v	
<	<	<	>	v	>	v	
^	<	<	>	>	>	>	
^	<	<	^	>	>	>	
^	<	^	>	>	>	^	
^	<	>	>	>	>	>	
expeced value MDP LP 28.07138339554319
mean w [ 0.28775475 -0.19831208 -0.17984436 -0.12384243]
Mean policy from posterior
>	^	<	>	^	<	<	
v	^	^	^	^	<	v	
<	<	<	>	v	v	v	
^	<	<	>	>	>	v	
^	^	<	<	^	>	>	
^	^	^	>	^	>	^	
^	^	^	>	>	>	>	
Mean rewards
-0.20	0.29	-0.12	-0.12	0.29	-0.18	-0.18	
-0.12	-0.20	-0.18	-0.18	-0.12	-0.12	-0.18	
0.29	-0.20	-0.20	-0.20	-0.18	-0.18	-0.20	
0.29	-0.12	-0.20	-0.18	0.29	-0.12	0.29	
-0.18	-0.12	0.29	-0.12	-0.12	-0.18	0.29	
-0.20	-0.12	-0.12	-0.18	0.29	-0.20	-0.18	
-0.18	-0.12	-0.12	-0.18	-0.20	-0.18	0.29	
mean = 0.04268868827475991, map = 0.12171253899224155
CVaR policy
>	^	<	>	^	<	<	
v	^	^	^	^	^	v	
<	<	<	v	v	v	v	
^	<	<	>	>	>	v	
^	<	<	<	>	>	>	
^	^	^	>	^	>	v	
^	<	^	>	>	>	>	
CVaR policy
>	^	<	>	^	<	<	
v	^	^	^	^	^	v	
<	<	<	v	v	v	v	
^	<	<	>	>	>	v	
^	<	<	<	>	>	>	
^	^	^	>	^	>	v	
^	^	^	>	>	>	>	
CVaR policy
>	^	<	>	^	<	<	
v	^	^	^	^	<	v	
<	<	<	v	v	v	v	
^	<	<	>	>	>	v	
^	^	<	<	^	>	>	
^	^	^	>	^	>	v	
^	^	^	>	>	>	>	
CVaR policy
>	^	<	>	^	<	<	
v	^	^	^	^	<	v	
<	<	<	v	v	v	v	
^	<	<	>	>	>	v	
^	^	<	<	^	>	>	
^	^	^	>	^	>	v	
^	^	^	>	>	>	>	
CVaR policy
>	^	<	>	^	<	<	
v	^	^	^	^	<	v	
<	<	<	v	v	v	v	
^	<	<	>	>	>	v	
^	^	<	<	^	>	>	
^	^	^	>	^	>	v	
^	^	^	>	>	>	>	
cvar = , 0.047441827803220704, 0.03694476957237747, 0.04270165483521815, 0.04268937568928166, 0.04269407802477865
==========
iteration 64
==========
weights [ 0.75598226  0.61795247 -0.02955392  0.2138975 ]
expeced value MDP LP 74.97350158703028
demonstration
[(24, 3), (31, 1), (32, 0), (31, 0), (30, 1), (31, 0), (30, 1), (31, 0), (30, 1), (31, 1), (32, 0), (31, 0), (30, 1), (31, 0), (30, 1), (31, 1), (32, 3), (39, 2), (32, 0), (31, 0), (30, 1), (31, 1), (32, 0), (31, 1), (32, 3), (39, 2), (32, 0), (31, 0), (30, 1), (31, 1), (32, 0), (31, 1), (32, 3), (39, 2), (32, 0), (31, 1), (32, 0), (31, 0), (30, 1), (31, 0), (30, 1), (31, 1), (32, 3), (39, 2), (32, 0), (31, 0), (30, 1), (31, 0), (30, 1)]
[-0.43497215 -0.1665579  -0.11477169 -0.28369826]
w_map [ 0.57056413 -0.17941078  0.03450604  0.21551906] loglik -22.87385646541952
accepted/total = 2827/3000 = 0.9423333333333334
-------
true weights [ 0.75598226  0.61795247 -0.02955392  0.2138975 ]
features
1 	3 	2 	1 	2 	3 	0 	
3 	0 	3 	1 	1 	2 	2 	
0 	2 	2 	3 	2 	2 	1 	
2 	3 	3 	2 	1 	0 	0 	
3 	1 	0 	0 	0 	2 	3 	
1 	3 	1 	3 	0 	1 	0 	
1 	3 	2 	0 	3 	3 	3 	
optimal policy
v	v	v	v	>	>	>	
v	<	<	>	v	^	^	
<	<	v	v	v	v	v	
^	v	v	v	>	>	>	
>	>	>	>	v	^	^	
>	>	^	v	^	<	>	
^	^	>	v	<	^	^	
optimal values
74.92	74.52	73.75	74.29	74.28	75.06	75.60	
75.06	75.06	74.52	74.42	74.55	74.28	74.81	
75.60	74.81	74.28	74.28	74.68	74.81	75.46	
74.81	74.92	75.06	74.81	75.46	75.60	75.60	
74.92	75.46	75.60	75.60	75.60	74.81	75.06	
74.79	74.92	75.46	75.06	75.60	75.46	75.60	
74.66	74.38	74.81	75.60	75.06	74.92	75.06	
map_weights [ 0.57056413 -0.17941078  0.03450604  0.21551906]
MAP reward
-0.18	0.22	0.03	-0.18	0.03	0.22	0.57	
0.22	0.57	0.22	-0.18	-0.18	0.03	0.03	
0.57	0.03	0.03	0.22	0.03	0.03	-0.18	
0.03	0.22	0.22	0.03	-0.18	0.57	0.57	
0.22	-0.18	0.57	0.57	0.57	0.03	0.22	
-0.18	0.22	-0.18	0.22	0.57	-0.18	0.57	
-0.18	0.22	0.03	0.57	0.22	0.22	0.22	
Map policy
v	v	v	>	>	>	>	
v	<	<	<	>	^	^	
<	<	v	v	>	v	v	
^	>	v	v	>	>	>	
^	>	>	>	v	^	^	
^	>	^	>	^	<	>	
>	>	>	v	^	<	^	
expeced value MDP LP 31.889342274260365
mean w [ 0.32487876 -0.16087465 -0.17050809 -0.10579128]
Mean policy from posterior
v	v	v	>	>	>	>	
v	<	<	<	>	^	^	
<	<	v	v	v	v	v	
^	>	v	v	>	>	>	
>	>	>	>	v	^	^	
>	>	^	>	^	<	>	
>	>	>	v	^	<	^	
Mean rewards
-0.16	-0.11	-0.17	-0.16	-0.17	-0.11	0.32	
-0.11	0.32	-0.11	-0.16	-0.16	-0.17	-0.17	
0.32	-0.17	-0.17	-0.11	-0.17	-0.17	-0.16	
-0.17	-0.11	-0.11	-0.17	-0.16	0.32	0.32	
-0.11	-0.16	0.32	0.32	0.32	-0.17	-0.11	
-0.16	-0.11	-0.16	-0.11	0.32	-0.16	0.32	
-0.16	-0.11	-0.17	0.32	-0.11	-0.11	-0.11	
mean = 0.04022200223904804, map = 0.07933822355727216
CVaR policy
v	v	<	>	>	>	>	
v	v	<	<	>	>	^	
<	<	<	v	>	v	v	
^	<	v	v	v	>	>	
^	>	>	>	<	<	^	
^	>	^	>	^	<	>	
>	>	>	v	^	^	^	
CVaR policy
v	v	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	v	v	v	v	v	
^	>	v	v	v	>	>	
>	>	>	>	v	^	^	
>	>	^	>	^	<	>	
>	>	>	v	^	>	^	
CVaR policy
v	v	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	v	v	v	v	v	
^	>	v	v	>	>	>	
>	>	>	>	v	^	^	
>	>	^	^	^	<	>	
>	>	>	v	^	<	^	
CVaR policy
v	v	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	v	v	v	v	v	
^	>	v	v	>	>	>	
>	>	>	>	<	^	^	
>	>	^	^	^	<	>	
>	>	>	v	^	<	^	
CVaR policy
v	v	<	>	>	>	>	
v	<	<	<	>	^	^	
<	<	v	v	v	v	v	
^	>	v	v	>	>	>	
>	>	>	>	v	^	^	
>	>	^	>	^	<	>	
>	>	>	v	^	<	^	
cvar = , 0.11509967366161789, 0.04035217636709376, 0.04022206915283277, 0.04022208933787397, 0.04022495249326141
==========
iteration 65
==========
weights [-0.6817857   0.18207286 -0.57525484 -0.41364187]
expeced value MDP LP 16.77307510034025
demonstration
[(24, 1), (25, 1), (26, 3), (33, 3), (40, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1), (41, 1)]
[-0.09344447 -0.03063495 -0.36619449 -0.50972609]
w_map [-0.36001928  0.50469575 -0.09510673 -0.04017824] loglik -0.693146448641528
accepted/total = 2403/3000 = 0.801
-------
true weights [-0.6817857   0.18207286 -0.57525484 -0.41364187]
features
0 	2 	1 	0 	3 	2 	1 	
3 	3 	0 	2 	1 	0 	2 	
3 	0 	0 	3 	2 	3 	0 	
0 	0 	0 	3 	3 	2 	2 	
0 	3 	0 	0 	0 	1 	3 	
0 	2 	0 	2 	2 	3 	1 	
2 	3 	1 	2 	0 	3 	2 	
optimal policy
>	>	^	<	>	>	>	
>	^	^	^	^	>	^	
^	^	^	^	^	v	^	
^	v	^	>	>	v	v	
>	v	v	>	>	>	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
optimal values
16.59	17.45	18.21	17.34	16.86	17.45	18.21	
16.28	16.86	17.34	16.59	16.88	16.59	17.45	
15.70	16.01	16.49	16.02	16.13	16.28	16.59	
14.86	15.43	15.64	15.71	16.28	16.87	16.86	
15.43	16.28	16.49	15.91	16.76	17.62	17.61	
16.01	16.86	17.34	16.70	16.86	17.61	18.21	
16.86	17.61	18.21	17.45	16.59	17.02	17.45	
map_weights [-0.36001928  0.50469575 -0.09510673 -0.04017824]
MAP reward
-0.36	-0.10	0.50	-0.36	-0.04	-0.10	0.50	
-0.04	-0.04	-0.36	-0.10	0.50	-0.36	-0.10	
-0.04	-0.36	-0.36	-0.04	-0.10	-0.04	-0.36	
-0.36	-0.36	-0.36	-0.04	-0.04	-0.10	-0.10	
-0.36	-0.04	-0.36	-0.36	-0.36	0.50	-0.04	
-0.36	-0.10	-0.36	-0.10	-0.10	-0.04	0.50	
-0.10	-0.04	0.50	-0.10	-0.36	-0.04	-0.10	
Map policy
>	>	^	<	>	>	>	
>	^	^	^	^	>	^	
^	^	^	^	^	v	^	
^	v	^	>	>	v	v	
>	v	v	v	>	>	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
expeced value MDP LP 28.975874133342458
mean w [-0.41268695  0.29770409 -0.03897237  0.01674516]
Mean policy from posterior
>	>	^	<	>	>	>	
>	^	^	>	^	>	^	
^	^	^	^	^	v	^	
^	v	>	>	>	v	v	
>	v	v	v	>	>	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
Mean rewards
-0.41	-0.04	0.30	-0.41	0.02	-0.04	0.30	
0.02	0.02	-0.41	-0.04	0.30	-0.41	-0.04	
0.02	-0.41	-0.41	0.02	-0.04	0.02	-0.41	
-0.41	-0.41	-0.41	0.02	0.02	-0.04	-0.04	
-0.41	0.02	-0.41	-0.41	-0.41	0.30	0.02	
-0.41	-0.04	-0.41	-0.04	-0.04	0.02	0.30	
-0.04	0.02	0.30	-0.04	-0.41	0.02	-0.04	
mean = 0.035799325877540866, map = 0.0012001040590270406
CVaR policy
>	>	^	<	>	>	>	
>	^	^	>	^	>	^	
^	^	^	>	^	v	^	
^	v	>	>	>	v	v	
>	v	<	v	>	>	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
CVaR policy
>	>	^	<	>	>	>	
>	^	^	>	^	>	^	
^	^	^	^	^	v	^	
^	v	>	>	>	v	v	
>	v	v	v	>	>	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
CVaR policy
>	>	^	<	>	>	>	
>	^	^	>	^	>	^	
^	^	^	^	^	v	^	
^	v	>	>	>	v	v	
>	v	v	v	>	>	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
CVaR policy
>	>	^	<	>	>	>	
>	^	^	>	^	>	^	
^	^	^	^	^	v	^	
^	v	>	>	>	v	v	
>	v	v	v	>	v	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
CVaR policy
>	>	^	<	>	>	>	
>	^	^	>	^	>	^	
^	^	^	^	^	v	^	
^	v	>	>	>	v	v	
>	v	v	v	>	v	v	
v	v	v	v	>	>	>	
>	>	v	<	<	^	^	
cvar = , 0.05732614949557302, 0.03579929949306404, 0.03579956680792762, 0.03579990325297899, 0.035802492822082144
==========
iteration 66
==========
weights [-0.31267016  0.31226303 -0.66067946 -0.60682108]
expeced value MDP LP 30.34136517706805
demonstration
[(24, 3), (31, 0), (30, 3), (37, 0), (36, 1), (37, 0), (36, 1), (37, 0), (36, 0), (35, 2), (28, 2), (21, 0), (21, 3), (28, 0), (28, 3), (35, 1), (36, 0), (35, 1), (36, 0), (35, 2), (28, 2), (21, 3), (28, 0), (28, 3), (35, 0), (35, 1), (36, 0), (35, 1), (36, 1), (37, 0), (36, 0), (35, 0), (35, 1), (36, 1), (37, 0), (36, 0), (35, 2), (28, 3), (35, 0), (35, 0), (35, 0), (35, 0), (35, 1), (36, 0), (35, 2), (28, 2), (21, 3), (28, 3), (35, 0)]
[-0.14106443 -0.08928455 -0.36628281 -0.40336821]
w_map [-0.00416508  0.90276445 -0.02129127 -0.0717792 ] loglik -39.304137868421094
accepted/total = 2536/3000 = 0.8453333333333334
-------
true weights [-0.31267016  0.31226303 -0.66067946 -0.60682108]
features
0 	0 	3 	1 	1 	3 	0 	
1 	0 	2 	1 	2 	3 	0 	
0 	2 	3 	0 	2 	0 	3 	
1 	3 	3 	2 	1 	0 	2 	
1 	3 	0 	1 	0 	3 	1 	
1 	1 	1 	3 	3 	1 	0 	
2 	2 	3 	2 	1 	2 	2 	
optimal policy
v	<	>	^	<	<	<	
<	<	>	^	^	^	^	
v	<	>	^	<	v	v	
<	<	v	v	v	v	v	
<	<	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
optimal values
30.60	29.98	30.31	31.23	31.23	30.31	29.69	
31.23	30.60	30.25	31.23	30.25	29.40	29.08	
30.60	29.63	29.69	30.60	29.63	29.08	29.34	
31.23	30.31	29.69	29.64	30.00	29.69	30.25	
31.23	30.31	30.60	30.61	29.99	30.31	31.23	
31.23	31.23	31.23	30.31	30.31	30.61	30.60	
30.25	30.25	30.31	30.25	31.23	30.25	29.63	
map_weights [-0.00416508  0.90276445 -0.02129127 -0.0717792 ]
MAP reward
-0.00	-0.00	-0.07	0.90	0.90	-0.07	-0.00	
0.90	-0.00	-0.02	0.90	-0.02	-0.07	-0.00	
-0.00	-0.02	-0.07	-0.00	-0.02	-0.00	-0.07	
0.90	-0.07	-0.07	-0.02	0.90	-0.00	-0.02	
0.90	-0.07	-0.00	0.90	-0.00	-0.07	0.90	
0.90	0.90	0.90	-0.07	-0.07	0.90	-0.00	
-0.02	-0.02	-0.07	-0.02	0.90	-0.02	-0.02	
Map policy
v	<	>	^	<	<	<	
<	<	>	^	^	<	^	
v	<	>	^	<	v	v	
<	<	v	v	v	>	v	
<	<	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
expeced value MDP LP 37.72930879294171
mean w [-0.00542142  0.38256753 -0.06278692 -0.27373547]
Mean policy from posterior
v	<	>	^	<	<	<	
<	<	>	^	<	<	^	
v	<	>	^	<	v	v	
<	<	v	v	v	>	v	
<	<	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
Mean rewards
-0.01	-0.01	-0.27	0.38	0.38	-0.27	-0.01	
0.38	-0.01	-0.06	0.38	-0.06	-0.27	-0.01	
-0.01	-0.06	-0.27	-0.01	-0.06	-0.01	-0.27	
0.38	-0.27	-0.27	-0.06	0.38	-0.01	-0.06	
0.38	-0.27	-0.01	0.38	-0.01	-0.27	0.38	
0.38	0.38	0.38	-0.27	-0.27	0.38	-0.01	
-0.06	-0.06	-0.27	-0.06	0.38	-0.06	-0.06	
mean = 0.003253621342878432, map = 0.0032535980188797
CVaR policy
v	<	>	^	<	<	<	
<	<	>	^	<	<	^	
v	<	>	^	<	v	v	
<	<	v	v	v	>	v	
<	v	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
CVaR policy
v	v	>	^	<	<	<	
<	<	>	^	^	<	^	
v	<	>	^	<	v	v	
v	<	v	v	v	>	v	
^	<	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
CVaR policy
v	<	>	^	^	<	<	
<	<	>	^	<	<	^	
v	<	>	^	<	v	v	
<	<	v	v	v	>	v	
<	<	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
CVaR policy
v	<	>	^	^	<	<	
<	<	>	^	<	<	^	
v	<	>	^	<	v	v	
v	<	v	v	v	>	v	
^	<	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
CVaR policy
v	<	>	^	<	<	<	
<	<	>	^	<	<	^	
v	<	>	^	<	v	v	
v	<	v	v	v	>	v	
^	<	v	<	<	>	>	
^	<	<	<	v	>	^	
^	^	^	>	v	<	^	
cvar = , 0.0033218344069112504, 0.0034148798017810122, 0.003253605035631324, 0.003256691552756763, 0.003254295413242403
==========
iteration 67
==========
weights [-0.27417221  0.12895001 -0.87124831 -0.38617079]
expeced value MDP LP 12.135931532335407
demonstration
[(24, 2), (17, 0), (16, 2), (9, 3), (16, 2), (9, 3), (16, 0), (15, 1), (16, 2), (9, 3), (16, 2), (9, 3), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 2), (9, 3), (16, 2), (9, 3), (16, 0), (15, 1), (16, 0), (15, 1), (16, 0), (15, 1), (16, 2), (9, 3), (16, 0), (15, 1), (16, 0), (15, 1), (16, 2), (9, 3), (16, 0), (15, 1), (16, 2), (9, 3), (16, 2), (9, 3), (16, 2), (9, 3), (16, 2)]
[-0.12501528 -0.64217812 -0.15214537 -0.08066123]
w_map [ 0.0126044   0.91864744  0.01619498 -0.05255318] loglik -17.328663758067705
accepted/total = 2805/3000 = 0.935
-------
true weights [-0.27417221  0.12895001 -0.87124831 -0.38617079]
features
3 	1 	0 	1 	0 	3 	0 	
0 	0 	1 	0 	2 	3 	2 	
3 	1 	1 	3 	3 	3 	2 	
1 	2 	3 	3 	3 	0 	1 	
0 	3 	3 	3 	3 	2 	0 	
3 	1 	3 	3 	0 	2 	3 	
1 	1 	1 	2 	3 	3 	2 	
optimal policy
>	^	<	^	<	<	<	
>	v	v	<	<	v	v	
>	>	^	<	<	v	v	
<	^	^	<	>	>	>	
^	v	v	<	^	>	^	
v	v	v	<	<	>	^	
<	<	<	<	<	<	^	
optimal values
12.38	12.90	12.49	12.90	12.49	11.98	11.59	
12.09	12.49	12.90	12.49	11.50	11.47	10.90	
12.38	12.90	12.90	12.38	11.87	11.98	11.89	
12.90	11.89	12.38	11.87	11.98	12.49	12.90	
12.49	12.38	11.87	11.37	11.47	11.50	12.49	
12.38	12.90	12.38	11.87	11.48	10.99	11.98	
12.90	12.90	12.90	11.89	11.39	10.89	10.99	
map_weights [ 0.0126044   0.91864744  0.01619498 -0.05255318]
MAP reward
-0.05	0.92	0.01	0.92	0.01	-0.05	0.01	
0.01	0.01	0.92	0.01	0.02	-0.05	0.02	
-0.05	0.92	0.92	-0.05	-0.05	-0.05	0.02	
0.92	0.02	-0.05	-0.05	-0.05	0.01	0.92	
0.01	-0.05	-0.05	-0.05	-0.05	0.02	0.01	
-0.05	0.92	-0.05	-0.05	0.01	0.02	-0.05	
0.92	0.92	0.92	0.02	-0.05	-0.05	0.02	
Map policy
>	^	<	^	<	<	v	
>	v	v	<	<	>	v	
v	>	^	<	<	>	v	
<	<	^	<	>	>	>	
^	v	v	v	>	^	^	
v	v	<	v	<	^	^	
<	<	<	<	<	<	^	
expeced value MDP LP 38.21709993492779
mean w [-0.11441223  0.38895778 -0.16027127 -0.09180726]
Mean policy from posterior
>	^	<	^	<	<	<	
v	v	v	<	<	^	v	
>	>	^	<	<	v	v	
<	^	^	<	>	>	>	
^	v	<	<	^	>	^	
v	v	<	<	<	>	^	
<	<	<	<	<	<	^	
Mean rewards
-0.09	0.39	-0.11	0.39	-0.11	-0.09	-0.11	
-0.11	-0.11	0.39	-0.11	-0.16	-0.09	-0.16	
-0.09	0.39	0.39	-0.09	-0.09	-0.09	-0.16	
0.39	-0.16	-0.09	-0.09	-0.09	-0.11	0.39	
-0.11	-0.09	-0.09	-0.09	-0.09	-0.16	-0.11	
-0.09	0.39	-0.09	-0.09	-0.11	-0.16	-0.09	
0.39	0.39	0.39	-0.16	-0.09	-0.09	-0.16	
mean = 0.002263470199309836, map = 0.1043527570571463
CVaR policy
>	^	>	^	<	<	v	
v	v	v	<	<	v	v	
>	>	^	<	<	>	v	
<	^	^	<	>	>	>	
^	v	<	<	^	>	^	
v	v	v	<	<	>	^	
<	<	<	<	<	<	^	
CVaR policy
>	^	v	^	<	<	<	
v	>	v	<	<	v	v	
>	>	<	<	<	v	v	
<	^	^	<	>	>	>	
^	v	<	<	^	^	^	
>	v	<	<	<	>	^	
<	<	<	<	<	<	^	
CVaR policy
>	^	v	^	<	<	<	
v	v	v	<	<	^	v	
>	>	^	<	<	v	v	
<	^	^	<	>	>	>	
^	v	<	<	^	>	^	
>	v	v	<	<	>	^	
<	<	<	<	<	<	^	
CVaR policy
>	^	v	^	<	<	<	
v	v	v	<	<	^	v	
>	>	^	<	<	v	v	
<	^	^	<	>	>	>	
^	v	<	<	^	>	^	
>	v	v	<	<	>	^	
<	<	<	<	<	<	^	
CVaR policy
>	^	v	^	<	<	<	
v	v	v	<	<	^	v	
>	>	^	<	<	v	v	
<	^	^	<	>	>	>	
^	v	<	<	^	^	^	
>	v	<	<	<	>	^	
<	<	<	<	<	<	^	
cvar = , 0.04801224610830346, 0.002306749922363238, 0.002265921374918989, 0.0022722660930813987, 0.0023028746702262737
==========
iteration 68
==========
weights [ 0.10464769  0.42897411 -0.64473749 -0.62397408]
expeced value MDP LP 42.127640770103696
demonstration
[(24, 0), (23, 0), (22, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 1), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 1), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 0), (21, 1), (22, 0), (21, 1), (22, 1), (23, 0), (22, 1), (23, 0), (22, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 1), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1)]
[-0.21821237 -0.09806942 -0.12172122 -0.56199698]
w_map [-0.09008218  0.52200733 -0.18166986  0.20624062] loglik -25.64644568071799
accepted/total = 2708/3000 = 0.9026666666666666
-------
true weights [ 0.10464769  0.42897411 -0.64473749 -0.62397408]
features
3 	0 	2 	1 	1 	3 	3 	
1 	3 	2 	1 	2 	2 	1 	
0 	3 	3 	0 	0 	0 	0 	
1 	1 	1 	0 	2 	2 	1 	
2 	2 	3 	1 	3 	2 	1 	
0 	1 	3 	3 	1 	3 	3 	
3 	1 	0 	3 	0 	0 	0 	
optimal policy
v	<	>	>	^	<	v	
<	<	>	^	^	>	>	
v	v	v	^	<	>	v	
<	<	<	<	<	>	>	
^	v	^	^	<	>	^	
>	v	<	^	^	>	^	
>	v	<	<	^	>	^	
optimal values
41.84	41.53	41.82	42.90	42.90	41.84	41.84	
42.90	41.84	41.82	42.90	41.82	41.82	42.90	
42.57	41.84	41.84	42.57	42.25	42.25	42.57	
42.90	42.90	42.90	42.57	41.50	41.82	42.90	
41.82	41.82	41.84	42.58	41.53	41.82	42.90	
42.57	42.90	41.84	41.53	41.54	40.80	41.84	
41.84	42.90	42.57	41.52	41.23	41.22	41.53	
map_weights [-0.09008218  0.52200733 -0.18166986  0.20624062]
MAP reward
0.21	-0.09	-0.18	0.52	0.52	0.21	0.21	
0.52	0.21	-0.18	0.52	-0.18	-0.18	0.52	
-0.09	0.21	0.21	-0.09	-0.09	-0.09	-0.09	
0.52	0.52	0.52	-0.09	-0.18	-0.18	0.52	
-0.18	-0.18	0.21	0.52	0.21	-0.18	0.52	
-0.09	0.52	0.21	0.21	0.52	0.21	0.21	
0.21	0.52	-0.09	0.21	-0.09	-0.09	-0.09	
Map policy
v	v	>	>	^	<	v	
<	<	>	^	^	>	>	
v	v	v	^	<	>	v	
<	<	<	<	<	>	>	
^	v	^	<	<	>	^	
>	v	<	^	<	>	^	
>	v	<	<	^	^	^	
expeced value MDP LP 37.9438730176566
mean w [-0.06526767  0.38404019 -0.10772703 -0.04053266]
Mean policy from posterior
v	<	>	>	^	<	v	
<	<	>	^	^	>	>	
v	v	v	^	<	>	v	
<	<	<	<	<	>	>	
^	^	^	<	<	>	^	
>	v	<	^	<	>	^	
>	v	<	<	^	^	^	
Mean rewards
-0.04	-0.07	-0.11	0.38	0.38	-0.04	-0.04	
0.38	-0.04	-0.11	0.38	-0.11	-0.11	0.38	
-0.07	-0.04	-0.04	-0.07	-0.07	-0.07	-0.07	
0.38	0.38	0.38	-0.07	-0.11	-0.11	0.38	
-0.11	-0.11	-0.04	0.38	-0.04	-0.11	0.38	
-0.07	0.38	-0.04	-0.04	0.38	-0.04	-0.04	
-0.04	0.38	-0.07	-0.04	-0.07	-0.07	-0.07	
mean = 0.08730225960860594, map = 0.08730219439986087
CVaR policy
v	>	>	>	^	<	v	
<	<	>	^	^	>	>	
v	v	v	^	^	^	v	
<	<	<	<	>	>	>	
^	^	^	<	>	>	^	
>	v	<	^	^	^	^	
>	v	<	<	^	^	^	
CVaR policy
v	<	>	>	^	<	v	
<	<	>	^	<	>	>	
v	v	v	^	<	>	v	
<	<	<	<	<	>	>	
^	v	^	<	<	>	^	
>	v	<	^	<	>	^	
>	v	<	<	^	^	^	
CVaR policy
v	<	>	>	^	<	v	
<	<	>	^	^	>	>	
v	v	v	^	<	>	v	
<	<	<	<	<	>	>	
^	v	^	<	<	>	^	
>	v	<	^	^	>	^	
>	v	<	<	^	^	^	
CVaR policy
v	<	>	>	^	<	v	
<	<	>	^	^	>	>	
v	v	v	^	<	>	v	
<	<	<	<	<	>	>	
^	v	^	<	<	>	^	
>	v	<	^	^	>	^	
>	v	<	<	^	^	^	
CVaR policy
v	<	>	>	^	<	v	
<	<	>	^	^	>	>	
v	v	v	^	<	>	v	
<	<	<	<	<	>	>	
^	v	^	<	<	>	^	
>	v	<	^	^	>	^	
>	v	<	<	^	^	^	
cvar = , 0.13585727591853214, 0.08730224677953657, 0.08732317206465012, 0.08730244555810884, 0.08730221169808061
==========
iteration 69
==========
weights [-0.57513739 -0.37883086  0.65270582 -0.31572024]
expeced value MDP LP 63.997206903246536
demonstration
[(24, 1), (25, 1), (26, 1), (27, 2), (20, 3), (27, 2), (20, 1), (20, 3), (27, 1), (27, 1), (27, 2), (20, 1), (20, 1), (20, 1), (20, 1), (20, 1), (20, 1), (20, 1), (20, 3), (27, 2), (20, 1), (20, 3), (27, 1), (27, 1), (27, 1), (27, 1), (27, 2), (20, 1), (20, 1), (20, 3), (27, 1), (27, 2), (20, 3), (27, 2), (20, 3), (27, 1), (27, 2), (20, 3), (27, 2), (20, 1), (20, 1), (20, 1), (20, 1), (20, 1), (20, 3), (27, 2), (20, 3), (27, 1), (27, 1)]
[-0.32893692 -0.26053988 -0.06338211 -0.3471411 ]
w_map [-0.27227483  0.16252714  0.50829111  0.05690692] loglik -31.884770296562237
accepted/total = 2501/3000 = 0.8336666666666667
-------
true weights [-0.57513739 -0.37883086  0.65270582 -0.31572024]
features
3 	2 	2 	2 	2 	2 	3 	
3 	1 	0 	0 	0 	1 	1 	
0 	0 	2 	0 	0 	1 	2 	
1 	0 	0 	0 	3 	3 	2 	
2 	1 	3 	3 	3 	1 	3 	
0 	3 	1 	1 	0 	3 	2 	
3 	1 	2 	1 	0 	1 	2 	
optimal policy
>	>	>	>	^	<	<	
^	^	^	^	^	^	v	
v	^	^	<	>	>	>	
v	<	^	>	>	>	^	
<	<	<	<	^	>	v	
^	v	v	v	>	>	v	
>	>	v	<	>	>	>	
optimal values
64.30	65.27	65.27	65.27	65.27	65.27	64.30	
63.34	64.24	64.04	64.04	64.04	64.24	64.24	
63.02	63.02	64.06	62.84	63.02	64.24	65.27	
64.24	63.02	62.84	62.13	63.34	64.30	65.27	
65.27	64.24	63.28	62.33	62.39	63.28	64.30	
64.04	63.28	64.24	63.22	63.08	64.30	65.27	
63.28	64.24	65.27	64.24	63.02	64.24	65.27	
map_weights [-0.27227483  0.16252714  0.50829111  0.05690692]
MAP reward
0.06	0.51	0.51	0.51	0.51	0.51	0.06	
0.06	0.16	-0.27	-0.27	-0.27	0.16	0.16	
-0.27	-0.27	0.51	-0.27	-0.27	0.16	0.51	
0.16	-0.27	-0.27	-0.27	0.06	0.06	0.51	
0.51	0.16	0.06	0.06	0.06	0.16	0.06	
-0.27	0.06	0.16	0.16	-0.27	0.06	0.51	
0.06	0.16	0.51	0.16	-0.27	0.16	0.51	
Map policy
>	>	>	>	^	<	<	
>	^	^	^	^	^	v	
v	^	^	<	>	>	>	
v	v	^	>	>	>	^	
<	<	<	v	>	>	v	
^	>	v	v	>	>	v	
>	>	v	<	>	>	>	
expeced value MDP LP 34.045945502018384
mean w [-0.22289894 -0.16245784  0.34575006  0.07149873]
Mean policy from posterior
>	>	>	>	^	<	<	
^	^	^	^	^	^	v	
v	^	^	<	>	>	>	
v	v	^	>	>	>	^	
<	<	v	<	^	>	v	
^	>	v	v	>	>	v	
>	>	v	<	>	>	>	
Mean rewards
0.07	0.35	0.35	0.35	0.35	0.35	0.07	
0.07	-0.16	-0.22	-0.22	-0.22	-0.16	-0.16	
-0.22	-0.22	0.35	-0.22	-0.22	-0.16	0.35	
-0.16	-0.22	-0.22	-0.22	0.07	0.07	0.35	
0.35	-0.16	0.07	0.07	0.07	-0.16	0.07	
-0.22	0.07	-0.16	-0.16	-0.22	0.07	0.35	
0.07	-0.16	0.35	-0.16	-0.22	-0.16	0.35	
mean = 1.5074519410518405e-09, map = 0.003825789364320542
CVaR policy
>	>	>	>	^	<	<	
^	^	^	^	^	^	v	
^	>	^	<	v	>	>	
v	<	^	>	>	>	^	
<	<	v	>	^	>	v	
^	<	v	<	>	>	v	
^	>	v	<	>	>	>	
CVaR policy
>	>	>	>	^	^	<	
^	^	^	^	^	^	v	
^	^	^	<	v	>	>	
v	<	^	>	>	>	>	
<	<	<	>	^	>	v	
^	^	v	v	>	>	v	
>	>	v	<	>	>	>	
CVaR policy
>	>	>	>	^	^	<	
^	^	^	^	^	^	v	
v	^	^	<	>	>	>	
v	v	^	>	>	>	^	
<	<	<	<	^	>	v	
^	^	v	v	>	>	v	
>	>	v	<	>	>	>	
CVaR policy
>	>	>	>	^	<	<	
^	^	^	^	^	^	v	
v	^	^	<	>	>	>	
v	v	^	>	>	>	^	
<	<	<	<	^	>	v	
^	v	v	v	>	>	v	
>	>	v	<	>	>	>	
CVaR policy
>	>	>	>	^	<	<	
^	^	^	^	^	^	v	
v	^	^	<	>	>	>	
v	v	^	>	>	>	^	
<	<	<	<	^	>	v	
^	^	v	v	>	>	v	
>	>	v	<	>	>	>	
cvar = , 0.06575579495060424, 0.054105259443545606, 9.622553420740587e-09, 3.544031569191475e-05, 3.586817605594206e-06
==========
iteration 70
==========
weights [ 0.60337786  0.70873724 -0.24972248 -0.26695575]
expeced value MDP LP 70.35282181792985
demonstration
[(24, 2), (17, 2), (10, 3), (17, 3), (24, 2), (17, 2), (10, 3), (17, 3), (24, 2), (17, 2), (10, 3), (17, 2), (10, 3), (17, 3), (24, 2), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 3), (24, 2), (17, 2), (10, 3), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 2), (10, 3), (17, 3), (24, 2), (17, 3), (24, 2), (17, 2), (10, 3), (17, 2), (10, 3), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2)]
[-0.36153464 -0.39921585 -0.01481481 -0.2244347 ]
w_map [ 0.00539643  0.82167115 -0.06304955  0.10988287] loglik -16.635532333420997
accepted/total = 2755/3000 = 0.9183333333333333
-------
true weights [ 0.60337786  0.70873724 -0.24972248 -0.26695575]
features
2 	2 	2 	2 	1 	2 	2 	
1 	1 	2 	1 	2 	1 	0 	
2 	1 	0 	1 	3 	1 	0 	
1 	1 	2 	1 	0 	3 	2 	
3 	1 	0 	3 	1 	1 	0 	
2 	1 	0 	3 	1 	3 	1 	
2 	0 	3 	0 	2 	0 	2 	
optimal policy
v	v	v	v	^	<	v	
<	<	<	v	<	v	<	
v	^	<	v	<	^	<	
<	<	<	^	<	v	v	
^	^	<	^	>	<	<	
>	^	<	>	^	^	>	
>	^	^	>	^	<	^	
optimal values
69.92	69.92	68.97	69.92	70.87	69.92	69.81	
70.87	70.87	69.92	70.87	69.92	70.87	70.77	
69.92	70.87	70.77	70.87	69.90	70.87	70.77	
70.87	70.87	69.92	70.87	70.77	69.90	69.81	
69.90	70.87	70.77	69.90	70.87	70.87	70.77	
69.92	70.87	70.77	69.90	70.87	69.90	70.87	
69.81	70.77	69.79	69.82	69.92	69.82	69.92	
map_weights [ 0.00539643  0.82167115 -0.06304955  0.10988287]
MAP reward
-0.06	-0.06	-0.06	-0.06	0.82	-0.06	-0.06	
0.82	0.82	-0.06	0.82	-0.06	0.82	0.01	
-0.06	0.82	0.01	0.82	0.11	0.82	0.01	
0.82	0.82	-0.06	0.82	0.01	0.11	-0.06	
0.11	0.82	0.01	0.11	0.82	0.82	0.01	
-0.06	0.82	0.01	0.11	0.82	0.11	0.82	
-0.06	0.01	0.11	0.01	-0.06	0.01	-0.06	
Map policy
v	v	v	v	^	<	v	
<	<	<	v	<	v	<	
^	^	<	v	<	^	<	
<	<	<	^	<	v	<	
^	^	<	^	>	<	<	
>	^	<	>	^	^	>	
>	^	^	^	^	^	^	
expeced value MDP LP 31.339055464247117
mean w [-0.14198553  0.31699219 -0.1266494  -0.15426387]
Mean policy from posterior
v	v	v	v	^	<	<	
<	<	<	v	<	v	<	
^	^	<	v	<	^	<	
<	<	<	^	<	v	v	
^	^	<	^	>	<	<	
>	^	<	>	^	^	>	
^	^	^	>	^	<	^	
Mean rewards
-0.13	-0.13	-0.13	-0.13	0.32	-0.13	-0.13	
0.32	0.32	-0.13	0.32	-0.13	0.32	-0.14	
-0.13	0.32	-0.14	0.32	-0.15	0.32	-0.14	
0.32	0.32	-0.13	0.32	-0.14	-0.15	-0.13	
-0.15	0.32	-0.14	-0.15	0.32	0.32	-0.14	
-0.13	0.32	-0.14	-0.15	0.32	-0.15	0.32	
-0.13	-0.14	-0.15	-0.14	-0.13	-0.14	-0.13	
mean = 0.03447221542138834, map = 0.01828065497817022
CVaR policy
v	v	v	v	^	<	<	
<	<	<	v	<	v	<	
v	^	<	v	<	^	<	
<	^	<	^	<	v	<	
>	^	<	^	>	<	<	
>	^	<	>	^	<	>	
^	^	^	>	^	<	^	
CVaR policy
v	v	v	v	^	<	<	
<	<	<	v	<	v	<	
^	^	<	v	<	^	<	
<	^	<	^	<	v	<	
^	^	<	>	>	<	<	
>	^	<	>	^	>	>	
^	^	<	>	^	>	^	
CVaR policy
v	v	v	v	^	<	<	
<	<	<	v	<	v	<	
v	^	<	v	<	^	<	
<	<	<	^	<	v	<	
^	^	<	^	>	<	<	
>	^	<	>	^	<	>	
^	^	<	>	^	<	^	
CVaR policy
v	v	v	v	^	<	<	
<	<	<	v	<	v	<	
>	^	<	v	<	^	<	
<	<	<	^	<	v	v	
^	^	<	^	>	<	<	
>	^	<	>	^	^	>	
^	^	^	>	^	<	^	
CVaR policy
v	v	v	v	^	<	<	
<	<	<	v	<	v	<	
>	^	<	v	<	^	<	
<	<	<	^	<	v	v	
^	^	<	^	>	<	<	
>	^	<	>	^	^	>	
^	^	^	>	^	<	^	
cvar = , 0.052056505240926754, 0.0520565529274819, 0.052070281077618574, 0.03447301007561521, 0.034473576063248856
==========
iteration 71
==========
weights [-0.51213339  0.39402856 -0.64219057 -0.41237381]
expeced value MDP LP 38.51389291377994
demonstration
[(24, 0), (23, 3), (30, 0), (29, 3), (36, 0), (35, 0), (35, 1), (36, 2), (29, 3), (36, 3), (43, 3), (43, 3), (43, 2), (36, 3), (43, 3), (43, 2), (36, 3), (43, 2), (36, 3), (43, 2), (36, 0), (35, 1), (36, 0), (35, 0), (35, 1), (36, 0), (35, 1), (36, 3), (43, 3), (43, 3), (43, 3), (43, 3), (43, 2), (36, 0), (35, 1), (36, 2), (29, 3), (36, 2), (29, 3), (36, 0), (35, 1), (36, 0), (35, 0), (35, 1), (36, 0), (35, 1), (36, 0), (35, 1), (36, 2)]
[-0.1298999  -0.14943287 -0.31013751 -0.41052972]
w_map [-0.20893838  0.56859798 -0.162768    0.05969564] loglik -36.41055300589778
accepted/total = 2494/3000 = 0.8313333333333334
-------
true weights [-0.51213339  0.39402856 -0.64219057 -0.41237381]
features
3 	1 	2 	1 	1 	1 	3 	
3 	2 	0 	2 	1 	1 	3 	
0 	2 	3 	3 	0 	3 	1 	
3 	2 	1 	3 	1 	0 	3 	
3 	1 	3 	3 	2 	0 	1 	
1 	1 	2 	2 	2 	0 	2 	
2 	1 	0 	1 	1 	3 	2 	
optimal policy
>	^	>	>	>	^	<	
^	^	>	>	^	^	<	
v	v	v	>	^	^	>	
v	v	v	<	^	^	^	
v	v	<	<	^	>	>	
<	v	<	v	v	v	^	
>	v	>	v	<	<	<	
optimal values
38.60	39.40	38.37	39.40	39.40	39.40	38.60	
37.80	38.37	37.47	38.37	39.40	39.40	38.60	
36.91	37.34	37.81	37.70	38.50	38.60	39.40	
37.80	38.37	38.60	37.81	38.51	37.70	38.60	
38.60	39.40	38.60	37.80	37.48	38.50	39.40	
39.40	39.40	38.37	38.37	38.37	37.70	38.37	
38.37	39.40	38.50	39.40	39.40	38.60	37.57	
map_weights [-0.20893838  0.56859798 -0.162768    0.05969564]
MAP reward
0.06	0.57	-0.16	0.57	0.57	0.57	0.06	
0.06	-0.16	-0.21	-0.16	0.57	0.57	0.06	
-0.21	-0.16	0.06	0.06	-0.21	0.06	0.57	
0.06	-0.16	0.57	0.06	0.57	-0.21	0.06	
0.06	0.57	0.06	0.06	-0.16	-0.21	0.57	
0.57	0.57	-0.16	-0.16	-0.16	-0.21	-0.16	
-0.16	0.57	-0.21	0.57	0.57	0.06	-0.16	
Map policy
>	^	>	>	>	<	<	
^	^	^	>	^	^	<	
v	v	v	^	^	^	>	
v	v	v	<	^	^	^	
v	v	<	<	v	>	>	
<	v	<	v	v	v	^	
>	v	>	v	<	<	<	
expeced value MDP LP 34.778774030777285
mean w [-0.20144855  0.35172621 -0.1762724   0.08932164]
Mean policy from posterior
>	^	>	>	^	<	<	
^	^	v	>	^	^	<	
v	>	v	v	^	^	>	
v	v	v	<	<	^	^	
v	v	<	<	^	>	>	
<	v	<	v	v	v	^	
^	v	>	v	<	<	<	
Mean rewards
0.09	0.35	-0.18	0.35	0.35	0.35	0.09	
0.09	-0.18	-0.20	-0.18	0.35	0.35	0.09	
-0.20	-0.18	0.09	0.09	-0.20	0.09	0.35	
0.09	-0.18	0.35	0.09	0.35	-0.20	0.09	
0.09	0.35	0.09	0.09	-0.18	-0.20	0.35	
0.35	0.35	-0.18	-0.18	-0.18	-0.20	-0.18	
-0.18	0.35	-0.20	0.35	0.35	0.09	-0.18	
mean = 0.06436943783974414, map = 0.005439390842468583
CVaR policy
>	^	>	>	^	<	<	
^	^	v	^	^	^	<	
v	>	v	<	^	^	>	
v	v	v	<	<	^	^	
v	v	<	<	^	>	>	
<	v	<	v	v	v	^	
^	v	>	v	<	<	<	
CVaR policy
>	^	>	>	<	<	<	
^	^	v	^	^	^	<	
v	>	v	v	^	^	>	
v	v	v	<	<	^	v	
>	v	<	<	^	>	>	
<	v	<	v	v	v	^	
>	v	>	v	<	<	<	
CVaR policy
>	^	>	>	<	<	<	
^	^	v	^	^	^	<	
v	>	v	v	^	^	>	
v	v	v	<	<	^	^	
>	v	<	<	^	>	>	
<	v	<	v	v	v	^	
>	v	>	v	<	<	<	
CVaR policy
>	^	>	>	<	<	<	
^	^	v	^	^	^	<	
v	>	v	v	^	^	>	
v	v	v	<	<	^	^	
>	v	<	<	^	>	>	
<	v	<	v	v	v	^	
>	v	>	v	<	<	<	
CVaR policy
>	^	>	>	<	<	<	
^	^	v	^	^	^	<	
v	>	v	v	^	^	>	
v	v	v	<	<	^	^	
>	v	<	<	^	>	>	
<	v	<	v	v	v	^	
>	v	>	v	<	<	<	
cvar = , 0.06437554658413092, 0.06436936120880432, 0.0643693311051976, 0.06437195479425384, 0.06436932133193807
==========
iteration 72
==========
weights [ 0.20924822 -0.80072576  0.55218653  0.10071479]
expeced value MDP LP 54.80757967308983
demonstration
[(24, 1), (25, 3), (32, 0), (31, 1), (32, 2), (25, 3), (32, 2), (25, 3), (32, 0), (31, 1), (32, 2), (25, 3), (32, 0), (31, 1), (32, 3), (39, 1), (40, 1), (41, 0), (40, 0), (39, 1), (40, 0), (39, 2), (32, 0), (31, 1), (32, 2), (25, 3), (32, 2), (25, 3), (32, 2), (25, 3), (32, 0), (31, 1), (32, 2), (25, 3), (32, 0), (31, 1), (32, 3), (39, 2), (32, 0), (31, 1), (32, 2), (25, 3), (32, 3), (39, 1), (40, 0), (39, 2), (32, 3), (39, 1), (40, 1)]
[-0.27323187 -0.06346529 -0.29497889 -0.36832395]
w_map [0.17553556 0.14302488 0.50675517 0.1746844 ] loglik -30.57769401252972
accepted/total = 2731/3000 = 0.9103333333333333
-------
true weights [ 0.20924822 -0.80072576  0.55218653  0.10071479]
features
0 	0 	2 	3 	0 	0 	2 	
2 	1 	0 	2 	0 	1 	2 	
2 	2 	3 	0 	0 	2 	1 	
2 	0 	3 	0 	2 	0 	2 	
2 	1 	1 	2 	2 	0 	3 	
2 	3 	1 	0 	2 	2 	2 	
3 	2 	0 	3 	3 	3 	3 	
optimal policy
v	>	^	<	>	>	>	
v	<	^	<	<	>	^	
v	<	<	^	v	v	^	
<	<	<	v	v	<	>	
^	<	>	>	v	<	v	
^	<	>	^	>	>	>	
^	v	<	^	^	^	^	
optimal values
54.88	54.88	55.22	54.77	54.54	54.88	55.22	
55.22	53.87	54.88	54.88	54.54	53.87	55.22	
55.22	55.22	54.77	54.54	54.88	54.88	53.87	
55.22	54.88	54.43	54.88	55.22	54.88	55.22	
55.22	53.87	53.87	55.22	55.22	54.88	54.77	
55.22	54.77	53.53	54.88	55.22	55.22	55.22	
54.77	55.22	54.88	54.43	54.77	54.77	54.77	
map_weights [0.17553556 0.14302488 0.50675517 0.1746844 ]
MAP reward
0.18	0.18	0.51	0.17	0.18	0.18	0.51	
0.51	0.14	0.18	0.51	0.18	0.14	0.51	
0.51	0.51	0.17	0.18	0.18	0.51	0.14	
0.51	0.18	0.17	0.18	0.51	0.18	0.51	
0.51	0.14	0.14	0.51	0.51	0.18	0.17	
0.51	0.17	0.14	0.18	0.51	0.51	0.51	
0.17	0.51	0.18	0.17	0.17	0.17	0.17	
Map policy
v	>	^	<	>	>	>	
v	<	^	<	<	>	^	
v	<	<	^	v	v	^	
<	<	<	>	v	<	>	
^	<	>	>	v	v	v	
^	<	>	>	>	>	>	
^	v	<	^	^	^	^	
expeced value MDP LP 28.995189418865227
mean w [-0.15403465 -0.13999888  0.2933651  -0.14711965]
Mean policy from posterior
v	>	^	<	<	>	>	
v	<	^	^	>	>	^	
v	<	<	^	v	>	^	
<	<	v	>	v	<	>	
^	<	>	>	v	v	v	
^	<	^	>	>	>	>	
^	v	<	>	^	^	^	
Mean rewards
-0.15	-0.15	0.29	-0.15	-0.15	-0.15	0.29	
0.29	-0.14	-0.15	0.29	-0.15	-0.14	0.29	
0.29	0.29	-0.15	-0.15	-0.15	0.29	-0.14	
0.29	-0.15	-0.15	-0.15	0.29	-0.15	0.29	
0.29	-0.14	-0.14	0.29	0.29	-0.15	-0.15	
0.29	-0.15	-0.14	-0.15	0.29	0.29	0.29	
-0.15	0.29	-0.15	-0.15	-0.15	-0.15	-0.15	
mean = 0.09044101781495328, map = -4.267519670975162e-11
CVaR policy
v	>	^	<	>	>	>	
v	v	^	<	<	>	^	
v	<	<	^	v	v	^	
<	<	<	v	v	<	>	
^	<	>	>	v	v	v	
^	<	>	>	>	>	>	
^	v	<	^	^	^	^	
CVaR policy
v	>	^	<	>	>	>	
v	<	^	<	<	>	^	
v	<	<	^	v	v	^	
<	^	<	>	v	<	>	
^	<	>	>	v	v	v	
^	<	>	>	>	>	>	
^	v	<	^	^	^	^	
CVaR policy
v	>	^	<	>	>	>	
v	v	^	<	<	>	^	
v	<	<	^	v	>	^	
<	<	v	v	v	<	>	
^	<	>	>	v	<	v	
^	<	^	^	>	>	>	
^	v	<	^	^	^	^	
CVaR policy
v	>	^	<	<	>	>	
v	<	^	^	>	>	^	
v	<	<	^	v	>	^	
<	<	v	>	v	<	>	
^	<	>	>	v	<	v	
^	<	^	^	>	>	>	
^	v	<	>	^	^	^	
CVaR policy
v	>	^	<	<	>	>	
v	<	^	^	>	>	^	
v	<	<	^	v	>	^	
<	<	v	>	v	<	>	
^	<	>	>	v	<	v	
^	<	^	^	>	>	>	
^	v	<	>	^	^	^	
cvar = , 4.2415697691922105e-06, 7.489162499041413e-07, 0.060825187575119344, 0.09043962622688184, 0.09037419891743781
==========
iteration 73
==========
weights [ 0.32849609  0.35566921 -0.43101621 -0.76145568]
expeced value MDP LP 34.65414559478792
demonstration
[(24, 0), (23, 0), (22, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 0), (21, 1), (22, 0), (21, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 0), (21, 1), (22, 0), (21, 1), (22, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 1), (22, 0), (21, 1), (22, 0)]
[-0.12296202 -0.31533903 -0.23268298 -0.32901597]
w_map [-0.06981579  0.82886766 -0.06975463 -0.03156192] loglik -22.180709276002744
accepted/total = 2702/3000 = 0.9006666666666666
-------
true weights [ 0.32849609  0.35566921 -0.43101621 -0.76145568]
features
3 	3 	1 	3 	0 	0 	1 	
1 	3 	2 	3 	3 	3 	2 	
0 	0 	2 	2 	2 	2 	2 	
1 	1 	3 	0 	3 	0 	0 	
3 	3 	3 	2 	3 	1 	2 	
1 	2 	0 	0 	0 	3 	1 	
1 	3 	0 	3 	0 	2 	3 	
optimal policy
v	>	^	<	>	>	>	
<	<	^	<	^	^	^	
v	v	<	<	^	v	^	
<	<	<	<	>	v	v	
v	^	v	v	>	>	v	
v	<	<	<	<	>	>	
<	<	^	<	^	<	^	
optimal values
34.45	34.45	35.57	34.45	35.51	35.54	35.57	
35.57	34.45	34.78	33.67	34.40	34.42	34.78	
35.54	35.54	34.75	33.97	33.62	33.99	34.00	
35.57	35.57	34.45	34.43	33.66	34.77	34.76	
34.45	34.45	33.65	33.96	33.68	34.79	34.78	
35.57	34.78	34.76	34.74	34.72	34.45	35.57	
35.57	34.45	34.74	33.63	34.70	33.93	34.45	
map_weights [-0.06981579  0.82886766 -0.06975463 -0.03156192]
MAP reward
-0.03	-0.03	0.83	-0.03	-0.07	-0.07	0.83	
0.83	-0.03	-0.07	-0.03	-0.03	-0.03	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	-0.07	-0.07	
0.83	0.83	-0.03	-0.07	-0.03	-0.07	-0.07	
-0.03	-0.03	-0.03	-0.07	-0.03	0.83	-0.07	
0.83	-0.07	-0.07	-0.07	-0.07	-0.03	0.83	
0.83	-0.03	-0.07	-0.03	-0.07	-0.07	-0.03	
Map policy
v	>	^	<	<	>	>	
<	<	^	^	<	>	^	
v	v	v	^	v	v	^	
<	<	<	<	v	v	v	
v	^	<	>	>	v	v	
v	<	<	>	>	>	>	
<	<	<	<	>	>	^	
expeced value MDP LP 33.639248499593094
mean w [-0.14122878  0.34370268 -0.19132655 -0.1290467 ]
Mean policy from posterior
v	>	^	<	<	>	>	
<	<	^	^	<	^	^	
v	v	v	^	v	v	^	
<	<	<	<	v	v	v	
v	^	<	>	>	v	v	
v	<	<	>	>	>	>	
<	<	<	<	^	>	^	
Mean rewards
-0.13	-0.13	0.34	-0.13	-0.14	-0.14	0.34	
0.34	-0.13	-0.19	-0.13	-0.13	-0.13	-0.19	
-0.14	-0.14	-0.19	-0.19	-0.19	-0.19	-0.19	
0.34	0.34	-0.13	-0.14	-0.13	-0.14	-0.14	
-0.13	-0.13	-0.13	-0.19	-0.13	0.34	-0.19	
0.34	-0.19	-0.14	-0.14	-0.14	-0.13	0.34	
0.34	-0.13	-0.14	-0.13	-0.14	-0.19	-0.13	
mean = 0.29149266815454666, map = 0.32196189492824345
CVaR policy
v	>	^	<	<	>	>	
<	<	^	^	<	^	^	
v	v	v	^	v	v	^	
<	<	<	<	v	v	v	
v	^	<	>	>	v	v	
v	<	<	>	>	>	>	
<	<	<	<	^	>	^	
CVaR policy
v	>	^	<	<	>	>	
<	<	^	^	<	^	^	
v	v	v	^	v	v	^	
<	<	<	<	v	v	v	
v	^	^	>	>	v	v	
v	<	<	>	>	>	>	
<	<	<	<	^	>	^	
CVaR policy
v	>	^	<	<	>	>	
<	<	^	^	<	^	^	
v	v	v	^	v	v	^	
<	<	<	<	v	v	v	
v	^	<	>	>	v	v	
v	<	<	>	>	>	>	
<	<	<	<	^	>	^	
CVaR policy
v	>	^	<	<	>	>	
<	<	^	^	<	^	^	
v	v	v	^	v	v	^	
<	<	<	<	v	v	v	
v	^	<	>	>	v	v	
v	<	<	>	>	>	>	
<	<	<	<	^	>	^	
CVaR policy
v	>	^	<	<	>	>	
<	<	^	^	<	^	^	
v	v	v	^	v	v	^	
<	<	<	<	v	v	v	
v	^	<	>	>	v	v	
v	<	<	>	>	>	>	
<	<	<	<	^	>	^	
cvar = , 0.2914926704320635, 0.29149048556682544, 0.29148103565981387, 0.29147367138450164, 0.2913640900869652
==========
iteration 74
==========
weights [-0.85781404 -0.0731534  -0.37778366 -0.34070979]
expeced value MDP LP -7.978158561428875
demonstration
[(24, 2), (17, 2), (10, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2), (3, 2)]
[-0.42757709 -0.2467502  -0.11601406 -0.20965865]
w_map [-0.40681041  0.07453559 -0.24382001 -0.27483398] loglik 0.0
accepted/total = 2684/3000 = 0.8946666666666667
-------
true weights [-0.85781404 -0.0731534  -0.37778366 -0.34070979]
features
0 	1 	0 	1 	0 	1 	0 	
1 	3 	1 	2 	2 	2 	0 	
3 	0 	3 	1 	0 	0 	2 	
2 	2 	2 	3 	3 	2 	0 	
0 	3 	1 	3 	0 	1 	3 	
1 	0 	2 	2 	0 	0 	3 	
1 	1 	2 	1 	1 	0 	2 	
optimal policy
>	^	<	^	>	^	<	
<	^	<	^	>	^	<	
^	^	^	^	<	^	^	
^	<	^	^	<	<	<	
v	>	>	v	<	^	<	
v	v	v	v	v	<	v	
<	<	<	v	v	<	<	
optimal values
-8.10	-7.32	-8.10	-7.32	-8.10	-7.32	-8.10	
-7.32	-7.58	-7.58	-7.62	-7.92	-7.62	-8.40	
-7.58	-8.36	-7.85	-7.62	-8.40	-8.40	-8.70	
-7.88	-8.18	-8.14	-7.88	-8.14	-8.44	-9.21	
-8.10	-8.14	-7.88	-7.88	-8.66	-8.43	-8.68	
-7.32	-8.10	-7.92	-7.62	-8.10	-8.88	-8.65	
-7.32	-7.32	-7.62	-7.32	-7.32	-8.10	-8.40	
map_weights [-0.40681041  0.07453559 -0.24382001 -0.27483398]
MAP reward
-0.41	0.07	-0.41	0.07	-0.41	0.07	-0.41	
0.07	-0.27	0.07	-0.24	-0.24	-0.24	-0.41	
-0.27	-0.41	-0.27	0.07	-0.41	-0.41	-0.24	
-0.24	-0.24	-0.24	-0.27	-0.27	-0.24	-0.41	
-0.41	-0.27	0.07	-0.27	-0.41	0.07	-0.27	
0.07	-0.41	-0.24	-0.24	-0.41	-0.41	-0.27	
0.07	0.07	-0.24	0.07	0.07	-0.41	-0.24	
Map policy
>	^	<	^	<	^	<	
<	^	>	^	<	^	<	
^	^	^	^	<	^	^	
^	<	v	^	<	^	^	
v	v	v	v	v	v	<	
v	v	v	v	v	<	v	
<	<	<	v	<	<	<	
expeced value MDP LP 41.72529134429726
mean w [-0.12916669  0.42441482  0.00541455 -0.08219624]
Mean policy from posterior
>	^	<	^	<	^	<	
<	^	>	^	<	^	<	
^	^	^	^	<	^	^	
^	<	v	^	<	^	^	
v	v	v	v	v	v	<	
v	v	v	v	v	<	v	
<	<	<	v	<	<	<	
Mean rewards
-0.13	0.42	-0.13	0.42	-0.13	0.42	-0.13	
0.42	-0.08	0.42	0.01	0.01	0.01	-0.13	
-0.08	-0.13	-0.08	0.42	-0.13	-0.13	0.01	
0.01	0.01	0.01	-0.08	-0.08	0.01	-0.13	
-0.13	-0.08	0.42	-0.08	-0.13	0.42	-0.08	
0.42	-0.13	0.01	0.01	-0.13	-0.13	-0.08	
0.42	0.42	0.01	0.42	0.42	-0.13	0.01	
mean = 0.04044145280177158, map = 0.040441550922806435
CVaR policy
>	^	<	^	<	^	<	
<	^	>	^	<	^	<	
^	^	^	^	<	^	^	
^	<	v	^	<	^	^	
v	v	v	v	v	v	<	
v	<	v	v	v	<	v	
<	<	<	v	v	<	<	
CVaR policy
>	^	<	^	>	^	<	
<	^	>	^	>	^	<	
^	^	^	^	<	^	^	
^	<	v	^	<	^	^	
v	v	v	v	v	v	<	
v	v	v	v	v	<	v	
<	<	<	v	<	<	<	
CVaR policy
>	^	<	^	>	^	<	
<	^	>	^	>	^	<	
^	^	^	^	<	^	^	
^	<	v	^	<	^	^	
v	v	v	v	v	v	<	
v	v	v	v	v	<	v	
<	<	<	v	<	<	<	
CVaR policy
>	^	<	^	>	^	<	
<	^	>	^	>	^	<	
^	^	>	^	<	^	^	
^	<	v	^	<	^	^	
v	v	v	v	v	v	<	
v	v	v	v	v	<	v	
<	<	<	v	<	<	<	
CVaR policy
>	^	<	^	>	^	<	
<	^	>	^	>	^	<	
^	^	^	^	<	^	^	
^	<	v	^	<	^	^	
v	v	v	v	v	v	<	
v	v	v	v	v	<	v	
<	<	<	v	<	<	<	
cvar = , 0.040441479578221085, 0.04044166007526062, 0.04044165824596302, 0.04044163852575178, 0.04044144187272547
==========
iteration 75
==========
weights [ 0.57403885  0.49315192  0.53047696 -0.38193556]
expeced value MDP LP 57.031567916907804
demonstration
[(24, 1), (25, 3), (32, 1), (33, 3), (40, 2), (33, 0), (32, 3), (39, 2), (32, 3), (39, 2), (32, 1), (33, 0), (32, 1), (33, 0), (32, 1), (33, 0), (32, 1), (33, 0), (32, 1), (33, 3), (40, 0), (39, 1), (40, 2), (33, 3), (40, 3), (47, 3), (47, 2), (40, 2), (33, 3), (40, 2), (33, 0), (32, 2), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 3), (32, 2), (25, 3), (32, 1), (33, 0), (32, 2), (25, 3), (32, 3), (39, 0), (38, 1), (39, 2)]
[-0.10680485 -0.30450583 -0.40249616 -0.18619315]
w_map [ 0.40012937 -0.42622715 -0.10755296  0.06609051] loglik -40.92278481309404
accepted/total = 2704/3000 = 0.9013333333333333
-------
true weights [ 0.57403885  0.49315192  0.53047696 -0.38193556]
features
0 	3 	3 	2 	0 	1 	2 	
3 	3 	3 	2 	1 	3 	0 	
2 	2 	3 	3 	0 	2 	0 	
2 	1 	1 	3 	0 	3 	0 	
3 	1 	2 	1 	0 	0 	3 	
1 	0 	3 	0 	0 	0 	3 	
1 	1 	2 	0 	1 	0 	3 	
optimal policy
<	<	>	>	^	<	v	
^	v	>	^	v	>	v	
v	v	v	>	v	>	v	
>	v	v	>	^	>	^	
v	v	>	v	>	<	^	
>	v	>	>	>	v	<	
>	>	>	^	>	v	<	
optimal values
57.40	56.45	56.40	57.36	57.40	57.32	57.36	
56.45	56.13	56.36	57.32	57.32	56.45	57.40	
57.04	57.08	56.25	56.45	57.40	57.36	57.40	
57.08	57.12	57.20	56.45	57.40	56.45	57.40	
56.25	57.20	57.28	57.32	57.40	57.40	56.45	
57.20	57.28	56.45	57.40	57.40	57.40	56.45	
57.20	57.28	57.36	57.40	57.32	57.40	56.45	
map_weights [ 0.40012937 -0.42622715 -0.10755296  0.06609051]
MAP reward
0.40	0.07	0.07	-0.11	0.40	-0.43	-0.11	
0.07	0.07	0.07	-0.11	-0.43	0.07	0.40	
-0.11	-0.11	0.07	0.07	0.40	-0.11	0.40	
-0.11	-0.43	-0.43	0.07	0.40	0.07	0.40	
0.07	-0.43	-0.11	-0.43	0.40	0.40	0.07	
-0.43	0.40	0.07	0.40	0.40	0.40	0.07	
-0.43	-0.43	-0.11	0.40	-0.43	0.40	0.07	
Map policy
<	<	<	>	^	<	v	
^	^	v	v	v	>	v	
^	>	>	>	v	>	>	
^	v	>	>	v	>	^	
v	v	v	>	>	v	^	
>	>	>	>	>	v	<	
>	^	>	v	>	v	<	
expeced value MDP LP 31.15883929445598
mean w [ 0.31638978 -0.19988724 -0.05423483 -0.07788905]
Mean policy from posterior
<	<	>	>	^	<	v	
^	^	>	^	v	>	v	
^	<	>	>	v	>	>	
^	v	>	>	v	>	^	
v	v	v	>	>	v	^	
>	>	>	>	>	v	<	
>	>	>	^	>	v	<	
Mean rewards
0.32	-0.08	-0.08	-0.05	0.32	-0.20	-0.05	
-0.08	-0.08	-0.08	-0.05	-0.20	-0.08	0.32	
-0.05	-0.05	-0.08	-0.08	0.32	-0.05	0.32	
-0.05	-0.20	-0.20	-0.08	0.32	-0.08	0.32	
-0.08	-0.20	-0.05	-0.20	0.32	0.32	-0.08	
-0.20	0.32	-0.08	0.32	0.32	0.32	-0.08	
-0.20	-0.20	-0.05	0.32	-0.20	0.32	-0.08	
mean = 0.1869965665185731, map = 0.3152849126453674
CVaR policy
<	<	<	>	^	<	v	
^	<	<	v	v	>	v	
^	>	>	>	v	>	>	
^	v	>	>	v	>	^	
v	v	v	v	>	v	^	
>	>	>	>	>	v	<	
^	^	>	v	<	v	<	
CVaR policy
<	<	<	>	^	<	v	
^	^	v	v	v	>	v	
^	>	>	>	v	>	>	
^	v	>	>	v	>	>	
>	v	v	>	>	v	^	
>	>	>	>	>	v	<	
>	^	>	v	>	v	<	
CVaR policy
<	<	<	>	^	<	v	
^	^	v	v	v	>	v	
^	>	>	>	v	>	>	
^	v	>	>	v	>	^	
>	v	v	>	>	v	^	
>	>	>	>	>	v	<	
>	^	>	v	>	v	<	
CVaR policy
<	<	>	>	^	<	v	
^	^	>	^	v	>	v	
^	<	>	>	v	>	>	
^	v	>	>	v	>	^	
v	v	v	>	>	v	^	
>	>	>	>	>	v	<	
>	>	>	^	>	v	<	
CVaR policy
<	<	>	>	^	<	v	
^	^	>	^	v	>	v	
^	<	>	>	v	>	>	
^	v	>	>	v	>	^	
v	v	v	>	>	v	^	
>	>	>	>	>	v	<	
>	>	>	^	>	v	<	
cvar = , 0.3152853745703794, 0.3152847167417576, 0.31528470657096364, 0.18699660151780506, 0.18700747779530502
==========
iteration 76
==========
weights [-0.02839589  0.61499088  0.40018801  0.6788442 ]
expeced value MDP LP 67.61210029535758
demonstration
[(24, 2), (17, 0), (16, 1), (17, 1), (18, 0), (17, 3), (24, 2), (17, 3), (24, 2), (17, 1), (18, 0), (17, 0), (16, 1), (17, 3), (24, 2), (17, 1), (18, 0), (17, 0), (16, 0), (15, 3), (22, 2), (15, 3), (22, 2), (15, 3), (22, 2), (15, 0), (14, 0), (14, 1), (15, 1), (16, 0), (15, 0), (14, 1), (15, 0), (14, 1), (15, 3), (22, 2), (15, 1), (16, 1), (17, 1), (18, 0), (17, 3), (24, 2), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1), (18, 0), (17, 1)]
[-0.22088149 -0.1208529  -0.39004277 -0.26822284]
w_map [-0.04574364 -0.47076218 -0.1902526   0.29324158] loglik -32.60501955307518
accepted/total = 2743/3000 = 0.9143333333333333
-------
true weights [-0.02839589  0.61499088  0.40018801  0.6788442 ]
features
2 	0 	1 	2 	0 	3 	3 	
1 	2 	2 	0 	0 	1 	2 	
3 	3 	3 	3 	3 	1 	3 	
1 	3 	1 	3 	0 	2 	3 	
1 	1 	0 	1 	0 	3 	3 	
3 	1 	1 	0 	0 	0 	2 	
3 	0 	1 	1 	2 	2 	3 	
optimal policy
v	v	v	<	>	>	>	
v	v	v	v	v	^	^	
<	<	>	>	<	>	>	
^	^	<	^	<	>	>	
v	^	^	^	>	>	>	
v	<	<	^	v	^	^	
<	<	^	<	<	>	>	
optimal values
67.54	66.90	67.54	67.27	67.18	67.88	67.88	
67.82	67.61	67.61	67.18	67.18	67.82	67.61	
67.88	67.88	67.88	67.88	67.88	67.82	67.88	
67.82	67.88	67.82	67.88	67.18	67.61	67.88	
67.82	67.82	67.11	67.82	67.18	67.88	67.88	
67.88	67.82	67.76	67.11	66.65	67.18	67.61	
67.88	67.18	67.69	67.63	67.36	67.61	67.88	
map_weights [-0.04574364 -0.47076218 -0.1902526   0.29324158]
MAP reward
-0.19	-0.05	-0.47	-0.19	-0.05	0.29	0.29	
-0.47	-0.19	-0.19	-0.05	-0.05	-0.47	-0.19	
0.29	0.29	0.29	0.29	0.29	-0.47	0.29	
-0.47	0.29	-0.47	0.29	-0.05	-0.19	0.29	
-0.47	-0.47	-0.05	-0.47	-0.05	0.29	0.29	
0.29	-0.47	-0.47	-0.05	-0.05	-0.05	-0.19	
0.29	-0.05	-0.47	-0.47	-0.19	-0.19	0.29	
Map policy
v	v	v	>	>	>	>	
v	v	v	v	v	^	^	
<	<	<	v	<	>	>	
^	^	>	^	^	>	^	
v	^	^	^	>	>	^	
v	<	<	>	^	^	^	
<	<	<	>	>	>	>	
expeced value MDP LP 30.816864713179232
mean w [-0.07552548 -0.14919008 -0.16893713  0.31221717]
Mean policy from posterior
v	v	v	>	>	>	>	
v	v	v	v	v	^	^	
<	<	<	<	<	>	>	
^	^	>	^	^	>	>	
v	^	^	^	>	>	^	
v	<	<	^	>	^	^	
<	<	<	^	>	>	>	
Mean rewards
-0.17	-0.08	-0.15	-0.17	-0.08	0.31	0.31	
-0.15	-0.17	-0.17	-0.08	-0.08	-0.15	-0.17	
0.31	0.31	0.31	0.31	0.31	-0.15	0.31	
-0.15	0.31	-0.15	0.31	-0.08	-0.17	0.31	
-0.15	-0.15	-0.08	-0.15	-0.08	0.31	0.31	
0.31	-0.15	-0.15	-0.08	-0.08	-0.08	-0.17	
0.31	-0.08	-0.15	-0.15	-0.17	-0.17	0.31	
mean = 0.03505477414962854, map = 0.057837410044470516
CVaR policy
v	v	v	>	>	>	>	
v	v	v	v	v	^	^	
>	<	<	<	<	>	>	
^	^	^	^	<	>	^	
v	^	^	^	>	>	^	
v	<	<	>	^	^	v	
<	<	<	^	^	>	>	
CVaR policy
v	v	v	>	>	>	>	
v	v	v	v	v	^	^	
<	<	<	<	<	>	>	
^	^	>	^	<	>	^	
v	^	^	^	>	>	^	
v	<	<	^	^	^	^	
<	<	<	^	>	>	>	
CVaR policy
v	v	v	>	>	>	>	
v	v	v	v	v	^	^	
<	<	<	>	<	>	>	
>	^	>	^	<	>	>	
v	^	^	^	>	>	^	
v	<	<	^	>	^	^	
<	<	<	^	>	>	>	
CVaR policy
v	v	v	>	>	>	>	
v	v	v	v	v	^	^	
<	<	<	>	<	>	>	
>	^	>	^	<	>	^	
v	^	^	^	>	>	^	
v	<	<	^	>	^	^	
<	<	<	^	>	>	>	
CVaR policy
v	v	v	>	>	>	>	
v	v	v	v	v	^	^	
<	<	<	v	<	>	>	
>	^	>	^	^	>	^	
v	^	^	^	>	>	^	
v	<	<	^	>	^	^	
<	<	<	^	>	>	>	
cvar = , 0.1118792808486262, 0.03506227913041471, 0.0350547785477886, 0.0350549688141939, 0.035054791771869986
==========
iteration 77
==========
weights [-0.1204424   0.10205021 -0.82887034  0.53670601]
expeced value MDP LP 52.613993874945265
demonstration
[(24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 3)]
[-0.45888107 -0.13434832 -0.36676759 -0.04000302]
w_map [-0.32496204 -0.30014745 -0.29260411  0.0822864 ] loglik 0.0
accepted/total = 2749/3000 = 0.9163333333333333
-------
true weights [-0.1204424   0.10205021 -0.82887034  0.53670601]
features
0 	3 	2 	3 	3 	2 	1 	
3 	2 	0 	2 	0 	3 	2 	
2 	1 	3 	2 	0 	1 	0 	
2 	0 	2 	3 	1 	2 	2 	
0 	0 	1 	3 	0 	0 	0 	
0 	3 	3 	2 	3 	2 	0 	
3 	3 	3 	3 	0 	0 	2 	
optimal policy
>	^	>	^	<	<	<	
<	^	v	^	^	<	<	
^	v	v	v	v	^	<	
v	v	>	v	<	<	^	
v	v	v	^	<	<	<	
v	v	v	<	v	<	^	
<	<	<	<	<	<	<	
optimal values
53.01	53.67	52.31	53.67	53.67	52.31	51.88	
53.67	52.31	51.68	52.31	53.01	53.02	51.66	
52.31	51.94	52.32	52.31	52.58	52.59	51.95	
51.01	52.36	52.31	53.67	53.24	51.87	50.60	
52.36	53.01	53.24	53.67	53.01	52.36	51.72	
53.01	53.67	53.67	52.31	53.02	51.66	51.08	
53.67	53.67	53.67	53.67	53.01	52.36	51.01	
map_weights [-0.32496204 -0.30014745 -0.29260411  0.0822864 ]
MAP reward
-0.32	0.08	-0.29	0.08	0.08	-0.29	-0.30	
0.08	-0.29	-0.32	-0.29	-0.32	0.08	-0.29	
-0.29	-0.30	0.08	-0.29	-0.32	-0.30	-0.32	
-0.29	-0.32	-0.29	0.08	-0.30	-0.29	-0.29	
-0.32	-0.32	-0.30	0.08	-0.32	-0.32	-0.32	
-0.32	0.08	0.08	-0.29	0.08	-0.29	-0.32	
0.08	0.08	0.08	0.08	-0.32	-0.32	-0.29	
Map policy
>	^	>	^	<	<	<	
<	^	v	^	^	^	<	
^	>	v	v	<	^	^	
^	>	>	v	<	<	<	
v	v	v	^	<	<	<	
v	v	v	<	<	<	<	
<	<	<	<	<	<	<	
expeced value MDP LP 31.124658675953643
mean w [-0.15011013 -0.16246677 -0.13355741  0.31684202]
Mean policy from posterior
>	^	>	^	<	<	<	
<	^	v	^	^	^	<	
^	>	v	v	<	^	^	
^	>	>	v	<	<	<	
v	v	v	^	<	<	<	
v	v	<	<	<	<	<	
<	<	<	<	<	<	<	
Mean rewards
-0.15	0.32	-0.13	0.32	0.32	-0.13	-0.16	
0.32	-0.13	-0.15	-0.13	-0.15	0.32	-0.13	
-0.13	-0.16	0.32	-0.13	-0.15	-0.16	-0.15	
-0.13	-0.15	-0.13	0.32	-0.16	-0.13	-0.13	
-0.15	-0.15	-0.16	0.32	-0.15	-0.15	-0.15	
-0.15	0.32	0.32	-0.13	0.32	-0.13	-0.15	
0.32	0.32	0.32	0.32	-0.15	-0.15	-0.13	
mean = 0.1557837711946064, map = 0.15578377030388424
CVaR policy
>	^	>	^	<	<	<	
<	^	v	^	^	^	<	
^	>	v	v	<	^	^	
^	>	>	v	<	<	<	
v	v	v	^	<	<	<	
>	v	v	v	<	<	<	
<	<	<	<	<	<	<	
CVaR policy
>	^	>	^	<	<	<	
<	^	v	^	^	^	<	
^	>	v	v	<	^	^	
^	>	>	v	<	<	<	
v	v	v	^	<	<	<	
>	v	v	v	<	<	<	
<	<	<	<	<	<	<	
CVaR policy
>	^	>	^	<	<	<	
<	^	v	^	^	^	<	
^	>	v	v	<	^	^	
^	>	>	v	<	<	<	
v	v	v	^	<	<	<	
v	v	v	<	<	<	<	
<	<	<	<	<	<	<	
CVaR policy
>	^	>	^	<	<	<	
<	^	v	^	^	^	<	
^	>	v	v	<	^	^	
^	>	>	v	<	<	<	
v	v	v	^	<	<	<	
v	v	v	<	<	<	<	
<	<	<	<	<	<	<	
CVaR policy
>	^	>	^	<	<	<	
<	^	v	^	^	^	<	
^	>	v	v	<	^	^	
^	>	>	v	<	<	<	
v	v	v	^	<	<	<	
v	v	v	<	<	<	<	
<	<	<	<	<	<	<	
cvar = , 0.1557837513025362, 0.15578381906117045, 0.15578394246023208, 0.15578352886833358, 0.1557838230386892
==========
iteration 78
==========
weights [-0.24775567 -0.311174   -0.90663386  0.14072284]
expeced value MDP LP 13.537845035815192
demonstration
[(24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3)]
[-0.25733988 -0.13160004 -0.31277552 -0.29828456]
w_map [-0.19110867 -0.16923009  0.19998206  0.43967918] loglik -0.6931460585510649
accepted/total = 2746/3000 = 0.9153333333333333
-------
true weights [-0.24775567 -0.311174   -0.90663386  0.14072284]
features
0 	3 	0 	0 	0 	1 	1 	
2 	0 	2 	0 	0 	1 	0 	
2 	2 	3 	1 	3 	0 	0 	
1 	0 	3 	0 	3 	0 	3 	
3 	3 	1 	1 	3 	3 	2 	
1 	0 	1 	2 	1 	1 	3 	
0 	0 	0 	1 	0 	3 	0 	
optimal policy
>	^	<	<	v	<	v	
>	^	v	>	v	v	v	
v	>	v	<	v	<	v	
v	>	^	>	v	v	>	
<	<	<	>	>	<	<	
^	^	<	^	^	^	>	
^	^	<	>	>	v	^	
optimal values
13.68	14.07	13.68	13.30	13.30	12.86	12.86	
12.64	13.68	13.02	13.30	13.68	13.24	13.30	
12.58	13.02	14.07	13.62	14.07	13.68	13.68	
13.62	13.68	14.07	13.68	14.07	13.68	14.07	
14.07	14.07	13.62	13.62	14.07	14.07	13.02	
13.62	13.68	13.24	12.58	13.62	13.62	14.07	
13.24	13.30	12.92	13.24	13.68	14.07	13.68	
map_weights [-0.19110867 -0.16923009  0.19998206  0.43967918]
MAP reward
-0.19	0.44	-0.19	-0.19	-0.19	-0.17	-0.17	
0.20	-0.19	0.20	-0.19	-0.19	-0.17	-0.19	
0.20	0.20	0.44	-0.17	0.44	-0.19	-0.19	
-0.17	-0.19	0.44	-0.19	0.44	-0.19	0.44	
0.44	0.44	-0.17	-0.17	0.44	0.44	0.20	
-0.17	-0.19	-0.17	0.20	-0.17	-0.17	0.44	
-0.19	-0.19	-0.19	-0.17	-0.19	0.44	-0.19	
Map policy
>	^	<	<	v	v	v	
v	^	v	<	v	v	v	
>	>	v	>	v	<	v	
v	v	^	>	v	v	>	
<	<	<	>	^	<	<	
^	^	^	>	^	^	>	
^	^	^	>	>	v	^	
expeced value MDP LP 33.16629397652477
mean w [-0.22331755 -0.10993516 -0.08430673  0.33729021]
Mean policy from posterior
>	^	<	<	v	v	v	
>	^	v	<	v	v	v	
>	>	v	>	v	<	v	
v	v	^	>	v	v	>	
<	<	<	>	>	<	<	
^	^	^	>	^	^	>	
^	^	^	>	>	v	^	
Mean rewards
-0.22	0.34	-0.22	-0.22	-0.22	-0.11	-0.11	
-0.08	-0.22	-0.08	-0.22	-0.22	-0.11	-0.22	
-0.08	-0.08	0.34	-0.11	0.34	-0.22	-0.22	
-0.11	-0.22	0.34	-0.22	0.34	-0.22	0.34	
0.34	0.34	-0.11	-0.11	0.34	0.34	-0.08	
-0.11	-0.22	-0.11	-0.08	-0.11	-0.11	0.34	
-0.22	-0.22	-0.22	-0.11	-0.22	0.34	-0.22	
mean = 0.030455175890754305, map = 0.06471682292145431
CVaR policy
>	^	<	<	v	v	v	
^	^	v	<	v	v	v	
>	>	v	>	v	<	v	
v	v	^	>	v	v	>	
<	<	<	>	^	<	<	
^	^	^	>	^	>	>	
^	^	^	>	>	v	^	
CVaR policy
>	^	<	<	v	v	v	
^	^	v	<	v	v	v	
>	>	v	>	v	<	v	
v	v	^	>	v	<	>	
<	<	<	>	>	<	v	
^	^	^	^	^	>	>	
^	^	^	>	>	v	^	
CVaR policy
>	^	<	<	v	v	v	
>	^	v	<	v	v	v	
>	>	v	>	v	<	v	
v	v	^	>	v	<	>	
<	<	<	>	>	<	v	
^	^	^	^	^	>	>	
^	^	^	>	>	v	^	
CVaR policy
>	^	<	<	v	v	v	
>	^	v	<	v	v	v	
>	>	v	>	v	<	v	
v	v	^	>	v	<	>	
<	<	<	>	>	<	<	
^	^	^	>	^	^	>	
^	^	^	>	>	v	^	
CVaR policy
>	^	<	<	v	v	v	
^	^	v	<	v	v	v	
>	>	v	>	v	<	v	
v	v	^	>	v	<	>	
<	<	<	>	>	<	<	
^	^	^	^	^	^	>	
^	^	^	>	>	v	^	
cvar = , 0.03045517225171679, 0.030455175521318267, 0.0304976378553512, 0.030466650819786167, 0.03046491867251433
==========
iteration 79
==========
weights [ 0.09842463 -0.16534341 -0.23362561 -0.95309665]
expeced value MDP LP 9.357105107782013
demonstration
[(24, 3), (31, 1), (32, 3), (39, 3), (46, 1), (47, 3), (47, 3), (47, 1), (48, 3), (48, 3), (48, 0), (47, 3), (47, 1), (48, 2), (41, 3), (48, 1), (48, 2), (41, 1), (41, 3), (48, 3), (48, 0), (47, 1), (48, 3), (48, 3), (48, 0), (47, 1), (48, 2), (41, 1), (41, 3), (48, 2), (41, 3), (48, 0), (47, 3), (47, 1), (48, 2), (41, 3), (48, 0), (47, 1), (48, 2), (41, 1), (41, 1), (41, 3), (48, 2), (41, 3), (48, 3), (48, 2), (41, 3), (48, 1), (48, 2)]
[-0.15286057 -0.02153355 -0.17488235 -0.65072353]
w_map [ 0.32080071  0.11506529 -0.25429523 -0.30983877] loglik -46.440857592425346
accepted/total = 2232/3000 = 0.744
-------
true weights [ 0.09842463 -0.16534341 -0.23362561 -0.95309665]
features
0 	0 	3 	2 	3 	2 	2 	
1 	3 	0 	0 	0 	1 	1 	
3 	0 	0 	3 	0 	2 	3 	
0 	0 	3 	2 	3 	2 	2 	
0 	3 	1 	0 	1 	3 	2 	
2 	0 	3 	1 	0 	2 	0 	
3 	2 	0 	2 	1 	0 	0 	
optimal policy
<	<	<	v	v	v	v	
^	^	v	<	<	<	<	
v	v	<	<	^	<	<	
<	<	<	v	^	^	v	
^	<	>	v	v	v	v	
^	<	v	>	v	v	v	
^	>	v	<	>	>	>	
optimal values
9.84	9.84	8.79	9.51	8.79	9.25	8.99	
9.58	8.79	9.84	9.84	9.84	9.58	9.32	
8.79	9.84	9.84	8.79	9.84	9.51	8.46	
9.84	9.84	8.79	9.00	8.79	9.18	9.18	
9.84	8.79	9.07	9.33	9.32	8.46	9.51	
9.51	9.51	8.79	9.32	9.58	9.51	9.84	
8.46	9.51	9.84	9.51	9.58	9.84	9.84	
map_weights [ 0.32080071  0.11506529 -0.25429523 -0.30983877]
MAP reward
0.32	0.32	-0.31	-0.25	-0.31	-0.25	-0.25	
0.12	-0.31	0.32	0.32	0.32	0.12	0.12	
-0.31	0.32	0.32	-0.31	0.32	-0.25	-0.31	
0.32	0.32	-0.31	-0.25	-0.31	-0.25	-0.25	
0.32	-0.31	0.12	0.32	0.12	-0.31	-0.25	
-0.25	0.32	-0.31	0.12	0.32	-0.25	0.32	
-0.31	-0.25	0.32	-0.25	0.12	0.32	0.32	
Map policy
<	<	<	v	v	v	v	
^	^	v	<	<	<	<	
v	v	<	<	^	<	^	
<	<	<	v	^	^	v	
^	^	>	v	v	<	v	
^	<	v	>	v	v	>	
^	>	v	<	>	>	>	
expeced value MDP LP 22.962124151375814
mean w [ 0.2333495   0.06258311 -0.23025288 -0.33556094]
Mean policy from posterior
<	<	<	v	v	v	v	
^	^	v	<	<	<	<	
v	v	<	<	^	<	^	
<	<	<	v	^	^	v	
^	^	>	>	v	<	v	
^	<	v	>	v	v	>	
^	>	v	<	>	>	>	
Mean rewards
0.23	0.23	-0.34	-0.23	-0.34	-0.23	-0.23	
0.06	-0.34	0.23	0.23	0.23	0.06	0.06	
-0.34	0.23	0.23	-0.34	0.23	-0.23	-0.34	
0.23	0.23	-0.34	-0.23	-0.34	-0.23	-0.23	
0.23	-0.34	0.06	0.23	0.06	-0.34	-0.23	
-0.23	0.23	-0.34	0.06	0.23	-0.23	0.23	
-0.34	-0.23	0.23	-0.23	0.06	0.23	0.23	
mean = 0.007739715534228253, map = 0.0077404458487357886
CVaR policy
<	<	<	v	v	v	v	
^	^	v	<	<	<	<	
>	v	<	>	^	<	^	
<	<	^	v	^	^	v	
^	^	>	v	v	<	v	
^	<	v	>	v	v	v	
^	>	v	<	>	>	>	
CVaR policy
>	<	<	v	v	v	v	
^	^	v	<	<	<	<	
v	v	<	<	^	<	^	
>	<	<	v	^	^	v	
^	<	>	v	v	<	v	
^	<	v	>	v	>	v	
^	>	v	<	>	>	^	
CVaR policy
<	<	<	v	v	v	v	
^	^	v	<	<	<	<	
v	v	<	^	^	<	^	
<	<	<	v	^	^	v	
^	<	>	v	v	<	v	
^	<	v	>	v	>	v	
^	>	v	<	>	>	^	
CVaR policy
<	<	<	v	v	v	v	
^	^	v	<	<	<	<	
v	v	<	>	^	<	^	
<	<	<	v	^	^	v	
^	^	>	v	v	<	v	
^	<	v	>	v	v	>	
^	>	v	<	>	>	>	
CVaR policy
<	<	<	v	v	v	v	
^	^	v	<	<	<	<	
v	v	<	>	^	<	^	
<	<	^	v	^	^	v	
^	^	>	v	v	<	v	
^	<	v	>	v	v	>	
^	>	v	<	>	>	>	
cvar = , 0.007739833102164084, 0.0077397043316196346, 0.007740704548858446, 0.007739791460947387, 0.007779178748124949
==========
iteration 80
==========
weights [ 0.49623729  0.8309511  -0.25069363  0.02053097]
expeced value MDP LP 82.49976982220983
demonstration
[(24, 3), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0), (31, 1), (32, 0)]
[-0.18235762 -0.22966954 -0.14415362 -0.44381922]
w_map [-0.08372042  0.57598293 -0.28741362 -0.05288303] loglik -0.693143547330692
accepted/total = 2721/3000 = 0.907
-------
true weights [ 0.49623729  0.8309511  -0.25069363  0.02053097]
features
0 	0 	2 	2 	2 	1 	1 	
2 	1 	2 	1 	2 	3 	2 	
0 	1 	1 	1 	0 	1 	0 	
0 	1 	0 	2 	0 	1 	2 	
1 	3 	3 	1 	1 	0 	0 	
0 	0 	1 	2 	3 	1 	2 	
2 	3 	3 	0 	0 	3 	1 	
optimal policy
>	v	<	v	>	>	>	
>	v	v	v	<	^	^	
>	>	<	<	<	v	<	
>	^	^	^	>	^	<	
<	^	>	>	<	^	<	
^	<	<	^	^	^	v	
^	^	^	^	>	>	>	
optimal values
82.43	82.76	81.68	82.01	82.01	83.10	83.10	
82.01	83.10	82.01	83.10	82.01	82.28	82.01	
82.76	83.10	83.10	83.10	82.76	83.10	82.76	
82.76	83.10	82.76	82.01	82.76	83.10	82.01	
83.10	82.28	82.28	83.10	83.10	82.76	82.43	
82.76	82.43	82.44	82.01	82.28	82.76	82.01	
81.68	81.63	81.63	81.69	81.96	82.28	83.10	
map_weights [-0.08372042  0.57598293 -0.28741362 -0.05288303]
MAP reward
-0.08	-0.08	-0.29	-0.29	-0.29	0.58	0.58	
-0.29	0.58	-0.29	0.58	-0.29	-0.05	-0.29	
-0.08	0.58	0.58	0.58	-0.08	0.58	-0.08	
-0.08	0.58	-0.08	-0.29	-0.08	0.58	-0.29	
0.58	-0.05	-0.05	0.58	0.58	-0.08	-0.08	
-0.08	-0.08	0.58	-0.29	-0.05	0.58	-0.29	
-0.29	-0.05	-0.05	-0.08	-0.08	-0.05	0.58	
Map policy
>	v	<	v	>	>	>	
>	v	>	v	<	^	^	
>	>	>	<	<	v	<	
>	^	^	^	>	^	<	
<	^	>	>	<	^	<	
^	>	^	^	^	v	v	
^	>	^	^	>	>	>	
expeced value MDP LP 37.05285936422125
mean w [-0.10395921  0.37479998 -0.1557237  -0.01681845]
Mean policy from posterior
>	v	<	v	>	>	>	
>	v	v	v	<	^	^	
>	>	>	<	<	v	<	
>	^	^	^	>	^	<	
<	^	>	>	<	^	<	
^	>	^	^	^	v	v	
^	>	^	^	>	>	>	
Mean rewards
-0.10	-0.10	-0.16	-0.16	-0.16	0.37	0.37	
-0.16	0.37	-0.16	0.37	-0.16	-0.02	-0.16	
-0.10	0.37	0.37	0.37	-0.10	0.37	-0.10	
-0.10	0.37	-0.10	-0.16	-0.10	0.37	-0.16	
0.37	-0.02	-0.02	0.37	0.37	-0.10	-0.10	
-0.10	-0.10	0.37	-0.16	-0.02	0.37	-0.16	
-0.16	-0.02	-0.02	-0.10	-0.10	-0.02	0.37	
mean = 0.043826521945305785, map = 0.043826282490741164
CVaR policy
>	v	<	v	>	>	>	
>	v	v	v	<	^	^	
>	>	>	<	<	v	<	
>	^	<	^	>	^	<	
<	^	>	>	<	^	<	
^	>	^	^	^	v	v	
^	>	^	<	>	>	>	
CVaR policy
>	v	<	v	>	>	>	
>	v	<	v	<	^	^	
>	>	>	<	<	v	<	
>	^	<	^	>	^	<	
<	^	>	>	<	^	<	
^	>	^	^	^	v	v	
^	>	^	^	>	>	>	
CVaR policy
>	v	<	v	>	>	>	
>	v	v	v	<	^	^	
>	>	>	<	<	v	<	
>	^	<	^	>	^	<	
<	^	>	>	<	^	<	
^	>	^	^	^	v	v	
^	>	^	^	>	>	>	
CVaR policy
>	v	<	v	>	>	>	
>	v	v	v	<	^	^	
>	>	>	<	<	v	<	
>	^	<	^	>	^	<	
<	^	>	>	<	^	<	
^	>	^	^	^	v	v	
^	>	^	^	>	>	>	
CVaR policy
>	v	<	v	>	>	>	
>	v	v	v	<	^	^	
>	>	>	<	<	v	<	
>	^	<	^	>	^	<	
<	^	>	>	<	^	<	
^	>	^	^	^	v	v	
^	>	^	^	>	>	>	
cvar = , 0.054395450578638815, 0.04383525831289603, 0.04382628450849779, 0.04382628430110458, 0.043828347898411835
==========
iteration 81
==========
weights [ 0.61667414 -0.16188743 -0.13203566 -0.7589941 ]
expeced value MDP LP 60.49793675756618
demonstration
[(24, 0), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 3), (30, 2), (23, 3), (30, 2), (23, 3), (30, 2), (23, 0), (22, 1), (23, 3), (30, 2), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 0), (22, 1), (23, 3), (30, 2), (23, 3), (30, 2), (23, 0), (22, 1), (23, 3), (30, 2)]
[-0.29400681 -0.0613488  -0.56342473 -0.08121966]
w_map [ 0.31610999 -0.06401908 -0.51576654  0.10410439] loglik -16.63553219110736
accepted/total = 2763/3000 = 0.921
-------
true weights [ 0.61667414 -0.16188743 -0.13203566 -0.7589941 ]
features
0 	0 	2 	3 	1 	1 	2 	
3 	1 	3 	1 	2 	2 	3 	
1 	3 	3 	2 	3 	1 	0 	
2 	0 	0 	3 	1 	2 	1 	
3 	3 	0 	2 	0 	3 	0 	
1 	2 	1 	1 	1 	0 	0 	
2 	0 	3 	1 	2 	3 	1 	
optimal policy
<	<	<	<	<	v	v	
^	^	^	v	>	v	v	
v	v	v	<	>	>	>	
>	>	v	<	v	>	v	
^	>	^	<	<	>	v	
>	v	^	^	>	>	^	
>	v	<	<	^	^	^	
optimal values
61.67	61.67	60.92	59.55	58.79	59.38	59.56	
60.29	60.89	59.55	58.80	59.41	60.15	60.29	
60.15	60.29	60.29	59.56	59.52	60.89	61.67	
60.92	61.67	61.67	60.29	60.16	60.15	60.89	
59.55	60.29	61.67	60.92	60.93	60.29	61.67	
60.15	60.92	60.89	60.15	60.89	61.67	61.67	
60.92	61.67	60.29	59.53	60.15	60.29	60.89	
map_weights [ 0.31610999 -0.06401908 -0.51576654  0.10410439]
MAP reward
0.32	0.32	-0.52	0.10	-0.06	-0.06	-0.52	
0.10	-0.06	0.10	-0.06	-0.52	-0.52	0.10	
-0.06	0.10	0.10	-0.52	0.10	-0.06	0.32	
-0.52	0.32	0.32	0.10	-0.06	-0.52	-0.06	
0.10	0.10	0.32	-0.52	0.32	0.10	0.32	
-0.06	-0.52	-0.06	-0.06	-0.06	0.32	0.32	
-0.52	0.32	0.10	-0.06	-0.52	0.10	-0.06	
Map policy
<	<	<	v	<	v	v	
^	^	v	<	v	>	v	
>	v	v	v	>	>	>	
>	>	v	<	v	v	v	
>	^	^	<	>	>	>	
^	v	^	>	>	>	>	
>	v	<	<	>	^	^	
expeced value MDP LP 34.41006688173009
mean w [ 0.34967382 -0.05619505 -0.16699727 -0.1043499 ]
Mean policy from posterior
<	<	<	<	<	v	v	
^	^	<	<	v	v	v	
^	v	v	<	>	>	>	
>	>	<	<	v	>	v	
>	>	^	<	v	>	>	
>	v	^	>	>	>	^	
>	v	<	<	^	^	^	
Mean rewards
0.35	0.35	-0.17	-0.10	-0.06	-0.06	-0.17	
-0.10	-0.06	-0.10	-0.06	-0.17	-0.17	-0.10	
-0.06	-0.10	-0.10	-0.17	-0.10	-0.06	0.35	
-0.17	0.35	0.35	-0.10	-0.06	-0.17	-0.06	
-0.10	-0.10	0.35	-0.17	0.35	-0.10	0.35	
-0.06	-0.17	-0.06	-0.06	-0.06	0.35	0.35	
-0.17	0.35	-0.10	-0.06	-0.17	-0.10	-0.06	
mean = 0.04113159914297171, map = 0.2876330037070858
CVaR policy
<	<	<	<	<	v	v	
^	^	<	<	v	v	v	
^	v	v	v	>	>	>	
>	>	v	<	v	>	v	
>	^	^	<	v	v	>	
v	v	^	<	>	>	^	
>	v	<	<	^	^	^	
CVaR policy
<	<	<	<	<	v	v	
^	^	<	<	v	v	v	
^	v	v	v	>	>	>	
>	>	<	<	v	>	v	
>	>	^	<	v	>	>	
v	v	^	>	>	>	^	
>	v	<	<	^	^	^	
CVaR policy
<	<	<	<	<	v	v	
^	^	<	<	v	v	v	
^	v	v	<	>	>	>	
>	>	<	<	v	>	v	
>	>	^	<	v	>	>	
>	v	^	>	>	>	^	
>	v	<	<	^	^	^	
CVaR policy
<	<	<	<	<	v	v	
^	^	<	<	v	v	v	
^	v	v	<	>	>	>	
>	>	<	<	v	>	v	
>	>	^	<	v	>	>	
>	v	^	>	>	>	^	
>	v	<	<	^	^	^	
CVaR policy
<	<	<	<	<	v	v	
^	^	<	<	v	v	v	
^	v	v	<	>	>	>	
>	>	<	<	v	>	v	
>	>	^	<	v	>	>	
>	v	^	>	>	>	^	
>	v	<	<	^	^	^	
cvar = , 0.041133177655140685, 0.04132108461181616, 0.04116967062819299, 0.04113160384157055, 0.04114464437662946
==========
iteration 82
==========
weights [-0.48669463  0.22149265  0.56724636 -0.62633929]
expeced value MDP LP 55.030858146417465
demonstration
[(24, 2), (17, 1), (18, 2), (11, 0), (10, 1), (11, 3), (18, 2), (11, 0), (10, 1), (11, 0), (10, 1), (11, 3), (18, 2), (11, 3), (18, 2), (11, 0), (10, 1), (11, 3), (18, 2), (11, 1), (12, 2), (5, 2), (5, 3), (12, 2), (5, 2), (5, 2), (5, 3), (12, 0), (11, 1), (12, 0), (11, 0), (10, 1), (11, 3), (18, 2), (11, 1), (12, 2), (5, 3), (12, 0), (11, 0), (10, 1), (11, 3), (18, 2), (11, 3), (18, 2), (11, 3), (18, 2), (11, 1), (12, 0), (11, 0)]
[-0.27857349 -0.5102755  -0.20448828 -0.00666272]
w_map [-0.26124208  0.05604618  0.36774322 -0.31496851] loglik -30.5776940125279
accepted/total = 2608/3000 = 0.8693333333333333
-------
true weights [-0.48669463  0.22149265  0.56724636 -0.62633929]
features
0 	3 	3 	1 	3 	2 	0 	
3 	0 	0 	2 	2 	2 	3 	
3 	0 	0 	1 	2 	1 	2 	
0 	0 	3 	3 	0 	1 	0 	
0 	1 	1 	1 	0 	1 	2 	
0 	3 	1 	2 	0 	1 	0 	
0 	3 	1 	1 	0 	3 	2 	
optimal policy
>	>	>	v	>	v	<	
>	>	>	>	>	^	<	
>	>	>	^	^	^	>	
>	v	^	^	^	^	^	
>	>	>	>	>	>	>	
>	>	>	^	>	^	v	
>	>	>	^	>	>	>	
optimal values
52.98	54.01	55.19	56.38	55.53	56.72	55.67	
53.45	54.63	55.67	56.72	56.72	56.72	55.53	
53.12	54.29	55.33	56.38	56.72	56.38	56.72	
52.29	53.31	54.15	55.19	55.67	56.04	55.67	
53.31	54.34	54.67	55.00	55.33	56.38	56.72	
52.49	53.51	54.69	55.01	54.99	56.04	55.67	
52.17	53.19	54.36	54.69	54.49	55.53	56.72	
map_weights [-0.26124208  0.05604618  0.36774322 -0.31496851]
MAP reward
-0.26	-0.31	-0.31	0.06	-0.31	0.37	-0.26	
-0.31	-0.26	-0.26	0.37	0.37	0.37	-0.31	
-0.31	-0.26	-0.26	0.06	0.37	0.06	0.37	
-0.26	-0.26	-0.31	-0.31	-0.26	0.06	-0.26	
-0.26	0.06	0.06	0.06	-0.26	0.06	0.37	
-0.26	-0.31	0.06	0.37	-0.26	0.06	-0.26	
-0.26	-0.31	0.06	0.06	-0.26	-0.31	0.37	
Map policy
>	>	>	v	>	^	<	
>	>	>	>	>	^	<	
>	>	>	>	^	^	>	
>	^	^	^	^	^	^	
>	>	>	>	>	>	>	
>	>	>	^	>	^	v	
>	>	>	^	>	>	>	
expeced value MDP LP 34.89136876933081
mean w [-0.21474006  0.03081752  0.35890558 -0.1265101 ]
Mean policy from posterior
>	>	>	v	>	^	<	
>	>	>	>	>	^	<	
>	>	>	>	^	^	>	
>	>	>	^	^	^	^	
>	>	>	^	>	>	>	
>	>	>	^	>	^	v	
>	>	>	>	>	>	>	
Mean rewards
-0.21	-0.13	-0.13	0.03	-0.13	0.36	-0.21	
-0.13	-0.21	-0.21	0.36	0.36	0.36	-0.13	
-0.13	-0.21	-0.21	0.03	0.36	0.03	0.36	
-0.21	-0.21	-0.13	-0.13	-0.21	0.03	-0.21	
-0.21	0.03	0.03	0.03	-0.21	0.03	0.36	
-0.21	-0.13	0.03	0.36	-0.21	0.03	-0.21	
-0.21	-0.13	0.03	0.03	-0.21	-0.13	0.36	
mean = 0.08009411734472138, map = 0.0021922401756953036
CVaR policy
>	>	>	v	>	^	<	
>	>	>	>	>	^	v	
>	>	>	^	^	>	>	
>	>	>	^	^	v	^	
>	>	>	^	>	>	>	
>	>	>	^	>	^	v	
>	>	>	>	>	>	>	
CVaR policy
>	>	>	v	v	^	<	
>	>	>	>	>	^	v	
>	>	>	>	^	>	>	
>	>	>	^	^	^	^	
>	>	>	^	>	>	>	
>	>	>	^	>	^	v	
>	>	>	>	>	>	>	
CVaR policy
>	>	>	v	v	^	<	
>	>	>	>	>	^	v	
>	>	>	>	^	>	>	
>	>	>	^	^	^	^	
>	>	>	^	>	>	>	
>	>	>	^	>	^	v	
>	>	>	>	>	>	>	
CVaR policy
>	>	>	v	v	^	<	
>	>	>	>	>	^	v	
>	>	>	>	^	>	>	
>	>	>	^	^	^	^	
>	>	>	^	>	>	>	
>	>	>	^	>	^	v	
>	>	>	>	>	>	>	
CVaR policy
>	>	>	v	v	^	<	
>	>	>	>	>	^	v	
>	>	>	>	^	>	>	
>	>	>	^	^	^	^	
>	>	>	^	>	>	>	
>	>	>	^	>	^	v	
>	>	>	>	>	>	>	
cvar = , 0.08009413292555934, 0.08011292668327741, 0.08009419497646064, 0.0800944792781948, 0.08009435581747226
==========
iteration 83
==========
weights [-0.02930422  0.05047086 -0.52421654  0.84958283]
expeced value MDP LP 83.15556184205207
demonstration
[(24, 2), (17, 1), (18, 2), (11, 3), (18, 2), (11, 3), (18, 2), (11, 3), (18, 2), (11, 3), (18, 2), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 3), (18, 2), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 3), (18, 2), (11, 3), (18, 2), (11, 1), (12, 0), (11, 1), (12, 0), (11, 1), (12, 0), (11, 3), (18, 2), (11, 3), (18, 2), (11, 1), (12, 0), (11, 3), (18, 2), (11, 1), (12, 0)]
[-0.66067593 -0.0709341  -0.15490992 -0.11348005]
w_map [-0.03670634  0.03521472 -0.32082256  0.60725639] loglik -15.9423851453721
accepted/total = 2586/3000 = 0.862
-------
true weights [-0.02930422  0.05047086 -0.52421654  0.84958283]
features
3 	1 	3 	2 	1 	1 	0 	
1 	1 	3 	1 	3 	3 	2 	
1 	1 	0 	0 	3 	0 	0 	
3 	2 	0 	0 	2 	3 	1 	
0 	1 	2 	1 	2 	1 	2 	
2 	3 	0 	0 	2 	2 	0 	
2 	2 	1 	0 	0 	0 	0 	
optimal policy
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	^	^	>	^	<	<	
<	<	^	^	^	^	<	
^	<	<	^	^	^	^	
^	^	<	<	^	^	^	
^	^	^	<	>	^	^	
optimal values
84.96	84.16	84.96	83.58	84.16	84.16	83.29	
84.16	84.16	84.96	84.16	84.96	84.96	83.58	
84.16	83.37	84.08	84.08	84.96	84.08	83.21	
84.96	83.58	83.21	83.21	83.58	84.09	83.30	
84.08	83.29	81.93	82.43	82.22	83.30	81.94	
82.71	83.31	82.44	81.59	80.88	81.94	81.09	
81.36	81.95	81.67	80.82	80.25	81.09	80.25	
map_weights [-0.03670634  0.03521472 -0.32082256  0.60725639]
MAP reward
0.61	0.04	0.61	-0.32	0.04	0.04	-0.04	
0.04	0.04	0.61	0.04	0.61	0.61	-0.32	
0.04	0.04	-0.04	-0.04	0.61	-0.04	-0.04	
0.61	-0.32	-0.04	-0.04	-0.32	0.61	0.04	
-0.04	0.04	-0.32	0.04	-0.32	0.04	-0.32	
-0.32	0.61	-0.04	-0.04	-0.32	-0.32	-0.04	
-0.32	-0.32	0.04	-0.04	-0.04	-0.04	-0.04	
Map policy
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	^	^	>	^	^	<	
<	<	^	^	^	^	<	
^	<	<	^	^	^	^	
^	^	<	<	^	^	^	
^	^	^	<	>	^	^	
expeced value MDP LP 32.964478112601
mean w [-0.04318134 -0.11542412 -0.31062821  0.33826446]
Mean policy from posterior
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	>	^	>	^	^	<	
<	<	^	^	^	^	<	
^	<	^	^	^	^	^	
^	^	<	<	^	^	^	
^	^	^	^	>	^	^	
Mean rewards
0.34	-0.12	0.34	-0.31	-0.12	-0.12	-0.04	
-0.12	-0.12	0.34	-0.12	0.34	0.34	-0.31	
-0.12	-0.12	-0.04	-0.04	0.34	-0.04	-0.04	
0.34	-0.31	-0.04	-0.04	-0.31	0.34	-0.12	
-0.04	-0.12	-0.31	-0.12	-0.31	-0.12	-0.31	
-0.31	0.34	-0.04	-0.04	-0.31	-0.31	-0.04	
-0.31	-0.31	-0.12	-0.04	-0.04	-0.04	-0.04	
mean = 0.0048353224118358185, map = 2.1486604850906588e-07
CVaR policy
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	>	^	>	^	^	<	
<	<	^	^	^	^	<	
^	<	^	^	^	^	^	
^	^	<	<	<	^	^	
^	^	^	^	<	^	^	
CVaR policy
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	>	^	>	^	^	<	
<	<	^	^	^	^	<	
^	<	^	^	^	^	^	
^	^	<	<	^	^	^	
^	^	^	^	>	^	^	
CVaR policy
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	>	^	>	^	^	<	
<	<	^	^	^	^	<	
^	<	^	^	^	^	^	
^	^	<	<	^	^	^	
^	^	^	^	>	^	^	
CVaR policy
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	>	^	>	^	^	<	
<	<	^	^	^	^	<	
^	<	^	^	^	^	^	
^	^	<	<	^	^	^	
^	^	^	^	>	^	^	
CVaR policy
<	<	^	<	v	v	<	
^	>	^	<	>	<	<	
v	>	^	>	^	^	<	
<	<	^	^	^	^	<	
^	<	^	^	^	^	^	
^	^	<	<	^	^	^	
^	^	^	^	>	^	^	
cvar = , 0.024679338630377856, 0.004863619150142995, 0.004837397417389866, 0.004835796535147097, 0.004835553242415358
==========
iteration 84
==========
weights [ 0.04521104 -0.6094532  -0.35420547  0.70785679]
expeced value MDP LP 69.86427027702693
demonstration
[(24, 1), (25, 3), (32, 2), (25, 2), (18, 0), (17, 1), (18, 3), (25, 2), (18, 0), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 1), (18, 2), (11, 2), (4, 3), (11, 2), (4, 2), (4, 3), (11, 3), (18, 0), (17, 1), (18, 3), (25, 3), (32, 2), (25, 3), (32, 2), (25, 3), (32, 2), (25, 2), (18, 3), (25, 0), (24, 2), (17, 3), (24, 1), (25, 3), (32, 2), (25, 3), (32, 2), (25, 2), (18, 3), (25, 2), (18, 0), (17, 1), (18, 2), (11, 2)]
[-0.04276512 -0.22503615 -0.12115239 -0.61104634]
w_map [-0.31681997  0.04058647  0.13985708  0.50273647] loglik -38.7255611424589
accepted/total = 2786/3000 = 0.9286666666666666
-------
true weights [ 0.04521104 -0.6094532  -0.35420547  0.70785679]
features
2 	1 	0 	2 	3 	1 	0 	
3 	0 	3 	2 	3 	2 	3 	
0 	3 	1 	3 	3 	0 	2 	
0 	0 	2 	3 	3 	0 	2 	
1 	2 	2 	0 	3 	1 	0 	
2 	0 	2 	0 	1 	3 	3 	
1 	3 	2 	1 	1 	0 	2 	
optimal policy
v	v	v	>	v	<	v	
<	<	<	>	^	<	>	
^	^	>	>	^	<	^	
^	^	>	^	^	<	<	
^	v	>	^	^	<	v	
>	v	<	^	^	>	>	
>	v	<	<	>	^	^	
optimal values
69.72	68.81	69.47	69.72	70.79	69.47	70.12	
70.79	70.12	70.13	69.72	70.79	69.72	70.79	
70.12	70.13	69.47	70.79	70.79	70.12	69.72	
69.47	69.47	69.72	70.79	70.79	70.12	69.07	
68.16	69.07	69.07	70.12	70.79	69.47	70.12	
69.07	70.12	69.07	69.47	69.47	70.79	70.79	
69.47	70.79	69.72	68.42	68.81	70.12	69.72	
map_weights [-0.31681997  0.04058647  0.13985708  0.50273647]
MAP reward
0.14	0.04	-0.32	0.14	0.50	0.04	-0.32	
0.50	-0.32	0.50	0.14	0.50	0.14	0.50	
-0.32	0.50	0.04	0.50	0.50	-0.32	0.14	
-0.32	-0.32	0.14	0.50	0.50	-0.32	0.14	
0.04	0.14	0.14	-0.32	0.50	0.04	-0.32	
0.14	-0.32	0.14	-0.32	0.04	0.50	0.50	
0.04	0.50	0.14	0.04	0.04	-0.32	0.14	
Map policy
v	<	v	>	^	<	v	
<	<	>	>	^	<	>	
^	>	>	>	^	<	^	
^	>	>	>	^	<	^	
v	>	^	^	^	<	v	
v	v	v	>	^	>	>	
>	v	<	<	^	^	^	
expeced value MDP LP 36.258952782594676
mean w [-0.08311737 -0.13056606 -0.10471787  0.36741007]
Mean policy from posterior
v	v	v	>	v	<	v	
<	<	<	>	v	<	>	
^	<	>	>	^	<	^	
^	^	>	^	^	<	<	
^	v	>	^	^	<	v	
>	v	<	^	^	>	>	
>	v	<	<	>	^	^	
Mean rewards
-0.10	-0.13	-0.08	-0.10	0.37	-0.13	-0.08	
0.37	-0.08	0.37	-0.10	0.37	-0.10	0.37	
-0.08	0.37	-0.13	0.37	0.37	-0.08	-0.10	
-0.08	-0.08	-0.10	0.37	0.37	-0.08	-0.10	
-0.13	-0.10	-0.10	-0.08	0.37	-0.13	-0.08	
-0.10	-0.08	-0.10	-0.08	-0.13	0.37	0.37	
-0.13	0.37	-0.10	-0.13	-0.13	-0.08	-0.10	
mean = 9.567975212121382e-08, map = 0.1599273699119408
CVaR policy
v	<	v	>	^	<	v	
<	<	>	v	^	<	>	
^	>	>	>	^	<	^	
^	>	>	>	^	<	^	
v	v	^	>	^	<	v	
v	v	v	>	^	>	>	
>	v	<	<	^	^	^	
CVaR policy
v	<	v	>	v	<	v	
<	<	>	>	v	<	>	
^	^	>	>	^	<	^	
^	>	>	>	^	<	^	
>	v	^	^	^	<	v	
>	v	v	^	^	>	>	
>	v	<	<	>	^	^	
CVaR policy
v	<	v	>	v	<	v	
<	<	>	>	v	<	>	
^	^	>	>	<	<	^	
^	^	>	>	^	<	^	
>	v	^	^	^	<	v	
>	v	v	^	^	>	>	
>	v	<	<	>	^	^	
CVaR policy
v	v	v	>	v	<	v	
<	<	<	>	v	<	>	
^	^	>	>	^	<	^	
^	^	>	^	^	<	<	
^	v	>	^	^	<	v	
>	v	<	^	^	>	>	
>	v	<	<	>	^	^	
CVaR policy
v	v	v	>	v	<	v	
<	<	<	>	v	<	>	
^	^	>	>	^	<	^	
^	^	>	^	^	<	<	
^	v	>	^	^	<	v	
>	v	<	^	^	>	>	
>	v	<	<	>	^	^	
cvar = , 0.13061412649499005, 0.0646147997542812, 0.05638177324594551, 4.20844799009501e-08, 1.4671815051769954e-07
==========
iteration 85
==========
weights [0.05219593 0.46829467 0.67689383 0.56550017]
expeced value MDP LP 67.38375093817783
demonstration
[(24, 3), (31, 2), (24, 3), (31, 2), (24, 3), (31, 2), (24, 0), (23, 1), (24, 3), (31, 2), (24, 0), (23, 1), (24, 3), (31, 2), (24, 3), (31, 2), (24, 0), (23, 1), (24, 3), (31, 2), (24, 0), (23, 1), (24, 3), (31, 2), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 3), (31, 2), (24, 3), (31, 2), (24, 0), (23, 1), (24, 0), (23, 1), (24, 0), (23, 1), (24, 3), (31, 2), (24, 3), (31, 2), (24, 0), (23, 1), (24, 3), (31, 2), (24, 0)]
[-0.5701895  -0.31814616 -0.00812491 -0.10353943]
w_map [ 0.10436985 -0.14863592  0.56588182  0.1811124 ] loglik -17.328679474777346
accepted/total = 2764/3000 = 0.9213333333333333
-------
true weights [0.05219593 0.46829467 0.67689383 0.56550017]
features
1 	0 	2 	1 	1 	0 	3 	
2 	0 	2 	2 	3 	0 	2 	
2 	3 	1 	1 	0 	3 	1 	
0 	3 	2 	2 	3 	3 	0 	
1 	1 	3 	2 	0 	3 	1 	
1 	2 	0 	0 	1 	3 	3 	
2 	1 	2 	1 	3 	3 	0 	
optimal policy
v	>	^	<	v	>	v	
<	<	^	<	<	>	>	
^	<	^	^	^	>	^	
^	>	>	v	<	<	^	
v	^	^	^	<	^	<	
v	v	v	^	v	^	<	
<	<	v	<	<	<	<	
optimal values
67.48	67.06	67.69	67.48	67.37	66.95	67.58	
67.69	67.06	67.69	67.69	67.58	67.06	67.69	
67.69	67.58	67.48	67.48	66.95	67.37	67.48	
67.06	67.58	67.69	67.69	67.58	67.47	66.86	
67.27	67.37	67.58	67.69	67.06	67.36	67.15	
67.48	67.48	67.06	67.06	67.17	67.25	67.14	
67.69	67.48	67.69	67.48	67.37	67.26	66.64	
map_weights [ 0.10436985 -0.14863592  0.56588182  0.1811124 ]
MAP reward
-0.15	0.10	0.57	-0.15	-0.15	0.10	0.18	
0.57	0.10	0.57	0.57	0.18	0.10	0.57	
0.57	0.18	-0.15	-0.15	0.10	0.18	-0.15	
0.10	0.18	0.57	0.57	0.18	0.18	0.10	
-0.15	-0.15	0.18	0.57	0.10	0.18	-0.15	
-0.15	0.57	0.10	0.10	-0.15	0.18	0.18	
0.57	-0.15	0.57	-0.15	0.18	0.18	0.10	
Map policy
v	>	^	<	v	>	v	
<	<	^	<	<	>	>	
^	<	^	^	^	^	^	
^	>	>	<	<	<	^	
^	^	^	^	<	<	<	
v	>	v	^	<	^	<	
<	<	v	<	<	<	<	
expeced value MDP LP 35.73124222813233
mean w [-0.14335669 -0.10048103  0.3629984  -0.06264275]
Mean policy from posterior
v	>	^	<	v	>	v	
<	<	^	<	<	>	>	
^	<	^	^	^	>	^	
^	>	>	<	<	<	^	
v	>	^	^	<	<	<	
v	v	v	^	<	^	<	
<	<	v	<	<	<	<	
Mean rewards
-0.10	-0.14	0.36	-0.10	-0.10	-0.14	-0.06	
0.36	-0.14	0.36	0.36	-0.06	-0.14	0.36	
0.36	-0.06	-0.10	-0.10	-0.14	-0.06	-0.10	
-0.14	-0.06	0.36	0.36	-0.06	-0.06	-0.14	
-0.10	-0.10	-0.06	0.36	-0.14	-0.06	-0.10	
-0.10	0.36	-0.14	-0.14	-0.10	-0.06	-0.06	
0.36	-0.10	0.36	-0.10	-0.06	-0.06	-0.14	
mean = 0.03844449715131759, map = 0.06366516051885185
CVaR policy
v	>	^	v	<	>	v	
<	<	^	<	<	>	>	
^	<	^	^	<	>	^	
^	>	>	<	<	<	^	
v	v	^	^	<	<	^	
v	v	v	^	<	<	^	
<	<	v	<	<	<	<	
CVaR policy
v	>	^	<	<	>	v	
<	<	^	<	<	>	>	
^	<	^	^	<	>	^	
^	>	>	v	<	<	^	
v	v	^	^	<	<	^	
v	v	v	^	<	<	^	
<	<	v	<	<	<	<	
CVaR policy
v	>	^	<	v	>	v	
<	<	^	<	<	>	>	
^	<	^	^	^	>	^	
^	>	>	v	<	<	^	
v	>	^	^	<	<	<	
v	v	v	^	<	^	<	
<	<	v	<	<	<	<	
CVaR policy
v	>	^	<	v	>	v	
<	<	^	<	<	>	>	
^	<	^	^	^	>	^	
^	>	>	<	<	<	^	
v	>	^	^	<	<	<	
v	v	v	^	<	^	<	
<	<	v	<	<	<	<	
CVaR policy
v	>	^	<	v	>	v	
<	<	^	<	<	>	>	
^	<	^	^	^	>	^	
^	>	>	<	<	<	^	
v	>	^	^	<	<	<	
v	v	v	^	<	^	<	
<	<	v	<	<	<	<	
cvar = , 0.05229777628295551, 0.05229768610736585, 0.038444573427582895, 0.03844454088310556, 0.038444494230304826
==========
iteration 86
==========
weights [-0.27958132  0.896489    0.07396007  0.33566599]
expeced value MDP LP 88.17650378526314
demonstration
[(24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2), (17, 3), (24, 2)]
[-0.46314837 -0.0575137  -0.09388898 -0.38544895]
w_map [-0.5152499   0.25569825 -0.14607307 -0.08297878] loglik 0.0
accepted/total = 2711/3000 = 0.9036666666666666
-------
true weights [-0.27958132  0.896489    0.07396007  0.33566599]
features
3 	0 	2 	3 	1 	3 	1 	
0 	0 	3 	2 	3 	3 	0 	
3 	3 	2 	1 	3 	1 	3 	
0 	2 	2 	1 	2 	2 	3 	
3 	1 	2 	2 	1 	0 	1 	
2 	3 	2 	3 	3 	0 	3 	
3 	2 	2 	0 	2 	2 	0 	
optimal policy
>	>	>	>	^	>	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	^	^	^	>	>	
^	^	>	^	^	>	^	
^	^	^	^	^	^	^	
optimal values
86.57	87.11	88.27	89.09	89.65	89.09	89.65	
86.57	87.11	88.27	88.83	89.09	88.54	88.47	
87.73	88.27	88.83	89.65	89.09	89.09	88.54	
86.85	88.01	88.83	89.65	88.83	88.28	89.09	
87.48	88.03	88.01	88.83	88.83	88.47	89.65	
86.68	87.48	87.46	88.27	88.28	87.92	89.09	
86.15	86.68	86.66	87.11	87.47	87.11	87.92	
map_weights [-0.5152499   0.25569825 -0.14607307 -0.08297878]
MAP reward
-0.08	-0.52	-0.15	-0.08	0.26	-0.08	0.26	
-0.52	-0.52	-0.08	-0.15	-0.08	-0.08	-0.52	
-0.08	-0.08	-0.15	0.26	-0.08	0.26	-0.08	
-0.52	-0.15	-0.15	0.26	-0.15	-0.15	-0.08	
-0.08	0.26	-0.15	-0.15	0.26	-0.52	0.26	
-0.15	-0.08	-0.15	-0.08	-0.08	-0.52	-0.08	
-0.08	-0.15	-0.15	-0.52	-0.15	-0.15	-0.52	
Map policy
>	>	>	>	^	>	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	^	^	^	>	>	
^	^	>	^	^	>	^	
^	^	^	^	^	>	^	
expeced value MDP LP 34.98427873820348
mean w [-0.09514717  0.35866409 -0.14285165 -0.04463769]
Mean policy from posterior
>	>	>	>	^	<	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	>	^	>	>	>	
^	^	>	^	^	>	^	
^	^	>	^	^	^	^	
Mean rewards
-0.04	-0.10	-0.14	-0.04	0.36	-0.04	0.36	
-0.10	-0.10	-0.04	-0.14	-0.04	-0.04	-0.10	
-0.04	-0.04	-0.14	0.36	-0.04	0.36	-0.04	
-0.10	-0.14	-0.14	0.36	-0.14	-0.14	-0.04	
-0.04	0.36	-0.14	-0.14	0.36	-0.10	0.36	
-0.14	-0.04	-0.14	-0.04	-0.04	-0.10	-0.04	
-0.04	-0.14	-0.14	-0.10	-0.14	-0.14	-0.10	
mean = 0.02835834193517428, map = 2.121680608979659e-11
CVaR policy
>	>	>	>	^	>	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	^	^	>	>	>	
^	^	>	^	^	>	^	
^	^	>	^	^	^	^	
CVaR policy
>	>	>	>	^	>	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	^	^	>	>	>	
^	^	>	^	^	>	^	
^	^	>	^	^	>	^	
CVaR policy
>	>	>	>	^	>	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	>	^	>	>	>	
^	^	>	^	^	>	^	
^	^	>	^	^	>	^	
CVaR policy
>	>	>	>	^	>	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	>	^	>	>	>	
^	^	>	^	^	>	^	
^	^	>	^	^	>	^	
CVaR policy
>	>	>	>	^	>	>	
v	>	>	v	^	v	^	
>	>	>	v	<	<	<	
>	>	>	^	<	^	v	
>	>	>	^	>	>	>	
^	^	>	^	^	>	^	
^	^	>	^	^	>	^	
cvar = , 0.028358376102502802, 0.028358492286500336, 0.028624657903009165, 0.02855935704683077, 0.028558517062506894
==========
iteration 87
==========
weights [ 0.44383032 -0.575832   -0.32695228  0.60376682]
expeced value MDP LP 59.52653047842325
demonstration
[(24, 3), (31, 0), (30, 1), (31, 0), (30, 0), (29, 1), (30, 0), (29, 1), (30, 1), (31, 0), (30, 1), (31, 0), (30, 0), (29, 1), (30, 1), (31, 0), (30, 0), (29, 1), (30, 1), (31, 0), (30, 0), (29, 1), (30, 1), (31, 0), (30, 0), (29, 1), (30, 0), (29, 1), (30, 0), (29, 1), (30, 1), (31, 0), (30, 0), (29, 1), (30, 0), (29, 1), (30, 1), (31, 0), (30, 1), (31, 0), (30, 0), (29, 1), (30, 1), (31, 0), (30, 1), (31, 0), (30, 0), (29, 1), (30, 1)]
[-0.26526363 -0.12509095 -0.06514927 -0.54449615]
w_map [ 0.06527018  0.03072424 -0.07063275  0.83337283] loglik -16.635532333420997
accepted/total = 2769/3000 = 0.923
-------
true weights [ 0.44383032 -0.575832   -0.32695228  0.60376682]
features
0 	2 	2 	0 	2 	1 	1 	
0 	0 	1 	3 	3 	2 	1 	
1 	2 	0 	2 	0 	1 	1 	
2 	2 	0 	1 	0 	2 	3 	
0 	3 	3 	3 	2 	2 	3 	
0 	0 	2 	1 	1 	2 	2 	
0 	0 	1 	1 	3 	0 	3 	
optimal policy
v	>	>	v	v	<	<	
>	>	>	>	<	<	<	
v	>	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	^	
^	^	^	^	v	v	^	
^	^	<	>	v	>	>	
optimal values
58.76	58.37	59.29	60.22	59.45	58.28	57.12	
58.90	59.05	59.20	60.38	60.38	59.45	58.28	
58.12	59.13	60.06	59.45	60.22	59.04	59.20	
59.29	59.45	60.22	59.20	60.06	59.45	60.38	
60.22	60.38	60.38	60.38	59.45	59.45	60.38	
60.06	60.22	59.45	59.20	59.20	59.29	59.45	
59.90	60.06	58.88	59.20	60.38	60.22	60.38	
map_weights [ 0.06527018  0.03072424 -0.07063275  0.83337283]
MAP reward
0.07	-0.07	-0.07	0.07	-0.07	0.03	0.03	
0.07	0.07	0.03	0.83	0.83	-0.07	0.03	
0.03	-0.07	0.07	-0.07	0.07	0.03	0.03	
-0.07	-0.07	0.07	0.03	0.07	-0.07	0.83	
0.07	0.83	0.83	0.83	-0.07	-0.07	0.83	
0.07	0.07	-0.07	0.03	0.03	-0.07	-0.07	
0.07	0.07	0.03	0.03	0.83	0.07	0.83	
Map policy
v	v	>	v	v	<	v	
>	>	>	>	<	<	v	
v	v	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	>	
^	^	^	^	v	v	^	
^	^	>	>	v	>	>	
expeced value MDP LP 33.040142384020896
mean w [-0.06743668 -0.12325741 -0.19533425  0.33648334]
Mean policy from posterior
v	v	>	v	v	v	v	
>	>	>	>	<	<	v	
v	v	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	>	
>	^	^	^	v	v	^	
^	^	>	>	v	>	>	
Mean rewards
-0.07	-0.20	-0.20	-0.07	-0.20	-0.12	-0.12	
-0.07	-0.07	-0.12	0.34	0.34	-0.20	-0.12	
-0.12	-0.20	-0.07	-0.20	-0.07	-0.12	-0.12	
-0.20	-0.20	-0.07	-0.12	-0.07	-0.20	0.34	
-0.07	0.34	0.34	0.34	-0.20	-0.20	0.34	
-0.07	-0.07	-0.20	-0.12	-0.12	-0.20	-0.20	
-0.07	-0.07	-0.12	-0.12	0.34	-0.07	0.34	
mean = 0.04460511501348918, map = 0.04460520443325322
CVaR policy
v	v	>	v	v	<	v	
>	>	>	>	<	<	v	
v	v	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	>	
>	^	^	^	v	v	v	
^	^	>	>	v	>	>	
CVaR policy
v	v	>	v	v	<	v	
>	>	>	>	<	<	v	
v	v	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	>	
^	^	^	^	v	v	v	
^	^	>	>	v	>	>	
CVaR policy
v	v	>	v	v	v	v	
>	>	>	>	<	<	v	
v	v	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	>	
>	^	^	^	v	v	^	
^	^	>	>	v	>	>	
CVaR policy
v	v	>	v	v	<	v	
>	>	>	>	<	<	v	
v	v	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	>	
>	^	^	^	v	v	^	
^	^	>	>	v	>	>	
CVaR policy
v	v	>	v	v	<	v	
>	>	>	>	<	<	v	
v	v	v	^	^	<	v	
v	v	v	v	^	>	v	
>	>	>	<	<	>	>	
>	^	^	^	v	v	^	
^	^	>	>	v	>	>	
cvar = , 0.044605052767295206, 0.044638035492546635, 0.04460512571483832, 0.044657209294648226, 0.04463156128499435
==========
iteration 88
==========
weights [ 0.77324191  0.31117166 -0.03979032  0.55107701]
expeced value MDP LP 76.77225380295229
demonstration
[(24, 2), (17, 0), (16, 0), (15, 2), (8, 0), (7, 0), (7, 1), (8, 0), (7, 1), (8, 2), (1, 1), (2, 2), (2, 2), (2, 2), (2, 1), (3, 2), (3, 2), (3, 0), (2, 1), (3, 0), (2, 1), (3, 2), (3, 0), (2, 1), (3, 2), (3, 2), (3, 0), (2, 0), (1, 3), (8, 0), (7, 0), (7, 1), (8, 2), (1, 2), (1, 1), (2, 0), (1, 2), (1, 3), (8, 0), (7, 1), (8, 2), (1, 1), (2, 2), (2, 0), (1, 2), (1, 1), (2, 1), (3, 0), (2, 0)]
[-0.56267421 -0.1060004  -0.13225804 -0.19906735]
w_map [ 0.53277888  0.03947028 -0.07573179  0.35201905] loglik -40.80499817294003
accepted/total = 1952/3000 = 0.6506666666666666
-------
true weights [ 0.77324191  0.31117166 -0.03979032  0.55107701]
features
2 	0 	0 	0 	2 	0 	2 	
0 	0 	1 	2 	0 	0 	2 	
1 	3 	3 	0 	1 	3 	0 	
0 	2 	0 	2 	3 	3 	3 	
0 	3 	1 	1 	2 	1 	2 	
3 	0 	1 	3 	2 	2 	3 	
1 	2 	1 	3 	1 	2 	0 	
optimal policy
>	>	^	^	<	^	<	
>	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	^	>	^	^	
^	<	<	<	^	^	v	
^	^	<	<	<	>	v	
^	^	^	^	>	>	>	
optimal values
76.51	77.32	77.32	77.32	76.51	77.32	76.51	
77.32	77.32	76.86	76.51	77.32	77.32	76.51	
76.86	77.10	76.88	76.89	76.86	77.10	77.32	
77.32	76.51	76.89	76.08	76.66	76.88	77.10	
77.32	77.10	76.64	76.19	75.86	76.42	76.29	
77.10	77.10	76.64	76.43	75.62	76.29	77.10	
76.64	76.29	76.19	76.22	76.06	76.51	77.32	
map_weights [ 0.53277888  0.03947028 -0.07573179  0.35201905]
MAP reward
-0.08	0.53	0.53	0.53	-0.08	0.53	-0.08	
0.53	0.53	0.04	-0.08	0.53	0.53	-0.08	
0.04	0.35	0.35	0.53	0.04	0.35	0.53	
0.53	-0.08	0.53	-0.08	0.35	0.35	0.35	
0.53	0.35	0.04	0.04	-0.08	0.04	-0.08	
0.35	0.53	0.04	0.35	-0.08	-0.08	0.35	
0.04	-0.08	0.04	0.35	0.04	-0.08	0.53	
Map policy
>	>	^	^	<	^	<	
<	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	^	>	^	^	
^	<	<	<	^	^	v	
^	^	<	<	>	>	v	
^	^	^	^	>	>	>	
expeced value MDP LP 29.766265855146987
mean w [ 0.30173719 -0.29511987 -0.16789502  0.17869179]
Mean policy from posterior
>	>	^	^	<	^	<	
<	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	^	>	^	^	
^	<	<	^	^	^	v	
^	^	<	<	>	>	v	
^	^	<	^	>	>	>	
Mean rewards
-0.17	0.30	0.30	0.30	-0.17	0.30	-0.17	
0.30	0.30	-0.30	-0.17	0.30	0.30	-0.17	
-0.30	0.18	0.18	0.30	-0.30	0.18	0.30	
0.30	-0.17	0.30	-0.17	0.18	0.18	0.18	
0.30	0.18	-0.30	-0.30	-0.17	-0.30	-0.17	
0.18	0.30	-0.30	0.18	-0.17	-0.17	0.18	
-0.30	-0.17	-0.30	0.18	-0.30	-0.17	0.30	
mean = 0.021276625055321574, map = 0.0027842363469687825
CVaR policy
v	>	>	^	<	^	<	
<	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	^	>	^	^	
^	<	<	^	^	^	v	
^	<	<	>	>	>	v	
^	^	<	^	>	>	>	
CVaR policy
>	>	>	^	<	^	<	
<	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	^	>	^	^	
^	<	<	^	^	^	v	
^	<	<	<	>	>	v	
^	^	<	^	>	>	>	
CVaR policy
>	>	>	^	<	^	<	
<	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	<	>	^	^	
^	<	<	^	^	^	v	
^	<	<	<	>	>	v	
^	^	<	^	>	>	>	
CVaR policy
>	>	>	^	<	^	<	
<	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	^	>	^	^	
^	<	<	^	^	^	v	
^	<	<	<	>	>	v	
^	^	<	^	>	>	>	
CVaR policy
>	>	>	^	<	^	<	
<	^	^	^	>	^	<	
^	^	<	<	^	^	>	
<	<	^	^	>	^	^	
^	<	<	^	^	^	v	
^	^	<	<	>	>	v	
^	^	<	^	>	>	>	
cvar = , 0.06774943284622736, 0.02127687721286975, 0.021276770509416565, 0.021276688926349152, 0.02127679400115312
==========
iteration 89
==========
weights [ 0.76232903  0.35580849  0.4977935  -0.21084686]
expeced value MDP LP 75.59893988288403
demonstration
[(24, 0), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3), (23, 2), (16, 3)]
[-0.4096741  -0.40158924 -0.06365425 -0.12508241]
w_map [ 0.07888608 -0.26062696 -0.3049728  -0.35551416] loglik 0.0
accepted/total = 2724/3000 = 0.908
-------
true weights [ 0.76232903  0.35580849  0.4977935  -0.21084686]
features
0 	1 	0 	0 	1 	1 	2 	
2 	2 	1 	3 	1 	3 	3 	
1 	3 	0 	3 	1 	2 	2 	
3 	3 	0 	1 	0 	2 	2 	
2 	3 	1 	0 	2 	3 	2 	
1 	3 	2 	1 	3 	1 	2 	
0 	0 	1 	1 	0 	2 	0 	
optimal policy
<	<	^	<	<	<	<	
^	<	^	^	^	<	v	
^	>	v	<	v	v	v	
v	>	^	<	<	<	v	
v	>	^	<	<	>	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
optimal values
76.23	75.83	76.23	76.23	75.83	75.42	75.17	
75.97	75.71	75.83	75.26	75.42	74.46	74.23	
75.56	75.26	76.23	75.26	75.43	75.31	75.19	
74.60	75.26	76.23	75.83	75.83	75.57	75.45	
75.57	74.86	75.83	75.83	75.57	74.74	75.71	
75.83	75.26	75.57	75.43	75.26	75.56	75.97	
76.23	76.23	75.83	75.83	76.23	75.97	76.23	
map_weights [ 0.07888608 -0.26062696 -0.3049728  -0.35551416]
MAP reward
0.08	-0.26	0.08	0.08	-0.26	-0.26	-0.30	
-0.30	-0.30	-0.26	-0.36	-0.26	-0.36	-0.36	
-0.26	-0.36	0.08	-0.36	-0.26	-0.30	-0.30	
-0.36	-0.36	0.08	-0.26	0.08	-0.30	-0.30	
-0.30	-0.36	-0.26	0.08	-0.30	-0.36	-0.30	
-0.26	-0.36	-0.30	-0.26	-0.36	-0.26	-0.30	
0.08	0.08	-0.26	-0.26	0.08	-0.30	0.08	
Map policy
<	<	^	<	<	<	<	
^	^	^	^	^	<	^	
^	>	v	<	v	<	<	
>	>	^	<	<	<	<	
v	>	^	<	<	<	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
expeced value MDP LP 39.050949657353286
mean w [ 0.39674885 -0.01051575 -0.07521199 -0.0923821 ]
Mean policy from posterior
<	<	^	<	<	<	<	
^	^	^	^	^	<	^	
^	>	v	<	v	<	<	
>	>	^	<	<	<	<	
v	>	^	<	<	<	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
Mean rewards
0.40	-0.01	0.40	0.40	-0.01	-0.01	-0.08	
-0.08	-0.08	-0.01	-0.09	-0.01	-0.09	-0.09	
-0.01	-0.09	0.40	-0.09	-0.01	-0.08	-0.08	
-0.09	-0.09	0.40	-0.01	0.40	-0.08	-0.08	
-0.08	-0.09	-0.01	0.40	-0.08	-0.09	-0.08	
-0.01	-0.09	-0.08	-0.01	-0.09	-0.01	-0.08	
0.40	0.40	-0.01	-0.01	0.40	-0.08	0.40	
mean = 0.02347760852386216, map = 0.023477629597778105
CVaR policy
<	<	^	<	<	<	<	
^	^	^	^	^	<	^	
^	>	v	<	v	<	<	
>	>	^	<	<	<	<	
v	>	^	<	<	<	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
CVaR policy
<	<	^	<	<	<	<	
^	^	^	^	^	<	^	
^	>	v	<	v	<	<	
>	>	^	<	<	<	<	
v	>	^	<	<	<	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
CVaR policy
<	<	^	<	<	<	<	
^	^	^	^	^	<	^	
^	>	v	<	v	<	<	
>	>	^	<	<	<	<	
v	>	^	<	<	<	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
CVaR policy
<	<	^	<	<	<	<	
^	^	^	^	^	<	^	
^	>	v	<	v	<	<	
>	>	^	<	<	<	<	
v	>	^	<	<	<	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
CVaR policy
<	<	^	<	<	<	<	
^	^	^	^	^	<	^	
^	>	v	<	v	<	<	
>	>	^	<	<	<	<	
v	>	^	<	<	<	v	
v	v	v	^	v	v	v	
<	<	<	>	v	>	>	
cvar = , 0.02347803696963524, 0.02350151360428754, 0.023498010024752602, 0.023480442725897888, 0.02347763079930587
==========
iteration 90
==========
weights [ 0.56319079  0.42042351 -0.60062614 -0.38119344]
expeced value MDP LP 55.084719770298356
demonstration
[(24, 3), (31, 1), (32, 1), (33, 1), (34, 1), (34, 2), (27, 3), (34, 1), (34, 2), (27, 1), (27, 1), (27, 0), (26, 1), (27, 3), (34, 2), (27, 1), (27, 1), (27, 0), (26, 1), (27, 3), (34, 1), (34, 1), (34, 2), (27, 1), (27, 1), (27, 3), (34, 2), (27, 1), (27, 1), (27, 0), (26, 1), (27, 0), (26, 1), (27, 3), (34, 2), (27, 1), (27, 3), (34, 1), (34, 1), (34, 1), (34, 1), (34, 2), (27, 3), (34, 2), (27, 3), (34, 2), (27, 0), (26, 1), (27, 0)]
[-0.57962098 -0.15265072 -0.04238681 -0.22534149]
w_map [ 0.47839983  0.20584284 -0.2891147  -0.02664263] loglik -37.74473163276707
accepted/total = 2188/3000 = 0.7293333333333333
-------
true weights [ 0.56319079  0.42042351 -0.60062614 -0.38119344]
features
1 	1 	3 	3 	3 	3 	3 	
3 	2 	2 	0 	2 	1 	1 	
2 	1 	1 	3 	0 	3 	3 	
0 	3 	1 	0 	2 	0 	0 	
2 	2 	3 	1 	0 	1 	0 	
2 	3 	1 	2 	1 	1 	3 	
1 	2 	2 	1 	1 	3 	1 	
optimal policy
>	v	v	v	>	v	v	
v	v	v	v	v	v	v	
v	>	v	v	>	v	v	
<	<	>	v	>	>	v	
^	^	>	>	>	^	>	
^	>	^	>	^	^	^	
^	>	>	>	^	^	^	
optimal values
54.22	54.34	53.68	54.18	53.38	54.31	54.31	
54.22	54.47	54.60	55.11	54.23	55.24	55.24	
55.16	55.62	55.76	55.10	55.38	55.37	55.37	
56.32	55.37	55.90	56.04	55.16	56.32	56.32	
55.16	54.22	55.09	56.04	56.18	56.18	56.32	
54.00	54.03	54.96	54.88	56.04	56.03	55.37	
53.88	53.45	54.60	55.76	55.90	55.09	55.24	
map_weights [ 0.47839983  0.20584284 -0.2891147  -0.02664263]
MAP reward
0.21	0.21	-0.03	-0.03	-0.03	-0.03	-0.03	
-0.03	-0.29	-0.29	0.48	-0.29	0.21	0.21	
-0.29	0.21	0.21	-0.03	0.48	-0.03	-0.03	
0.48	-0.03	0.21	0.48	-0.29	0.48	0.48	
-0.29	-0.29	-0.03	0.21	0.48	0.21	0.48	
-0.29	-0.03	0.21	-0.29	0.21	0.21	-0.03	
0.21	-0.29	-0.29	0.21	0.21	-0.03	0.21	
Map policy
v	<	>	v	v	v	v	
v	v	>	v	v	v	v	
v	v	v	>	>	v	v	
<	<	<	v	>	>	>	
^	^	>	>	>	>	^	
^	^	^	>	^	^	^	
^	^	>	>	^	^	^	
expeced value MDP LP 25.75065519656901
mean w [ 0.26524799  0.09879175 -0.31592012 -0.22722851]
Mean policy from posterior
v	<	v	v	v	v	v	
v	v	v	v	v	v	v	
v	v	v	v	>	v	v	
<	<	>	v	>	>	>	
^	^	>	>	>	>	^	
^	>	^	>	^	^	^	
^	>	>	>	^	^	^	
Mean rewards
0.10	0.10	-0.23	-0.23	-0.23	-0.23	-0.23	
-0.23	-0.32	-0.32	0.27	-0.32	0.10	0.10	
-0.32	0.10	0.10	-0.23	0.27	-0.23	-0.23	
0.27	-0.23	0.10	0.27	-0.32	0.27	0.27	
-0.32	-0.32	-0.23	0.10	0.27	0.10	0.27	
-0.32	-0.23	0.10	-0.32	0.10	0.10	-0.23	
0.10	-0.32	-0.32	0.10	0.10	-0.23	0.10	
mean = 0.026911473516875617, map = 0.18183972221776656
CVaR policy
v	<	>	v	>	v	v	
v	v	v	v	v	v	v	
v	v	v	v	>	v	v	
<	<	>	v	>	>	>	
^	^	>	>	>	>	^	
^	>	^	>	^	^	^	
^	>	>	>	^	^	^	
CVaR policy
v	<	>	v	v	v	v	
v	v	v	v	v	v	v	
v	v	v	v	>	v	v	
<	<	>	v	>	>	>	
^	^	>	>	>	>	>	
^	>	^	>	^	^	^	
^	>	>	>	^	^	^	
CVaR policy
v	<	v	v	v	v	v	
v	v	v	v	v	v	v	
v	v	v	v	>	v	v	
<	<	>	v	>	>	>	
^	^	>	>	>	>	>	
^	>	^	>	^	^	^	
^	>	>	>	^	^	^	
CVaR policy
v	<	v	v	v	v	v	
v	v	v	v	v	v	v	
v	v	v	v	>	v	v	
<	<	>	v	>	>	<	
^	^	>	>	>	>	^	
^	>	^	>	^	^	^	
^	>	>	>	^	^	^	
CVaR policy
v	<	v	v	v	v	v	
v	v	v	v	v	v	v	
v	v	v	v	>	v	v	
<	<	>	v	>	>	<	
^	^	>	>	>	>	^	
^	>	^	>	^	^	^	
^	>	>	>	^	^	^	
cvar = , 0.03392187974938565, 0.03549603636464127, 0.02691154588119815, 0.0269363138207126, 0.026912134951835753
==========
iteration 91
==========
weights [-0.53343529  0.23727694  0.63893951  0.50090194]
expeced value MDP LP 63.388128835039154
demonstration
[(24, 3), (31, 3), (38, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3)]
[-0.27934615 -0.13993877 -0.27040618 -0.3103089 ]
w_map [-0.02967969 -0.05073322  0.91465215  0.00493494] loglik -0.345006330342585
accepted/total = 2583/3000 = 0.861
-------
true weights [-0.53343529  0.23727694  0.63893951  0.50090194]
features
0 	3 	3 	3 	0 	2 	3 	
1 	0 	2 	2 	1 	1 	2 	
2 	1 	1 	3 	1 	0 	2 	
1 	0 	1 	0 	1 	3 	0 	
3 	2 	1 	2 	1 	2 	2 	
1 	1 	3 	3 	0 	1 	0 	
1 	3 	1 	2 	3 	1 	1 	
optimal policy
>	>	v	v	>	^	v	
v	>	>	<	<	>	>	
<	<	^	^	<	>	>	
^	^	^	v	>	v	v	
^	<	>	v	>	>	<	
^	>	>	v	v	^	^	
>	>	>	v	<	<	<	
optimal values
62.45	63.62	63.76	63.76	62.72	63.89	63.76	
63.49	62.72	63.89	63.89	63.49	63.49	63.89	
63.89	63.49	63.49	63.76	63.36	62.72	63.89	
63.49	62.32	63.09	62.59	63.36	63.76	62.72	
63.36	63.36	63.36	63.76	63.49	63.89	63.89	
62.96	63.22	63.62	63.76	62.58	63.49	62.72	
62.96	63.36	63.49	63.89	63.76	63.36	62.96	
map_weights [-0.02967969 -0.05073322  0.91465215  0.00493494]
MAP reward
-0.03	0.00	0.00	0.00	-0.03	0.91	0.00	
-0.05	-0.03	0.91	0.91	-0.05	-0.05	0.91	
0.91	-0.05	-0.05	0.00	-0.05	-0.03	0.91	
-0.05	-0.03	-0.05	-0.03	-0.05	0.00	-0.03	
0.00	0.91	-0.05	0.91	-0.05	0.91	0.91	
-0.05	-0.05	0.00	0.00	-0.03	-0.05	-0.03	
-0.05	0.00	-0.05	0.91	0.00	-0.05	-0.05	
Map policy
v	>	v	v	>	^	v	
v	>	>	<	<	>	>	
<	<	^	^	<	>	^	
^	^	^	v	>	v	^	
^	>	>	v	>	>	>	
^	>	>	v	v	^	^	
>	>	>	v	<	<	^	
expeced value MDP LP 41.54399905411382
mean w [-0.15985521 -0.20087636  0.42174606  0.06046929]
Mean policy from posterior
v	>	v	v	>	^	v	
v	>	>	<	<	>	>	
<	<	^	^	<	>	>	
^	^	^	v	>	v	^	
^	>	>	v	>	>	>	
^	>	>	v	v	^	^	
>	>	>	v	<	<	^	
Mean rewards
-0.16	0.06	0.06	0.06	-0.16	0.42	0.06	
-0.20	-0.16	0.42	0.42	-0.20	-0.20	0.42	
0.42	-0.20	-0.20	0.06	-0.20	-0.16	0.42	
-0.20	-0.16	-0.20	-0.16	-0.20	0.06	-0.16	
0.06	0.42	-0.20	0.42	-0.20	0.42	0.42	
-0.20	-0.20	0.06	0.06	-0.16	-0.20	-0.16	
-0.20	0.06	-0.20	0.42	0.06	-0.20	-0.20	
mean = 0.015401419999719224, map = 0.01540153804357658
CVaR policy
>	>	v	v	>	^	v	
v	>	>	<	<	>	v	
<	<	^	^	<	>	^	
^	^	^	v	>	v	^	
^	<	>	v	>	>	>	
^	>	>	v	v	^	^	
>	>	>	v	<	<	^	
CVaR policy
>	>	v	v	>	^	v	
v	>	>	<	<	>	>	
<	<	^	^	<	>	^	
^	^	^	v	>	v	^	
^	<	>	v	>	>	>	
^	>	>	v	v	^	^	
>	>	>	v	<	<	^	
CVaR policy
v	>	v	v	>	^	v	
v	>	>	<	<	>	>	
<	<	^	^	<	>	>	
^	^	^	v	>	v	^	
^	>	>	v	>	>	>	
^	>	>	v	v	^	^	
>	>	>	v	<	<	^	
CVaR policy
v	>	v	v	>	^	v	
v	>	>	<	<	>	>	
<	<	^	^	<	>	>	
^	^	^	v	>	v	^	
^	>	>	v	>	>	>	
^	>	>	v	v	^	^	
>	>	>	v	<	<	^	
CVaR policy
v	>	v	v	>	^	v	
v	>	>	<	<	>	>	
<	<	^	^	<	>	>	
^	^	^	v	>	v	^	
^	>	>	v	>	>	>	
^	>	>	v	v	^	^	
>	>	>	v	<	<	^	
cvar = , 0.012810559740557892, 0.012832486519094743, 0.015420017867569413, 0.01540175849630998, 0.015456444809558434
==========
iteration 92
==========
weights [0.54074328 0.52068441 0.62423467 0.21636897]
expeced value MDP LP 62.272714422967184
demonstration
[(24, 1), (25, 2), (18, 1), (19, 0), (18, 3), (25, 0), (24, 1), (25, 0), (24, 1), (25, 2), (18, 3), (25, 2), (18, 1), (19, 2), (12, 3), (19, 2), (12, 3), (19, 0), (18, 1), (19, 2), (12, 3), (19, 0), (18, 1), (19, 0), (18, 1), (19, 2), (12, 3), (19, 0), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 3), (25, 2), (18, 1), (19, 0), (18, 3), (25, 2), (18, 3), (25, 0), (24, 1), (25, 0), (24, 1), (25, 2), (18, 1), (19, 2), (12, 3)]
[-0.57525265 -0.00588983 -0.33631025 -0.08254726]
w_map [ 0.21882884 -0.02265891  0.66910459  0.08940767] loglik -27.032738570778747
accepted/total = 2753/3000 = 0.9176666666666666
-------
true weights [0.54074328 0.52068441 0.62423467 0.21636897]
features
2 	0 	3 	0 	2 	1 	0 	
1 	1 	2 	3 	3 	2 	0 	
0 	3 	1 	3 	2 	2 	3 	
0 	3 	1 	2 	2 	1 	2 	
2 	0 	1 	0 	3 	1 	3 	
2 	1 	2 	2 	2 	2 	1 	
2 	3 	2 	2 	0 	0 	3 	
optimal policy
<	<	<	>	^	v	v	
^	^	<	^	v	v	<	
v	<	v	v	>	<	<	
v	v	>	>	^	<	>	
v	<	v	v	v	v	^	
v	<	v	v	<	<	<	
<	<	v	<	<	^	<	
optimal values
62.42	62.34	61.93	62.34	62.42	62.32	62.26	
62.32	62.24	62.24	61.93	62.02	62.42	62.34	
62.26	61.85	62.22	62.02	62.42	62.42	62.02	
62.34	61.93	62.32	62.42	62.42	62.32	62.42	
62.42	62.34	62.32	62.34	62.02	62.32	62.02	
62.42	62.32	62.42	62.42	62.42	62.42	62.32	
62.42	62.02	62.42	62.42	62.34	62.34	61.93	
map_weights [ 0.21882884 -0.02265891  0.66910459  0.08940767]
MAP reward
0.67	0.22	0.09	0.22	0.67	-0.02	0.22	
-0.02	-0.02	0.67	0.09	0.09	0.67	0.22	
0.22	0.09	-0.02	0.09	0.67	0.67	0.09	
0.22	0.09	-0.02	0.67	0.67	-0.02	0.67	
0.67	0.22	-0.02	0.22	0.09	-0.02	0.09	
0.67	-0.02	0.67	0.67	0.67	0.67	-0.02	
0.67	0.09	0.67	0.67	0.22	0.22	0.09	
Map policy
<	<	<	>	^	v	v	
^	^	^	^	v	v	<	
v	<	>	v	v	<	<	
v	v	>	>	^	<	>	
v	<	v	v	v	v	^	
v	>	v	v	<	<	<	
<	<	v	<	<	^	<	
expeced value MDP LP 34.15253759444197
mean w [-0.09323655 -0.07625152  0.34551824 -0.1930166 ]
Mean policy from posterior
<	<	<	>	^	v	<	
^	<	v	^	v	v	<	
^	>	v	v	>	<	<	
v	>	>	>	^	<	>	
v	<	v	v	v	v	^	
v	<	v	v	<	<	<	
<	<	v	<	<	^	^	
Mean rewards
0.35	-0.09	-0.19	-0.09	0.35	-0.08	-0.09	
-0.08	-0.08	0.35	-0.19	-0.19	0.35	-0.09	
-0.09	-0.19	-0.08	-0.19	0.35	0.35	-0.19	
-0.09	-0.19	-0.08	0.35	0.35	-0.08	0.35	
0.35	-0.09	-0.08	-0.09	-0.19	-0.08	-0.19	
0.35	-0.08	0.35	0.35	0.35	0.35	-0.08	
0.35	-0.19	0.35	0.35	-0.09	-0.09	-0.19	
mean = 0.003234061643318853, map = 0.01229661328910936
CVaR policy
<	<	<	>	^	v	v	
^	^	<	^	>	v	<	
v	<	v	v	v	<	<	
v	v	>	>	^	<	>	
v	<	v	v	v	v	^	
v	>	v	v	<	<	<	
<	>	v	<	^	^	<	
CVaR policy
<	<	<	>	^	v	<	
^	<	v	^	v	v	<	
^	>	v	>	>	<	<	
v	>	>	>	^	<	>	
v	<	v	v	v	v	^	
v	<	v	v	<	<	<	
<	<	v	<	^	^	^	
CVaR policy
<	<	<	>	^	v	<	
^	<	v	^	>	v	<	
^	>	v	v	>	<	<	
v	>	>	>	^	<	>	
v	<	v	v	v	v	^	
v	<	v	v	<	<	<	
<	<	v	<	<	^	^	
CVaR policy
<	<	<	>	^	v	<	
^	<	v	^	>	v	<	
^	>	v	v	>	<	<	
v	>	>	>	^	<	>	
v	<	v	v	v	v	^	
v	<	v	v	<	<	<	
<	<	v	<	<	^	^	
CVaR policy
<	<	<	>	^	v	<	
^	<	v	^	v	v	<	
^	>	v	v	>	<	<	
v	>	>	>	^	<	>	
v	<	v	v	v	v	^	
v	<	v	v	<	<	<	
<	<	v	<	<	^	^	
cvar = , 5.2758608148906205e-08, 0.003234126676616711, 0.0032349910634366097, 0.003235434031466866, 0.0032340602443952093
==========
iteration 93
==========
weights [ 0.7976449  -0.55034766 -0.23571966 -0.072913  ]
expeced value MDP LP 78.41921404240674
demonstration
[(24, 3), (31, 3), (38, 0), (37, 0), (36, 1), (37, 1), (38, 0), (37, 0), (36, 1), (37, 0), (36, 0), (35, 1), (36, 1), (37, 0), (36, 1), (37, 1), (38, 0), (37, 1), (38, 0), (37, 0), (36, 0), (35, 1), (36, 1), (37, 1), (38, 1), (39, 0), (38, 1), (39, 0), (38, 0), (37, 0), (36, 0), (35, 0), (35, 0), (35, 1), (36, 0), (35, 0), (35, 0), (35, 1), (36, 0), (35, 0), (35, 0), (35, 1), (36, 0), (35, 1), (36, 1), (37, 0), (36, 1), (37, 1), (38, 1)]
[-0.46533775 -0.36082215 -0.05042688 -0.12341322]
w_map [ 0.66090521 -0.12974774 -0.12138402  0.08796303] loglik -31.191622344818825
accepted/total = 2441/3000 = 0.8136666666666666
-------
true weights [ 0.7976449  -0.55034766 -0.23571966 -0.072913  ]
features
1 	2 	1 	3 	1 	0 	0 	
1 	3 	0 	1 	1 	3 	3 	
1 	1 	2 	2 	0 	0 	1 	
0 	1 	1 	1 	1 	3 	1 	
2 	3 	1 	3 	2 	3 	3 	
0 	0 	0 	0 	0 	3 	2 	
3 	1 	1 	2 	2 	0 	2 	
optimal policy
v	v	v	>	>	>	>	
v	>	v	v	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
optimal values
75.76	75.87	76.40	77.56	78.42	79.76	79.76	
77.08	76.88	77.73	77.39	78.42	78.89	78.89	
78.42	77.08	77.71	78.73	79.76	79.76	78.42	
79.76	78.42	77.08	77.55	78.42	78.89	77.55	
78.73	78.89	78.42	78.89	78.73	78.03	77.18	
79.76	79.76	79.76	79.76	79.76	78.89	77.87	
78.89	78.42	78.42	78.73	78.73	79.76	78.73	
map_weights [ 0.66090521 -0.12974774 -0.12138402  0.08796303]
MAP reward
-0.13	-0.12	-0.13	0.09	-0.13	0.66	0.66	
-0.13	0.09	0.66	-0.13	-0.13	0.09	0.09	
-0.13	-0.13	-0.12	-0.12	0.66	0.66	-0.13	
0.66	-0.13	-0.13	-0.13	-0.13	0.09	-0.13	
-0.12	0.09	-0.13	0.09	-0.12	0.09	0.09	
0.66	0.66	0.66	0.66	0.66	0.09	-0.12	
0.09	-0.13	-0.13	-0.12	-0.12	0.66	-0.12	
Map policy
v	v	>	>	>	>	>	
v	>	v	v	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
expeced value MDP LP 34.15539260307891
mean w [ 0.34757079 -0.25876834 -0.20530325  0.04723786]
Mean policy from posterior
v	v	>	>	>	>	>	
v	>	v	v	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
Mean rewards
-0.26	-0.21	-0.26	0.05	-0.26	0.35	0.35	
-0.26	0.05	0.35	-0.26	-0.26	0.05	0.05	
-0.26	-0.26	-0.21	-0.21	0.35	0.35	-0.26	
0.35	-0.26	-0.26	-0.26	-0.26	0.05	-0.26	
-0.21	0.05	-0.26	0.05	-0.21	0.05	0.05	
0.35	0.35	0.35	0.35	0.35	0.05	-0.21	
0.05	-0.26	-0.26	-0.21	-0.21	0.35	-0.21	
mean = 0.003419299028621481, map = 0.0034204264950687957
CVaR policy
v	v	>	>	>	>	>	
v	>	>	>	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
CVaR policy
v	v	>	>	>	>	>	
v	>	>	>	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
CVaR policy
v	v	>	>	>	>	<	
v	>	v	v	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
CVaR policy
v	v	>	>	>	>	>	
v	>	v	v	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
CVaR policy
v	v	>	>	>	>	>	
v	>	v	v	v	^	^	
v	v	>	>	>	<	<	
<	<	v	v	^	^	<	
v	v	v	v	v	v	<	
<	<	<	<	<	<	<	
^	^	^	^	^	v	<	
cvar = , 0.04736477724377153, 0.04735004774101981, 0.0036085126419180824, 0.003433894407876892, 0.003423394514683764
==========
iteration 94
==========
weights [-0.14607925 -0.47373744 -0.51379926  0.70017427]
expeced value MDP LP 68.7658810043972
demonstration
[(24, 0), (23, 0), (22, 0), (21, 0), (21, 1), (22, 1), (23, 0), (22, 0), (21, 1), (22, 1), (23, 0), (22, 0), (21, 0), (21, 1), (22, 0), (21, 1), (22, 1), (23, 2), (16, 1), (17, 0), (16, 2), (9, 3), (16, 1), (17, 0), (16, 1), (17, 0), (16, 3), (23, 0), (22, 1), (23, 0), (22, 0), (21, 1), (22, 0), (21, 0), (21, 0), (21, 0), (21, 1), (22, 1), (23, 0), (22, 0), (21, 1), (22, 1), (23, 0), (22, 1), (23, 2), (16, 3), (23, 2), (16, 2), (9, 3)]
[-0.42200505 -0.15446172 -0.16983038 -0.25370285]
w_map [ 1.45205177e-04 -1.84079002e-01 -2.14333369e-03  8.13632459e-01] loglik -33.33673099107182
accepted/total = 2753/3000 = 0.9176666666666666
-------
true weights [-0.14607925 -0.47373744 -0.51379926  0.70017427]
features
2 	2 	2 	1 	0 	0 	1 	
1 	0 	3 	2 	3 	1 	3 	
0 	2 	3 	3 	1 	0 	2 	
3 	3 	3 	1 	3 	2 	0 	
0 	2 	1 	0 	2 	3 	1 	
0 	0 	1 	0 	0 	0 	3 	
0 	1 	1 	2 	3 	3 	3 	
optimal policy
v	v	v	v	v	>	v	
v	>	v	v	v	>	>	
v	v	v	<	<	<	^	
>	>	<	<	<	v	v	
^	^	^	^	>	v	v	
^	^	^	>	v	>	v	
^	^	>	>	>	>	>	
optimal values
66.81	67.97	68.80	67.64	68.02	68.01	68.84	
68.01	69.17	70.02	68.80	68.86	68.84	70.02	
69.17	68.80	70.02	70.02	68.84	68.01	68.80	
70.02	70.02	70.02	68.84	68.86	67.97	68.01	
69.17	68.80	68.84	68.01	67.97	69.18	68.84	
68.33	67.97	67.68	68.33	69.17	69.17	70.02	
67.50	66.82	67.64	68.80	70.02	70.02	70.02	
map_weights [ 1.45205177e-04 -1.84079002e-01 -2.14333369e-03  8.13632459e-01]
MAP reward
-0.00	-0.00	-0.00	-0.18	0.00	0.00	-0.18	
-0.18	0.00	0.81	-0.00	0.81	-0.18	0.81	
0.00	-0.00	0.81	0.81	-0.18	0.00	-0.00	
0.81	0.81	0.81	-0.18	0.81	-0.00	0.00	
0.00	-0.00	-0.18	0.00	-0.00	0.81	-0.18	
0.00	0.00	-0.18	0.00	0.00	0.00	0.81	
0.00	-0.18	-0.18	-0.00	0.81	0.81	0.81	
Map policy
>	v	v	<	v	v	v	
v	>	v	v	<	>	>	
v	>	v	<	<	>	^	
<	<	<	<	<	v	^	
^	^	^	^	>	v	v	
^	^	^	>	v	>	v	
^	^	>	>	>	>	>	
expeced value MDP LP 32.07794814027566
mean w [-0.09639125 -0.09168018 -0.1784489   0.32605091]
Mean policy from posterior
v	v	v	v	v	v	v	
v	>	v	v	v	>	>	
v	>	v	<	<	<	^	
>	<	<	^	<	v	v	
^	^	^	^	>	>	v	
^	^	^	>	v	>	v	
^	>	>	>	>	>	>	
Mean rewards
-0.18	-0.18	-0.18	-0.09	-0.10	-0.10	-0.09	
-0.09	-0.10	0.33	-0.18	0.33	-0.09	0.33	
-0.10	-0.18	0.33	0.33	-0.09	-0.10	-0.18	
0.33	0.33	0.33	-0.09	0.33	-0.18	-0.10	
-0.10	-0.18	-0.09	-0.10	-0.18	0.33	-0.09	
-0.10	-0.10	-0.09	-0.10	-0.10	-0.10	0.33	
-0.10	-0.09	-0.09	-0.18	0.33	0.33	0.33	
mean = 0.026347731317656553, map = 0.004040493615079299
CVaR policy
>	v	v	v	v	>	v	
v	>	v	<	<	>	>	
v	>	v	<	<	>	^	
>	<	<	<	<	v	^	
^	^	^	^	>	v	v	
^	^	^	>	v	v	v	
^	^	>	>	>	>	>	
CVaR policy
>	v	v	<	v	>	v	
v	>	v	v	<	>	>	
v	v	v	<	<	>	^	
>	<	<	<	<	v	^	
^	^	^	^	>	v	v	
^	^	^	>	v	>	v	
^	^	>	>	>	>	>	
CVaR policy
v	v	v	v	v	>	v	
v	>	v	<	v	>	>	
v	>	>	<	<	<	^	
<	<	<	^	<	v	v	
^	^	^	^	>	v	v	
^	^	^	>	v	v	v	
^	^	>	>	>	>	>	
CVaR policy
v	v	v	v	v	>	v	
v	>	v	v	v	>	>	
v	>	v	<	<	<	^	
>	<	<	^	<	v	v	
^	^	^	^	>	v	v	
^	^	^	>	v	v	v	
^	^	>	>	>	>	>	
CVaR policy
v	v	v	v	v	>	v	
v	>	v	v	v	>	>	
v	>	v	<	<	<	^	
>	<	<	^	<	v	v	
^	^	^	^	>	v	v	
^	^	^	>	v	v	v	
^	^	>	>	>	>	>	
cvar = , 0.004038971937049496, 0.004039765946501461, 9.297969788235605e-05, 3.664884164322757e-05, 8.685021641952062e-05
==========
iteration 95
==========
weights [-0.54099776 -0.21286853 -0.7979314   0.15910342]
expeced value MDP LP 14.885158158227686
demonstration
[(24, 3), (31, 1), (32, 3), (39, 3), (46, 3), (46, 2), (39, 1), (40, 0), (39, 1), (40, 0), (39, 1), (40, 0), (39, 1), (40, 0), (39, 1), (40, 0), (39, 3), (46, 0), (45, 3), (45, 3), (45, 3), (45, 3), (45, 3), (45, 1), (46, 2), (39, 1), (40, 0), (39, 1), (40, 0), (39, 3), (46, 0), (45, 3), (45, 1), (46, 3), (46, 3), (46, 2), (39, 1), (40, 0), (39, 1), (40, 0), (39, 1), (40, 0), (39, 3), (46, 0), (45, 1), (46, 0), (45, 3), (45, 1), (46, 0)]
[-0.23975939 -0.49886958 -0.05205705 -0.20931398]
w_map [ 0.03558417 -0.31049796 -0.25116281  0.40275506] loglik -29.413414689354795
accepted/total = 2511/3000 = 0.837
-------
true weights [-0.54099776 -0.21286853 -0.7979314   0.15910342]
features
1 	0 	3 	1 	1 	0 	2 	
1 	0 	0 	3 	0 	0 	1 	
2 	0 	3 	1 	0 	0 	2 	
2 	0 	2 	0 	1 	3 	0 	
2 	2 	1 	3 	0 	2 	0 	
3 	3 	2 	2 	3 	3 	0 	
0 	2 	1 	3 	3 	2 	0 	
optimal policy
>	>	^	<	<	<	<	
^	^	^	^	<	<	<	
>	>	^	^	<	v	v	
v	v	^	v	v	v	<	
v	v	>	>	v	v	v	
<	<	<	v	v	<	<	
^	^	>	v	<	<	^	
optimal values
14.85	15.21	15.91	15.54	15.17	14.48	13.53	
14.48	14.52	15.21	15.54	14.85	14.16	13.80	
13.58	14.52	15.22	15.17	14.48	14.27	13.33	
14.01	14.26	14.27	14.52	14.85	14.96	14.27	
14.95	14.95	14.85	15.22	15.21	14.95	14.52	
15.91	15.91	14.95	14.95	15.91	15.91	15.21	
15.21	14.95	15.54	15.91	15.91	14.95	14.52	
map_weights [ 0.03558417 -0.31049796 -0.25116281  0.40275506]
MAP reward
-0.31	0.04	0.40	-0.31	-0.31	0.04	-0.25	
-0.31	0.04	0.04	0.40	0.04	0.04	-0.31	
-0.25	0.04	0.40	-0.31	0.04	0.04	-0.25	
-0.25	0.04	-0.25	0.04	-0.31	0.40	0.04	
-0.25	-0.25	-0.31	0.40	0.04	-0.25	0.04	
0.40	0.40	-0.25	-0.25	0.40	0.40	0.04	
0.04	-0.25	-0.31	0.40	0.40	-0.25	0.04	
Map policy
>	>	^	<	<	v	<	
>	^	^	<	<	<	<	
>	>	^	<	^	v	v	
v	v	^	v	v	v	<	
v	v	>	>	v	v	v	
<	<	<	v	v	<	<	
^	^	>	v	<	<	^	
expeced value MDP LP 33.573995831036896
mean w [-0.00286091 -0.20434112 -0.23563206  0.34244743]
Mean policy from posterior
>	>	^	<	<	v	<	
>	^	^	<	<	<	<	
>	>	^	<	^	v	v	
v	v	^	v	v	v	<	
v	v	>	>	v	v	v	
<	<	<	v	v	<	<	
^	^	>	v	<	<	^	
Mean rewards
-0.20	-0.00	0.34	-0.20	-0.20	-0.00	-0.24	
-0.20	-0.00	-0.00	0.34	-0.00	-0.00	-0.20	
-0.24	-0.00	0.34	-0.20	-0.00	-0.00	-0.24	
-0.24	-0.00	-0.24	-0.00	-0.20	0.34	-0.00	
-0.24	-0.24	-0.20	0.34	-0.00	-0.24	-0.00	
0.34	0.34	-0.24	-0.24	0.34	0.34	-0.00	
-0.00	-0.24	-0.20	0.34	0.34	-0.24	-0.00	
mean = 0.10600904445616521, map = 0.10600904391102439
CVaR policy
>	>	^	<	v	v	<	
>	^	^	<	<	<	<	
>	>	^	<	^	v	v	
v	^	^	v	v	v	v	
v	v	>	>	v	v	v	
<	<	<	v	v	<	<	
^	^	>	v	<	<	^	
CVaR policy
>	>	^	<	<	v	<	
>	^	^	<	<	<	<	
>	>	^	<	^	v	v	
v	v	^	v	v	v	<	
v	v	>	>	v	v	v	
<	<	<	v	v	<	<	
^	^	>	v	<	<	^	
CVaR policy
>	>	^	<	<	v	<	
>	^	^	<	<	<	<	
>	>	^	<	^	v	v	
v	v	^	v	v	v	<	
v	v	>	>	v	v	v	
<	<	<	v	v	<	<	
^	^	>	v	<	<	^	
CVaR policy
>	>	^	<	<	v	<	
>	^	^	<	<	<	<	
>	>	^	<	^	v	v	
v	v	^	v	v	v	<	
v	v	>	>	v	v	v	
<	<	<	>	v	<	<	
^	^	>	v	v	<	^	
CVaR policy
>	>	^	<	<	v	<	
>	^	^	<	<	<	<	
>	>	^	<	^	v	v	
v	v	^	v	v	v	<	
v	v	>	>	v	v	v	
<	<	<	>	v	<	<	
^	^	>	v	v	<	^	
cvar = , 0.15310764647492192, 0.10600947597126442, 0.10601059774809052, 0.10600909837631001, 0.10602091144992087
==========
iteration 96
==========
weights [ 0.40563662 -0.25973646  0.51924904 -0.70595775]
expeced value MDP LP 51.25223836320424
demonstration
[(24, 2), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1)]
[-0.06654302 -0.16054421 -0.6855222  -0.08739057]
w_map [-0.35031023 -0.28992132  0.06476489 -0.29500356] loglik 0.0
accepted/total = 2769/3000 = 0.923
-------
true weights [ 0.40563662 -0.25973646  0.51924904 -0.70595775]
features
3 	3 	3 	3 	3 	3 	3 	
3 	3 	0 	1 	0 	2 	3 	
0 	0 	2 	2 	0 	0 	2 	
1 	3 	1 	0 	2 	0 	3 	
0 	2 	1 	1 	3 	2 	0 	
0 	1 	2 	2 	3 	0 	1 	
2 	1 	1 	2 	0 	2 	0 	
optimal policy
v	v	v	v	v	v	v	
v	>	v	v	>	v	v	
>	>	>	<	<	>	>	
v	^	^	^	^	<	^	
v	<	v	v	>	v	<	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
optimal values
49.27	49.38	50.59	49.93	50.48	50.59	49.49	
50.48	50.59	51.81	51.15	51.70	51.81	50.70	
51.70	51.81	51.92	51.92	51.81	51.81	51.92	
50.92	50.59	51.15	51.81	51.81	51.70	50.70	
51.70	51.70	51.15	51.15	50.59	51.81	51.70	
51.81	51.15	51.92	51.92	50.70	51.81	51.03	
51.92	51.15	51.15	51.92	51.81	51.92	51.81	
map_weights [-0.35031023 -0.28992132  0.06476489 -0.29500356]
MAP reward
-0.30	-0.30	-0.30	-0.30	-0.30	-0.30	-0.30	
-0.30	-0.30	-0.35	-0.29	-0.35	0.06	-0.30	
-0.35	-0.35	0.06	0.06	-0.35	-0.35	0.06	
-0.29	-0.30	-0.29	-0.35	0.06	-0.35	-0.30	
-0.35	0.06	-0.29	-0.29	-0.30	0.06	-0.35	
-0.35	-0.29	0.06	0.06	-0.30	-0.35	-0.29	
0.06	-0.29	-0.29	0.06	-0.35	0.06	-0.35	
Map policy
>	v	v	v	<	v	v	
>	v	v	v	<	>	v	
>	>	>	<	<	>	>	
>	v	^	^	<	>	^	
>	>	v	v	<	v	^	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
expeced value MDP LP 39.26799920280695
mean w [-0.09786845 -0.08674328  0.39894607 -0.05358579]
Mean policy from posterior
>	v	v	v	>	v	v	
>	v	v	v	>	>	v	
>	>	>	<	<	>	>	
>	v	^	^	<	>	^	
>	v	v	v	v	v	^	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
Mean rewards
-0.05	-0.05	-0.05	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.05	-0.10	-0.09	-0.10	0.40	-0.05	
-0.10	-0.10	0.40	0.40	-0.10	-0.10	0.40	
-0.09	-0.05	-0.09	-0.10	0.40	-0.10	-0.05	
-0.10	0.40	-0.09	-0.09	-0.05	0.40	-0.10	
-0.10	-0.09	0.40	0.40	-0.05	-0.10	-0.09	
0.40	-0.09	-0.09	0.40	-0.10	0.40	-0.10	
mean = 0.296889257397396, map = 0.2704255503266779
CVaR policy
v	v	v	v	v	v	v	
v	>	v	v	>	v	v	
>	>	>	<	<	>	>	
v	^	^	^	^	<	^	
v	<	v	v	^	v	<	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
CVaR policy
v	v	v	v	v	v	v	
v	>	v	v	>	v	v	
>	>	>	<	<	>	>	
v	^	^	^	^	<	^	
v	v	v	v	^	v	<	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
CVaR policy
>	v	v	v	>	v	v	
>	>	v	v	>	>	v	
>	>	>	<	<	>	>	
>	^	^	^	^	<	^	
v	v	v	v	^	v	<	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
CVaR policy
>	v	v	v	>	v	v	
>	>	v	v	>	>	v	
>	>	>	<	<	>	>	
>	^	^	^	<	>	^	
v	v	v	v	v	v	^	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
CVaR policy
>	v	v	v	>	v	v	
>	v	v	v	>	>	v	
>	>	>	<	<	>	>	
>	v	^	^	<	>	^	
>	>	v	v	v	v	^	
v	>	>	v	<	v	v	
<	<	>	v	<	v	<	
cvar = , -5.137663521281866e-07, 0.011251467359400635, 0.18867328612969203, 0.2571630390632862, 0.29688874471811744
==========
iteration 97
==========
weights [0.76702529 0.60006062 0.14111081 0.17800898]
expeced value MDP LP 76.29914897280885
demonstration
[(24, 2), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 2), (10, 3), (17, 2), (10, 3), (17, 2), (10, 3), (17, 0), (16, 1), (17, 2), (10, 3), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 2), (10, 3), (17, 0), (16, 1), (17, 2), (10, 3), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 0), (16, 1), (17, 2), (10, 3), (17, 0), (16, 1), (17, 2), (10, 3), (17, 0), (16, 1)]
[-0.1068405  -0.63515326 -0.22455484 -0.0334514 ]
w_map [ 0.67010445 -0.03411217  0.10491733 -0.19086605] loglik -16.635532026215515
accepted/total = 2831/3000 = 0.9436666666666667
-------
true weights [0.76702529 0.60006062 0.14111081 0.17800898]
features
2 	0 	2 	2 	0 	1 	1 	
2 	0 	1 	0 	2 	2 	1 	
3 	2 	0 	0 	2 	2 	1 	
0 	2 	2 	2 	3 	3 	0 	
2 	3 	3 	3 	0 	0 	2 	
2 	2 	3 	3 	2 	0 	1 	
0 	0 	1 	0 	2 	3 	0 	
optimal policy
>	^	<	v	^	<	<	
>	^	<	v	<	^	v	
v	^	>	<	<	>	v	
<	<	^	^	v	v	>	
^	v	>	>	>	v	<	
v	v	v	v	>	^	v	
<	<	<	v	<	>	>	
optimal values
76.08	76.70	76.08	76.08	76.70	76.54	76.37	
76.08	76.70	76.54	76.70	76.08	75.91	76.37	
76.11	76.08	76.70	76.70	76.08	75.91	76.54	
76.70	76.08	76.08	76.08	76.11	76.11	76.70	
76.08	75.49	75.53	76.11	76.70	76.70	76.08	
76.08	76.08	75.95	76.11	76.08	76.70	76.54	
76.70	76.70	76.54	76.70	76.08	76.11	76.70	
map_weights [ 0.67010445 -0.03411217  0.10491733 -0.19086605]
MAP reward
0.10	0.67	0.10	0.10	0.67	-0.03	-0.03	
0.10	0.67	-0.03	0.67	0.10	0.10	-0.03	
-0.19	0.10	0.67	0.67	0.10	0.10	-0.03	
0.67	0.10	0.10	0.10	-0.19	-0.19	0.67	
0.10	-0.19	-0.19	-0.19	0.67	0.67	0.10	
0.10	0.10	-0.19	-0.19	0.10	0.67	-0.03	
0.67	0.67	-0.03	0.67	0.10	-0.19	0.67	
Map policy
>	^	<	v	^	<	<	
>	^	<	v	<	<	v	
v	^	>	^	<	<	v	
<	<	^	^	v	v	>	
^	v	^	>	>	v	<	
v	v	<	v	>	^	v	
<	<	<	v	<	>	>	
expeced value MDP LP 38.951453041341715
mean w [ 0.39354133 -0.09394327 -0.11900104 -0.03568965]
Mean policy from posterior
>	^	<	v	^	<	<	
>	^	<	v	<	^	v	
v	^	>	<	<	v	v	
<	<	^	^	v	v	>	
^	v	>	>	>	v	<	
v	v	>	v	>	^	v	
<	<	<	v	<	>	>	
Mean rewards
-0.12	0.39	-0.12	-0.12	0.39	-0.09	-0.09	
-0.12	0.39	-0.09	0.39	-0.12	-0.12	-0.09	
-0.04	-0.12	0.39	0.39	-0.12	-0.12	-0.09	
0.39	-0.12	-0.12	-0.12	-0.04	-0.04	0.39	
-0.12	-0.04	-0.04	-0.04	0.39	0.39	-0.12	
-0.12	-0.12	-0.04	-0.04	-0.12	0.39	-0.09	
0.39	0.39	-0.09	0.39	-0.12	-0.04	0.39	
mean = 0.017054333449991077, map = 0.02856353934042488
CVaR policy
>	^	<	v	^	<	<	
>	^	<	v	<	^	v	
v	^	>	^	<	>	v	
<	<	^	^	v	v	>	
^	v	^	>	>	v	<	
v	v	v	v	^	^	v	
<	<	<	v	<	>	>	
CVaR policy
>	^	<	v	^	<	<	
>	^	>	v	<	^	v	
v	>	>	^	<	v	v	
<	<	^	^	v	v	>	
^	v	>	>	>	v	<	
v	v	>	v	>	^	v	
<	<	<	v	<	>	>	
CVaR policy
>	^	<	v	^	<	<	
>	^	<	v	<	^	v	
v	^	>	^	<	v	v	
<	<	^	^	v	v	>	
^	v	>	>	>	v	<	
v	v	>	v	>	^	v	
<	<	<	v	<	>	>	
CVaR policy
>	^	<	v	^	<	<	
>	^	<	v	<	^	v	
v	^	>	^	<	v	v	
<	<	^	^	v	v	>	
^	v	>	>	>	v	<	
v	v	>	v	>	^	v	
<	<	<	v	<	>	>	
CVaR policy
>	^	<	v	^	<	<	
>	^	<	v	<	^	v	
v	^	>	^	<	v	v	
<	<	^	^	v	v	>	
^	v	>	>	>	v	<	
v	v	>	v	>	^	v	
<	<	<	v	<	>	>	
cvar = , 0.0007455211497244818, 0.017053523254944025, 0.017056013153862182, 0.017070354623399453, 0.017054384610744933
==========
iteration 98
==========
weights [ 0.62396586 -0.24810212  0.37342348  0.64005222]
expeced value MDP LP 63.649765949921864
demonstration
[(24, 2), (17, 1), (18, 1), (19, 3), (26, 3), (33, 1), (34, 1), (34, 0), (33, 1), (34, 1), (34, 0), (33, 1), (34, 1), (34, 1), (34, 1), (34, 1), (34, 1), (34, 1), (34, 0), (33, 1), (34, 0), (33, 1), (34, 1), (34, 1), (34, 1), (34, 1), (34, 0), (33, 1), (34, 0), (33, 1), (34, 0), (33, 1), (34, 1), (34, 0), (33, 1), (34, 1), (34, 0), (33, 1), (34, 0), (33, 1), (34, 1), (34, 1), (34, 1), (34, 1), (34, 0), (33, 1), (34, 1), (34, 1), (34, 0)]
[-0.13583248 -0.32415626 -0.26193983 -0.27807143]
w_map [ 0.04312479 -0.59094267 -0.19172571  0.17420683] loglik -22.180842576103487
accepted/total = 1323/3000 = 0.441
-------
true weights [ 0.62396586 -0.24810212  0.37342348  0.64005222]
features
2 	3 	2 	1 	0 	3 	2 	
3 	2 	3 	0 	1 	1 	2 	
0 	0 	1 	2 	0 	0 	2 	
1 	2 	2 	1 	1 	0 	2 	
1 	2 	0 	1 	0 	3 	3 	
3 	2 	2 	2 	2 	0 	2 	
0 	3 	1 	1 	0 	0 	0 	
optimal policy
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	>	v	v	
v	v	<	>	>	>	<	
<	<	<	>	^	^	^	
^	v	<	>	>	^	<	
optimal values
63.74	64.01	63.74	63.10	63.99	64.01	63.74	
64.01	63.74	63.74	63.73	63.10	63.12	63.47	
63.99	63.97	63.09	63.69	63.96	63.97	63.71	
63.10	63.71	63.44	62.81	63.10	63.99	63.74	
63.12	63.47	63.46	63.10	63.99	64.01	64.01	
64.01	63.74	63.47	63.46	63.72	63.99	63.74	
63.99	64.01	63.12	63.07	63.96	63.97	63.96	
map_weights [ 0.04312479 -0.59094267 -0.19172571  0.17420683]
MAP reward
-0.19	0.17	-0.19	-0.59	0.04	0.17	-0.19	
0.17	-0.19	0.17	0.04	-0.59	-0.59	-0.19	
0.04	0.04	-0.59	-0.19	0.04	0.04	-0.19	
-0.59	-0.19	-0.19	-0.59	-0.59	0.04	-0.19	
-0.59	-0.19	0.04	-0.59	0.04	0.17	0.17	
0.17	-0.19	-0.19	-0.19	-0.19	0.04	-0.19	
0.04	0.17	-0.59	-0.59	0.04	0.04	0.04	
Map policy
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	>	v	v	
v	v	<	>	>	>	>	
<	<	<	>	^	^	^	
^	v	<	>	>	^	<	
expeced value MDP LP 18.856696598540342
mean w [ 0.1043648  -0.49118857 -0.12633439  0.19262339]
Mean policy from posterior
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	>	v	v	
v	v	<	>	>	>	>	
<	<	<	>	^	^	^	
^	v	<	>	>	^	<	
Mean rewards
-0.13	0.19	-0.13	-0.49	0.10	0.19	-0.13	
0.19	-0.13	0.19	0.10	-0.49	-0.49	-0.13	
0.10	0.10	-0.49	-0.13	0.10	0.10	-0.13	
-0.49	-0.13	-0.13	-0.49	-0.49	0.10	-0.13	
-0.49	-0.13	0.10	-0.49	0.10	0.19	0.19	
0.19	-0.13	-0.13	-0.13	-0.13	0.10	-0.13	
0.10	0.19	-0.49	-0.49	0.10	0.10	0.10	
mean = -6.100194127611758e-09, map = -1.1987246750777558e-08
CVaR policy
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	v	v	v	
v	v	v	>	>	>	>	
<	<	<	>	^	^	^	
>	v	<	>	>	^	<	
CVaR policy
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	v	v	v	
v	v	v	>	>	>	>	
<	<	<	>	^	^	^	
^	v	<	>	>	^	<	
CVaR policy
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	v	v	v	
v	v	<	>	>	>	>	
<	<	<	>	^	^	^	
^	v	<	>	>	^	<	
CVaR policy
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	v	v	v	
v	v	<	>	>	>	>	
<	<	<	>	^	^	^	
^	v	<	>	>	^	<	
CVaR policy
>	^	<	>	>	^	<	
<	^	<	<	^	^	^	
^	<	<	>	>	v	<	
^	^	<	^	v	v	v	
v	v	<	>	>	>	>	
<	<	<	>	^	^	^	
^	v	<	>	>	^	<	
cvar = , 4.2456143489744136e-08, -1.2459025811040192e-08, -1.3261463038816146e-08, 8.678860723421167e-06, 2.3148111338855415e-06
==========
iteration 99
==========
weights [ 0.16259108 -0.69162232 -0.24317244 -0.66037101]
expeced value MDP LP 15.071515724244675
demonstration
[(24, 1), (25, 1), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 0), (26, 1), (27, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 0), (26, 1), (27, 1), (27, 1), (27, 1), (27, 0)]
[-0.26389978 -0.13820331 -0.50068449 -0.09721242]
w_map [ 0.7360333  -0.00101467 -0.00166074  0.2612913 ] loglik -22.873856662058643
accepted/total = 2579/3000 = 0.8596666666666667
-------
true weights [ 0.16259108 -0.69162232 -0.24317244 -0.66037101]
features
2 	2 	1 	2 	2 	2 	2 	
2 	0 	3 	1 	3 	1 	3 	
0 	3 	3 	1 	2 	3 	2 	
0 	1 	0 	1 	3 	0 	0 	
1 	2 	1 	3 	2 	2 	1 	
1 	1 	1 	2 	0 	3 	0 	
1 	1 	2 	2 	3 	2 	0 	
optimal policy
v	v	<	<	>	>	v	
v	<	<	<	v	v	v	
v	<	<	>	>	v	v	
^	<	<	>	>	>	>	
^	<	^	>	>	^	^	
^	^	>	>	^	>	v	
^	>	>	^	>	>	^	
optimal values
15.45	15.46	14.61	14.22	13.87	14.25	14.64	
15.85	15.86	15.04	14.20	14.23	14.59	15.03	
16.26	15.44	14.62	14.20	15.04	15.44	15.85	
16.26	15.40	15.41	14.59	15.44	16.26	16.26	
15.40	15.01	14.57	14.64	15.45	15.85	15.40	
14.56	14.17	14.22	15.06	15.46	15.44	16.26	
13.72	13.44	14.28	14.67	15.03	15.85	16.26	
map_weights [ 0.7360333  -0.00101467 -0.00166074  0.2612913 ]
MAP reward
-0.00	-0.00	-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	0.74	0.26	-0.00	0.26	-0.00	0.26	
0.74	0.26	0.26	-0.00	-0.00	0.26	-0.00	
0.74	-0.00	0.74	-0.00	0.26	0.74	0.74	
-0.00	-0.00	-0.00	0.26	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	0.74	0.26	0.74	
-0.00	-0.00	-0.00	-0.00	0.26	-0.00	0.74	
Map policy
v	v	v	<	v	v	v	
v	v	<	<	>	v	v	
v	<	<	<	v	v	v	
<	<	<	>	>	>	>	
^	^	^	>	v	^	v	
^	<	>	>	>	>	v	
^	^	>	>	^	>	>	
expeced value MDP LP 34.39189763575445
mean w [ 0.35175011 -0.27267974 -0.03910789  0.02313083]
Mean policy from posterior
v	v	v	<	v	v	v	
v	v	<	<	v	v	v	
v	<	<	<	v	v	v	
^	<	<	>	>	>	>	
^	^	^	>	v	^	^	
^	^	>	>	>	>	v	
^	>	>	>	^	>	>	
Mean rewards
-0.04	-0.04	-0.27	-0.04	-0.04	-0.04	-0.04	
-0.04	0.35	0.02	-0.27	0.02	-0.27	0.02	
0.35	0.02	0.02	-0.27	-0.04	0.02	-0.04	
0.35	-0.27	0.35	-0.27	0.02	0.35	0.35	
-0.27	-0.04	-0.27	0.02	-0.04	-0.04	-0.27	
-0.27	-0.27	-0.27	-0.04	0.35	0.02	0.35	
-0.27	-0.27	-0.04	-0.04	0.02	-0.04	0.35	
mean = 0.12810573383817392, map = 0.15785225862184937
CVaR policy
v	v	v	v	v	v	v	
v	v	<	<	v	v	v	
v	<	<	<	v	v	v	
<	<	^	>	>	>	>	
^	^	^	>	v	^	v	
^	^	>	>	>	>	v	
^	>	>	>	^	>	>	
CVaR policy
v	v	v	v	v	v	v	
v	v	<	<	v	v	v	
<	<	<	<	v	v	v	
<	<	^	>	>	>	>	
^	^	^	>	v	^	v	
^	^	>	>	>	>	v	
^	>	>	>	^	>	>	
CVaR policy
v	v	v	<	v	v	v	
v	v	<	<	v	v	v	
<	<	<	<	v	v	v	
<	<	<	>	>	>	>	
^	<	^	>	v	^	v	
^	^	>	>	>	>	v	
^	>	>	>	^	>	>	
CVaR policy
v	v	v	<	v	v	v	
v	v	<	<	v	v	v	
<	<	<	<	v	v	v	
<	<	<	>	>	>	>	
^	<	^	>	v	^	v	
^	^	>	>	>	>	v	
^	>	>	>	^	>	>	
CVaR policy
v	v	v	<	v	v	v	
v	v	<	<	v	v	v	
<	<	<	<	v	v	v	
<	<	<	>	>	>	>	
^	<	^	>	v	^	v	
^	^	>	>	>	>	v	
^	>	>	>	^	>	>	
cvar = , 0.15960625587664445, 0.159610570612287, 0.12810557573687298, 0.1281125010687365, 0.12811137177364706
