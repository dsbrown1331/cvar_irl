==========
iteration 0
==========
weights [-0.08410756 -0.31708325 -0.40439986 -0.52150567 -0.58915789  0.3313151 ]
expeced value MDP LP -1.640679819826811
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.66021017 -0.61116448  0.20087784 -0.15924064  0.34185926  0.08957272]
w_map [-0.56349409 -0.51671001 -0.38115742 -0.36042038 -0.35917425  0.10627897] loglik 0.0
accepted/total = 2469/3000 = 0.823
-------
true weights [-0.08410756 -0.31708325 -0.40439986 -0.52150567 -0.58915789  0.3313151 ]
features
0 	1 	0 	4 	3 	4 	3 	2 	
4 	3 	3 	0 	4 	1 	3 	3 	
3 	2 	1 	4 	0 	3 	3 	2 	
4 	3 	1 	1 	4 	0 	3 	2 	
0 	4 	0 	0 	4 	2 	4 	3 	
2 	0 	1 	4 	0 	0 	1 	3 	
1 	1 	0 	2 	4 	1 	2 	0 	
2 	0 	4 	2 	0 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	<	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.88	-2.82	-2.53	-2.80	-2.68	-2.41	-2.85	-2.48	
-3.41	-2.85	-2.47	-2.23	-2.18	-1.84	-2.35	-2.09	
-2.85	-2.35	-1.97	-2.17	-1.61	-1.54	-2.05	-1.59	
-2.57	-2.17	-1.67	-1.60	-1.61	-1.03	-1.54	-1.20	
-2.00	-1.94	-1.36	-1.29	-1.22	-0.96	-1.06	-0.80	
-1.98	-1.59	-1.52	-1.22	-0.64	-0.56	-0.48	-0.28	
-2.12	-1.82	-1.52	-1.45	-1.06	-0.48	-0.16	0.24	
-2.09	-1.70	-1.63	-1.05	-0.66	-0.58	0.01	0.33	
map_weights [-0.56349409 -0.51671001 -0.38115742 -0.36042038 -0.35917425  0.10627897]
MAP reward
-0.56	-0.52	-0.56	-0.36	-0.36	-0.36	-0.36	-0.38	
-0.36	-0.36	-0.36	-0.56	-0.36	-0.52	-0.36	-0.36	
-0.36	-0.38	-0.52	-0.36	-0.56	-0.36	-0.36	-0.38	
-0.36	-0.36	-0.52	-0.52	-0.36	-0.56	-0.36	-0.38	
-0.56	-0.36	-0.56	-0.56	-0.36	-0.38	-0.36	-0.36	
-0.38	-0.56	-0.52	-0.36	-0.56	-0.56	-0.52	-0.36	
-0.52	-0.52	-0.56	-0.38	-0.36	-0.52	-0.38	-0.56	
-0.38	-0.56	-0.36	-0.38	-0.56	-0.36	-0.52	0.11	
Map policy
v	>	>	>	>	>	v	v	
v	>	v	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0298252404992154
mean w [-0.25495071 -0.41625482 -0.35488705 -0.43265162 -0.27560371  0.05395662]
Mean policy from posterior
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.25	-0.42	-0.25	-0.28	-0.43	-0.28	-0.43	-0.35	
-0.28	-0.43	-0.43	-0.25	-0.28	-0.42	-0.43	-0.43	
-0.43	-0.35	-0.42	-0.28	-0.25	-0.43	-0.43	-0.35	
-0.28	-0.43	-0.42	-0.42	-0.28	-0.25	-0.43	-0.35	
-0.25	-0.28	-0.25	-0.25	-0.28	-0.35	-0.28	-0.43	
-0.35	-0.25	-0.42	-0.28	-0.25	-0.25	-0.42	-0.43	
-0.42	-0.42	-0.25	-0.35	-0.28	-0.42	-0.35	-0.25	
-0.35	-0.25	-0.28	-0.35	-0.25	-0.28	-0.42	0.05	
mean = 0.428147352143492, map = 0.9506531317353175
CVaR policy
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
v	>	>	>	>	>	>	v	
v	v	v	>	>	v	v	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
v	>	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.5585622402736539, 0.40812879566876004, 0.43169906455953133, 0.42814735226798306, 0.4281473521431509
==========
iteration 1
==========
weights [-0.33233128 -0.19253504 -0.18998862 -0.66592242 -0.41271132  0.4501191 ]
expeced value MDP LP -1.6399212618541847
demonstration
[(56, 2), (48, 2), (40, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.03730017  0.50527812  0.19539653 -0.76502865  0.31664009  0.13997547]
w_map [-0.43582146 -0.21270508 -0.20695964 -0.69871273 -0.40622784  0.26222772] loglik -0.6931472441373652
accepted/total = 1693/3000 = 0.5643333333333334
-------
true weights [-0.33233128 -0.19253504 -0.18998862 -0.66592242 -0.41271132  0.4501191 ]
features
1 	3 	4 	1 	4 	2 	0 	2 	
2 	0 	0 	4 	0 	4 	2 	0 	
2 	4 	2 	1 	0 	4 	4 	3 	
1 	0 	1 	3 	3 	4 	2 	0 	
1 	2 	2 	2 	1 	3 	2 	4 	
1 	3 	1 	3 	1 	2 	2 	3 	
2 	3 	0 	3 	1 	4 	3 	1 	
2 	3 	2 	3 	0 	3 	0 	5 	
optimal policy
v	v	v	v	>	>	v	v	
v	>	v	v	>	>	v	<	
v	>	v	<	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
optimal values
-2.54	-3.12	-2.56	-2.59	-2.43	-2.03	-1.86	-2.03	
-2.37	-2.48	-2.17	-2.42	-2.26	-1.94	-1.55	-1.86	
-2.20	-2.25	-1.86	-2.03	-2.08	-1.77	-1.37	-1.80	
-2.03	-2.00	-1.69	-1.98	-1.81	-1.37	-0.97	-1.15	
-1.86	-1.68	-1.51	-1.33	-1.15	-1.44	-0.79	-0.82	
-2.03	-2.33	-1.69	-1.63	-0.97	-0.79	-0.60	-0.42	
-2.20	-2.63	-1.98	-1.66	-1.01	-0.82	-0.42	0.25	
-2.37	-2.36	-1.71	-1.54	-0.88	-0.55	0.11	0.45	
map_weights [-0.43582146 -0.21270508 -0.20695964 -0.69871273 -0.40622784  0.26222772]
MAP reward
-0.21	-0.70	-0.41	-0.21	-0.41	-0.21	-0.44	-0.21	
-0.21	-0.44	-0.44	-0.41	-0.44	-0.41	-0.21	-0.44	
-0.21	-0.41	-0.21	-0.21	-0.44	-0.41	-0.41	-0.70	
-0.21	-0.44	-0.21	-0.70	-0.70	-0.41	-0.21	-0.44	
-0.21	-0.21	-0.21	-0.21	-0.21	-0.70	-0.21	-0.41	
-0.21	-0.70	-0.21	-0.70	-0.21	-0.21	-0.21	-0.70	
-0.21	-0.70	-0.44	-0.70	-0.21	-0.41	-0.70	-0.21	
-0.21	-0.70	-0.21	-0.70	-0.44	-0.70	-0.44	0.26	
Map policy
v	<	v	>	>	v	v	v	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.4259147125357732
mean w [-0.49179267 -0.1552209  -0.10339399 -0.53147704 -0.39551321  0.18792958]
Mean policy from posterior
v	<	v	v	>	v	v	<	
v	<	v	v	>	>	v	<	
v	>	v	<	v	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	>	v	
^	<	^	>	^	>	>	v	
^	<	^	>	>	>	>	.	
Mean rewards
-0.16	-0.53	-0.40	-0.16	-0.40	-0.10	-0.49	-0.10	
-0.10	-0.49	-0.49	-0.40	-0.49	-0.40	-0.10	-0.49	
-0.10	-0.40	-0.10	-0.16	-0.49	-0.40	-0.40	-0.53	
-0.16	-0.49	-0.16	-0.53	-0.53	-0.40	-0.10	-0.49	
-0.16	-0.10	-0.10	-0.10	-0.16	-0.53	-0.10	-0.40	
-0.16	-0.53	-0.16	-0.53	-0.16	-0.10	-0.10	-0.53	
-0.10	-0.53	-0.49	-0.53	-0.16	-0.40	-0.53	-0.16	
-0.10	-0.53	-0.10	-0.53	-0.49	-0.53	-0.49	0.19	
mean = 0.038241466103785315, map = 0.005841859963669371
CVaR policy
v	<	v	>	>	v	v	v	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	v	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	<	
v	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	v	v	
^	<	^	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	<	>	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	v	v	
^	<	^	>	>	>	>	v	
^	<	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	v	
v	<	v	v	>	>	v	<	
v	>	v	<	v	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	>	v	
^	<	^	>	^	>	>	v	
^	<	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	<	
v	<	v	v	>	>	v	<	
v	>	v	<	v	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	v	v	
^	<	^	>	^	>	>	v	
^	<	^	>	>	>	>	.	
cvar = , 0.007967798536998139, 0.013048562317232504, 0.024184633731432204, 0.031077950022315726, 0.03824147051052984
==========
iteration 2
==========
weights [-0.17656382 -0.09591669 -0.40970874 -0.69359356 -0.55654522  0.03081159]
expeced value MDP LP -2.1813044872200917
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.10449919  0.8480101  -0.11813262  0.4644925  -0.13450211 -0.14886026]
w_map [-0.49887516 -0.35531796 -0.52493963 -0.40734555 -0.40495933  0.13924313] loglik 0.0
accepted/total = 2368/3000 = 0.7893333333333333
-------
true weights [-0.17656382 -0.09591669 -0.40970874 -0.69359356 -0.55654522  0.03081159]
features
3 	4 	4 	2 	3 	2 	2 	2 	
4 	4 	3 	3 	2 	1 	4 	0 	
3 	0 	1 	4 	3 	4 	3 	3 	
3 	0 	4 	4 	2 	0 	0 	4 	
1 	4 	1 	1 	0 	3 	4 	3 	
2 	1 	3 	0 	4 	0 	1 	0 	
1 	4 	2 	3 	2 	2 	2 	4 	
3 	0 	4 	4 	0 	1 	4 	5 	
optimal policy
v	v	v	>	>	v	<	v	
>	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.30	-3.64	-3.63	-3.70	-3.33	-2.66	-3.04	-3.13	
-3.64	-3.11	-3.10	-3.33	-2.66	-2.27	-2.71	-2.75	
-3.25	-2.58	-2.43	-2.82	-2.73	-2.20	-2.18	-2.60	
-3.10	-2.51	-2.36	-2.28	-2.05	-1.66	-1.50	-1.93	
-2.43	-2.36	-1.82	-1.74	-1.66	-1.64	-1.33	-1.38	
-2.80	-2.41	-2.34	-1.66	-1.50	-0.95	-0.79	-0.70	
-2.64	-2.57	-2.26	-1.87	-1.19	-1.02	-0.93	-0.53	
-2.71	-2.04	-1.88	-1.34	-0.79	-0.62	-0.53	0.03	
map_weights [-0.49887516 -0.35531796 -0.52493963 -0.40734555 -0.40495933  0.13924313]
MAP reward
-0.41	-0.40	-0.40	-0.52	-0.41	-0.52	-0.52	-0.52	
-0.40	-0.40	-0.41	-0.41	-0.52	-0.36	-0.40	-0.50	
-0.41	-0.50	-0.36	-0.40	-0.41	-0.40	-0.41	-0.41	
-0.41	-0.50	-0.40	-0.40	-0.52	-0.50	-0.50	-0.40	
-0.36	-0.40	-0.36	-0.36	-0.50	-0.41	-0.40	-0.41	
-0.52	-0.36	-0.41	-0.50	-0.40	-0.50	-0.36	-0.50	
-0.36	-0.40	-0.52	-0.41	-0.52	-0.52	-0.52	-0.40	
-0.41	-0.50	-0.40	-0.40	-0.50	-0.36	-0.40	0.14	
Map policy
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	v	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1665199086893017
mean w [-0.32198715 -0.31283337 -0.47290061 -0.41183587 -0.25019415 -0.1583582 ]
Mean policy from posterior
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
v	v	>	v	v	>	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.41	-0.25	-0.25	-0.47	-0.41	-0.47	-0.47	-0.47	
-0.25	-0.25	-0.41	-0.41	-0.47	-0.31	-0.25	-0.32	
-0.41	-0.32	-0.31	-0.25	-0.41	-0.25	-0.41	-0.41	
-0.41	-0.32	-0.25	-0.25	-0.47	-0.32	-0.32	-0.25	
-0.31	-0.25	-0.31	-0.31	-0.32	-0.41	-0.25	-0.41	
-0.47	-0.31	-0.41	-0.32	-0.25	-0.32	-0.31	-0.32	
-0.31	-0.25	-0.47	-0.41	-0.47	-0.47	-0.47	-0.25	
-0.41	-0.32	-0.25	-0.25	-0.32	-0.31	-0.25	-0.16	
mean = 0.18275506256142782, map = 0.45215499207510135
CVaR policy
v	>	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	v	>	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	v	>	v	v	
>	v	>	v	v	>	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
v	v	>	v	v	>	v	v	
>	v	>	v	v	>	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
v	v	>	v	v	>	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.37563611341245107, 0.166528895871501, 0.18951782907947834, 0.18275506260671026, 0.18275506207304826
==========
iteration 3
==========
weights [-0.19171245 -0.31367861 -0.54062804 -0.33488767 -0.23603901  0.63616761]
expeced value MDP LP -1.1948997534909562
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.35876884 -0.11819347  0.11854638 -0.719397    0.55996149  0.11033163]
w_map [-0.1195765  -0.35114088 -0.52290497 -0.64863134 -0.38380738  0.1447108 ] loglik 0.0
accepted/total = 2312/3000 = 0.7706666666666667
-------
true weights [-0.19171245 -0.31367861 -0.54062804 -0.33488767 -0.23603901  0.63616761]
features
3 	2 	4 	2 	0 	4 	3 	4 	
0 	0 	1 	0 	0 	0 	3 	0 	
4 	3 	4 	2 	0 	4 	0 	0 	
4 	3 	4 	4 	1 	1 	0 	1 	
3 	3 	1 	1 	3 	1 	2 	4 	
3 	3 	1 	2 	1 	4 	0 	0 	
1 	3 	0 	1 	3 	2 	0 	3 	
4 	1 	3 	2 	1 	4 	3 	5 	
optimal policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.54	-2.58	-2.10	-2.11	-1.59	-1.45	-1.37	-1.05	
-2.23	-2.06	-1.89	-1.59	-1.41	-1.23	-1.15	-0.82	
-2.44	-2.23	-1.91	-1.76	-1.23	-1.05	-0.82	-0.64	
-2.23	-2.01	-1.69	-1.47	-1.25	-0.94	-0.64	-0.45	
-2.21	-1.89	-1.57	-1.27	-0.97	-0.64	-0.63	-0.14	
-2.02	-1.71	-1.38	-1.17	-0.64	-0.33	-0.09	0.10	
-1.70	-1.41	-1.08	-0.90	-0.59	-0.44	0.10	0.29	
-1.65	-1.43	-1.12	-0.80	-0.26	0.06	0.29	0.64	
map_weights [-0.1195765  -0.35114088 -0.52290497 -0.64863134 -0.38380738  0.1447108 ]
MAP reward
-0.65	-0.52	-0.38	-0.52	-0.12	-0.38	-0.65	-0.38	
-0.12	-0.12	-0.35	-0.12	-0.12	-0.12	-0.65	-0.12	
-0.38	-0.65	-0.38	-0.52	-0.12	-0.38	-0.12	-0.12	
-0.38	-0.65	-0.38	-0.38	-0.35	-0.35	-0.12	-0.35	
-0.65	-0.65	-0.35	-0.35	-0.65	-0.35	-0.52	-0.38	
-0.65	-0.65	-0.35	-0.52	-0.35	-0.38	-0.12	-0.12	
-0.35	-0.65	-0.12	-0.35	-0.65	-0.52	-0.12	-0.65	
-0.38	-0.35	-0.65	-0.52	-0.35	-0.38	-0.65	0.14	
Map policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	v	v	
^	>	>	>	>	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7522250745788919
mean w [-0.41723598 -0.27861357 -0.32619331 -0.34098501 -0.26965147  0.33129223]
Mean policy from posterior
>	>	v	v	>	v	>	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	v	>	v	>	v	
v	>	>	>	>	v	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.34	-0.33	-0.27	-0.33	-0.42	-0.27	-0.34	-0.27	
-0.42	-0.42	-0.28	-0.42	-0.42	-0.42	-0.34	-0.42	
-0.27	-0.34	-0.27	-0.33	-0.42	-0.27	-0.42	-0.42	
-0.27	-0.34	-0.27	-0.27	-0.28	-0.28	-0.42	-0.28	
-0.34	-0.34	-0.28	-0.28	-0.34	-0.28	-0.33	-0.27	
-0.34	-0.34	-0.28	-0.33	-0.28	-0.27	-0.42	-0.42	
-0.28	-0.34	-0.42	-0.28	-0.34	-0.33	-0.42	-0.34	
-0.27	-0.28	-0.34	-0.33	-0.28	-0.27	-0.34	0.33	
mean = 0.30976427875433843, map = 0.11497457473095007
CVaR policy
>	>	>	>	>	v	>	v	
>	>	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	v	v	
v	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	v	>	v	
v	v	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	>	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	>	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	>	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.29782193310600436, 0.2814314959221713, 0.3097642787375512, 0.3097642786978956, 0.3097642786759951
==========
iteration 4
==========
weights [-0.28507528 -0.44883582 -0.47654705 -0.19042536 -0.65668186  0.15062708]
expeced value MDP LP -2.4888440735753576
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.01264683  0.77521783 -0.075454   -0.28125817  0.4245342   0.36585327]
w_map [-0.34369434 -0.60807499 -0.50673213 -0.34876922 -0.07951695  0.35690145] loglik 0.0
accepted/total = 2542/3000 = 0.8473333333333334
-------
true weights [-0.28507528 -0.44883582 -0.47654705 -0.19042536 -0.65668186  0.15062708]
features
1 	1 	4 	0 	2 	3 	4 	2 	
2 	4 	2 	4 	3 	4 	1 	4 	
1 	1 	2 	0 	1 	2 	1 	4 	
2 	4 	1 	1 	3 	3 	2 	0 	
0 	2 	0 	4 	1 	4 	2 	2 	
1 	2 	0 	4 	2 	1 	0 	1 	
2 	0 	2 	3 	2 	1 	1 	4 	
3 	2 	3 	0 	0 	2 	1 	5 	
optimal policy
v	>	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.90	-4.61	-4.20	-3.58	-3.33	-3.37	-3.43	-3.41	
-4.49	-4.40	-3.81	-3.51	-2.88	-3.21	-2.81	-2.96	
-4.06	-3.78	-3.36	-2.98	-2.72	-2.58	-2.38	-2.33	
-3.64	-3.54	-2.92	-2.72	-2.29	-2.12	-1.95	-1.69	
-3.20	-2.94	-2.49	-2.78	-2.35	-2.10	-1.49	-1.42	
-3.01	-2.68	-2.23	-2.14	-1.92	-1.46	-1.02	-0.95	
-2.59	-2.23	-1.96	-1.50	-1.52	-1.19	-0.75	-0.51	
-2.13	-1.96	-1.50	-1.33	-1.05	-0.77	-0.30	0.15	
map_weights [-0.34369434 -0.60807499 -0.50673213 -0.34876922 -0.07951695  0.35690145]
MAP reward
-0.61	-0.61	-0.08	-0.34	-0.51	-0.35	-0.08	-0.51	
-0.51	-0.08	-0.51	-0.08	-0.35	-0.08	-0.61	-0.08	
-0.61	-0.61	-0.51	-0.34	-0.61	-0.51	-0.61	-0.08	
-0.51	-0.08	-0.61	-0.61	-0.35	-0.35	-0.51	-0.34	
-0.34	-0.51	-0.34	-0.08	-0.61	-0.08	-0.51	-0.51	
-0.61	-0.51	-0.34	-0.08	-0.51	-0.61	-0.34	-0.61	
-0.51	-0.34	-0.51	-0.35	-0.51	-0.61	-0.61	-0.08	
-0.35	-0.51	-0.35	-0.34	-0.34	-0.51	-0.61	0.36	
Map policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	v	
v	v	>	^	v	v	>	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	v	v	
>	>	>	v	>	>	v	v	
>	>	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.2695717133411906
mean w [-0.27706758 -0.43508593 -0.34997344 -0.38125414 -0.36495272  0.09441377]
Mean policy from posterior
v	v	v	v	>	>	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.44	-0.44	-0.36	-0.28	-0.35	-0.38	-0.36	-0.35	
-0.35	-0.36	-0.35	-0.36	-0.38	-0.36	-0.44	-0.36	
-0.44	-0.44	-0.35	-0.28	-0.44	-0.35	-0.44	-0.36	
-0.35	-0.36	-0.44	-0.44	-0.38	-0.38	-0.35	-0.28	
-0.28	-0.35	-0.28	-0.36	-0.44	-0.36	-0.35	-0.35	
-0.44	-0.35	-0.28	-0.36	-0.35	-0.44	-0.28	-0.44	
-0.35	-0.28	-0.35	-0.38	-0.35	-0.44	-0.44	-0.36	
-0.38	-0.35	-0.38	-0.28	-0.28	-0.35	-0.44	0.09	
mean = 0.1552308079198057, map = 0.49465189455972247
CVaR policy
v	>	>	>	v	v	>	v	
>	>	>	>	v	v	>	v	
v	>	>	v	v	v	>	v	
v	v	>	>	>	v	>	v	
v	>	>	v	v	>	v	v	
v	v	>	v	v	>	v	v	
v	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	>	v	v	
v	v	>	v	v	>	v	v	
v	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	>	v	
v	>	v	>	>	v	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.22798202313133142, 0.18999403303108453, 0.13617474018818498, 0.13578341033957653, 0.13578337737665214
==========
iteration 5
==========
weights [-0.29346291 -0.17950379 -0.77916458 -0.47179135 -0.1487552   0.17275788]
expeced value MDP LP -1.6481470730449301
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.03204997 -0.02574625 -0.57804975  0.50055726 -0.51890099  0.37993764]
w_map [-0.391652   -0.38295779 -0.66100335 -0.34010664 -0.31727827 -0.2160754 ] loglik -0.6931471805648073
accepted/total = 2098/3000 = 0.6993333333333334
-------
true weights [-0.29346291 -0.17950379 -0.77916458 -0.47179135 -0.1487552   0.17275788]
features
1 	1 	0 	0 	0 	3 	3 	4 	
2 	3 	4 	2 	2 	4 	3 	2 	
4 	0 	0 	2 	4 	1 	0 	3 	
0 	2 	1 	1 	0 	1 	2 	0 	
0 	4 	4 	4 	2 	3 	2 	1 	
0 	4 	2 	3 	3 	2 	3 	0 	
4 	4 	1 	4 	0 	1 	1 	0 	
1 	2 	0 	2 	2 	1 	0 	5 	
optimal policy
>	>	v	<	>	v	v	v	
v	>	v	<	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	<	>	>	v	
>	v	<	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
optimal values
-2.77	-2.61	-2.46	-2.73	-2.64	-2.37	-2.53	-2.23	
-2.93	-2.64	-2.19	-2.95	-2.67	-1.91	-2.08	-2.11	
-2.17	-2.33	-2.06	-2.43	-1.91	-1.78	-1.62	-1.34	
-2.04	-2.25	-1.78	-1.67	-1.95	-1.81	-1.65	-0.88	
-1.77	-1.49	-1.62	-1.50	-2.00	-1.71	-1.36	-0.59	
-1.63	-1.35	-1.85	-1.37	-1.23	-1.25	-0.77	-0.41	
-1.35	-1.22	-1.08	-0.91	-0.77	-0.48	-0.30	-0.12	
-1.52	-1.98	-1.36	-1.68	-1.08	-0.30	-0.12	0.17	
map_weights [-0.391652   -0.38295779 -0.66100335 -0.34010664 -0.31727827 -0.2160754 ]
MAP reward
-0.38	-0.38	-0.39	-0.39	-0.39	-0.34	-0.34	-0.32	
-0.66	-0.34	-0.32	-0.66	-0.66	-0.32	-0.34	-0.66	
-0.32	-0.39	-0.39	-0.66	-0.32	-0.38	-0.39	-0.34	
-0.39	-0.66	-0.38	-0.38	-0.39	-0.38	-0.66	-0.39	
-0.39	-0.32	-0.32	-0.32	-0.66	-0.34	-0.66	-0.38	
-0.39	-0.32	-0.66	-0.34	-0.34	-0.66	-0.34	-0.39	
-0.32	-0.32	-0.38	-0.32	-0.39	-0.38	-0.38	-0.39	
-0.38	-0.66	-0.39	-0.66	-0.66	-0.38	-0.39	-0.22	
Map policy
>	v	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	>	v	>	v	
>	>	>	v	v	>	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.5907339663788655
mean w [-0.24887957 -0.21520821 -0.60952797 -0.32692667 -0.21252456  0.15799415]
Mean policy from posterior
>	>	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	>	>	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.22	-0.22	-0.25	-0.25	-0.25	-0.33	-0.33	-0.21	
-0.61	-0.33	-0.21	-0.61	-0.61	-0.21	-0.33	-0.61	
-0.21	-0.25	-0.25	-0.61	-0.21	-0.22	-0.25	-0.33	
-0.25	-0.61	-0.22	-0.22	-0.25	-0.22	-0.61	-0.25	
-0.25	-0.21	-0.21	-0.21	-0.61	-0.33	-0.61	-0.22	
-0.25	-0.21	-0.61	-0.33	-0.33	-0.61	-0.33	-0.25	
-0.21	-0.21	-0.22	-0.21	-0.25	-0.22	-0.22	-0.25	
-0.22	-0.61	-0.25	-0.61	-0.61	-0.22	-0.25	0.16	
mean = 0.01369926363633378, map = 0.1205097204686878
CVaR policy
>	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
v	>	v	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	>	>	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	>	>	>	v	
>	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
cvar = , 0.12666028410330177, 0.02390100749889079, 0.0156354147311073, 0.014608906695638302, 0.013699263682239504
==========
iteration 6
==========
weights [-0.41968772 -0.19150613 -0.43021903 -0.38564015 -0.19455769  0.64461476]
expeced value MDP LP -1.1838108390563458
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.42905162  0.54947596 -0.16301717 -0.36081074 -0.47914771  0.3572805 ]
w_map [-0.47018838 -0.24301364 -0.35386758 -0.14453091 -0.44950808  0.60967065] loglik 0.0
accepted/total = 2425/3000 = 0.8083333333333333
-------
true weights [-0.41968772 -0.19150613 -0.43021903 -0.38564015 -0.19455769  0.64461476]
features
2 	3 	0 	0 	4 	4 	3 	4 	
0 	2 	4 	2 	2 	0 	4 	2 	
2 	3 	2 	4 	1 	1 	0 	0 	
2 	2 	1 	0 	4 	0 	0 	1 	
4 	2 	2 	2 	0 	1 	4 	4 	
1 	4 	1 	2 	1 	4 	2 	1 	
0 	4 	4 	3 	2 	0 	0 	1 	
2 	4 	4 	0 	2 	0 	2 	5 	
optimal policy
>	>	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.06	-2.66	-2.29	-2.11	-1.70	-1.52	-1.53	-1.17	
-2.70	-2.30	-1.89	-1.71	-1.53	-1.34	-1.16	-0.98	
-2.49	-2.08	-1.71	-1.30	-1.11	-0.93	-0.97	-0.56	
-2.12	-1.94	-1.52	-1.35	-0.94	-0.75	-0.56	-0.14	
-1.71	-1.77	-1.59	-1.17	-0.75	-0.33	-0.14	0.05	
-1.53	-1.35	-1.17	-0.99	-0.56	-0.37	-0.18	0.25	
-1.93	-1.53	-1.35	-1.20	-0.82	-0.40	0.02	0.45	
-1.84	-1.42	-1.24	-1.06	-0.64	-0.21	0.21	0.64	
map_weights [-0.47018838 -0.24301364 -0.35386758 -0.14453091 -0.44950808  0.60967065]
MAP reward
-0.35	-0.14	-0.47	-0.47	-0.45	-0.45	-0.14	-0.45	
-0.47	-0.35	-0.45	-0.35	-0.35	-0.47	-0.45	-0.35	
-0.35	-0.14	-0.35	-0.45	-0.24	-0.24	-0.47	-0.47	
-0.35	-0.35	-0.24	-0.47	-0.45	-0.47	-0.47	-0.24	
-0.45	-0.35	-0.35	-0.35	-0.47	-0.24	-0.45	-0.45	
-0.24	-0.45	-0.24	-0.35	-0.24	-0.45	-0.35	-0.24	
-0.47	-0.45	-0.45	-0.14	-0.35	-0.47	-0.47	-0.24	
-0.35	-0.45	-0.45	-0.47	-0.35	-0.47	-0.35	0.61	
Map policy
>	v	v	v	v	>	v	v	
v	v	v	>	v	v	>	v	
>	v	v	>	>	v	v	v	
>	>	v	v	v	v	>	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.067376720530925
mean w [-0.29424434 -0.36452529 -0.28229361 -0.40762557 -0.41776937  0.03556994]
Mean policy from posterior
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.28	-0.41	-0.29	-0.29	-0.42	-0.42	-0.41	-0.42	
-0.29	-0.28	-0.42	-0.28	-0.28	-0.29	-0.42	-0.28	
-0.28	-0.41	-0.28	-0.42	-0.36	-0.36	-0.29	-0.29	
-0.28	-0.28	-0.36	-0.29	-0.42	-0.29	-0.29	-0.36	
-0.42	-0.28	-0.28	-0.28	-0.29	-0.36	-0.42	-0.42	
-0.36	-0.42	-0.36	-0.28	-0.36	-0.42	-0.28	-0.36	
-0.29	-0.42	-0.42	-0.41	-0.28	-0.29	-0.29	-0.36	
-0.28	-0.42	-0.42	-0.29	-0.28	-0.29	-0.28	0.04	
mean = 0.7289966409560655, map = 0.18156569180891258
CVaR policy
>	v	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
v	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.3677964223113521, 0.48658458623631895, 0.7137284458019593, 0.7219876133479484, 0.7273629781209434
==========
iteration 7
==========
weights [-0.25808208 -0.27387159 -0.54798065 -0.11659288 -0.73758758  0.02181429]
expeced value MDP LP -1.7243150469732487
demonstration
[(56, 1), (57, 2), (49, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.48458429 -0.69686725  0.01937839  0.06967702 -0.27751778 -0.44419317]
w_map [-0.14878795 -0.2815126  -0.48767297 -0.1328834  -0.77459073 -0.20769949] loglik -4.906420095096564e-06
accepted/total = 1156/3000 = 0.38533333333333336
-------
true weights [-0.25808208 -0.27387159 -0.54798065 -0.11659288 -0.73758758  0.02181429]
features
0 	3 	1 	1 	4 	0 	0 	4 	
3 	2 	1 	4 	1 	2 	1 	2 	
2 	2 	3 	1 	1 	4 	0 	1 	
3 	4 	0 	4 	4 	3 	1 	2 	
1 	1 	4 	4 	4 	3 	3 	1 	
0 	3 	3 	1 	3 	1 	4 	1 	
4 	0 	4 	1 	2 	1 	1 	3 	
2 	2 	2 	1 	4 	0 	3 	5 	
optimal policy
v	>	v	>	>	>	v	<	
v	>	v	>	v	>	v	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	>	v	
>	^	>	>	>	>	>	.	
optimal values
-2.91	-2.89	-2.80	-2.97	-2.72	-2.01	-1.77	-2.49	
-2.68	-3.07	-2.55	-2.92	-2.20	-2.06	-1.52	-1.98	
-2.59	-2.82	-2.30	-2.20	-1.95	-1.69	-1.26	-1.44	
-2.07	-2.45	-2.33	-2.41	-1.69	-0.97	-1.01	-1.18	
-1.97	-1.73	-2.09	-1.99	-1.59	-0.86	-0.75	-0.64	
-1.71	-1.47	-1.37	-1.26	-1.00	-0.89	-1.10	-0.37	
-2.43	-1.71	-2.09	-1.43	-1.16	-0.62	-0.37	-0.09	
-2.77	-2.24	-1.88	-1.35	-1.09	-0.35	-0.09	0.02	
map_weights [-0.14878795 -0.2815126  -0.48767297 -0.1328834  -0.77459073 -0.20769949]
MAP reward
-0.15	-0.13	-0.28	-0.28	-0.77	-0.15	-0.15	-0.77	
-0.13	-0.49	-0.28	-0.77	-0.28	-0.49	-0.28	-0.49	
-0.49	-0.49	-0.13	-0.28	-0.28	-0.77	-0.15	-0.28	
-0.13	-0.77	-0.15	-0.77	-0.77	-0.13	-0.28	-0.49	
-0.28	-0.28	-0.77	-0.77	-0.77	-0.13	-0.13	-0.28	
-0.15	-0.13	-0.13	-0.28	-0.13	-0.28	-0.77	-0.28	
-0.77	-0.15	-0.77	-0.28	-0.49	-0.28	-0.28	-0.13	
-0.49	-0.49	-0.49	-0.28	-0.77	-0.15	-0.13	-0.21	
Map policy
v	<	v	>	>	>	v	<	
v	<	v	>	>	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	v	v	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.6899681514484626
mean w [-0.08739639 -0.18148476 -0.38745449 -0.0868663  -0.61615095 -0.57302038]
Mean policy from posterior
v	<	v	>	>	>	v	<	
v	<	v	>	>	>	v	v	
v	>	v	>	>	v	v	<	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	>	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.09	-0.09	-0.18	-0.18	-0.62	-0.09	-0.09	-0.62	
-0.09	-0.39	-0.18	-0.62	-0.18	-0.39	-0.18	-0.39	
-0.39	-0.39	-0.09	-0.18	-0.18	-0.62	-0.09	-0.18	
-0.09	-0.62	-0.09	-0.62	-0.62	-0.09	-0.18	-0.39	
-0.18	-0.18	-0.62	-0.62	-0.62	-0.09	-0.09	-0.18	
-0.09	-0.09	-0.09	-0.18	-0.09	-0.18	-0.62	-0.18	
-0.62	-0.09	-0.62	-0.18	-0.39	-0.18	-0.18	-0.09	
-0.39	-0.39	-0.39	-0.18	-0.62	-0.09	-0.09	-0.57	
mean = 0.01717264661434892, map = 0.015559808962724242
CVaR policy
v	<	v	>	>	>	v	<	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	>	v	<	
v	<	v	>	>	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	>	v	<	
v	<	v	>	>	>	v	v	
v	>	v	>	>	v	v	<	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	>	v	<	
v	<	v	>	>	>	v	v	
v	>	v	>	>	v	v	<	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	>	v	<	
v	<	v	>	>	>	v	v	
v	>	v	>	>	v	v	<	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	>	v	
>	^	>	>	>	>	>	.	
cvar = , 0.012072801682155854, 0.014619656685777027, 0.017172646601019137, 0.017172646888947263, 0.017172646691871574
==========
iteration 8
==========
weights [-0.26192152 -0.1579791  -0.15035918 -0.61724254 -0.70747556  0.04818527]
expeced value MDP LP -1.9647020575994665
demonstration
[(56, 2), (48, 1), (49, 2), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.22144333 -0.55877557  0.52407627  0.0824541  -0.24995124  0.54295716]
w_map [-0.28320147 -0.07299992 -0.23493342 -0.70472465 -0.53226764 -0.28165325] loglik -4.21886170443031e-09
accepted/total = 1459/3000 = 0.48633333333333334
-------
true weights [-0.26192152 -0.1579791  -0.15035918 -0.61724254 -0.70747556  0.04818527]
features
3 	4 	4 	4 	1 	3 	2 	4 	
1 	2 	0 	4 	0 	1 	0 	3 	
0 	0 	2 	0 	4 	3 	3 	3 	
1 	0 	3 	1 	4 	0 	3 	3 	
0 	3 	4 	0 	0 	0 	4 	4 	
0 	2 	2 	1 	3 	4 	0 	3 	
2 	1 	3 	0 	3 	1 	2 	2 	
0 	3 	3 	1 	2 	3 	4 	5 	
optimal policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	<	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
optimal values
-3.27	-3.23	-3.10	-3.40	-2.72	-2.94	-2.71	-3.39	
-2.68	-2.55	-2.42	-2.74	-2.59	-2.35	-2.59	-3.18	
-2.55	-2.42	-2.18	-2.05	-2.74	-2.21	-2.42	-2.62	
-2.31	-2.53	-2.41	-1.81	-2.30	-1.61	-1.82	-2.02	
-2.18	-2.29	-2.25	-1.67	-1.61	-1.36	-1.21	-1.42	
-1.93	-1.69	-1.55	-1.42	-1.63	-1.11	-0.51	-0.72	
-1.96	-1.83	-1.88	-1.27	-1.02	-0.41	-0.25	-0.10	
-2.20	-2.43	-1.91	-1.31	-1.16	-1.02	-0.66	0.05	
map_weights [-0.28320147 -0.07299992 -0.23493342 -0.70472465 -0.53226764 -0.28165325]
MAP reward
-0.70	-0.53	-0.53	-0.53	-0.07	-0.70	-0.23	-0.53	
-0.07	-0.23	-0.28	-0.53	-0.28	-0.07	-0.28	-0.70	
-0.28	-0.28	-0.23	-0.28	-0.53	-0.70	-0.70	-0.70	
-0.07	-0.28	-0.70	-0.07	-0.53	-0.28	-0.70	-0.70	
-0.28	-0.70	-0.53	-0.28	-0.28	-0.28	-0.53	-0.53	
-0.28	-0.23	-0.23	-0.07	-0.70	-0.53	-0.28	-0.70	
-0.23	-0.07	-0.70	-0.28	-0.70	-0.07	-0.23	-0.23	
-0.28	-0.70	-0.70	-0.07	-0.23	-0.70	-0.53	-0.28	
Map policy
v	v	v	>	v	v	v	<	
v	v	v	v	>	v	<	<	
v	>	>	v	v	v	v	v	
v	<	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
expeced value MDP LP -1.7807511137641256
mean w [-0.24616483 -0.10727514 -0.11435609 -0.48791863 -0.56135273 -0.18080289]
Mean policy from posterior
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
Mean rewards
-0.49	-0.56	-0.56	-0.56	-0.11	-0.49	-0.11	-0.56	
-0.11	-0.11	-0.25	-0.56	-0.25	-0.11	-0.25	-0.49	
-0.25	-0.25	-0.11	-0.25	-0.56	-0.49	-0.49	-0.49	
-0.11	-0.25	-0.49	-0.11	-0.56	-0.25	-0.49	-0.49	
-0.25	-0.49	-0.56	-0.25	-0.25	-0.25	-0.56	-0.56	
-0.25	-0.11	-0.11	-0.11	-0.49	-0.56	-0.25	-0.49	
-0.11	-0.11	-0.49	-0.25	-0.49	-0.11	-0.11	-0.11	
-0.25	-0.49	-0.49	-0.11	-0.11	-0.49	-0.56	-0.18	
mean = 0.0005228556099590165, map = 0.005780241739168579
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	^	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
cvar = , 0.03170344478341258, 0.005798662791253406, 0.0005228553239244871, 0.0005228550811864352, 0.0005228551111458035
==========
iteration 9
==========
weights [-0.5868067  -0.02628692 -0.22383965 -0.35914696 -0.68844222  0.04385757]
expeced value MDP LP -1.8970149022680243
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.04355189  0.30289432 -0.41419436  0.50825155  0.18396492 -0.66531088]
w_map [-0.38338326 -0.63247726 -0.38791167 -0.38666584 -0.39051201 -0.02245491] loglik 0.0
accepted/total = 2396/3000 = 0.7986666666666666
-------
true weights [-0.5868067  -0.02628692 -0.22383965 -0.35914696 -0.68844222  0.04385757]
features
2 	1 	3 	2 	4 	2 	0 	0 	
2 	2 	1 	4 	1 	1 	2 	0 	
2 	3 	3 	1 	1 	2 	3 	4 	
0 	2 	1 	1 	4 	3 	4 	1 	
4 	2 	1 	4 	4 	2 	2 	2 	
2 	2 	2 	0 	3 	3 	3 	3 	
3 	2 	4 	0 	2 	0 	3 	4 	
4 	1 	3 	3 	2 	2 	3 	5 	
optimal policy
>	v	v	v	v	v	<	v	
>	>	v	>	v	v	<	v	
>	v	>	>	>	v	v	v	
>	>	>	^	>	v	>	v	
>	v	^	v	v	>	v	v	
>	v	<	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.75	-2.56	-2.69	-2.88	-2.68	-2.21	-2.77	-3.03	
-2.75	-2.55	-2.35	-2.68	-2.01	-2.01	-2.21	-2.47	
-2.76	-2.57	-2.35	-2.01	-2.01	-2.00	-2.24	-1.90	
-2.79	-2.23	-2.02	-2.02	-2.46	-1.79	-1.90	-1.23	
-2.76	-2.09	-2.03	-2.56	-2.00	-1.45	-1.24	-1.21	
-2.09	-1.88	-2.09	-1.89	-1.32	-1.37	-1.02	-1.00	
-2.02	-1.68	-2.13	-1.55	-0.97	-1.12	-0.67	-0.65	
-2.14	-1.47	-1.45	-1.11	-0.75	-0.54	-0.32	0.04	
map_weights [-0.38338326 -0.63247726 -0.38791167 -0.38666584 -0.39051201 -0.02245491]
MAP reward
-0.39	-0.63	-0.39	-0.39	-0.39	-0.39	-0.38	-0.38	
-0.39	-0.39	-0.63	-0.39	-0.63	-0.63	-0.39	-0.38	
-0.39	-0.39	-0.39	-0.63	-0.63	-0.39	-0.39	-0.39	
-0.38	-0.39	-0.63	-0.63	-0.39	-0.39	-0.39	-0.63	
-0.39	-0.39	-0.63	-0.39	-0.39	-0.39	-0.39	-0.39	
-0.39	-0.39	-0.39	-0.38	-0.39	-0.39	-0.39	-0.39	
-0.39	-0.39	-0.39	-0.38	-0.39	-0.38	-0.39	-0.39	
-0.39	-0.63	-0.39	-0.39	-0.39	-0.39	-0.39	-0.02	
Map policy
v	v	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.221063607894905
mean w [-0.44951471 -0.33651219 -0.35620569 -0.26478686 -0.43287485 -0.1346314 ]
Mean policy from posterior
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.36	-0.34	-0.26	-0.36	-0.43	-0.36	-0.45	-0.45	
-0.36	-0.36	-0.34	-0.43	-0.34	-0.34	-0.36	-0.45	
-0.36	-0.26	-0.26	-0.34	-0.34	-0.36	-0.26	-0.43	
-0.45	-0.36	-0.34	-0.34	-0.43	-0.26	-0.43	-0.34	
-0.43	-0.36	-0.34	-0.43	-0.43	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.45	-0.26	-0.26	-0.26	-0.26	
-0.26	-0.36	-0.43	-0.45	-0.36	-0.45	-0.26	-0.43	
-0.43	-0.34	-0.26	-0.26	-0.36	-0.36	-0.26	-0.13	
mean = 0.173616627655643, map = 0.5881033925343799
CVaR policy
>	>	v	v	v	v	>	v	
>	>	v	v	>	>	>	v	
v	>	v	v	v	>	v	v	
>	>	v	v	>	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.707076302678681, 0.18237885350856686, 0.18130340145906487, 0.17361662647171583, 0.1736166273515618
==========
iteration 10
==========
weights [-0.34467623 -0.45326995 -0.65711793 -0.41425282 -0.26255775  0.05829845]
expeced value MDP LP -2.4697658965533784
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.48165205 -0.5412006   0.12192639  0.43710632  0.08772943 -0.51135974]
w_map [-0.25336159 -0.43673331 -0.49329315 -0.40586923 -0.49596276  0.30170332] loglik 0.0
accepted/total = 2364/3000 = 0.788
-------
true weights [-0.34467623 -0.45326995 -0.65711793 -0.41425282 -0.26255775  0.05829845]
features
0 	1 	1 	4 	4 	2 	2 	1 	
2 	4 	2 	1 	2 	4 	1 	4 	
4 	4 	0 	0 	4 	4 	1 	2 	
3 	1 	3 	4 	1 	1 	1 	0 	
3 	2 	1 	4 	2 	3 	4 	3 	
1 	3 	1 	3 	1 	1 	0 	1 	
4 	0 	1 	4 	3 	1 	0 	2 	
1 	4 	3 	2 	4 	1 	3 	5 	
optimal policy
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.40	-4.10	-3.98	-3.56	-3.47	-3.24	-3.23	-3.08	
-4.30	-3.68	-3.85	-3.33	-3.24	-2.61	-2.60	-2.66	
-3.68	-3.45	-3.22	-2.91	-2.61	-2.37	-2.16	-2.42	
-3.78	-3.40	-2.98	-2.59	-2.56	-2.13	-1.73	-1.78	
-3.52	-3.41	-2.78	-2.35	-2.33	-1.69	-1.29	-1.45	
-3.14	-2.86	-2.54	-2.11	-1.90	-1.48	-1.04	-1.05	
-2.71	-2.47	-2.15	-1.71	-1.46	-1.14	-0.70	-0.60	
-2.78	-2.35	-2.10	-1.71	-1.06	-0.81	-0.36	0.06	
map_weights [-0.25336159 -0.43673331 -0.49329315 -0.40586923 -0.49596276  0.30170332]
MAP reward
-0.25	-0.44	-0.44	-0.50	-0.50	-0.49	-0.49	-0.44	
-0.49	-0.50	-0.49	-0.44	-0.49	-0.50	-0.44	-0.50	
-0.50	-0.50	-0.25	-0.25	-0.50	-0.50	-0.44	-0.49	
-0.41	-0.44	-0.41	-0.50	-0.44	-0.44	-0.44	-0.25	
-0.41	-0.49	-0.44	-0.50	-0.49	-0.41	-0.50	-0.41	
-0.44	-0.41	-0.44	-0.41	-0.44	-0.44	-0.25	-0.44	
-0.50	-0.25	-0.44	-0.50	-0.41	-0.44	-0.25	-0.49	
-0.44	-0.50	-0.41	-0.49	-0.50	-0.44	-0.41	0.30	
Map policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.028186323724858
mean w [-0.32946321 -0.43954655 -0.31256626 -0.34186267 -0.31991091  0.27261604]
Mean policy from posterior
v	v	v	>	v	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.33	-0.44	-0.44	-0.32	-0.32	-0.31	-0.31	-0.44	
-0.31	-0.32	-0.31	-0.44	-0.31	-0.32	-0.44	-0.32	
-0.32	-0.32	-0.33	-0.33	-0.32	-0.32	-0.44	-0.31	
-0.34	-0.44	-0.34	-0.32	-0.44	-0.44	-0.44	-0.33	
-0.34	-0.31	-0.44	-0.32	-0.31	-0.34	-0.32	-0.34	
-0.44	-0.34	-0.44	-0.34	-0.44	-0.44	-0.33	-0.44	
-0.32	-0.33	-0.44	-0.32	-0.34	-0.44	-0.33	-0.31	
-0.44	-0.32	-0.34	-0.31	-0.32	-0.44	-0.34	0.27	
mean = 0.3149224403644033, map = 0.21866204853021198
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	>	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	v	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	v	v	
v	v	>	v	v	>	v	v	
>	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.2786237313110518, 0.2594189566642533, 0.23911882850348976, 0.26185803218052817, 0.3149224403304012
==========
iteration 11
==========
weights [-0.58041721 -0.34975756 -0.46086771 -0.14345249 -0.29632741  0.46903935]
expeced value MDP LP -1.45901076468911
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.0950597   0.32980473 -0.74074469 -0.44340207 -0.20107218 -0.31057106]
w_map [-0.29615921 -0.062434   -0.65534324 -0.44024831 -0.38442335 -0.37056318] loglik 0.0
accepted/total = 2395/3000 = 0.7983333333333333
-------
true weights [-0.58041721 -0.34975756 -0.46086771 -0.14345249 -0.29632741  0.46903935]
features
0 	3 	2 	2 	3 	2 	1 	0 	
3 	0 	2 	0 	3 	1 	3 	1 	
4 	3 	0 	2 	3 	0 	4 	2 	
1 	2 	0 	1 	1 	0 	4 	4 	
4 	0 	1 	0 	4 	3 	2 	1 	
4 	4 	3 	1 	1 	2 	2 	0 	
3 	4 	4 	4 	4 	3 	0 	2 	
3 	3 	4 	2 	3 	4 	3 	5 	
optimal policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.12	-2.73	-2.62	-2.18	-1.73	-2.18	-2.20	-2.55	
-2.56	-3.10	-2.61	-2.17	-1.61	-1.94	-1.87	-1.99	
-2.44	-2.54	-2.49	-1.92	-1.48	-1.86	-1.74	-1.66	
-2.17	-2.42	-2.04	-1.68	-1.35	-1.29	-1.46	-1.21	
-1.84	-1.98	-1.47	-1.58	-1.01	-0.72	-1.17	-0.92	
-1.56	-1.42	-1.13	-1.05	-0.76	-0.58	-0.72	-0.58	
-1.27	-1.29	-1.00	-0.71	-0.42	-0.12	-0.26	0.00	
-1.14	-1.01	-0.87	-0.58	-0.12	0.02	0.32	0.47	
map_weights [-0.29615921 -0.062434   -0.65534324 -0.44024831 -0.38442335 -0.37056318]
MAP reward
-0.30	-0.44	-0.66	-0.66	-0.44	-0.66	-0.06	-0.30	
-0.44	-0.30	-0.66	-0.30	-0.44	-0.06	-0.44	-0.06	
-0.38	-0.44	-0.30	-0.66	-0.44	-0.30	-0.38	-0.66	
-0.06	-0.66	-0.30	-0.06	-0.06	-0.30	-0.38	-0.38	
-0.38	-0.30	-0.06	-0.30	-0.38	-0.44	-0.66	-0.06	
-0.38	-0.38	-0.44	-0.06	-0.06	-0.66	-0.66	-0.30	
-0.44	-0.38	-0.38	-0.38	-0.38	-0.44	-0.30	-0.66	
-0.44	-0.44	-0.38	-0.66	-0.44	-0.38	-0.44	-0.37	
Map policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	v	v	
v	>	>	v	<	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.40480386673877
mean w [-0.28747057 -0.42206405 -0.35513141 -0.30395241 -0.40866943 -0.21775702]
Mean policy from posterior
>	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.29	-0.30	-0.36	-0.36	-0.30	-0.36	-0.42	-0.29	
-0.30	-0.29	-0.36	-0.29	-0.30	-0.42	-0.30	-0.42	
-0.41	-0.30	-0.29	-0.36	-0.30	-0.29	-0.41	-0.36	
-0.42	-0.36	-0.29	-0.42	-0.42	-0.29	-0.41	-0.41	
-0.41	-0.29	-0.42	-0.29	-0.41	-0.30	-0.36	-0.42	
-0.41	-0.41	-0.30	-0.42	-0.42	-0.36	-0.36	-0.29	
-0.30	-0.41	-0.41	-0.41	-0.41	-0.30	-0.29	-0.36	
-0.30	-0.30	-0.41	-0.36	-0.30	-0.41	-0.30	-0.22	
mean = 0.4230451826014656, map = 0.5651828422996044
CVaR policy
v	v	>	>	>	>	>	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	v	v	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	>	v	
>	v	v	>	>	v	>	v	
>	>	v	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	v	>	>	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.6026843569445153, 0.6242481472993284, 0.5098707829351294, 0.42612874297786996, 0.4230451748869246
==========
iteration 12
==========
weights [-0.52224542 -0.36797392 -0.51471427 -0.42126538 -0.30986084  0.23118365]
expeced value MDP LP -2.5537975529811376
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.57418339  0.38701315 -0.53958112  0.03555347 -0.35164342 -0.32321715]
w_map [-0.47086823 -0.52109508 -0.35901084 -0.35805095 -0.09822867 -0.48990294] loglik 0.0
accepted/total = 2404/3000 = 0.8013333333333333
-------
true weights [-0.52224542 -0.36797392 -0.51471427 -0.42126538 -0.30986084  0.23118365]
features
2 	1 	3 	1 	0 	4 	0 	3 	
1 	3 	4 	4 	4 	2 	0 	2 	
0 	2 	0 	4 	4 	0 	1 	0 	
2 	4 	4 	3 	1 	2 	3 	4 	
2 	0 	4 	3 	4 	0 	0 	2 	
1 	2 	0 	4 	3 	1 	2 	0 	
1 	0 	1 	0 	3 	0 	4 	0 	
0 	1 	2 	3 	1 	1 	2 	5 	
optimal policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.78	-4.31	-3.98	-3.65	-3.53	-3.63	-3.36	-3.01	
-4.31	-3.98	-3.59	-3.32	-3.04	-3.35	-2.86	-2.62	
-4.33	-3.85	-3.53	-3.04	-2.75	-2.86	-2.37	-2.12	
-3.85	-3.37	-3.09	-2.87	-2.47	-2.46	-2.02	-1.62	
-3.78	-3.30	-2.81	-2.52	-2.12	-1.97	-1.61	-1.32	
-3.45	-3.11	-2.62	-2.12	-1.83	-1.46	-1.10	-0.81	
-3.11	-2.77	-2.27	-1.93	-1.42	-1.11	-0.59	-0.29	
-2.77	-2.27	-1.92	-1.42	-1.01	-0.65	-0.29	0.23	
map_weights [-0.47086823 -0.52109508 -0.35901084 -0.35805095 -0.09822867 -0.48990294]
MAP reward
-0.36	-0.52	-0.36	-0.52	-0.47	-0.10	-0.47	-0.36	
-0.52	-0.36	-0.10	-0.10	-0.10	-0.36	-0.47	-0.36	
-0.47	-0.36	-0.47	-0.10	-0.10	-0.47	-0.52	-0.47	
-0.36	-0.10	-0.10	-0.36	-0.52	-0.36	-0.36	-0.10	
-0.36	-0.47	-0.10	-0.36	-0.10	-0.47	-0.47	-0.36	
-0.52	-0.36	-0.47	-0.10	-0.36	-0.52	-0.36	-0.47	
-0.52	-0.47	-0.52	-0.47	-0.36	-0.47	-0.10	-0.47	
-0.47	-0.52	-0.36	-0.36	-0.52	-0.52	-0.36	-0.49	
Map policy
>	>	v	v	v	v	>	v	
>	>	>	v	v	<	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0455309220612916
mean w [-0.5283259  -0.27827733 -0.30223373 -0.29076602 -0.33324095  0.13078603]
Mean policy from posterior
>	>	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
v	>	>	v	v	v	v	v	
v	>	v	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.30	-0.28	-0.29	-0.28	-0.53	-0.33	-0.53	-0.29	
-0.28	-0.29	-0.33	-0.33	-0.33	-0.30	-0.53	-0.30	
-0.53	-0.30	-0.53	-0.33	-0.33	-0.53	-0.28	-0.53	
-0.30	-0.33	-0.33	-0.29	-0.28	-0.30	-0.29	-0.33	
-0.30	-0.53	-0.33	-0.29	-0.33	-0.53	-0.53	-0.30	
-0.28	-0.30	-0.53	-0.33	-0.29	-0.28	-0.30	-0.53	
-0.28	-0.53	-0.28	-0.53	-0.29	-0.53	-0.33	-0.53	
-0.53	-0.28	-0.30	-0.29	-0.28	-0.28	-0.30	0.13	
mean = 0.03809163479691646, map = 0.0676197402779164
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
v	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
v	>	>	v	v	v	v	v	
v	>	v	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
v	>	>	v	v	v	v	v	
v	>	v	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.0354420708540264, 0.023233467555906273, 0.02486258928333651, 0.038091630410216126, 0.03809164878142246
==========
iteration 13
==========
weights [-0.16465125 -0.39456712 -0.30432771 -0.20456629 -0.82626103  0.00606123]
expeced value MDP LP -1.792204919258439
demonstration
[(56, 1), (57, 1), (58, 1), (59, 2), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.49320386 -0.41444286  0.45963641 -0.39923176  0.40340147  0.22716225]
w_map [-0.29159679 -0.40095229 -0.34174028 -0.23091152 -0.75185326  0.13718125] loglik -0.693147084370306
accepted/total = 1763/3000 = 0.5876666666666667
-------
true weights [-0.16465125 -0.39456712 -0.30432771 -0.20456629 -0.82626103  0.00606123]
features
3 	3 	1 	2 	1 	3 	0 	0 	
2 	1 	3 	1 	2 	2 	1 	0 	
3 	4 	3 	3 	4 	3 	2 	1 	
0 	0 	2 	4 	2 	0 	1 	4 	
0 	4 	4 	1 	0 	4 	0 	2 	
2 	0 	2 	1 	2 	2 	0 	3 	
0 	4 	4 	3 	2 	0 	3 	3 	
3 	3 	0 	1 	3 	1 	4 	5 	
optimal policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	<	
v	<	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
optimal values
-3.08	-3.13	-2.95	-2.59	-2.31	-1.93	-1.93	-2.07	
-2.91	-2.95	-2.59	-2.40	-2.03	-1.74	-1.78	-1.93	
-2.63	-3.39	-2.63	-2.45	-2.27	-1.45	-1.40	-1.78	
-2.45	-2.59	-2.64	-2.36	-1.55	-1.26	-1.11	-1.52	
-2.31	-2.69	-2.50	-1.69	-1.31	-1.54	-0.72	-0.70	
-2.16	-1.88	-1.73	-1.44	-1.16	-0.86	-0.56	-0.40	
-2.11	-2.59	-1.87	-1.06	-0.86	-0.56	-0.40	-0.20	
-1.97	-1.78	-1.59	-1.44	-1.06	-0.95	-0.82	0.01	
map_weights [-0.29159679 -0.40095229 -0.34174028 -0.23091152 -0.75185326  0.13718125]
MAP reward
-0.23	-0.23	-0.40	-0.34	-0.40	-0.23	-0.29	-0.29	
-0.34	-0.40	-0.23	-0.40	-0.34	-0.34	-0.40	-0.29	
-0.23	-0.75	-0.23	-0.23	-0.75	-0.23	-0.34	-0.40	
-0.29	-0.29	-0.34	-0.75	-0.34	-0.29	-0.40	-0.75	
-0.29	-0.75	-0.75	-0.40	-0.29	-0.75	-0.29	-0.34	
-0.34	-0.29	-0.34	-0.40	-0.34	-0.34	-0.29	-0.23	
-0.29	-0.75	-0.75	-0.23	-0.34	-0.29	-0.23	-0.23	
-0.23	-0.23	-0.29	-0.40	-0.23	-0.40	-0.75	0.14	
Map policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	v	>	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
expeced value MDP LP -1.4533108837978417
mean w [-0.27559938 -0.35515077 -0.16984389 -0.13344046 -0.70212071  0.07915265]
Mean policy from posterior
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	v	>	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
Mean rewards
-0.13	-0.13	-0.36	-0.17	-0.36	-0.13	-0.28	-0.28	
-0.17	-0.36	-0.13	-0.36	-0.17	-0.17	-0.36	-0.28	
-0.13	-0.70	-0.13	-0.13	-0.70	-0.13	-0.17	-0.36	
-0.28	-0.28	-0.17	-0.70	-0.17	-0.28	-0.36	-0.70	
-0.28	-0.70	-0.70	-0.36	-0.28	-0.70	-0.28	-0.17	
-0.17	-0.28	-0.17	-0.36	-0.17	-0.17	-0.28	-0.13	
-0.28	-0.70	-0.70	-0.13	-0.17	-0.28	-0.13	-0.13	
-0.13	-0.13	-0.28	-0.36	-0.13	-0.36	-0.70	0.08	
mean = 0.08326231363274861, map = 0.018010552905419974
CVaR policy
v	>	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	v	>	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	v	>	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
CVaR policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	v	>	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
CVaR policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	v	>	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
CVaR policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	v	>	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
cvar = , 0.09344193052771121, 0.08326235077773791, 0.0832623136430759, 0.08326231364181003, 0.0832623136415438
==========
iteration 14
==========
weights [-0.67693597 -0.56792764 -0.00178009 -0.1635496  -0.26157735  0.35219531]
expeced value MDP LP -0.6461380456201834
demonstration
[(56, 1), (57, 3), (57, 3), (57, 3), (57, 1), (58, 0), (57, 1), (58, 2), (50, 3), (58, 3), (58, 3), (58, 2), (50, 3), (58, 2), (50, 2), (42, 3), (50, 3), (58, 3), (58, 2), (50, 3), (58, 0), (57, 3), (57, 3), (57, 1), (58, 3), (58, 2), (50, 3), (58, 0), (57, 3), (57, 3)]
[-0.3885703   0.02021908  0.42607052 -0.1144785   0.60191146  0.54006063]
w_map [-0.19453621 -0.16197327  0.84398055 -0.17186182 -0.44028239  0.01523461] loglik -24.9667970093833
accepted/total = 2800/3000 = 0.9333333333333333
-------
true weights [-0.67693597 -0.56792764 -0.00178009 -0.1635496  -0.26157735  0.35219531]
features
0 	4 	1 	1 	2 	0 	4 	3 	
2 	2 	3 	1 	4 	4 	2 	1 	
1 	1 	3 	0 	3 	3 	1 	0 	
4 	4 	1 	3 	3 	3 	1 	1 	
3 	4 	1 	1 	3 	0 	3 	1 	
3 	4 	2 	0 	2 	2 	0 	2 	
2 	4 	2 	1 	0 	3 	2 	1 	
3 	2 	2 	0 	0 	0 	1 	5 	
optimal policy
v	v	v	>	^	<	v	<	
>	<	<	<	^	<	<	<	
^	^	^	<	^	<	^	^	
v	v	^	>	v	<	<	v	
v	v	v	>	v	v	>	v	
v	>	v	>	>	<	>	>	
<	v	v	<	^	^	>	v	
>	v	<	<	<	>	>	.	
optimal values
-0.85	-0.44	-0.90	-0.74	-0.18	-0.85	-0.94	-1.10	
-0.18	-0.18	-0.34	-0.90	-0.44	-0.70	-0.69	-1.25	
-0.74	-0.74	-0.50	-1.17	-0.60	-0.75	-1.25	-1.92	
-0.76	-0.95	-1.06	-0.66	-0.50	-0.66	-1.22	-1.30	
-0.50	-0.70	-0.74	-0.90	-0.34	-0.85	-0.90	-0.74	
-0.34	-0.44	-0.18	-0.85	-0.18	-0.18	-0.85	-0.18	
-0.18	-0.44	-0.18	-0.74	-0.85	-0.34	-0.22	-0.22	
-0.34	-0.18	-0.18	-0.85	-1.52	-0.89	-0.22	0.35	
map_weights [-0.19453621 -0.16197327  0.84398055 -0.17186182 -0.44028239  0.01523461]
MAP reward
-0.19	-0.44	-0.16	-0.16	0.84	-0.19	-0.44	-0.17	
0.84	0.84	-0.17	-0.16	-0.44	-0.44	0.84	-0.16	
-0.16	-0.16	-0.17	-0.19	-0.17	-0.17	-0.16	-0.19	
-0.44	-0.44	-0.16	-0.17	-0.17	-0.17	-0.16	-0.16	
-0.17	-0.44	-0.16	-0.16	-0.17	-0.19	-0.17	-0.16	
-0.17	-0.44	0.84	-0.19	0.84	0.84	-0.19	0.84	
0.84	-0.44	0.84	-0.16	-0.19	-0.17	0.84	-0.16	
-0.17	0.84	0.84	-0.19	-0.19	-0.19	-0.16	0.02	
Map policy
v	v	>	>	^	<	<	<	
<	<	<	^	^	^	<	<	
^	^	<	^	^	v	^	v	
^	^	v	v	v	v	>	v	
v	>	v	<	v	v	>	v	
v	>	v	<	>	<	<	>	
<	>	v	<	^	^	>	^	
>	v	<	<	<	^	^	.	
expeced value MDP LP 41.67240171798401
mean w [-0.19490499 -0.23879804  0.432267   -0.16940589 -0.27531264  0.00886281]
Mean policy from posterior
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	>	v	<	^	^	<	^	
>	v	<	<	<	^	^	.	
Mean rewards
-0.19	-0.28	-0.24	-0.24	0.43	-0.19	-0.28	-0.17	
0.43	0.43	-0.17	-0.24	-0.28	-0.28	0.43	-0.24	
-0.24	-0.24	-0.17	-0.19	-0.17	-0.17	-0.24	-0.19	
-0.28	-0.28	-0.24	-0.17	-0.17	-0.17	-0.24	-0.24	
-0.17	-0.28	-0.24	-0.24	-0.17	-0.19	-0.17	-0.24	
-0.17	-0.28	0.43	-0.19	0.43	0.43	-0.19	0.43	
0.43	-0.28	0.43	-0.24	-0.19	-0.17	0.43	-0.24	
-0.17	0.43	0.43	-0.19	-0.19	-0.19	-0.24	0.01	
mean = 0.08582470242374296, map = 0.15108766790099426
CVaR policy
v	v	v	>	^	<	<	<	
>	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	v	<	>	<	>	>	
<	>	v	<	^	^	^	^	
>	v	^	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
>	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	v	<	>	<	>	>	
<	v	^	<	^	^	^	^	
>	v	v	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	<	>	<	>	>	
<	>	v	<	^	^	^	^	
>	v	^	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	v	v	<	^	^	<	^	
>	v	v	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	v	v	<	^	^	<	^	
>	v	v	<	<	^	^	.	
cvar = , 0.10483177678101308, 0.10483247371156168, 0.10956959964128754, 0.08583082317151203, 0.0858249698535668
==========
iteration 15
==========
weights [-0.17112211 -0.63055689 -0.18926621 -0.15706279 -0.03268814  0.71523164]
expeced value MDP LP -0.3725553197038518
demonstration
[(56, 2), (48, 1), (49, 2), (41, 1), (42, 1), (43, 1), (44, 3), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.04448336 -0.19479622  0.00876614  0.13439286  0.50684707 -0.8276735 ]
w_map [-0.23564916 -0.82853075 -0.38712857 -0.25399755 -0.11286621 -0.175739  ] loglik -2.869356631407527e-05
accepted/total = 1156/3000 = 0.38533333333333336
-------
true weights [-0.17112211 -0.63055689 -0.18926621 -0.15706279 -0.03268814  0.71523164]
features
1 	4 	3 	3 	4 	4 	1 	2 	
2 	3 	0 	1 	2 	1 	2 	2 	
4 	1 	1 	2 	4 	0 	3 	0 	
4 	3 	1 	0 	4 	1 	1 	2 	
1 	1 	1 	1 	1 	4 	0 	0 	
0 	0 	4 	2 	4 	1 	2 	3 	
0 	4 	1 	3 	3 	4 	2 	4 	
1 	2 	1 	2 	0 	0 	4 	5 	
optimal policy
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.54	-0.92	-0.90	-0.75	-0.60	-0.62	-1.00	-0.41	
-1.14	-1.07	-1.06	-1.20	-0.57	-0.98	-0.37	-0.22	
-0.96	-1.53	-1.20	-0.57	-0.38	-0.36	-0.19	-0.03	
-0.94	-0.91	-1.19	-0.57	-0.40	-0.51	-0.47	0.14	
-0.93	-0.76	-0.59	-0.56	-0.37	0.13	0.16	0.34	
-0.30	-0.13	0.04	0.07	0.26	-0.18	0.32	0.51	
-0.34	-0.17	-0.49	0.14	0.30	0.46	0.48	0.68	
-0.96	-0.35	-0.50	0.13	0.32	0.50	0.68	0.72	
map_weights [-0.23564916 -0.82853075 -0.38712857 -0.25399755 -0.11286621 -0.175739  ]
MAP reward
-0.83	-0.11	-0.25	-0.25	-0.11	-0.11	-0.83	-0.39	
-0.39	-0.25	-0.24	-0.83	-0.39	-0.83	-0.39	-0.39	
-0.11	-0.83	-0.83	-0.39	-0.11	-0.24	-0.25	-0.24	
-0.11	-0.25	-0.83	-0.24	-0.11	-0.83	-0.83	-0.39	
-0.83	-0.83	-0.83	-0.83	-0.83	-0.11	-0.24	-0.24	
-0.24	-0.24	-0.11	-0.39	-0.11	-0.83	-0.39	-0.25	
-0.24	-0.11	-0.83	-0.25	-0.25	-0.11	-0.39	-0.11	
-0.83	-0.39	-0.83	-0.39	-0.24	-0.24	-0.11	-0.18	
Map policy
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.2151070298495166
mean w [-0.160981   -0.62905173 -0.32542807 -0.29481654 -0.04859148  0.13023412]
Mean policy from posterior
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	<	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.63	-0.05	-0.29	-0.29	-0.05	-0.05	-0.63	-0.33	
-0.33	-0.29	-0.16	-0.63	-0.33	-0.63	-0.33	-0.33	
-0.05	-0.63	-0.63	-0.33	-0.05	-0.16	-0.29	-0.16	
-0.05	-0.29	-0.63	-0.16	-0.05	-0.63	-0.63	-0.33	
-0.63	-0.63	-0.63	-0.63	-0.63	-0.05	-0.16	-0.16	
-0.16	-0.16	-0.05	-0.33	-0.05	-0.63	-0.33	-0.29	
-0.16	-0.05	-0.63	-0.29	-0.29	-0.05	-0.33	-0.05	
-0.63	-0.33	-0.63	-0.33	-0.16	-0.16	-0.05	0.13	
mean = 0.010404395023667412, map = 0.0008570158002709771
CVaR policy
>	>	>	>	v	<	v	v	
v	^	>	v	v	v	v	v	
v	v	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	^	>	v	v	v	v	v	
v	<	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	<	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	<	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	<	>	>	v	>	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.017057177047543504, 0.01520089582067935, 0.010404395054223192, 0.010404395026423818, 0.010404395058046245
==========
iteration 16
==========
weights [-0.2406627  -0.2448106  -0.93441745 -0.05926605 -0.02083348  0.07118127]
expeced value MDP LP -0.8498426005386726
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.24728227  0.32260628  0.58781628 -0.29179958  0.56296726  0.29524492]
w_map [-0.5866961  -0.50999834 -0.22794491 -0.3578473  -0.42107558  0.19588558] loglik 0.0
accepted/total = 2394/3000 = 0.798
-------
true weights [-0.2406627  -0.2448106  -0.93441745 -0.05926605 -0.02083348  0.07118127]
features
4 	1 	1 	2 	2 	4 	3 	0 	
1 	4 	1 	3 	2 	4 	3 	1 	
3 	1 	3 	0 	3 	1 	4 	1 	
3 	4 	3 	1 	3 	1 	2 	3 	
2 	2 	2 	3 	1 	4 	3 	3 	
3 	4 	2 	3 	0 	0 	2 	4 	
4 	2 	3 	0 	0 	1 	1 	1 	
3 	1 	1 	0 	4 	4 	1 	5 	
optimal policy
v	v	v	v	>	v	v	<	
v	v	v	v	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	<	>	^	v	^	>	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.26	-1.40	-1.43	-1.88	-1.58	-0.65	-0.67	-0.91	
-1.26	-1.17	-1.19	-0.96	-1.56	-0.63	-0.62	-0.79	
-1.02	-1.16	-0.96	-0.91	-0.67	-0.80	-0.56	-0.55	
-0.97	-0.92	-0.91	-0.86	-0.62	-0.57	-1.24	-0.31	
-1.90	-1.85	-1.55	-0.62	-0.57	-0.33	-0.31	-0.25	
-1.04	-1.05	-1.60	-0.67	-0.69	-0.56	-1.13	-0.19	
-0.99	-1.67	-0.74	-0.69	-0.45	-0.44	-0.42	-0.17	
-0.98	-0.93	-0.69	-0.45	-0.21	-0.19	-0.17	0.07	
map_weights [-0.5866961  -0.50999834 -0.22794491 -0.3578473  -0.42107558  0.19588558]
MAP reward
-0.42	-0.51	-0.51	-0.23	-0.23	-0.42	-0.36	-0.59	
-0.51	-0.42	-0.51	-0.36	-0.23	-0.42	-0.36	-0.51	
-0.36	-0.51	-0.36	-0.59	-0.36	-0.51	-0.42	-0.51	
-0.36	-0.42	-0.36	-0.51	-0.36	-0.51	-0.23	-0.36	
-0.23	-0.23	-0.23	-0.36	-0.51	-0.42	-0.36	-0.36	
-0.36	-0.42	-0.23	-0.36	-0.59	-0.59	-0.23	-0.42	
-0.42	-0.23	-0.36	-0.59	-0.59	-0.51	-0.51	-0.51	
-0.36	-0.51	-0.51	-0.59	-0.42	-0.42	-0.51	0.20	
Map policy
>	>	>	>	v	>	v	v	
v	>	>	>	>	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.103284766305066
mean w [-0.39313732 -0.29762467 -0.36921673 -0.42464143 -0.2711288   0.04092519]
Mean policy from posterior
>	v	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	v	>	v	>	v	>	v	
>	v	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.27	-0.30	-0.30	-0.37	-0.37	-0.27	-0.42	-0.39	
-0.30	-0.27	-0.30	-0.42	-0.37	-0.27	-0.42	-0.30	
-0.42	-0.30	-0.42	-0.39	-0.42	-0.30	-0.27	-0.30	
-0.42	-0.27	-0.42	-0.30	-0.42	-0.30	-0.37	-0.42	
-0.37	-0.37	-0.37	-0.42	-0.30	-0.27	-0.42	-0.42	
-0.42	-0.27	-0.37	-0.42	-0.39	-0.39	-0.37	-0.27	
-0.27	-0.37	-0.42	-0.39	-0.39	-0.30	-0.30	-0.30	
-0.42	-0.30	-0.30	-0.39	-0.27	-0.27	-0.30	0.04	
mean = 0.5472059993080647, map = 1.1850593153710018
CVaR policy
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	v	>	v	>	v	>	v	
>	v	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 1.097889535140065, 1.040521978195455, 0.6120607242571527, 0.612060723555955, 0.5472059993139302
==========
iteration 17
==========
weights [-0.79093218 -0.13423246 -0.04159454 -0.24763618 -0.37166243  0.3939812 ]
expeced value MDP LP -0.9781668881702649
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.28104899  0.82814557  0.33714117 -0.14930145 -0.09356733  0.30079302]
w_map [-0.52343469 -0.069487   -0.09325627 -0.28485847 -0.75912337  0.2346877 ] loglik -0.22992778117368554
accepted/total = 1596/3000 = 0.532
-------
true weights [-0.79093218 -0.13423246 -0.04159454 -0.24763618 -0.37166243  0.3939812 ]
features
4 	2 	4 	3 	4 	2 	0 	1 	
4 	3 	0 	4 	3 	0 	0 	3 	
2 	0 	4 	3 	1 	3 	1 	0 	
1 	2 	4 	1 	4 	1 	0 	1 	
1 	2 	3 	1 	2 	4 	1 	2 	
4 	0 	3 	2 	0 	1 	0 	0 	
0 	1 	0 	4 	2 	2 	1 	3 	
0 	2 	4 	0 	1 	0 	1 	5 	
optimal policy
v	v	>	v	v	<	<	v	
v	<	v	v	v	v	v	v	
v	v	>	v	>	v	<	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	<	<	
^	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
optimal values
-1.70	-1.60	-1.83	-1.48	-1.53	-1.55	-2.33	-1.86	
-1.34	-1.57	-2.02	-1.24	-1.17	-1.58	-1.71	-1.74	
-0.98	-1.60	-1.24	-0.88	-0.93	-0.80	-0.93	-1.51	
-0.94	-0.82	-1.00	-0.64	-0.83	-0.56	-1.35	-0.72	
-0.91	-0.78	-0.75	-0.51	-0.47	-0.43	-0.56	-0.60	
-1.27	-1.40	-0.62	-0.38	-0.76	-0.06	-0.67	-0.65	
-2.03	-1.25	-1.13	-0.34	0.03	0.08	0.12	0.14	
-2.06	-1.28	-1.25	-0.89	-0.10	-0.54	0.26	0.39	
map_weights [-0.52343469 -0.069487   -0.09325627 -0.28485847 -0.75912337  0.2346877 ]
MAP reward
-0.76	-0.09	-0.76	-0.28	-0.76	-0.09	-0.52	-0.07	
-0.76	-0.28	-0.52	-0.76	-0.28	-0.52	-0.52	-0.28	
-0.09	-0.52	-0.76	-0.28	-0.07	-0.28	-0.07	-0.52	
-0.07	-0.09	-0.76	-0.07	-0.76	-0.07	-0.52	-0.07	
-0.07	-0.09	-0.28	-0.07	-0.09	-0.76	-0.07	-0.09	
-0.76	-0.52	-0.28	-0.09	-0.52	-0.07	-0.52	-0.52	
-0.52	-0.07	-0.52	-0.76	-0.09	-0.09	-0.07	-0.28	
-0.52	-0.09	-0.76	-0.52	-0.07	-0.52	-0.07	0.23	
Map policy
>	v	<	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	<	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
expeced value MDP LP -0.9876143150698887
mean w [-0.48870525 -0.10334759 -0.09789651 -0.51889712 -0.37563835  0.41939795]
Mean policy from posterior
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	<	
^	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
Mean rewards
-0.38	-0.10	-0.38	-0.52	-0.38	-0.10	-0.49	-0.10	
-0.38	-0.52	-0.49	-0.38	-0.52	-0.49	-0.49	-0.52	
-0.10	-0.49	-0.38	-0.52	-0.10	-0.52	-0.10	-0.49	
-0.10	-0.10	-0.38	-0.10	-0.38	-0.10	-0.49	-0.10	
-0.10	-0.10	-0.52	-0.10	-0.10	-0.38	-0.10	-0.10	
-0.38	-0.49	-0.52	-0.10	-0.49	-0.10	-0.49	-0.49	
-0.49	-0.10	-0.49	-0.38	-0.10	-0.10	-0.10	-0.52	
-0.49	-0.10	-0.38	-0.49	-0.10	-0.49	-0.10	0.42	
mean = 0.1226350265974171, map = 0.30750302348645986
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
^	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
CVaR policy
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	<	
^	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
CVaR policy
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	<	
^	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
CVaR policy
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	<	
^	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
cvar = , 0.06914925711636177, 0.0772426732319641, 0.1129138374955655, 0.11291383693765511, 0.12263503632546413
==========
iteration 18
==========
weights [-0.44412316 -0.0720415  -0.15141447 -0.47629387 -0.45369036  0.58476277]
expeced value MDP LP -1.0142718814466793
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.14722831 -0.18277789  0.10845285 -0.26249133 -0.70178113  0.60971763]
w_map [-0.37185118 -0.19014698 -0.23165426 -0.39746135 -0.73089176 -0.28236293] loglik -0.6931475021248659
accepted/total = 1702/3000 = 0.5673333333333334
-------
true weights [-0.44412316 -0.0720415  -0.15141447 -0.47629387 -0.45369036  0.58476277]
features
3 	1 	1 	3 	4 	3 	4 	4 	
4 	0 	1 	1 	2 	0 	3 	4 	
4 	3 	0 	4 	0 	3 	4 	2 	
0 	0 	4 	1 	0 	0 	4 	3 	
2 	1 	3 	2 	0 	4 	3 	4 	
0 	1 	4 	4 	1 	0 	2 	3 	
2 	3 	1 	1 	1 	4 	0 	0 	
1 	2 	2 	4 	4 	1 	1 	5 	
optimal policy
>	>	v	v	v	v	v	v	
>	>	>	v	<	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-1.96	-1.50	-1.44	-1.79	-1.91	-2.35	-2.36	-2.28	
-2.25	-1.82	-1.39	-1.33	-1.47	-1.90	-1.92	-1.84	
-1.84	-1.72	-1.70	-1.27	-1.49	-1.82	-1.46	-1.40	
-1.40	-1.26	-1.27	-0.82	-1.05	-1.36	-1.02	-1.26	
-0.97	-0.82	-1.16	-0.76	-0.61	-0.92	-0.57	-0.79	
-1.19	-0.76	-0.69	-0.62	-0.17	-0.47	-0.09	-0.34	
-0.75	-0.72	-0.24	-0.17	-0.10	-0.03	0.06	0.13	
-0.60	-0.54	-0.39	-0.48	-0.03	0.43	0.51	0.58	
map_weights [-0.37185118 -0.19014698 -0.23165426 -0.39746135 -0.73089176 -0.28236293]
MAP reward
-0.40	-0.19	-0.19	-0.40	-0.73	-0.40	-0.73	-0.73	
-0.73	-0.37	-0.19	-0.19	-0.23	-0.37	-0.40	-0.73	
-0.73	-0.40	-0.37	-0.73	-0.37	-0.40	-0.73	-0.23	
-0.37	-0.37	-0.73	-0.19	-0.37	-0.37	-0.73	-0.40	
-0.23	-0.19	-0.40	-0.23	-0.37	-0.73	-0.40	-0.73	
-0.37	-0.19	-0.73	-0.73	-0.19	-0.37	-0.23	-0.40	
-0.23	-0.40	-0.19	-0.19	-0.19	-0.73	-0.37	-0.37	
-0.19	-0.23	-0.23	-0.73	-0.73	-0.19	-0.19	-0.28	
Map policy
>	>	v	v	v	v	v	v	
>	>	>	>	v	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	v	v	>	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.7120536366201111
mean w [-0.39758184 -0.09771544 -0.24811496 -0.48477787 -0.51435554  0.04752808]
Mean policy from posterior
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.48	-0.10	-0.10	-0.48	-0.51	-0.48	-0.51	-0.51	
-0.51	-0.40	-0.10	-0.10	-0.25	-0.40	-0.48	-0.51	
-0.51	-0.48	-0.40	-0.51	-0.40	-0.48	-0.51	-0.25	
-0.40	-0.40	-0.51	-0.10	-0.40	-0.40	-0.51	-0.48	
-0.25	-0.10	-0.48	-0.25	-0.40	-0.51	-0.48	-0.51	
-0.40	-0.10	-0.51	-0.51	-0.10	-0.40	-0.25	-0.48	
-0.25	-0.48	-0.10	-0.10	-0.10	-0.51	-0.40	-0.40	
-0.10	-0.25	-0.25	-0.51	-0.51	-0.10	-0.10	0.05	
mean = 0.01419611341794802, map = 0.21084360091007315
CVaR policy
>	>	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.05046751421764273, 0.023005989798501103, 0.019246443629925114, 0.014196112664462301, 0.014196124092119566
==========
iteration 19
==========
weights [-3.86537140e-01 -3.90661519e-01 -3.04991611e-01 -4.46380362e-05
 -6.17544866e-01  4.72854175e-01]
expeced value MDP LP -0.437465840610313
demonstration
[(56, 1), (57, 1), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3), (58, 3)]
[-0.17316451  0.21474849 -0.69486615  0.00992958 -0.43170079  0.50457309]
w_map [-0.0150933  -0.50754697  0.11782383  0.55103632 -0.65054371  0.03791663] loglik -0.6930918254392964
accepted/total = 2792/3000 = 0.9306666666666666
-------
true weights [-3.86537140e-01 -3.90661519e-01 -3.04991611e-01 -4.46380362e-05
 -6.17544866e-01  4.72854175e-01]
features
3 	4 	4 	0 	2 	3 	4 	2 	
4 	2 	0 	0 	0 	4 	1 	0 	
2 	0 	3 	3 	4 	1 	3 	1 	
1 	1 	1 	4 	0 	0 	1 	2 	
1 	2 	1 	3 	4 	3 	2 	3 	
3 	0 	0 	4 	3 	3 	0 	4 	
0 	1 	4 	1 	3 	0 	3 	2 	
1 	0 	3 	2 	1 	4 	2 	5 	
optimal policy
<	<	v	>	>	^	<	<	
^	>	v	v	^	^	<	v	
>	>	>	<	<	v	v	v	
v	^	^	^	>	v	v	v	
v	v	^	>	v	v	>	>	
<	<	<	>	v	<	v	v	
^	v	v	>	^	>	v	v	
>	>	v	<	^	>	>	.	
optimal values
-0.00	-0.62	-1.00	-0.69	-0.31	-0.00	-0.62	-0.92	
-0.62	-0.69	-0.39	-0.39	-0.69	-0.62	-1.01	-1.08	
-0.69	-0.39	-0.00	-0.00	-0.62	-0.78	-0.69	-0.70	
-0.78	-0.78	-0.40	-0.62	-0.77	-0.39	-0.70	-0.31	
-0.40	-0.69	-0.78	-0.62	-0.62	-0.00	-0.31	-0.00	
-0.00	-0.39	-0.77	-0.62	-0.00	-0.00	-0.23	-0.46	
-0.39	-0.78	-0.62	-0.40	-0.00	-0.23	0.16	0.16	
-0.78	-0.39	-0.00	-0.31	-0.40	-0.46	0.16	0.47	
map_weights [-0.0150933  -0.50754697  0.11782383  0.55103632 -0.65054371  0.03791663]
MAP reward
0.55	-0.65	-0.65	-0.02	0.12	0.55	-0.65	0.12	
-0.65	0.12	-0.02	-0.02	-0.02	-0.65	-0.51	-0.02	
0.12	-0.02	0.55	0.55	-0.65	-0.51	0.55	-0.51	
-0.51	-0.51	-0.51	-0.65	-0.02	-0.02	-0.51	0.12	
-0.51	0.12	-0.51	0.55	-0.65	0.55	0.12	0.55	
0.55	-0.02	-0.02	-0.65	0.55	0.55	-0.02	-0.65	
-0.02	-0.51	-0.65	-0.51	0.55	-0.02	0.55	0.12	
-0.51	-0.02	0.55	0.12	-0.51	-0.65	0.12	0.04	
Map policy
<	<	v	>	>	^	<	<	
^	>	v	v	^	^	<	v	
>	>	>	<	<	v	v	v	
^	^	^	^	>	v	v	v	
v	v	<	>	v	v	<	>	
<	<	<	>	>	<	<	^	
^	v	v	>	^	<	<	<	
>	>	v	<	^	^	^	.	
expeced value MDP LP 49.08766091788304
mean w [-0.22910925 -0.04183073 -0.15353405  0.50641277 -0.14083184 -0.10469   ]
Mean policy from posterior
<	<	<	>	>	^	<	<	
^	^	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	v	v	<	>	
<	<	>	>	>	^	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
Mean rewards
0.51	-0.14	-0.14	-0.23	-0.15	0.51	-0.14	-0.15	
-0.14	-0.15	-0.23	-0.23	-0.23	-0.14	-0.04	-0.23	
-0.15	-0.23	0.51	0.51	-0.14	-0.04	0.51	-0.04	
-0.04	-0.04	-0.04	-0.14	-0.23	-0.23	-0.04	-0.15	
-0.04	-0.15	-0.04	0.51	-0.14	0.51	-0.15	0.51	
0.51	-0.23	-0.23	-0.14	0.51	0.51	-0.23	-0.14	
-0.23	-0.04	-0.14	-0.04	0.51	-0.23	0.51	-0.15	
-0.04	-0.23	0.51	-0.15	-0.04	-0.14	-0.15	-0.10	
mean = 0.09471858139342137, map = 0.06066095039535879
CVaR policy
<	<	<	<	>	^	<	<	
^	<	v	v	v	^	^	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	>	>	>	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	^	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	^	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	>	^	v	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	<	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	^	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	>	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	>	v	<	>	
<	<	>	>	>	<	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	v	v	<	>	
<	<	>	>	>	<	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
cvar = , 0.1124240427715425, 0.09813336731189493, 0.094734517562442, 0.0947258345898706, 0.09472239481510358
==========
iteration 20
==========
weights [-0.56426135 -0.10031456 -0.05486098 -0.05723787 -0.54609201  0.60584135]
expeced value MDP LP -0.14838274413885394
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.16245979  0.00563443  0.86834337  0.24891363 -0.01407472  0.39673513]
w_map [-0.19703668 -0.48070067 -0.42392819 -0.57568998 -0.46640079 -0.0379429 ] loglik 0.0
accepted/total = 2454/3000 = 0.818
-------
true weights [-0.56426135 -0.10031456 -0.05486098 -0.05723787 -0.54609201  0.60584135]
features
4 	2 	4 	1 	2 	2 	1 	4 	
4 	0 	4 	2 	1 	2 	3 	4 	
0 	3 	3 	1 	0 	4 	3 	0 	
4 	2 	1 	2 	3 	3 	3 	4 	
0 	4 	1 	3 	4 	2 	2 	2 	
1 	0 	1 	2 	3 	4 	4 	1 	
0 	2 	3 	3 	0 	3 	2 	3 	
3 	1 	2 	4 	3 	2 	3 	5 	
optimal policy
>	>	>	>	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	<	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	v	
>	>	>	^	<	v	v	v	
>	>	>	^	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.26	-0.72	-0.67	-0.13	-0.03	0.03	0.04	-0.51	
-1.23	-0.69	-0.62	-0.07	-0.02	0.08	0.14	-0.41	
-0.69	-0.13	-0.07	-0.02	-0.42	-0.35	0.20	-0.37	
-0.62	-0.07	-0.02	0.09	0.14	0.20	0.26	-0.17	
-1.17	-0.62	-0.07	0.03	-0.29	0.26	0.32	0.38	
-0.78	-0.69	-0.13	-0.03	-0.08	-0.13	-0.07	0.44	
-0.76	-0.19	-0.14	-0.08	-0.15	0.42	0.48	0.54	
-0.34	-0.28	-0.18	-0.13	0.42	0.48	0.54	0.61	
map_weights [-0.19703668 -0.48070067 -0.42392819 -0.57568998 -0.46640079 -0.0379429 ]
MAP reward
-0.47	-0.42	-0.47	-0.48	-0.42	-0.42	-0.48	-0.47	
-0.47	-0.20	-0.47	-0.42	-0.48	-0.42	-0.58	-0.47	
-0.20	-0.58	-0.58	-0.48	-0.20	-0.47	-0.58	-0.20	
-0.47	-0.42	-0.48	-0.42	-0.58	-0.58	-0.58	-0.47	
-0.20	-0.47	-0.48	-0.58	-0.47	-0.42	-0.42	-0.42	
-0.48	-0.20	-0.48	-0.42	-0.58	-0.47	-0.47	-0.48	
-0.20	-0.42	-0.58	-0.58	-0.20	-0.58	-0.42	-0.58	
-0.58	-0.48	-0.42	-0.47	-0.58	-0.42	-0.58	-0.04	
Map policy
v	v	v	>	v	>	>	v	
v	>	>	v	v	v	>	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.2127057301169466
mean w [-0.39349097 -0.28854221 -0.39692286 -0.39335427 -0.28872342  0.086953  ]
Mean policy from posterior
v	>	v	v	v	>	>	v	
>	>	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
v	>	v	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.29	-0.40	-0.29	-0.29	-0.40	-0.40	-0.29	-0.29	
-0.29	-0.39	-0.29	-0.40	-0.29	-0.40	-0.39	-0.29	
-0.39	-0.39	-0.39	-0.29	-0.39	-0.29	-0.39	-0.39	
-0.29	-0.40	-0.29	-0.40	-0.39	-0.39	-0.39	-0.29	
-0.39	-0.29	-0.29	-0.39	-0.29	-0.40	-0.40	-0.40	
-0.29	-0.39	-0.29	-0.40	-0.39	-0.29	-0.29	-0.29	
-0.39	-0.40	-0.39	-0.39	-0.39	-0.39	-0.40	-0.39	
-0.39	-0.29	-0.40	-0.29	-0.39	-0.40	-0.39	0.09	
mean = 0.9443061303663529, map = 0.8450556021891802
CVaR policy
v	v	>	>	v	>	>	v	
v	>	>	v	v	v	>	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	>	>	>	v	v	>	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	>	>	v	
v	>	v	v	v	v	>	v	
v	v	v	>	>	v	>	v	
v	>	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	>	v	
v	>	v	>	v	v	>	v	
v	v	v	>	>	v	>	v	
v	>	v	v	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	>	v	
v	>	v	>	v	v	>	v	
v	v	v	>	>	v	>	v	
v	>	v	v	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.9626982862935474, 0.9827356253841644, 0.8966094261887989, 0.8824380287060305, 0.8824380274639432
==========
iteration 21
==========
weights [-0.10255413 -0.17061703 -0.26391579 -0.67031573 -0.31661541  0.58408257]
expeced value MDP LP -0.7636550177943935
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[ 0.55415926 -0.02421975 -0.08065829 -0.05151678  0.77438215 -0.28895236]
w_map [-0.15077997 -0.33658233 -0.26157685 -0.86117256 -0.23027079  0.03020709] loglik -5.978153005870013e-05
accepted/total = 1000/3000 = 0.3333333333333333
-------
true weights [-0.10255413 -0.17061703 -0.26391579 -0.67031573 -0.31661541  0.58408257]
features
0 	0 	1 	2 	4 	3 	3 	4 	
3 	2 	3 	3 	4 	3 	2 	0 	
1 	2 	0 	1 	4 	0 	4 	1 	
4 	4 	0 	0 	0 	1 	4 	0 	
1 	2 	2 	1 	1 	3 	3 	1 	
3 	1 	4 	0 	4 	0 	0 	1 	
1 	2 	2 	2 	2 	4 	0 	3 	
0 	4 	0 	3 	4 	2 	1 	5 	
optimal policy
>	v	<	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	<	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
optimal values
-1.47	-1.38	-1.54	-1.67	-1.42	-2.07	-1.44	-0.83	
-1.86	-1.29	-1.45	-1.42	-1.11	-1.42	-0.77	-0.52	
-1.20	-1.04	-0.78	-0.76	-0.81	-0.76	-0.73	-0.42	
-1.30	-1.00	-0.69	-0.59	-0.49	-0.66	-0.56	-0.25	
-1.17	-1.01	-0.75	-0.49	-0.39	-0.58	-0.48	-0.15	
-1.47	-0.80	-0.64	-0.33	-0.23	0.09	0.20	0.02	
-1.22	-1.06	-0.80	-0.54	-0.28	-0.02	0.30	-0.09	
-1.29	-1.20	-0.90	-0.85	-0.18	0.14	0.41	0.58	
map_weights [-0.15077997 -0.33658233 -0.26157685 -0.86117256 -0.23027079  0.03020709]
MAP reward
-0.15	-0.15	-0.34	-0.26	-0.23	-0.86	-0.86	-0.23	
-0.86	-0.26	-0.86	-0.86	-0.23	-0.86	-0.26	-0.15	
-0.34	-0.26	-0.15	-0.34	-0.23	-0.15	-0.23	-0.34	
-0.23	-0.23	-0.15	-0.15	-0.15	-0.34	-0.23	-0.15	
-0.34	-0.26	-0.26	-0.34	-0.34	-0.86	-0.86	-0.34	
-0.86	-0.34	-0.23	-0.15	-0.23	-0.15	-0.15	-0.34	
-0.34	-0.26	-0.26	-0.26	-0.26	-0.23	-0.15	-0.86	
-0.15	-0.23	-0.15	-0.86	-0.23	-0.26	-0.34	0.03	
Map policy
>	v	>	>	v	<	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	<	v	v	
>	>	>	>	v	<	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.421843341833763
mean w [-0.10765912 -0.41199901 -0.17280385 -0.68930445 -0.25243198  0.2006275 ]
Mean policy from posterior
>	v	<	>	v	<	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	<	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.11	-0.11	-0.41	-0.17	-0.25	-0.69	-0.69	-0.25	
-0.69	-0.17	-0.69	-0.69	-0.25	-0.69	-0.17	-0.11	
-0.41	-0.17	-0.11	-0.41	-0.25	-0.11	-0.25	-0.41	
-0.25	-0.25	-0.11	-0.11	-0.11	-0.41	-0.25	-0.11	
-0.41	-0.17	-0.17	-0.41	-0.41	-0.69	-0.69	-0.41	
-0.69	-0.41	-0.25	-0.11	-0.25	-0.11	-0.11	-0.41	
-0.41	-0.17	-0.17	-0.17	-0.17	-0.25	-0.11	-0.69	
-0.11	-0.25	-0.11	-0.69	-0.25	-0.17	-0.41	0.20	
mean = 0.10214423705548858, map = 0.03126106431868603
CVaR policy
>	v	<	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	>	v	<	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	>	v	<	v	v	
>	v	v	v	v	v	v	<	
>	>	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	>	v	<	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	<	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	>	v	<	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	<	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.13491912373229786, 0.120782729220966, 0.12348257481637881, 0.10214423589887989, 0.1021442361333661
==========
iteration 22
==========
weights [-0.73515571 -0.56581614 -0.06623291 -0.25651433 -0.24663134  0.09156831]
expeced value MDP LP -1.9062027144048383
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-4.27788784e-04 -4.80228339e-01 -4.43839524e-02 -3.39222108e-01
 -7.51878745e-01  2.94987014e-01]
w_map [-0.27929918 -0.39789523 -0.40077239 -0.47199207 -0.6140194  -0.05706586] loglik 0.0
accepted/total = 2428/3000 = 0.8093333333333333
-------
true weights [-0.73515571 -0.56581614 -0.06623291 -0.25651433 -0.24663134  0.09156831]
features
0 	2 	2 	3 	4 	1 	3 	3 	
1 	1 	1 	4 	4 	2 	4 	4 	
1 	2 	3 	3 	2 	0 	4 	4 	
1 	1 	1 	2 	4 	2 	1 	2 	
3 	2 	4 	3 	1 	3 	1 	4 	
3 	0 	2 	1 	2 	1 	4 	0 	
3 	0 	0 	3 	3 	0 	4 	0 	
2 	2 	1 	2 	2 	4 	1 	5 	
optimal policy
>	>	>	>	v	v	v	v	
v	v	>	>	v	<	v	v	
>	>	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.39	-2.68	-2.64	-2.60	-2.37	-2.73	-2.57	-2.35	
-3.51	-2.97	-2.91	-2.37	-2.14	-2.19	-2.34	-2.11	
-2.97	-2.43	-2.39	-2.15	-1.91	-2.52	-2.11	-1.89	
-2.74	-2.51	-2.46	-1.91	-1.87	-1.80	-2.06	-1.66	
-2.20	-1.96	-1.92	-1.88	-1.64	-1.75	-1.51	-1.61	
-1.98	-2.40	-1.69	-1.64	-1.08	-1.51	-0.96	-1.37	
-1.74	-2.17	-1.81	-1.08	-1.02	-1.45	-0.72	-0.64	
-1.50	-1.44	-1.39	-0.83	-0.78	-0.72	-0.48	0.09	
map_weights [-0.27929918 -0.39789523 -0.40077239 -0.47199207 -0.6140194  -0.05706586]
MAP reward
-0.28	-0.40	-0.40	-0.47	-0.61	-0.40	-0.47	-0.47	
-0.40	-0.40	-0.40	-0.61	-0.61	-0.40	-0.61	-0.61	
-0.40	-0.40	-0.47	-0.47	-0.40	-0.28	-0.61	-0.61	
-0.40	-0.40	-0.40	-0.40	-0.61	-0.40	-0.40	-0.40	
-0.47	-0.40	-0.61	-0.47	-0.40	-0.47	-0.40	-0.61	
-0.47	-0.28	-0.40	-0.40	-0.40	-0.40	-0.61	-0.28	
-0.47	-0.28	-0.28	-0.47	-0.47	-0.28	-0.61	-0.28	
-0.40	-0.40	-0.40	-0.40	-0.40	-0.61	-0.40	-0.06	
Map policy
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	v	v	>	v	v	
>	v	v	>	v	v	v	v	
>	v	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.673927560718166
mean w [-0.51278649 -0.23176672 -0.24822557 -0.43850266 -0.27223325  0.13117123]
Mean policy from posterior
v	v	v	>	>	v	v	v	
v	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	<	
>	>	v	v	v	v	v	<	
v	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.51	-0.25	-0.25	-0.44	-0.27	-0.23	-0.44	-0.44	
-0.23	-0.23	-0.23	-0.27	-0.27	-0.25	-0.27	-0.27	
-0.23	-0.25	-0.44	-0.44	-0.25	-0.51	-0.27	-0.27	
-0.23	-0.23	-0.23	-0.25	-0.27	-0.25	-0.23	-0.25	
-0.44	-0.25	-0.27	-0.44	-0.23	-0.44	-0.23	-0.27	
-0.44	-0.51	-0.25	-0.23	-0.25	-0.23	-0.27	-0.51	
-0.44	-0.51	-0.51	-0.44	-0.44	-0.51	-0.27	-0.51	
-0.25	-0.25	-0.23	-0.25	-0.25	-0.27	-0.23	0.13	
mean = 0.4150037202681993, map = 1.1146583515792252
CVaR policy
>	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
v	v	>	v	v	>	v	v	
v	>	v	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
v	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	v	>	>	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	v	>	>	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	>	>	v	>	v	<	
>	>	v	v	v	v	v	v	
v	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.09898167333225105, 0.19273358901186377, 0.2779388841641173, 0.38496948044794577, 0.41284517472220283
==========
iteration 23
==========
weights [-0.07457546 -0.5154494  -0.65889459 -0.23983411 -0.44147547  0.20539556]
expeced value MDP LP -1.8144946294694744
demonstration
[(56, 1), (57, 1), (58, 1), (59, 2), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.202932    0.02734787 -0.39533692 -0.41079574 -0.78912904 -0.10149698]
w_map [-0.21212236 -0.38898313 -0.77093889 -0.18641829 -0.21672136  0.35725275] loglik -1.3209334781549842e-07
accepted/total = 1757/3000 = 0.5856666666666667
-------
true weights [-0.07457546 -0.5154494  -0.65889459 -0.23983411 -0.44147547  0.20539556]
features
1 	2 	1 	0 	0 	0 	3 	0 	
4 	0 	2 	1 	1 	0 	0 	1 	
4 	2 	3 	1 	1 	1 	1 	2 	
4 	3 	0 	4 	0 	3 	1 	0 	
2 	3 	3 	4 	2 	0 	2 	1 	
0 	3 	3 	4 	2 	2 	4 	2 	
4 	0 	1 	4 	3 	0 	0 	1 	
0 	0 	0 	3 	1 	4 	2 	5 	
optimal policy
>	>	>	>	>	v	v	<	
>	v	v	^	>	v	<	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
optimal values
-3.73	-3.25	-2.61	-2.12	-2.07	-2.01	-2.23	-2.28	
-3.26	-2.85	-2.80	-2.61	-2.45	-1.96	-2.01	-2.51	
-3.00	-2.80	-2.16	-2.38	-1.96	-1.90	-2.40	-2.18	
-2.58	-2.16	-1.94	-1.89	-1.46	-1.40	-1.90	-1.53	
-2.46	-1.98	-2.00	-1.98	-1.82	-1.17	-1.47	-1.47	
-1.82	-1.76	-1.78	-1.55	-1.34	-1.11	-0.82	-0.97	
-1.96	-1.53	-1.63	-1.12	-0.69	-0.45	-0.38	-0.31	
-1.53	-1.47	-1.41	-1.35	-1.20	-0.89	-0.46	0.21	
map_weights [-0.21212236 -0.38898313 -0.77093889 -0.18641829 -0.21672136  0.35725275]
MAP reward
-0.39	-0.77	-0.39	-0.21	-0.21	-0.21	-0.19	-0.21	
-0.22	-0.21	-0.77	-0.39	-0.39	-0.21	-0.21	-0.39	
-0.22	-0.77	-0.19	-0.39	-0.39	-0.39	-0.39	-0.77	
-0.22	-0.19	-0.21	-0.22	-0.21	-0.19	-0.39	-0.21	
-0.77	-0.19	-0.19	-0.22	-0.77	-0.21	-0.77	-0.39	
-0.21	-0.19	-0.19	-0.22	-0.77	-0.77	-0.22	-0.77	
-0.22	-0.21	-0.39	-0.22	-0.19	-0.21	-0.21	-0.39	
-0.21	-0.21	-0.21	-0.19	-0.39	-0.22	-0.77	0.36	
Map policy
v	v	>	v	>	v	v	<	
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	<	v	v	v	
>	v	v	v	<	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	.	
expeced value MDP LP -1.5976607988032039
mean w [-0.13847487 -0.44213903 -0.69723437 -0.14855154 -0.24351706  0.23261986]
Mean policy from posterior
v	v	v	>	>	v	v	<	
v	<	v	v	>	v	<	<	
v	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	^	^	>	>	.	
Mean rewards
-0.44	-0.70	-0.44	-0.14	-0.14	-0.14	-0.15	-0.14	
-0.24	-0.14	-0.70	-0.44	-0.44	-0.14	-0.14	-0.44	
-0.24	-0.70	-0.15	-0.44	-0.44	-0.44	-0.44	-0.70	
-0.24	-0.15	-0.14	-0.24	-0.14	-0.15	-0.44	-0.14	
-0.70	-0.15	-0.15	-0.24	-0.70	-0.14	-0.70	-0.44	
-0.14	-0.15	-0.15	-0.24	-0.70	-0.70	-0.24	-0.70	
-0.24	-0.14	-0.44	-0.24	-0.15	-0.14	-0.14	-0.44	
-0.14	-0.14	-0.14	-0.15	-0.44	-0.24	-0.70	0.23	
mean = 0.10331742643982356, map = 0.24652995613240614
CVaR policy
v	v	>	>	>	v	v	<	
v	v	v	v	>	v	<	v	
v	>	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	<	
v	v	v	v	>	v	<	v	
v	>	v	<	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	^	^	>	>	.	
CVaR policy
v	v	v	>	>	v	v	<	
v	v	v	v	>	v	<	v	
v	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	^	^	>	>	.	
CVaR policy
v	v	v	>	>	v	v	<	
v	v	v	v	>	v	<	<	
v	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	^	^	>	>	.	
CVaR policy
v	v	v	>	>	v	v	<	
v	v	v	v	>	v	<	<	
v	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	^	^	>	>	.	
cvar = , 0.06586506129642689, 0.07226753820116194, 0.08716530521328303, 0.08461627560264229, 0.08461625663827976
==========
iteration 24
==========
weights [-0.3701546  -0.72728912 -0.3132146  -0.20491508 -0.35992136  0.25376989]
expeced value MDP LP -1.807155645393347
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.0073972   0.80005355 -0.37104863 -0.36783258  0.00495745 -0.29471534]
w_map [-0.37230427 -0.38050853 -0.64111791 -0.38450284 -0.3962545  -0.0266562 ] loglik 0.0
accepted/total = 2322/3000 = 0.774
-------
true weights [-0.3701546  -0.72728912 -0.3132146  -0.20491508 -0.35992136  0.25376989]
features
1 	2 	2 	3 	2 	2 	4 	2 	
0 	1 	1 	2 	4 	4 	4 	4 	
0 	0 	0 	3 	1 	0 	0 	2 	
3 	4 	1 	0 	0 	4 	2 	4 	
3 	0 	2 	3 	1 	2 	1 	2 	
4 	1 	2 	4 	1 	3 	2 	1 	
2 	0 	4 	4 	0 	3 	3 	3 	
3 	4 	0 	1 	3 	0 	2 	5 	
optimal policy
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.86	-3.16	-2.88	-2.59	-2.51	-2.22	-2.53	-2.27	
-3.32	-3.51	-3.12	-2.41	-2.27	-1.92	-2.19	-1.98	
-2.98	-2.81	-2.47	-2.12	-2.29	-1.58	-1.85	-1.64	
-2.63	-2.61	-2.63	-1.93	-1.58	-1.22	-1.49	-1.34	
-2.45	-2.27	-1.92	-1.62	-1.59	-0.87	-1.19	-0.99	
-2.42	-2.44	-1.73	-1.43	-1.29	-0.56	-0.47	-0.68	
-2.08	-1.79	-1.43	-1.08	-0.73	-0.36	-0.16	0.05	
-2.24	-2.05	-1.71	-1.35	-0.63	-0.43	-0.06	0.25	
map_weights [-0.37230427 -0.38050853 -0.64111791 -0.38450284 -0.3962545  -0.0266562 ]
MAP reward
-0.38	-0.64	-0.64	-0.38	-0.64	-0.64	-0.40	-0.64	
-0.37	-0.38	-0.38	-0.64	-0.40	-0.40	-0.40	-0.40	
-0.37	-0.37	-0.37	-0.38	-0.38	-0.37	-0.37	-0.64	
-0.38	-0.40	-0.38	-0.37	-0.37	-0.40	-0.64	-0.40	
-0.38	-0.37	-0.64	-0.38	-0.38	-0.64	-0.38	-0.64	
-0.40	-0.38	-0.64	-0.40	-0.38	-0.38	-0.64	-0.38	
-0.64	-0.37	-0.40	-0.40	-0.37	-0.38	-0.38	-0.38	
-0.38	-0.40	-0.37	-0.38	-0.38	-0.37	-0.64	-0.03	
Map policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	v	v	v	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9518668823826406
mean w [-0.27345636 -0.25844712 -0.31953694 -0.45315416 -0.39675269  0.17844719]
Mean policy from posterior
v	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.26	-0.32	-0.32	-0.45	-0.32	-0.32	-0.40	-0.32	
-0.27	-0.26	-0.26	-0.32	-0.40	-0.40	-0.40	-0.40	
-0.27	-0.27	-0.27	-0.45	-0.26	-0.27	-0.27	-0.32	
-0.45	-0.40	-0.26	-0.27	-0.27	-0.40	-0.32	-0.40	
-0.45	-0.27	-0.32	-0.45	-0.26	-0.32	-0.26	-0.32	
-0.40	-0.26	-0.32	-0.40	-0.26	-0.45	-0.32	-0.26	
-0.32	-0.27	-0.40	-0.40	-0.27	-0.45	-0.45	-0.45	
-0.45	-0.40	-0.27	-0.26	-0.45	-0.27	-0.32	0.18	
mean = 0.8657083246228026, map = 0.526078225328626
CVaR policy
v	v	v	v	v	v	v	v	
>	v	v	>	v	>	>	v	
>	v	v	>	v	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	v	v	>	>	v	
>	v	>	v	>	>	>	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
v	v	>	>	>	>	v	v	
>	v	>	v	v	>	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	>	>	v	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	>	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.7602866140875628, 0.9129175275236929, 0.9293715285763757, 0.8797383995388199, 0.8657083246786006
==========
iteration 25
==========
weights [-0.14094762 -0.52527219 -0.49991509 -0.08408736 -0.62976705  0.22501239]
expeced value MDP LP -1.1743076201594294
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.60552738 -0.19137586  0.70608659  0.09526169 -0.17556529 -0.24136204]
w_map [-0.41923476 -0.38222554 -0.3446083  -0.18272532 -0.72481303  0.02546565] loglik -9.748765350536814e-08
accepted/total = 1889/3000 = 0.6296666666666667
-------
true weights [-0.14094762 -0.52527219 -0.49991509 -0.08408736 -0.62976705  0.22501239]
features
4 	3 	1 	1 	2 	4 	1 	4 	
3 	1 	0 	3 	0 	4 	3 	3 	
2 	1 	0 	4 	1 	2 	1 	2 	
4 	0 	3 	1 	2 	3 	4 	0 	
2 	4 	0 	4 	0 	4 	0 	2 	
1 	1 	0 	3 	2 	1 	1 	3 	
0 	2 	0 	1 	0 	3 	3 	3 	
0 	0 	2 	3 	0 	3 	4 	5 	
optimal policy
>	>	v	v	v	>	v	v	
>	>	v	<	<	>	>	v	
>	v	v	<	v	v	>	v	
>	>	v	<	v	v	v	v	
>	>	v	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
optimal values
-2.53	-1.92	-1.86	-1.93	-2.03	-2.35	-1.74	-1.77	
-1.92	-1.86	-1.35	-1.42	-1.54	-1.84	-1.22	-1.15	
-2.21	-1.73	-1.22	-1.83	-1.81	-1.74	-1.59	-1.08	
-1.83	-1.22	-1.09	-1.60	-1.30	-1.25	-1.21	-0.58	
-2.12	-1.63	-1.01	-1.37	-0.80	-1.18	-0.58	-0.45	
-1.74	-1.40	-0.88	-0.75	-0.67	-0.56	-0.47	0.05	
-1.23	-1.32	-0.83	-0.70	-0.17	-0.03	0.05	0.14	
-1.10	-0.97	-0.83	-0.34	-0.25	-0.12	-0.41	0.23	
map_weights [-0.41923476 -0.38222554 -0.3446083  -0.18272532 -0.72481303  0.02546565]
MAP reward
-0.72	-0.18	-0.38	-0.38	-0.34	-0.72	-0.38	-0.72	
-0.18	-0.38	-0.42	-0.18	-0.42	-0.72	-0.18	-0.18	
-0.34	-0.38	-0.42	-0.72	-0.38	-0.34	-0.38	-0.34	
-0.72	-0.42	-0.18	-0.38	-0.34	-0.18	-0.72	-0.42	
-0.34	-0.72	-0.42	-0.72	-0.42	-0.72	-0.42	-0.34	
-0.38	-0.38	-0.42	-0.18	-0.34	-0.38	-0.38	-0.18	
-0.42	-0.34	-0.42	-0.38	-0.42	-0.18	-0.18	-0.18	
-0.42	-0.42	-0.34	-0.18	-0.42	-0.18	-0.72	0.03	
Map policy
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
expeced value MDP LP -1.691985952066268
mean w [-0.40786773 -0.44206745 -0.33191356 -0.09527367 -0.54405713  0.019308  ]
Mean policy from posterior
v	>	v	v	v	>	v	v	
v	>	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	v	>	v	v	v	
v	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
Mean rewards
-0.54	-0.10	-0.44	-0.44	-0.33	-0.54	-0.44	-0.54	
-0.10	-0.44	-0.41	-0.10	-0.41	-0.54	-0.10	-0.10	
-0.33	-0.44	-0.41	-0.54	-0.44	-0.33	-0.44	-0.33	
-0.54	-0.41	-0.10	-0.44	-0.33	-0.10	-0.54	-0.41	
-0.33	-0.54	-0.41	-0.54	-0.41	-0.54	-0.41	-0.33	
-0.44	-0.44	-0.41	-0.10	-0.33	-0.44	-0.44	-0.10	
-0.41	-0.33	-0.41	-0.44	-0.41	-0.10	-0.10	-0.10	
-0.41	-0.41	-0.33	-0.10	-0.41	-0.10	-0.54	0.02	
mean = 0.08184213683533548, map = 0.252064102965901
CVaR policy
v	>	>	v	>	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	>	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
CVaR policy
v	>	>	v	>	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	>	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
CVaR policy
v	>	>	v	>	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	>	>	v	>	v	
>	>	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	v	>	v	v	v	
v	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
v	>	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	v	>	v	v	v	
v	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
cvar = , 0.28607266612112214, 0.2683736706835551, 0.26865140211918304, 0.10771274216003035, 0.08184211983001766
==========
iteration 26
==========
weights [-0.5936157  -0.01014504 -0.01389096 -0.2426591  -0.22906451  0.73210008]
expeced value MDP LP -0.33357797309954346
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.18928863  0.19232223 -0.09470785  0.83542194  0.43521408 -0.17570226]
w_map [-0.72144031 -0.15964272 -0.28698394 -0.43365316 -0.35805556 -0.23541336] loglik -9.407667768357442e-06
accepted/total = 1400/3000 = 0.4666666666666667
-------
true weights [-0.5936157  -0.01014504 -0.01389096 -0.2426591  -0.22906451  0.73210008]
features
4 	1 	1 	1 	4 	1 	0 	3 	
3 	2 	1 	3 	4 	2 	0 	1 	
0 	4 	1 	2 	3 	2 	1 	3 	
1 	3 	0 	4 	0 	4 	4 	0 	
3 	4 	1 	4 	0 	1 	4 	0 	
2 	0 	3 	4 	4 	0 	1 	0 	
2 	2 	4 	2 	3 	2 	0 	1 	
1 	1 	0 	0 	3 	3 	2 	5 	
optimal policy
>	>	v	<	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	<	v	v	>	v	v	<	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
optimal values
-0.76	-0.53	-0.53	-0.53	-0.61	-0.39	-0.98	-0.84	
-0.77	-0.53	-0.52	-0.75	-0.61	-0.38	-0.95	-0.61	
-0.93	-0.74	-0.52	-0.51	-0.61	-0.37	-0.36	-0.60	
-0.34	-0.58	-0.88	-0.51	-0.95	-0.36	-0.36	-0.95	
-0.33	-0.51	-0.29	-0.28	-0.63	-0.14	-0.13	-0.48	
-0.09	-0.66	-0.29	-0.05	-0.04	-0.16	0.10	0.11	
-0.08	-0.06	-0.05	0.18	0.20	0.44	0.11	0.71	
-0.08	-0.07	-0.64	-0.38	0.21	0.46	0.71	0.73	
map_weights [-0.72144031 -0.15964272 -0.28698394 -0.43365316 -0.35805556 -0.23541336]
MAP reward
-0.36	-0.16	-0.16	-0.16	-0.36	-0.16	-0.72	-0.43	
-0.43	-0.29	-0.16	-0.43	-0.36	-0.29	-0.72	-0.16	
-0.72	-0.36	-0.16	-0.29	-0.43	-0.29	-0.16	-0.43	
-0.16	-0.43	-0.72	-0.36	-0.72	-0.36	-0.36	-0.72	
-0.43	-0.36	-0.16	-0.36	-0.72	-0.16	-0.36	-0.72	
-0.29	-0.72	-0.43	-0.36	-0.36	-0.72	-0.16	-0.72	
-0.29	-0.29	-0.36	-0.29	-0.43	-0.29	-0.72	-0.16	
-0.16	-0.16	-0.72	-0.72	-0.43	-0.43	-0.29	-0.24	
Map policy
>	>	>	>	>	v	<	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	<	
v	v	v	v	>	v	v	v	
v	>	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.8148530492958848
mean w [-0.65689737 -0.12278402 -0.19466737 -0.36469885 -0.22857212 -0.05977394]
Mean policy from posterior
>	>	>	>	>	v	<	v	
>	>	v	>	>	v	v	v	
>	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
>	>	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.23	-0.12	-0.12	-0.12	-0.23	-0.12	-0.66	-0.36	
-0.36	-0.19	-0.12	-0.36	-0.23	-0.19	-0.66	-0.12	
-0.66	-0.23	-0.12	-0.19	-0.36	-0.19	-0.12	-0.36	
-0.12	-0.36	-0.66	-0.23	-0.66	-0.23	-0.23	-0.66	
-0.36	-0.23	-0.12	-0.23	-0.66	-0.12	-0.23	-0.66	
-0.19	-0.66	-0.36	-0.23	-0.23	-0.66	-0.12	-0.66	
-0.19	-0.19	-0.23	-0.19	-0.36	-0.19	-0.66	-0.12	
-0.12	-0.12	-0.66	-0.66	-0.36	-0.36	-0.19	-0.06	
mean = 0.03051480869670148, map = 0.03571078422320556
CVaR policy
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	<	v	
>	>	v	>	>	v	v	v	
>	>	>	v	>	>	v	<	
>	v	v	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	<	v	
>	>	v	>	>	v	v	v	
>	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
>	>	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	<	v	
>	>	v	>	>	v	v	v	
>	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
>	>	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	<	v	
>	>	v	>	>	v	v	v	
>	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
>	>	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
cvar = , 0.05317854740124267, 0.037573514296972865, 0.03563077909022466, 0.030514817162389463, 0.03051480873626533
==========
iteration 27
==========
weights [-0.01850592 -0.18489183 -0.17904333 -0.63184099 -0.50212388  0.53109754]
expeced value MDP LP -0.5220450183958818
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.21721785  0.53497449  0.14740003 -0.42309552  0.6793232  -0.06634835]
w_map [-0.27385787 -0.56363844 -0.18898184 -0.42445026 -0.53637859 -0.32208605] loglik -2.5941915282601258e-11
accepted/total = 1893/3000 = 0.631
-------
true weights [-0.01850592 -0.18489183 -0.17904333 -0.63184099 -0.50212388  0.53109754]
features
0 	2 	3 	1 	3 	4 	3 	4 	
0 	3 	3 	4 	4 	4 	0 	1 	
2 	1 	3 	4 	1 	0 	1 	1 	
0 	2 	4 	3 	3 	2 	1 	2 	
0 	1 	2 	4 	2 	2 	2 	1 	
3 	2 	1 	3 	3 	1 	4 	3 	
0 	2 	2 	1 	2 	0 	1 	4 	
4 	4 	3 	3 	1 	0 	0 	5 	
optimal policy
v	<	<	v	v	v	v	v	
v	<	>	>	v	v	v	<	
v	v	>	>	>	v	<	<	
v	v	v	v	v	v	v	<	
>	v	v	>	>	v	<	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
optimal values
-0.84	-1.01	-1.64	-1.46	-1.42	-1.11	-0.94	-0.99	
-0.83	-1.46	-1.90	-1.29	-0.79	-0.61	-0.31	-0.49	
-0.82	-0.98	-1.42	-0.79	-0.29	-0.11	-0.29	-0.47	
-0.65	-0.80	-0.95	-1.22	-0.72	-0.09	-0.27	-0.45	
-0.64	-0.63	-0.45	-0.59	-0.09	0.09	-0.09	-0.27	
-0.91	-0.45	-0.27	-0.54	-0.36	0.27	-0.19	-0.61	
-0.28	-0.27	-0.09	0.09	0.28	0.46	0.32	0.02	
-0.78	-0.77	-0.72	-0.34	0.29	0.48	0.51	0.53	
map_weights [-0.27385787 -0.56363844 -0.18898184 -0.42445026 -0.53637859 -0.32208605]
MAP reward
-0.27	-0.19	-0.42	-0.56	-0.42	-0.54	-0.42	-0.54	
-0.27	-0.42	-0.42	-0.54	-0.54	-0.54	-0.27	-0.56	
-0.19	-0.56	-0.42	-0.54	-0.56	-0.27	-0.56	-0.56	
-0.27	-0.19	-0.54	-0.42	-0.42	-0.19	-0.56	-0.19	
-0.27	-0.56	-0.19	-0.54	-0.19	-0.19	-0.19	-0.56	
-0.42	-0.19	-0.56	-0.42	-0.42	-0.56	-0.54	-0.42	
-0.27	-0.19	-0.19	-0.56	-0.19	-0.27	-0.56	-0.54	
-0.54	-0.54	-0.42	-0.42	-0.56	-0.27	-0.27	-0.32	
Map policy
v	<	v	>	>	v	v	v	
v	v	v	v	>	v	<	v	
v	v	v	v	>	v	<	v	
v	v	v	>	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.7914585247665593
mean w [-0.15828798 -0.46146319 -0.17072791 -0.46479481 -0.40475599 -0.05857726]
Mean policy from posterior
v	<	<	v	>	v	v	v	
v	<	>	>	>	v	<	v	
v	v	v	>	>	v	<	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.16	-0.17	-0.46	-0.46	-0.46	-0.40	-0.46	-0.40	
-0.16	-0.46	-0.46	-0.40	-0.40	-0.40	-0.16	-0.46	
-0.17	-0.46	-0.46	-0.40	-0.46	-0.16	-0.46	-0.46	
-0.16	-0.17	-0.40	-0.46	-0.46	-0.17	-0.46	-0.17	
-0.16	-0.46	-0.17	-0.40	-0.17	-0.17	-0.17	-0.46	
-0.46	-0.17	-0.46	-0.46	-0.46	-0.46	-0.40	-0.46	
-0.16	-0.17	-0.17	-0.46	-0.17	-0.16	-0.46	-0.40	
-0.40	-0.40	-0.46	-0.46	-0.46	-0.16	-0.16	-0.06	
mean = 0.14961685144944092, map = 0.29991628565178086
CVaR policy
v	v	v	v	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	v	
v	v	v	>	>	v	<	v	
v	v	v	>	>	v	<	v	
>	v	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	<	v	>	v	v	v	
v	v	>	>	>	v	<	v	
v	v	v	>	>	v	<	v	
>	v	v	v	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	<	v	>	v	v	v	
v	v	>	>	>	v	<	v	
v	v	v	>	>	v	<	v	
>	v	v	v	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	<	v	>	v	v	v	
v	<	>	>	>	v	<	v	
v	v	v	>	>	v	<	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.1957586053403959, 0.15916090658233584, 0.13429035014300372, 0.13429035002004852, 0.12052761228632014
==========
iteration 28
==========
weights [-0.5100235  -0.34968658 -0.25484199 -0.37175441 -0.63466652  0.10792566]
expeced value MDP LP -2.4444006017462887
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.13281335 -0.55666561 -0.10373567  0.55277748  0.35501261 -0.47971453]
w_map [-0.55055611 -0.55668232 -0.26695227 -0.23750592 -0.50880074  0.02102383] loglik -1.3088552464068925e-10
accepted/total = 2027/3000 = 0.6756666666666666
-------
true weights [-0.5100235  -0.34968658 -0.25484199 -0.37175441 -0.63466652  0.10792566]
features
2 	2 	3 	3 	3 	1 	2 	2 	
4 	2 	0 	1 	0 	4 	0 	0 	
4 	1 	1 	4 	4 	3 	3 	4 	
4 	4 	0 	2 	4 	1 	4 	2 	
2 	4 	2 	1 	4 	4 	4 	4 	
1 	0 	0 	3 	0 	2 	0 	3 	
2 	4 	2 	4 	2 	2 	2 	3 	
4 	4 	4 	1 	2 	4 	0 	5 	
optimal policy
>	v	>	v	>	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
optimal values
-4.21	-3.99	-4.04	-3.70	-3.59	-3.26	-3.07	-2.84	
-4.37	-3.78	-3.72	-3.36	-3.42	-2.94	-2.96	-2.61	
-4.16	-3.56	-3.24	-3.04	-2.94	-2.32	-2.47	-2.12	
-3.87	-3.53	-2.92	-2.43	-2.59	-1.97	-2.12	-1.50	
-3.27	-3.04	-2.43	-2.20	-2.13	-1.64	-1.65	-1.26	
-3.05	-2.85	-2.36	-1.87	-1.51	-1.01	-1.02	-0.63	
-2.72	-2.49	-1.88	-1.64	-1.01	-0.77	-0.52	-0.26	
-3.33	-2.83	-2.21	-1.60	-1.26	-1.03	-0.40	0.11	
map_weights [-0.55055611 -0.55668232 -0.26695227 -0.23750592 -0.50880074  0.02102383]
MAP reward
-0.27	-0.27	-0.24	-0.24	-0.24	-0.56	-0.27	-0.27	
-0.51	-0.27	-0.55	-0.56	-0.55	-0.51	-0.55	-0.55	
-0.51	-0.56	-0.56	-0.51	-0.51	-0.24	-0.24	-0.51	
-0.51	-0.51	-0.55	-0.27	-0.51	-0.56	-0.51	-0.27	
-0.27	-0.51	-0.27	-0.56	-0.51	-0.51	-0.51	-0.51	
-0.56	-0.55	-0.55	-0.24	-0.55	-0.27	-0.55	-0.24	
-0.27	-0.51	-0.27	-0.51	-0.27	-0.27	-0.27	-0.24	
-0.51	-0.51	-0.51	-0.56	-0.27	-0.51	-0.55	0.02	
Map policy
>	>	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	>	v	>	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	>	>	.	
expeced value MDP LP -1.6862266320439967
mean w [-0.50794047 -0.54965307 -0.13213629 -0.18679282 -0.3544756  -0.0456934 ]
Mean policy from posterior
>	>	>	>	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	>	v	v	>	v	
>	>	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	^	>	.	
Mean rewards
-0.13	-0.13	-0.19	-0.19	-0.19	-0.55	-0.13	-0.13	
-0.35	-0.13	-0.51	-0.55	-0.51	-0.35	-0.51	-0.51	
-0.35	-0.55	-0.55	-0.35	-0.35	-0.19	-0.19	-0.35	
-0.35	-0.35	-0.51	-0.13	-0.35	-0.55	-0.35	-0.13	
-0.13	-0.35	-0.13	-0.55	-0.35	-0.35	-0.35	-0.35	
-0.55	-0.51	-0.51	-0.19	-0.51	-0.13	-0.51	-0.19	
-0.13	-0.35	-0.13	-0.35	-0.13	-0.13	-0.13	-0.19	
-0.35	-0.35	-0.35	-0.55	-0.13	-0.35	-0.51	-0.05	
mean = 0.24976569621443456, map = 0.22554784293802488
CVaR policy
>	>	>	v	>	>	>	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	>	^	>	>	.	
CVaR policy
>	>	>	v	v	>	>	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	>	>	.	
CVaR policy
>	>	>	>	v	v	>	v	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	>	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	^	>	.	
CVaR policy
>	>	>	>	v	v	>	v	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	>	v	v	>	v	
>	>	v	v	>	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	^	>	^	^	>	.	
cvar = , 0.09077370734891455, 0.11808531861184601, 0.15109577485972503, 0.20826357020971953, 0.24976569620590094
==========
iteration 29
==========
weights [-0.52871357 -0.78382907 -0.29487581 -0.01777343 -0.12584647  0.05448833]
expeced value MDP LP -1.3249246001259218
demonstration
[(56, 1), (57, 1), (58, 1), (59, 2), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.03848452  0.20364453  0.53257885 -0.55687396  0.23746906 -0.55399228]
w_map [-0.59663816 -0.68452529 -0.26791662 -0.13794055 -0.27198716 -0.10326716] loglik -1.007478545034246e-10
accepted/total = 1987/3000 = 0.6623333333333333
-------
true weights [-0.52871357 -0.78382907 -0.29487581 -0.01777343 -0.12584647  0.05448833]
features
3 	4 	2 	4 	1 	3 	1 	1 	
1 	3 	2 	0 	1 	0 	0 	1 	
1 	0 	2 	0 	1 	3 	2 	2 	
2 	3 	0 	0 	3 	4 	4 	2 	
3 	1 	1 	4 	0 	1 	4 	2 	
0 	1 	4 	1 	2 	3 	3 	4 	
1 	2 	0 	2 	4 	2 	4 	0 	
2 	4 	4 	4 	1 	1 	4 	5 	
optimal policy
<	<	<	>	>	v	<	v	
^	^	<	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	v	<	
<	^	>	>	v	v	v	v	
^	>	>	>	>	>	v	<	
>	v	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
optimal values
-1.78	-1.89	-2.16	-2.00	-1.89	-1.12	-1.89	-2.58	
-2.54	-1.88	-2.16	-2.15	-1.89	-1.12	-1.27	-1.81	
-2.68	-2.15	-1.91	-1.63	-1.37	-0.59	-0.75	-1.04	
-1.91	-1.63	-1.63	-1.12	-0.59	-0.58	-0.46	-0.75	
-1.78	-2.40	-1.93	-1.16	-1.04	-1.01	-0.34	-0.63	
-2.29	-2.18	-1.41	-1.30	-0.52	-0.23	-0.21	-0.34	
-2.30	-1.53	-1.42	-0.90	-0.61	-0.49	-0.20	-0.47	
-1.53	-1.25	-1.13	-1.02	-1.39	-0.86	-0.07	0.05	
map_weights [-0.59663816 -0.68452529 -0.26791662 -0.13794055 -0.27198716 -0.10326716]
MAP reward
-0.14	-0.27	-0.27	-0.27	-0.68	-0.14	-0.68	-0.68	
-0.68	-0.14	-0.27	-0.60	-0.68	-0.60	-0.60	-0.68	
-0.68	-0.60	-0.27	-0.60	-0.68	-0.14	-0.27	-0.27	
-0.27	-0.14	-0.60	-0.60	-0.14	-0.27	-0.27	-0.27	
-0.14	-0.68	-0.68	-0.27	-0.60	-0.68	-0.27	-0.27	
-0.60	-0.68	-0.27	-0.68	-0.27	-0.14	-0.14	-0.27	
-0.68	-0.27	-0.60	-0.27	-0.27	-0.27	-0.27	-0.60	
-0.27	-0.27	-0.27	-0.27	-0.68	-0.68	-0.27	-0.10	
Map policy
>	v	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	>	>	>	.	
expeced value MDP LP -1.6426523466372753
mean w [-0.45346342 -0.58059336 -0.22462605 -0.40615063 -0.13469512  0.07662878]
Mean policy from posterior
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
Mean rewards
-0.41	-0.13	-0.22	-0.13	-0.58	-0.41	-0.58	-0.58	
-0.58	-0.41	-0.22	-0.45	-0.58	-0.45	-0.45	-0.58	
-0.58	-0.45	-0.22	-0.45	-0.58	-0.41	-0.22	-0.22	
-0.22	-0.41	-0.45	-0.45	-0.41	-0.13	-0.13	-0.22	
-0.41	-0.58	-0.58	-0.13	-0.45	-0.58	-0.13	-0.22	
-0.45	-0.58	-0.13	-0.58	-0.22	-0.41	-0.41	-0.13	
-0.58	-0.22	-0.45	-0.22	-0.13	-0.22	-0.13	-0.45	
-0.22	-0.13	-0.13	-0.13	-0.58	-0.58	-0.13	0.08	
mean = 0.38173115352691833, map = 0.10612388940246653
CVaR policy
>	>	v	>	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	>	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	v	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	>	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	v	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	>	v	v	
v	>	v	>	v	>	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	>	v	v	
v	>	v	>	v	>	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
cvar = , 0.182612884257483, 0.16535164757067045, 0.2080169786064594, 0.35252233169590874, 0.38173115364134325
==========
iteration 30
==========
weights [-0.31004306 -0.29634479 -0.21476876 -0.59271379 -0.28141103  0.58260248]
expeced value MDP LP -1.3929046319665854
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.23068883  0.19557829  0.26641489  0.74741671 -0.34817381  0.39711229]
w_map [-0.5646029  -0.39971739 -0.35919824 -0.3478328  -0.45511208 -0.25359721] loglik 0.0
accepted/total = 2314/3000 = 0.7713333333333333
-------
true weights [-0.31004306 -0.29634479 -0.21476876 -0.59271379 -0.28141103  0.58260248]
features
0 	3 	2 	3 	2 	4 	3 	2 	
3 	3 	2 	3 	0 	1 	0 	2 	
1 	3 	3 	1 	3 	1 	1 	2 	
1 	2 	2 	1 	2 	4 	0 	1 	
2 	1 	3 	4 	4 	4 	2 	0 	
4 	4 	0 	2 	1 	0 	3 	4 	
3 	4 	0 	3 	2 	3 	4 	0 	
1 	3 	1 	1 	2 	0 	0 	5 	
optimal policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.32	-3.25	-2.68	-2.65	-2.08	-1.88	-1.82	-1.24	
-3.04	-3.06	-2.49	-2.38	-1.91	-1.62	-1.34	-1.04	
-2.47	-2.50	-2.30	-1.81	-1.82	-1.37	-1.12	-0.83	
-2.20	-1.92	-1.73	-1.53	-1.24	-1.09	-0.84	-0.62	
-2.00	-1.82	-1.82	-1.24	-1.04	-0.81	-0.54	-0.33	
-1.81	-1.54	-1.27	-0.97	-0.76	-0.91	-0.61	-0.02	
-1.99	-1.42	-1.15	-1.06	-0.47	-0.61	-0.02	0.27	
-1.71	-1.43	-0.84	-0.55	-0.26	-0.05	0.27	0.58	
map_weights [-0.5646029  -0.39971739 -0.35919824 -0.3478328  -0.45511208 -0.25359721]
MAP reward
-0.56	-0.35	-0.36	-0.35	-0.36	-0.46	-0.35	-0.36	
-0.35	-0.35	-0.36	-0.35	-0.56	-0.40	-0.56	-0.36	
-0.40	-0.35	-0.35	-0.40	-0.35	-0.40	-0.40	-0.36	
-0.40	-0.36	-0.36	-0.40	-0.36	-0.46	-0.56	-0.40	
-0.36	-0.40	-0.35	-0.46	-0.46	-0.46	-0.36	-0.56	
-0.46	-0.46	-0.56	-0.36	-0.40	-0.56	-0.35	-0.46	
-0.35	-0.46	-0.56	-0.35	-0.36	-0.35	-0.46	-0.56	
-0.40	-0.35	-0.40	-0.40	-0.36	-0.56	-0.56	-0.25	
Map policy
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	>	v	v	
v	>	>	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.053302130226112
mean w [-0.29387821 -0.24431041 -0.42157242 -0.40149042 -0.37493015  0.04067981]
Mean policy from posterior
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	v	v	v	>	>	>	v	
>	v	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.29	-0.40	-0.42	-0.40	-0.42	-0.37	-0.40	-0.42	
-0.40	-0.40	-0.42	-0.40	-0.29	-0.24	-0.29	-0.42	
-0.24	-0.40	-0.40	-0.24	-0.40	-0.24	-0.24	-0.42	
-0.24	-0.42	-0.42	-0.24	-0.42	-0.37	-0.29	-0.24	
-0.42	-0.24	-0.40	-0.37	-0.37	-0.37	-0.42	-0.29	
-0.37	-0.37	-0.29	-0.42	-0.24	-0.29	-0.40	-0.37	
-0.40	-0.37	-0.29	-0.40	-0.42	-0.40	-0.37	-0.29	
-0.24	-0.40	-0.24	-0.24	-0.42	-0.29	-0.29	0.04	
mean = 0.20978527054918472, map = 0.46707153442473714
CVaR policy
v	v	>	v	>	v	>	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	>	>	v	
>	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	v	v	v	>	>	>	v	
>	v	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	v	v	v	>	>	>	v	
>	v	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.3554441259786407, 0.317812568627289, 0.25333319920052966, 0.20978527065890717, 0.20978527039571082
==========
iteration 31
==========
weights [-0.05358524 -0.25940048 -0.6119391  -0.55979138 -0.36199438  0.33311294]
expeced value MDP LP -1.3212044259615796
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.14825215 -0.02758484 -0.80714138  0.37191167  0.28866152  0.32270637]
w_map [-0.2615796  -0.27395155 -0.66310111 -0.60528594 -0.22375187  0.0196877 ] loglik -8.359535286217579e-11
accepted/total = 1919/3000 = 0.6396666666666667
-------
true weights [-0.05358524 -0.25940048 -0.6119391  -0.55979138 -0.36199438  0.33311294]
features
4 	1 	0 	2 	1 	0 	0 	0 	
0 	1 	4 	2 	0 	4 	0 	0 	
0 	0 	3 	3 	0 	0 	3 	2 	
0 	2 	1 	4 	1 	2 	1 	0 	
1 	1 	2 	1 	2 	4 	3 	0 	
1 	4 	4 	1 	1 	4 	2 	4 	
2 	1 	1 	0 	4 	3 	4 	4 	
2 	0 	3 	2 	0 	1 	1 	5 	
optimal policy
v	>	>	>	>	>	>	v	
v	v	>	>	v	>	>	v	
v	<	v	>	>	>	v	v	
v	>	>	v	>	>	>	v	
v	v	>	v	v	>	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-2.43	-2.38	-2.14	-2.11	-1.51	-1.27	-1.23	-1.18	
-2.09	-2.33	-2.36	-2.02	-1.42	-1.53	-1.18	-1.14	
-2.06	-2.09	-2.29	-1.93	-1.38	-1.34	-1.30	-1.10	
-2.02	-2.34	-1.75	-1.50	-1.60	-1.35	-0.75	-0.49	
-1.99	-1.75	-1.75	-1.15	-1.46	-1.35	-1.00	-0.44	
-1.75	-1.50	-1.25	-0.90	-0.85	-1.10	-0.90	-0.39	
-1.75	-1.15	-0.90	-0.65	-0.60	-0.75	-0.29	-0.03	
-1.79	-1.19	-1.40	-0.85	-0.24	-0.19	0.07	0.33	
map_weights [-0.2615796  -0.27395155 -0.66310111 -0.60528594 -0.22375187  0.0196877 ]
MAP reward
-0.22	-0.27	-0.26	-0.66	-0.27	-0.26	-0.26	-0.26	
-0.26	-0.27	-0.22	-0.66	-0.26	-0.22	-0.26	-0.26	
-0.26	-0.26	-0.61	-0.61	-0.26	-0.26	-0.61	-0.66	
-0.26	-0.66	-0.27	-0.22	-0.27	-0.66	-0.27	-0.26	
-0.27	-0.27	-0.66	-0.27	-0.66	-0.22	-0.61	-0.26	
-0.27	-0.22	-0.22	-0.27	-0.27	-0.22	-0.66	-0.22	
-0.66	-0.27	-0.27	-0.26	-0.22	-0.61	-0.22	-0.22	
-0.66	-0.26	-0.61	-0.66	-0.26	-0.27	-0.27	0.02	
Map policy
v	>	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	>	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.284555805068015
mean w [-0.16390651 -0.15301737 -0.63824436 -0.44921235 -0.27952532  0.19624816]
Mean policy from posterior
v	v	v	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	v	>	v	v	
v	>	>	v	<	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.28	-0.15	-0.16	-0.64	-0.15	-0.16	-0.16	-0.16	
-0.16	-0.15	-0.28	-0.64	-0.16	-0.28	-0.16	-0.16	
-0.16	-0.16	-0.45	-0.45	-0.16	-0.16	-0.45	-0.64	
-0.16	-0.64	-0.15	-0.28	-0.15	-0.64	-0.15	-0.16	
-0.15	-0.15	-0.64	-0.15	-0.64	-0.28	-0.45	-0.16	
-0.15	-0.28	-0.28	-0.15	-0.15	-0.28	-0.64	-0.28	
-0.64	-0.15	-0.15	-0.16	-0.28	-0.45	-0.28	-0.28	
-0.64	-0.16	-0.45	-0.64	-0.16	-0.15	-0.15	0.20	
mean = 0.14240691938489514, map = 0.10157403427218137
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	v	>	>	v	
>	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	<	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	<	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	>	v	v	v	>	v	v	
v	>	>	v	<	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	^	>	>	>	>	.	
cvar = , 0.08421984504574853, 0.10100894200040234, 0.10478746448282106, 0.10556933911920208, 0.14240692230869723
==========
iteration 32
==========
weights [-0.4094745  -0.40134772 -0.10730514 -0.40448845 -0.6221851   0.33016821]
expeced value MDP LP -1.3553792537867473
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.30043577 -0.65684421  0.62308963  0.24672462  0.08147473 -0.15014054]
w_map [-0.16101667 -0.39146164 -0.46957445 -0.55073527 -0.52141234 -0.15859093] loglik 0.0
accepted/total = 2413/3000 = 0.8043333333333333
-------
true weights [-0.4094745  -0.40134772 -0.10730514 -0.40448845 -0.6221851   0.33016821]
features
3 	1 	0 	2 	1 	0 	2 	3 	
2 	3 	4 	0 	3 	1 	2 	1 	
1 	1 	2 	4 	0 	0 	3 	1 	
1 	4 	0 	0 	1 	2 	1 	1 	
3 	2 	3 	1 	2 	2 	0 	2 	
3 	1 	0 	3 	3 	2 	2 	4 	
3 	3 	1 	0 	3 	1 	1 	2 	
1 	1 	2 	2 	4 	2 	2 	5 	
optimal policy
v	v	>	>	v	v	v	<	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	^	v	>	>	>	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.09	-3.01	-2.64	-2.25	-2.17	-1.79	-1.58	-1.96	
-2.71	-2.64	-2.48	-2.18	-1.78	-1.39	-1.48	-1.69	
-2.63	-2.26	-1.87	-2.00	-1.39	-1.00	-1.39	-1.30	
-2.26	-2.09	-1.79	-1.39	-0.99	-0.60	-0.99	-0.90	
-1.87	-1.48	-1.39	-0.99	-0.60	-0.50	-0.70	-0.51	
-2.26	-1.87	-1.51	-1.19	-0.79	-0.39	-0.29	-0.40	
-1.89	-1.50	-1.11	-1.02	-0.69	-0.29	-0.18	0.22	
-1.50	-1.11	-0.72	-0.62	-0.51	0.11	0.22	0.33	
map_weights [-0.16101667 -0.39146164 -0.46957445 -0.55073527 -0.52141234 -0.15859093]
MAP reward
-0.55	-0.39	-0.16	-0.47	-0.39	-0.16	-0.47	-0.55	
-0.47	-0.55	-0.52	-0.16	-0.55	-0.39	-0.47	-0.39	
-0.39	-0.39	-0.47	-0.52	-0.16	-0.16	-0.55	-0.39	
-0.39	-0.52	-0.16	-0.16	-0.39	-0.47	-0.39	-0.39	
-0.55	-0.47	-0.55	-0.39	-0.47	-0.47	-0.16	-0.47	
-0.55	-0.39	-0.16	-0.55	-0.55	-0.47	-0.47	-0.52	
-0.55	-0.55	-0.39	-0.16	-0.55	-0.39	-0.39	-0.47	
-0.39	-0.39	-0.47	-0.47	-0.52	-0.47	-0.47	-0.16	
Map policy
>	>	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.13152922710889
mean w [-0.42879236 -0.39993938 -0.27329198 -0.39483545 -0.32280419  0.00621972]
Mean policy from posterior
v	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.39	-0.40	-0.43	-0.27	-0.40	-0.43	-0.27	-0.39	
-0.27	-0.39	-0.32	-0.43	-0.39	-0.40	-0.27	-0.40	
-0.40	-0.40	-0.27	-0.32	-0.43	-0.43	-0.39	-0.40	
-0.40	-0.32	-0.43	-0.43	-0.40	-0.27	-0.40	-0.40	
-0.39	-0.27	-0.39	-0.40	-0.27	-0.27	-0.43	-0.27	
-0.39	-0.40	-0.43	-0.39	-0.39	-0.27	-0.27	-0.32	
-0.39	-0.39	-0.40	-0.43	-0.39	-0.40	-0.40	-0.27	
-0.40	-0.40	-0.27	-0.27	-0.32	-0.27	-0.27	0.01	
mean = 0.1943920032798434, map = 0.4366726139512298
CVaR policy
>	>	v	v	>	v	>	v	
>	>	>	v	>	v	>	v	
v	v	>	>	>	>	v	v	
>	>	>	v	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	v	v	
>	>	v	v	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	v	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	v	v	
>	>	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	>	>	v	
v	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	>	v	v	v	>	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	>	v	v	v	>	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.7056471292124105, 0.26509885862003113, 0.19513485889090054, 0.19439200315784388, 0.19439200314823735
==========
iteration 33
==========
weights [-0.5241945  -0.77962364 -0.22290199 -0.13972452 -0.17733602  0.12942492]
expeced value MDP LP -1.9563915891969281
demonstration
[(56, 1), (57, 2), (49, 1), (50, 2), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.46799735 -0.58018185  0.27775586  0.46854692 -0.34175137  0.17575257]
w_map [-0.4375139  -0.77054702 -0.30084473 -0.13408608 -0.28332773 -0.16148576] loglik -2.9748886554159526e-06
accepted/total = 1244/3000 = 0.4146666666666667
-------
true weights [-0.5241945  -0.77962364 -0.22290199 -0.13972452 -0.17733602  0.12942492]
features
2 	1 	4 	3 	1 	2 	3 	0 	
1 	0 	0 	0 	4 	3 	2 	1 	
0 	3 	0 	1 	0 	0 	3 	4 	
1 	4 	2 	1 	1 	3 	3 	4 	
3 	1 	2 	1 	1 	2 	3 	2 	
1 	2 	3 	4 	3 	0 	0 	1 	
1 	3 	3 	1 	1 	0 	0 	1 	
2 	0 	4 	1 	1 	4 	3 	5 	
optimal policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	^	v	v	<	
>	>	v	<	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-3.64	-3.45	-2.70	-2.55	-2.69	-1.98	-1.77	-2.28	
-3.58	-2.83	-2.93	-2.43	-1.93	-1.77	-1.65	-2.36	
-2.83	-2.33	-2.56	-3.19	-2.43	-1.95	-1.44	-1.60	
-2.97	-2.21	-2.05	-2.81	-2.20	-1.44	-1.31	-1.48	
-2.73	-2.61	-1.85	-2.28	-2.12	-1.39	-1.18	-1.39	
-2.61	-1.85	-1.64	-1.52	-1.36	-1.23	-1.05	-1.42	
-2.65	-1.89	-1.77	-2.25	-1.48	-0.71	-0.54	-0.65	
-2.59	-2.39	-1.90	-1.74	-0.97	-0.19	-0.01	0.13	
map_weights [-0.4375139  -0.77054702 -0.30084473 -0.13408608 -0.28332773 -0.16148576]
MAP reward
-0.30	-0.77	-0.28	-0.13	-0.77	-0.30	-0.13	-0.44	
-0.77	-0.44	-0.44	-0.44	-0.28	-0.13	-0.30	-0.77	
-0.44	-0.13	-0.44	-0.77	-0.44	-0.44	-0.13	-0.28	
-0.77	-0.28	-0.30	-0.77	-0.77	-0.13	-0.13	-0.28	
-0.13	-0.77	-0.30	-0.77	-0.77	-0.30	-0.13	-0.30	
-0.77	-0.30	-0.13	-0.28	-0.13	-0.44	-0.44	-0.77	
-0.77	-0.13	-0.13	-0.77	-0.77	-0.44	-0.44	-0.77	
-0.30	-0.44	-0.28	-0.77	-0.77	-0.28	-0.13	-0.16	
Map policy
>	>	>	v	v	>	v	<	
>	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	^	>	>	>	>	.	
expeced value MDP LP -1.3973652129198415
mean w [-0.33453095 -0.71806328 -0.34327414 -0.06188861 -0.16193474  0.10833782]
Mean policy from posterior
>	>	>	v	v	v	v	<	
v	v	>	>	>	v	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	^	>	v	v	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.34	-0.72	-0.16	-0.06	-0.72	-0.34	-0.06	-0.33	
-0.72	-0.33	-0.33	-0.33	-0.16	-0.06	-0.34	-0.72	
-0.33	-0.06	-0.33	-0.72	-0.33	-0.33	-0.06	-0.16	
-0.72	-0.16	-0.34	-0.72	-0.72	-0.06	-0.06	-0.16	
-0.06	-0.72	-0.34	-0.72	-0.72	-0.34	-0.06	-0.34	
-0.72	-0.34	-0.06	-0.16	-0.06	-0.33	-0.33	-0.72	
-0.72	-0.06	-0.06	-0.72	-0.72	-0.33	-0.33	-0.72	
-0.34	-0.33	-0.16	-0.72	-0.72	-0.16	-0.06	0.11	
mean = 0.04926010190566332, map = 0.0033406019743393855
CVaR policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	^	>	v	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	^	>	v	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	^	>	v	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	<	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	^	>	v	v	v	
>	^	^	>	>	>	>	.	
cvar = , 0.0028651999282884866, 0.0038942805916883927, 0.003894273363551193, 0.0038942768468994693, 0.04926010355252375
==========
iteration 34
==========
weights [-0.62439498 -0.16258313 -0.103938   -0.22088694 -0.30853585  0.65491153]
expeced value MDP LP -0.671030005140768
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[ 0.19246773  0.26120419  0.4608694  -0.50253157 -0.2645748  -0.59982508]
w_map [-0.14360729 -0.22590955 -0.05264336 -0.63483144 -0.72090206 -0.05347679] loglik -0.4466839639343334
accepted/total = 1921/3000 = 0.6403333333333333
-------
true weights [-0.62439498 -0.16258313 -0.103938   -0.22088694 -0.30853585  0.65491153]
features
3 	4 	4 	0 	4 	4 	4 	1 	
4 	4 	4 	0 	1 	2 	4 	2 	
1 	3 	3 	3 	0 	0 	2 	2 	
0 	3 	2 	1 	1 	3 	0 	1 	
1 	4 	1 	4 	0 	1 	4 	1 	
2 	1 	4 	2 	2 	3 	4 	0 	
4 	2 	3 	1 	2 	2 	2 	4 	
4 	1 	3 	2 	3 	4 	3 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
v	>	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	>	>	>	.	
optimal values
-1.75	-1.69	-1.48	-2.01	-1.55	-1.40	-1.25	-0.96	
-1.54	-1.39	-1.18	-1.40	-1.26	-1.11	-1.10	-0.80	
-1.25	-1.09	-0.88	-0.78	-1.17	-1.01	-0.80	-0.70	
-1.41	-0.88	-0.67	-0.57	-0.55	-0.39	-0.92	-0.61	
-0.79	-0.84	-0.57	-0.41	-0.62	-0.17	-0.30	-0.45	
-0.64	-0.54	-0.41	-0.10	0.00	-0.01	0.01	-0.29	
-0.68	-0.38	-0.28	-0.06	0.11	0.21	0.32	0.34	
-0.84	-0.54	-0.38	-0.16	-0.11	0.11	0.43	0.65	
map_weights [-0.14360729 -0.22590955 -0.05264336 -0.63483144 -0.72090206 -0.05347679]
MAP reward
-0.63	-0.72	-0.72	-0.14	-0.72	-0.72	-0.72	-0.23	
-0.72	-0.72	-0.72	-0.14	-0.23	-0.05	-0.72	-0.05	
-0.23	-0.63	-0.63	-0.63	-0.14	-0.14	-0.05	-0.05	
-0.14	-0.63	-0.05	-0.23	-0.23	-0.63	-0.14	-0.23	
-0.23	-0.72	-0.23	-0.72	-0.14	-0.23	-0.72	-0.23	
-0.05	-0.23	-0.72	-0.05	-0.05	-0.63	-0.72	-0.14	
-0.72	-0.05	-0.63	-0.23	-0.05	-0.05	-0.05	-0.72	
-0.72	-0.23	-0.63	-0.05	-0.63	-0.72	-0.63	-0.05	
Map policy
v	>	>	v	v	v	>	v	
v	>	>	>	v	v	v	v	
v	>	v	>	v	<	>	v	
v	>	>	>	v	v	>	v	
v	>	^	v	v	<	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
expeced value MDP LP -1.9170299817132106
mean w [-0.38985469 -0.16324274 -0.13850095 -0.38025402 -0.63413339 -0.20666256]
Mean policy from posterior
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
Mean rewards
-0.38	-0.63	-0.63	-0.39	-0.63	-0.63	-0.63	-0.16	
-0.63	-0.63	-0.63	-0.39	-0.16	-0.14	-0.63	-0.14	
-0.16	-0.38	-0.38	-0.38	-0.39	-0.39	-0.14	-0.14	
-0.39	-0.38	-0.14	-0.16	-0.16	-0.38	-0.39	-0.16	
-0.16	-0.63	-0.16	-0.63	-0.39	-0.16	-0.63	-0.16	
-0.14	-0.16	-0.63	-0.14	-0.14	-0.38	-0.63	-0.39	
-0.63	-0.14	-0.38	-0.16	-0.14	-0.14	-0.14	-0.63	
-0.63	-0.16	-0.38	-0.14	-0.38	-0.63	-0.38	-0.21	
mean = 0.10222371105132055, map = 0.28489840475512507
CVaR policy
v	>	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
v	>	>	>	v	v	>	v	
v	v	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	>	>	v	
v	>	>	>	v	v	>	v	
v	v	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	>	>	v	
v	>	>	>	v	v	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	^	>	>	.	
cvar = , 0.19681770820782107, 0.1627699763966064, 0.15917750711541778, 0.13338852694832204, 0.11506203244658963
==========
iteration 35
==========
weights [-0.06684751 -0.23871016 -0.10837398 -0.73125052 -0.41222717  0.47132302]
expeced value MDP LP -0.6096881849589867
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.09008644  0.03140573  0.30363949 -0.92021684 -0.04078635  0.2241397 ]
w_map [-0.46012952 -0.11527958 -0.36604353 -0.53961711 -0.53290744 -0.25656699] loglik 0.0
accepted/total = 2400/3000 = 0.8
-------
true weights [-0.06684751 -0.23871016 -0.10837398 -0.73125052 -0.41222717  0.47132302]
features
1 	1 	4 	0 	1 	1 	0 	3 	
0 	3 	4 	0 	4 	1 	0 	4 	
0 	1 	4 	2 	1 	3 	4 	4 	
0 	2 	3 	0 	1 	0 	3 	2 	
3 	0 	1 	4 	2 	2 	4 	3 	
2 	4 	2 	0 	2 	0 	4 	3 	
3 	0 	4 	4 	3 	2 	2 	2 	
4 	3 	2 	4 	0 	0 	2 	5 	
optimal policy
v	<	>	v	<	<	<	<	
v	<	>	v	<	v	<	<	
v	v	>	v	v	v	<	v	
>	v	>	>	>	v	<	<	
>	>	v	v	>	v	<	v	
>	>	>	>	>	v	v	v	
>	>	^	^	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-0.99	-1.22	-1.00	-0.60	-0.83	-1.06	-1.12	-1.84	
-0.76	-1.48	-0.94	-0.54	-0.94	-1.03	-1.09	-1.49	
-0.70	-0.81	-0.88	-0.47	-0.54	-0.80	-1.20	-1.30	
-0.64	-0.58	-1.10	-0.37	-0.31	-0.07	-0.80	-0.90	
-1.20	-0.48	-0.41	-0.48	-0.11	-0.00	-0.41	-1.10	
-0.69	-0.59	-0.18	-0.07	-0.00	0.11	-0.17	-0.38	
-1.37	-0.65	-0.59	-0.48	-0.52	0.18	0.25	0.36	
-1.43	-1.03	-0.30	-0.20	0.22	0.29	0.36	0.47	
map_weights [-0.46012952 -0.11527958 -0.36604353 -0.53961711 -0.53290744 -0.25656699]
MAP reward
-0.12	-0.12	-0.53	-0.46	-0.12	-0.12	-0.46	-0.54	
-0.46	-0.54	-0.53	-0.46	-0.53	-0.12	-0.46	-0.53	
-0.46	-0.12	-0.53	-0.37	-0.12	-0.54	-0.53	-0.53	
-0.46	-0.37	-0.54	-0.46	-0.12	-0.46	-0.54	-0.37	
-0.54	-0.46	-0.12	-0.53	-0.37	-0.37	-0.53	-0.54	
-0.37	-0.53	-0.37	-0.46	-0.37	-0.46	-0.53	-0.54	
-0.54	-0.46	-0.53	-0.53	-0.54	-0.37	-0.37	-0.37	
-0.53	-0.54	-0.37	-0.53	-0.46	-0.46	-0.37	-0.26	
Map policy
>	v	>	>	v	v	<	v	
v	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.847327855814132
mean w [-0.2736168  -0.30550491 -0.28018256 -0.443381   -0.37850138  0.17867454]
Mean policy from posterior
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.31	-0.31	-0.38	-0.27	-0.31	-0.31	-0.27	-0.44	
-0.27	-0.44	-0.38	-0.27	-0.38	-0.31	-0.27	-0.38	
-0.27	-0.31	-0.38	-0.28	-0.31	-0.44	-0.38	-0.38	
-0.27	-0.28	-0.44	-0.27	-0.31	-0.27	-0.44	-0.28	
-0.44	-0.27	-0.31	-0.38	-0.28	-0.28	-0.38	-0.44	
-0.28	-0.38	-0.28	-0.27	-0.28	-0.27	-0.38	-0.44	
-0.44	-0.27	-0.38	-0.38	-0.44	-0.28	-0.28	-0.28	
-0.38	-0.44	-0.28	-0.38	-0.27	-0.27	-0.28	0.18	
mean = 0.0877765303324366, map = 0.2569479360346595
CVaR policy
>	>	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	v	v	v	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	v	>	v	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 1.1810206551751783, 0.4100205281274306, 0.16047564889286925, 0.08777653031170918, 0.08777653032342048
==========
iteration 36
==========
weights [-0.01995221 -0.56123947 -0.57293475 -0.57445404 -0.15730936  0.04017778]
expeced value MDP LP -1.6125031222031943
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.04982166  0.32317317  0.74454599  0.5736291   0.0378186  -0.09081622]
w_map [-0.37843528 -0.39179509 -0.39705979 -0.36789911 -0.44503679 -0.46067274] loglik 0.0
accepted/total = 2171/3000 = 0.7236666666666667
-------
true weights [-0.01995221 -0.56123947 -0.57293475 -0.57445404 -0.15730936  0.04017778]
features
4 	2 	2 	4 	1 	2 	0 	4 	
0 	4 	0 	4 	2 	3 	1 	2 	
2 	2 	4 	3 	3 	4 	3 	4 	
3 	0 	2 	4 	1 	0 	3 	4 	
0 	0 	4 	2 	4 	3 	4 	3 	
0 	3 	3 	1 	3 	0 	4 	2 	
3 	0 	2 	1 	3 	3 	1 	4 	
2 	0 	0 	3 	1 	0 	0 	5 	
optimal policy
v	<	v	v	<	v	^	<	
<	<	<	<	v	v	v	v	
^	v	^	v	>	v	<	v	
v	v	<	>	>	v	v	v	
v	v	<	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.13	-2.68	-2.68	-2.40	-2.94	-2.43	-2.00	-2.13	
-2.00	-2.13	-2.13	-2.27	-2.43	-1.87	-2.42	-2.10	
-2.55	-2.28	-2.27	-2.41	-1.87	-1.31	-1.87	-1.54	
-2.28	-1.72	-2.28	-1.86	-1.72	-1.17	-1.41	-1.40	
-1.72	-1.72	-1.86	-1.86	-1.30	-1.16	-0.84	-1.26	
-1.72	-1.72	-2.26	-1.71	-1.16	-0.59	-0.69	-0.69	
-1.72	-1.16	-1.70	-1.68	-1.13	-0.57	-0.54	-0.12	
-1.71	-1.15	-1.14	-1.13	-0.56	-0.00	0.02	0.04	
map_weights [-0.37843528 -0.39179509 -0.39705979 -0.36789911 -0.44503679 -0.46067274]
MAP reward
-0.45	-0.40	-0.40	-0.45	-0.39	-0.40	-0.38	-0.45	
-0.38	-0.45	-0.38	-0.45	-0.40	-0.37	-0.39	-0.40	
-0.40	-0.40	-0.45	-0.37	-0.37	-0.45	-0.37	-0.45	
-0.37	-0.38	-0.40	-0.45	-0.39	-0.38	-0.37	-0.45	
-0.38	-0.38	-0.45	-0.40	-0.45	-0.37	-0.45	-0.37	
-0.38	-0.37	-0.37	-0.39	-0.37	-0.38	-0.45	-0.40	
-0.37	-0.38	-0.40	-0.39	-0.37	-0.37	-0.39	-0.45	
-0.40	-0.38	-0.38	-0.37	-0.39	-0.38	-0.38	-0.46	
Map policy
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8674872920168242
mean w [-0.21402518 -0.24510297 -0.31607282 -0.34079713 -0.30542315 -0.10048495]
Mean policy from posterior
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.31	-0.32	-0.32	-0.31	-0.25	-0.32	-0.21	-0.31	
-0.21	-0.31	-0.21	-0.31	-0.32	-0.34	-0.25	-0.32	
-0.32	-0.32	-0.31	-0.34	-0.34	-0.31	-0.34	-0.31	
-0.34	-0.21	-0.32	-0.31	-0.25	-0.21	-0.34	-0.31	
-0.21	-0.21	-0.31	-0.32	-0.31	-0.34	-0.31	-0.34	
-0.21	-0.34	-0.34	-0.25	-0.34	-0.21	-0.31	-0.32	
-0.34	-0.21	-0.32	-0.25	-0.34	-0.34	-0.25	-0.31	
-0.32	-0.21	-0.21	-0.34	-0.25	-0.21	-0.21	-0.10	
mean = 0.17984969700323528, map = 0.5730164449088886
CVaR policy
v	>	>	>	>	v	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	v	v	>	v	
>	>	v	v	v	>	>	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	v	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	v	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.790828415890872, 0.6554722752700879, 0.5005826723000848, 0.19900849282784439, 0.17984969652166605
==========
iteration 37
==========
weights [-0.13292463 -0.35914369 -0.00912155 -0.1627433  -0.90904404  0.02042576]
expeced value MDP LP -1.1796173475983402
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.8342167  -0.16050755 -0.38394118  0.07904606  0.34976809  0.04819748]
w_map [-0.51114611 -0.67030002 -0.36568587 -0.34685988 -0.18627831 -0.02626788] loglik 0.0
accepted/total = 2367/3000 = 0.789
-------
true weights [-0.13292463 -0.35914369 -0.00912155 -0.1627433  -0.90904404  0.02042576]
features
3 	3 	0 	0 	1 	1 	3 	3 	
0 	4 	1 	3 	2 	4 	4 	4 	
1 	3 	4 	3 	1 	1 	0 	2 	
1 	0 	3 	1 	3 	3 	3 	1 	
0 	2 	1 	4 	3 	2 	1 	4 	
4 	0 	1 	0 	4 	4 	3 	1 	
0 	4 	2 	0 	1 	4 	3 	0 	
0 	1 	3 	0 	3 	3 	2 	5 	
optimal policy
v	>	>	v	v	<	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	>	
v	v	<	>	v	v	v	<	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.93	-1.86	-1.71	-1.59	-1.67	-2.02	-1.99	-1.96	
-1.79	-2.22	-1.82	-1.48	-1.33	-2.08	-1.85	-1.81	
-1.67	-1.32	-2.22	-1.48	-1.33	-1.18	-0.95	-0.91	
-1.52	-1.17	-1.32	-1.33	-0.98	-0.83	-0.83	-1.18	
-1.17	-1.05	-1.28	-1.60	-0.83	-0.67	-0.67	-1.38	
-1.95	-1.05	-0.93	-0.70	-1.57	-1.22	-0.31	-0.47	
-1.20	-1.48	-0.57	-0.57	-0.67	-1.06	-0.15	-0.11	
-1.08	-0.95	-0.60	-0.44	-0.31	-0.15	0.01	0.02	
map_weights [-0.51114611 -0.67030002 -0.36568587 -0.34685988 -0.18627831 -0.02626788]
MAP reward
-0.35	-0.35	-0.51	-0.51	-0.67	-0.67	-0.35	-0.35	
-0.51	-0.19	-0.67	-0.35	-0.37	-0.19	-0.19	-0.19	
-0.67	-0.35	-0.19	-0.35	-0.67	-0.67	-0.51	-0.37	
-0.67	-0.51	-0.35	-0.67	-0.35	-0.35	-0.35	-0.67	
-0.51	-0.37	-0.67	-0.19	-0.35	-0.37	-0.67	-0.19	
-0.19	-0.51	-0.67	-0.51	-0.19	-0.19	-0.35	-0.67	
-0.51	-0.19	-0.37	-0.51	-0.67	-0.19	-0.35	-0.51	
-0.51	-0.67	-0.35	-0.51	-0.35	-0.35	-0.37	-0.03	
Map policy
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.2281001777397353
mean w [-0.41845095 -0.33906296 -0.34680697 -0.2694077  -0.43319988 -0.09321423]
Mean policy from posterior
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.27	-0.27	-0.42	-0.42	-0.34	-0.34	-0.27	-0.27	
-0.42	-0.43	-0.34	-0.27	-0.35	-0.43	-0.43	-0.43	
-0.34	-0.27	-0.43	-0.27	-0.34	-0.34	-0.42	-0.35	
-0.34	-0.42	-0.27	-0.34	-0.27	-0.27	-0.27	-0.34	
-0.42	-0.35	-0.34	-0.43	-0.27	-0.35	-0.34	-0.43	
-0.43	-0.42	-0.34	-0.42	-0.43	-0.43	-0.27	-0.34	
-0.42	-0.43	-0.35	-0.42	-0.34	-0.43	-0.27	-0.42	
-0.42	-0.34	-0.27	-0.42	-0.27	-0.27	-0.35	-0.09	
mean = 0.2048394265558242, map = 1.5485213556715904
CVaR policy
v	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	v	v	>	v	>	v	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.9954618412935523, 0.8554670790481431, 0.32381858725110635, 0.21558213056672715, 0.20483953681521028
==========
iteration 38
==========
weights [-0.24018941 -0.09776579 -0.34570597 -0.20586074 -0.69557571  0.5357556 ]
expeced value MDP LP -0.8340895143841156
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.1683453  -0.49579218 -0.17168215  0.74198253 -0.05093659 -0.37847406]
w_map [-0.55972963 -0.39581548 -0.2277379  -0.46888696 -0.37639545 -0.34152568] loglik 0.0
accepted/total = 2381/3000 = 0.7936666666666666
-------
true weights [-0.24018941 -0.09776579 -0.34570597 -0.20586074 -0.69557571  0.5357556 ]
features
1 	2 	4 	3 	1 	1 	3 	3 	
2 	2 	4 	0 	1 	2 	1 	4 	
0 	3 	1 	4 	0 	4 	3 	0 	
0 	0 	1 	2 	2 	4 	4 	2 	
3 	1 	0 	4 	0 	4 	4 	2 	
4 	2 	0 	1 	1 	1 	4 	3 	
4 	2 	0 	0 	1 	0 	0 	0 	
0 	4 	4 	3 	1 	1 	3 	5 	
optimal policy
v	v	>	>	v	<	v	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	<	>	v	
v	v	v	>	v	<	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.67	-1.69	-1.94	-1.26	-1.07	-1.15	-1.32	-1.51	
-1.59	-1.36	-1.51	-1.21	-0.98	-1.31	-1.13	-1.53	
-1.25	-1.02	-0.83	-1.51	-0.89	-1.58	-1.04	-0.84	
-1.17	-0.97	-0.74	-0.99	-0.66	-1.34	-1.30	-0.61	
-0.93	-0.74	-0.65	-0.86	-0.31	-0.81	-0.96	-0.27	
-1.44	-0.75	-0.41	-0.17	-0.07	-0.12	-0.61	0.08	
-1.48	-0.80	-0.45	-0.22	0.02	-0.02	0.08	0.29	
-1.69	-1.47	-0.78	-0.08	0.12	0.22	0.32	0.54	
map_weights [-0.55972963 -0.39581548 -0.2277379  -0.46888696 -0.37639545 -0.34152568]
MAP reward
-0.40	-0.23	-0.38	-0.47	-0.40	-0.40	-0.47	-0.47	
-0.23	-0.23	-0.38	-0.56	-0.40	-0.23	-0.40	-0.38	
-0.56	-0.47	-0.40	-0.38	-0.56	-0.38	-0.47	-0.56	
-0.56	-0.56	-0.40	-0.23	-0.23	-0.38	-0.38	-0.23	
-0.47	-0.40	-0.56	-0.38	-0.56	-0.38	-0.38	-0.23	
-0.38	-0.23	-0.56	-0.40	-0.40	-0.40	-0.38	-0.47	
-0.38	-0.23	-0.56	-0.56	-0.40	-0.56	-0.56	-0.56	
-0.56	-0.38	-0.38	-0.47	-0.40	-0.40	-0.47	-0.34	
Map policy
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
v	v	>	v	>	>	>	v	
>	v	>	>	v	>	>	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1075641157777856
mean w [-0.51476362 -0.27957017 -0.42750839 -0.23410098 -0.27808935 -0.0672723 ]
Mean policy from posterior
>	>	>	>	>	>	v	<	
>	v	v	v	>	v	v	<	
>	>	>	v	>	>	v	v	
v	>	>	v	>	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	>	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.28	-0.43	-0.28	-0.23	-0.28	-0.28	-0.23	-0.23	
-0.43	-0.43	-0.28	-0.51	-0.28	-0.43	-0.28	-0.28	
-0.51	-0.23	-0.28	-0.28	-0.51	-0.28	-0.23	-0.51	
-0.51	-0.51	-0.28	-0.43	-0.43	-0.28	-0.28	-0.43	
-0.23	-0.28	-0.51	-0.28	-0.51	-0.28	-0.28	-0.43	
-0.28	-0.43	-0.51	-0.28	-0.28	-0.28	-0.28	-0.23	
-0.28	-0.43	-0.51	-0.51	-0.28	-0.51	-0.51	-0.51	
-0.51	-0.28	-0.28	-0.23	-0.28	-0.28	-0.23	-0.07	
mean = 0.7913340029515461, map = 1.0122361490181035
CVaR policy
>	>	>	>	>	>	v	v	
>	v	v	>	>	v	v	v	
v	>	>	v	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	v	
>	v	v	>	>	v	v	v	
>	>	>	v	>	>	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	<	
>	v	v	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.8127695190205937, 0.8114711001366848, 0.7715238601821717, 0.771523854755783, 0.7854907771385544
==========
iteration 39
==========
weights [-0.04764988 -0.63173347 -0.26004631 -0.41853618 -0.52868348  0.27629598]
expeced value MDP LP -2.0470749509994812
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.23817682  0.16341088 -0.23043163 -0.46030564  0.43411073  0.68054137]
w_map [-0.18852514 -0.4769162  -0.65287233 -0.43767902 -0.3360859  -0.07905871] loglik -1.9337448691203463e-06
accepted/total = 1532/3000 = 0.5106666666666667
-------
true weights [-0.04764988 -0.63173347 -0.26004631 -0.41853618 -0.52868348  0.27629598]
features
2 	0 	3 	1 	1 	0 	1 	1 	
4 	4 	2 	3 	2 	0 	0 	0 	
0 	3 	2 	1 	4 	0 	3 	2 	
2 	3 	0 	4 	4 	1 	4 	1 	
2 	2 	0 	4 	0 	3 	2 	1 	
3 	1 	2 	0 	2 	1 	2 	3 	
0 	0 	4 	4 	1 	0 	1 	1 	
2 	2 	3 	1 	4 	4 	3 	5 	
optimal policy
>	>	v	v	>	v	v	v	
v	>	v	<	>	v	v	<	
v	>	v	<	v	>	v	v	
>	>	v	<	v	v	v	v	
>	>	v	>	v	>	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
optimal values
-3.04	-2.81	-2.79	-3.40	-2.88	-2.27	-2.83	-2.85	
-3.08	-2.90	-2.40	-2.79	-2.48	-2.24	-2.22	-2.24	
-2.58	-2.56	-2.16	-2.77	-2.64	-2.22	-2.19	-2.25	
-2.55	-2.32	-1.92	-2.43	-2.13	-2.30	-1.79	-2.01	
-2.37	-2.13	-1.89	-2.13	-1.62	-1.68	-1.28	-1.40	
-2.76	-2.47	-1.86	-1.62	-1.58	-1.34	-1.03	-0.77	
-2.41	-2.39	-2.36	-1.85	-1.34	-0.71	-0.78	-0.36	
-2.65	-2.45	-2.21	-1.81	-1.19	-0.67	-0.15	0.28	
map_weights [-0.18852514 -0.4769162  -0.65287233 -0.43767902 -0.3360859  -0.07905871]
MAP reward
-0.65	-0.19	-0.44	-0.48	-0.48	-0.19	-0.48	-0.48	
-0.34	-0.34	-0.65	-0.44	-0.65	-0.19	-0.19	-0.19	
-0.19	-0.44	-0.65	-0.48	-0.34	-0.19	-0.44	-0.65	
-0.65	-0.44	-0.19	-0.34	-0.34	-0.48	-0.34	-0.48	
-0.65	-0.65	-0.19	-0.34	-0.19	-0.44	-0.65	-0.48	
-0.44	-0.48	-0.65	-0.19	-0.65	-0.48	-0.65	-0.44	
-0.19	-0.19	-0.34	-0.34	-0.48	-0.19	-0.48	-0.48	
-0.65	-0.65	-0.44	-0.48	-0.34	-0.34	-0.44	-0.08	
Map policy
>	v	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	>	v	
v	>	>	v	>	v	>	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -2.0508094317956598
mean w [-0.08369339 -0.57788584 -0.3991495  -0.40669203 -0.21872649 -0.04951121]
Mean policy from posterior
>	v	v	>	>	v	<	v	
v	>	v	>	v	v	<	<	
v	>	v	v	v	<	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.40	-0.08	-0.41	-0.58	-0.58	-0.08	-0.58	-0.58	
-0.22	-0.22	-0.40	-0.41	-0.40	-0.08	-0.08	-0.08	
-0.08	-0.41	-0.40	-0.58	-0.22	-0.08	-0.41	-0.40	
-0.40	-0.41	-0.08	-0.22	-0.22	-0.58	-0.22	-0.58	
-0.40	-0.40	-0.08	-0.22	-0.08	-0.41	-0.40	-0.58	
-0.41	-0.58	-0.40	-0.08	-0.40	-0.58	-0.40	-0.41	
-0.08	-0.08	-0.22	-0.22	-0.58	-0.08	-0.58	-0.58	
-0.40	-0.40	-0.41	-0.58	-0.22	-0.22	-0.41	-0.05	
mean = 0.24591403636790865, map = 0.29319144490044646
CVaR policy
>	v	v	>	>	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
v	>	v	>	v	v	v	<	
v	>	v	v	v	<	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	v	v	v	
v	>	v	>	v	v	<	<	
v	>	v	v	v	<	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	v	<	v	
v	>	v	>	v	v	<	<	
v	>	v	v	v	<	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.19342671420164326, 0.19342672447339204, 0.2131847608501256, 0.2459140363861656, 0.24591403638077125
==========
iteration 40
==========
weights [-0.16999147 -0.30044224 -0.18595266 -0.18621804 -0.86748449  0.24300713]
expeced value MDP LP -1.2911925434252285
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.36494151 -0.195508    0.56725186  0.38301814 -0.50988508  0.31643949]
w_map [-0.36670646 -0.45647457 -0.45449457 -0.60922126 -0.21281749  0.18479775] loglik 0.0
accepted/total = 2275/3000 = 0.7583333333333333
-------
true weights [-0.16999147 -0.30044224 -0.18595266 -0.18621804 -0.86748449  0.24300713]
features
2 	4 	1 	0 	3 	4 	0 	0 	
2 	4 	1 	2 	3 	4 	3 	1 	
2 	0 	3 	3 	1 	2 	4 	2 	
3 	0 	2 	1 	2 	3 	3 	0 	
3 	3 	2 	1 	4 	3 	0 	2 	
2 	0 	1 	4 	1 	3 	1 	4 	
2 	2 	0 	2 	3 	2 	1 	0 	
0 	3 	3 	0 	3 	0 	1 	5 	
optimal policy
v	>	>	v	v	<	>	v	
v	v	v	v	v	v	>	v	
>	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	<	>	v	v	<	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.24	-3.04	-2.20	-1.91	-1.76	-2.61	-1.81	-1.65	
-2.08	-2.59	-2.01	-1.76	-1.59	-1.99	-1.67	-1.50	
-1.91	-1.74	-1.73	-1.59	-1.42	-1.13	-1.73	-1.21	
-1.76	-1.59	-1.56	-1.42	-1.13	-0.95	-0.87	-1.03	
-1.60	-1.43	-1.39	-1.67	-1.64	-0.78	-0.69	-0.87	
-1.43	-1.26	-1.21	-1.62	-0.89	-0.60	-0.53	-0.80	
-1.27	-1.10	-0.92	-0.76	-0.60	-0.41	-0.23	0.07	
-1.10	-0.94	-0.76	-0.58	-0.41	-0.23	-0.06	0.24	
map_weights [-0.36670646 -0.45647457 -0.45449457 -0.60922126 -0.21281749  0.18479775]
MAP reward
-0.45	-0.21	-0.46	-0.37	-0.61	-0.21	-0.37	-0.37	
-0.45	-0.21	-0.46	-0.45	-0.61	-0.21	-0.61	-0.46	
-0.45	-0.37	-0.61	-0.61	-0.46	-0.45	-0.21	-0.45	
-0.61	-0.37	-0.45	-0.46	-0.45	-0.61	-0.61	-0.37	
-0.61	-0.61	-0.45	-0.46	-0.21	-0.61	-0.37	-0.45	
-0.45	-0.37	-0.46	-0.21	-0.46	-0.61	-0.46	-0.21	
-0.45	-0.45	-0.37	-0.45	-0.61	-0.45	-0.46	-0.37	
-0.37	-0.61	-0.61	-0.37	-0.61	-0.37	-0.46	0.18	
Map policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	>	v	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1016315712209295
mean w [-0.2845212  -0.30923979 -0.52049721 -0.31844066 -0.35185626  0.03632514]
Mean policy from posterior
>	v	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	v	v	>	>	>	v	v	
>	>	v	>	>	>	v	v	
v	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.52	-0.35	-0.31	-0.28	-0.32	-0.35	-0.28	-0.28	
-0.52	-0.35	-0.31	-0.52	-0.32	-0.35	-0.32	-0.31	
-0.52	-0.28	-0.32	-0.32	-0.31	-0.52	-0.35	-0.52	
-0.32	-0.28	-0.52	-0.31	-0.52	-0.32	-0.32	-0.28	
-0.32	-0.32	-0.52	-0.31	-0.35	-0.32	-0.28	-0.52	
-0.52	-0.28	-0.31	-0.35	-0.31	-0.32	-0.31	-0.35	
-0.52	-0.52	-0.28	-0.52	-0.32	-0.52	-0.31	-0.28	
-0.28	-0.32	-0.32	-0.28	-0.32	-0.28	-0.31	0.04	
mean = 0.27474416780562727, map = 0.8622581290849327
CVaR policy
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	v	v	>	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	v	v	>	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	>	v	v	
>	v	v	>	>	>	v	v	
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	v	>	>	>	v	v	
>	>	v	>	v	>	v	v	
v	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	v	>	>	>	v	v	
>	>	v	>	v	>	v	v	
v	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.7582022387679048, 0.7560527535428894, 0.6623586061421494, 0.27817580618512316, 0.27122382992361516
==========
iteration 41
==========
weights [-0.31750265 -0.52318306 -0.21493703 -0.01627862 -0.06694747  0.75797537]
expeced value MDP LP -0.4478254943158449
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.19307513 -0.59653329  0.03019293 -0.25231536  0.61937335 -0.39833657]
w_map [-0.24585252 -0.87483768 -0.27742523 -0.2037841  -0.13790221  0.19158768] loglik -9.27025003338855e-06
accepted/total = 1456/3000 = 0.48533333333333334
-------
true weights [-0.31750265 -0.52318306 -0.21493703 -0.01627862 -0.06694747  0.75797537]
features
4 	4 	3 	1 	3 	3 	0 	3 	
3 	4 	1 	4 	1 	4 	1 	4 	
1 	0 	1 	4 	0 	0 	2 	3 	
2 	4 	3 	0 	1 	3 	1 	3 	
3 	3 	1 	0 	2 	0 	0 	3 	
4 	4 	4 	1 	3 	0 	4 	2 	
1 	1 	2 	3 	2 	2 	2 	1 	
3 	4 	3 	1 	4 	2 	0 	5 	
optimal policy
v	v	<	>	>	>	>	v	
>	v	<	v	^	^	>	v	
v	v	v	v	>	>	>	v	
v	v	<	<	v	v	>	v	
>	v	v	>	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
optimal values
-0.94	-0.93	-0.94	-0.98	-0.46	-0.45	-0.44	-0.12	
-0.88	-0.87	-1.39	-0.94	-0.98	-0.51	-0.63	-0.11	
-1.18	-0.82	-1.03	-0.89	-0.88	-0.57	-0.25	-0.04	
-0.66	-0.50	-0.51	-0.83	-0.82	-0.50	-0.55	-0.02	
-0.45	-0.44	-0.88	-0.62	-0.30	-0.49	-0.17	-0.01	
-0.49	-0.43	-0.37	-0.61	-0.09	-0.17	0.14	0.01	
-0.91	-0.82	-0.30	-0.09	-0.07	-0.00	0.21	0.23	
-0.39	-0.38	-0.31	-0.38	0.14	0.21	0.43	0.76	
map_weights [-0.24585252 -0.87483768 -0.27742523 -0.2037841  -0.13790221  0.19158768]
MAP reward
-0.14	-0.14	-0.20	-0.87	-0.20	-0.20	-0.25	-0.20	
-0.20	-0.14	-0.87	-0.14	-0.87	-0.14	-0.87	-0.14	
-0.87	-0.25	-0.87	-0.14	-0.25	-0.25	-0.28	-0.20	
-0.28	-0.14	-0.20	-0.25	-0.87	-0.20	-0.87	-0.20	
-0.20	-0.20	-0.87	-0.25	-0.28	-0.25	-0.25	-0.20	
-0.14	-0.14	-0.14	-0.87	-0.20	-0.25	-0.14	-0.28	
-0.87	-0.87	-0.28	-0.20	-0.28	-0.28	-0.28	-0.87	
-0.20	-0.14	-0.20	-0.87	-0.14	-0.28	-0.25	0.19	
Map policy
>	v	<	v	>	v	>	v	
>	v	>	v	>	v	>	v	
>	v	>	>	>	v	>	v	
>	v	>	v	>	v	v	v	
v	v	v	>	v	v	v	<	
>	>	v	>	>	>	v	<	
^	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.5022730125869934
mean w [-0.28582434 -0.77197244 -0.19904917 -0.19274433 -0.10477367 -0.00109791]
Mean policy from posterior
>	v	<	v	>	v	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	<	v	v	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	<	
^	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.10	-0.10	-0.19	-0.77	-0.19	-0.19	-0.29	-0.19	
-0.19	-0.10	-0.77	-0.10	-0.77	-0.10	-0.77	-0.10	
-0.77	-0.29	-0.77	-0.10	-0.29	-0.29	-0.20	-0.19	
-0.20	-0.10	-0.19	-0.29	-0.77	-0.19	-0.77	-0.19	
-0.19	-0.19	-0.77	-0.29	-0.20	-0.29	-0.29	-0.19	
-0.10	-0.10	-0.10	-0.77	-0.19	-0.29	-0.10	-0.20	
-0.77	-0.77	-0.20	-0.19	-0.20	-0.20	-0.20	-0.77	
-0.19	-0.10	-0.19	-0.77	-0.10	-0.20	-0.29	-0.00	
mean = 0.06751237560822182, map = 0.12271170314469887
CVaR policy
>	v	<	v	>	v	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	v	<	v	v	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	v	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	<	v	v	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	<	
^	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	v	>	v	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	<	v	v	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	<	
^	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	v	>	v	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	<	v	v	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	<	
^	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	<	v	>	v	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	<	v	v	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	<	
^	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.03759537656062106, 0.050032630675898526, 0.0675123753202212, 0.06751237526737613, 0.06751238854253089
==========
iteration 42
==========
weights [-0.48168162 -0.00285144 -0.40146165 -0.62732878 -0.19438844  0.41889732]
expeced value MDP LP -0.6802611196993763
demonstration
[(56, 1), (57, 2), (49, 2), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1), (42, 0), (41, 1)]
[ 0.26156956  0.17585796  0.76115433 -0.22572511 -0.39827927 -0.33424725]
w_map [ 0.15620636  0.68505417 -0.26712995 -0.04630837  0.1295844  -0.64498478] loglik 0.0
accepted/total = 2668/3000 = 0.8893333333333333
-------
true weights [-0.48168162 -0.00285144 -0.40146165 -0.62732878 -0.19438844  0.41889732]
features
4 	2 	3 	4 	2 	4 	2 	0 	
3 	2 	2 	2 	3 	3 	1 	3 	
1 	1 	3 	1 	3 	2 	1 	2 	
2 	2 	3 	1 	3 	1 	4 	0 	
3 	2 	3 	1 	1 	1 	3 	3 	
2 	1 	1 	4 	3 	0 	4 	1 	
2 	4 	4 	4 	4 	0 	4 	3 	
2 	4 	3 	4 	2 	2 	0 	5 	
optimal policy
v	v	>	v	>	>	v	<	
v	v	>	v	<	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	v	v	v	<	<	
v	v	>	>	>	<	<	v	
>	>	<	^	^	^	>	v	
>	^	^	^	<	>	v	v	
>	^	^	^	>	>	>	.	
optimal values
-1.09	-1.08	-1.49	-0.87	-1.26	-0.87	-0.68	-1.16	
-0.91	-0.68	-1.08	-0.68	-1.30	-0.91	-0.29	-0.91	
-0.29	-0.29	-0.91	-0.29	-0.91	-0.68	-0.29	-0.68	
-0.68	-0.68	-0.91	-0.29	-0.91	-0.29	-0.48	-0.95	
-1.30	-0.68	-0.91	-0.29	-0.29	-0.29	-0.91	-0.84	
-0.68	-0.29	-0.29	-0.48	-0.91	-0.76	-0.41	-0.21	
-0.87	-0.48	-0.48	-0.67	-0.85	-0.74	-0.26	-0.21	
-1.06	-0.67	-1.10	-0.85	-0.86	-0.47	-0.07	0.42	
map_weights [ 0.15620636  0.68505417 -0.26712995 -0.04630837  0.1295844  -0.64498478]
MAP reward
0.13	-0.27	-0.05	0.13	-0.27	0.13	-0.27	0.16	
-0.05	-0.27	-0.27	-0.27	-0.05	-0.05	0.69	-0.05	
0.69	0.69	-0.05	0.69	-0.05	-0.27	0.69	-0.27	
-0.27	-0.27	-0.05	0.69	-0.05	0.69	0.13	0.16	
-0.05	-0.27	-0.05	0.69	0.69	0.69	-0.05	-0.05	
-0.27	0.69	0.69	0.13	-0.05	0.16	0.13	0.69	
-0.27	0.13	0.13	0.13	0.13	0.16	0.13	-0.05	
-0.27	0.13	-0.05	0.13	-0.27	-0.27	0.16	-0.64	
Map policy
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
<	<	<	v	<	v	^	<	
^	^	>	v	v	v	<	<	
^	v	>	>	>	^	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
expeced value MDP LP 53.709722654645745
mean w [-0.01254747  0.55451117 -0.37748479 -0.1571289  -0.01044263 -0.16532503]
Mean policy from posterior
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	v	<	v	<	<	
^	v	>	>	<	<	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
Mean rewards
-0.01	-0.38	-0.16	-0.01	-0.38	-0.01	-0.38	-0.01	
-0.16	-0.38	-0.38	-0.38	-0.16	-0.16	0.55	-0.16	
0.55	0.55	-0.16	0.55	-0.16	-0.38	0.55	-0.38	
-0.38	-0.38	-0.16	0.55	-0.16	0.55	-0.01	-0.01	
-0.16	-0.38	-0.16	0.55	0.55	0.55	-0.16	-0.16	
-0.38	0.55	0.55	-0.01	-0.16	-0.01	-0.01	0.55	
-0.38	-0.01	-0.01	-0.01	-0.01	-0.01	-0.01	-0.16	
-0.38	-0.01	-0.16	-0.01	-0.38	-0.38	-0.01	-0.17	
mean = 0.09415734474543502, map = 0.09415733837821727
CVaR policy
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
<	<	<	v	<	v	^	<	
^	^	>	v	<	v	<	<	
^	v	>	>	>	^	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	v	<	v	<	<	
^	v	>	>	>	<	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	^	<	v	<	<	
^	v	>	^	<	^	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	^	<	v	<	<	
^	v	>	^	<	^	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	v	v	v	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	^	<	v	<	<	
^	v	>	^	<	^	<	v	
>	>	<	^	^	^	>	>	
>	^	^	^	^	^	^	^	
>	^	^	^	^	^	^	.	
cvar = , 0.09415734102978268, 0.09415734054158842, 0.09415774447201797, 0.09415746684011095, 0.09415734964668265
==========
iteration 43
==========
weights [-0.43255586 -0.48379263 -0.00750224 -0.64402311 -0.34845398  0.20639255]
expeced value MDP LP -1.2029239591397993
demonstration
[(56, 2), (48, 2), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0), (40, 0)]
[ 0.3906694  -0.54578307  0.46643941  0.46318181 -0.33190676  0.08504734]
w_map [ 0.21917652 -0.79158846  0.5412815   0.11830302 -0.13461845 -0.01568335] loglik 0.0
accepted/total = 2727/3000 = 0.909
-------
true weights [-0.43255586 -0.48379263 -0.00750224 -0.64402311 -0.34845398  0.20639255]
features
4 	2 	3 	3 	1 	3 	2 	0 	
2 	3 	2 	0 	2 	3 	0 	4 	
2 	0 	2 	1 	2 	4 	4 	3 	
2 	1 	0 	2 	0 	0 	3 	2 	
0 	1 	3 	3 	3 	3 	1 	4 	
2 	0 	1 	3 	1 	0 	2 	3 	
1 	4 	1 	4 	1 	3 	3 	0 	
1 	4 	0 	3 	4 	4 	4 	5 	
optimal policy
>	^	<	v	v	>	^	<	
v	^	v	>	v	<	^	^	
^	>	^	>	^	<	<	v	
^	<	^	>	^	^	>	>	
v	<	^	^	^	v	v	^	
<	<	<	v	>	>	v	v	
^	^	<	>	v	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.09	-0.75	-1.39	-1.81	-1.23	-1.39	-0.75	-1.18	
-0.75	-1.39	-0.75	-1.18	-0.75	-1.39	-1.18	-1.51	
-0.75	-1.18	-0.75	-1.23	-0.75	-1.09	-1.43	-1.39	
-0.75	-1.23	-1.18	-1.17	-1.18	-1.51	-1.39	-0.75	
-1.18	-1.65	-1.81	-1.80	-1.81	-1.84	-1.26	-1.09	
-0.75	-1.18	-1.65	-2.27	-1.68	-1.21	-0.79	-0.87	
-1.23	-1.51	-1.98	-1.65	-1.31	-1.13	-0.79	-0.23	
-1.70	-1.85	-1.89	-1.47	-0.83	-0.49	-0.14	0.21	
map_weights [ 0.21917652 -0.79158846  0.5412815   0.11830302 -0.13461845 -0.01568335]
MAP reward
-0.13	0.54	0.12	0.12	-0.79	0.12	0.54	0.22	
0.54	0.12	0.54	0.22	0.54	0.12	0.22	-0.13	
0.54	0.22	0.54	-0.79	0.54	-0.13	-0.13	0.12	
0.54	-0.79	0.22	0.54	0.22	0.22	0.12	0.54	
0.22	-0.79	0.12	0.12	0.12	0.12	-0.79	-0.13	
0.54	0.22	-0.79	0.12	-0.79	0.22	0.54	0.12	
-0.79	-0.13	-0.79	-0.13	-0.79	0.12	0.12	0.22	
-0.79	-0.13	0.22	0.12	-0.13	-0.13	-0.13	-0.02	
Map policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
<	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	^	^	
<	<	<	^	^	^	>	^	
^	^	<	^	>	^	^	^	
^	^	<	^	<	^	^	.	
expeced value MDP LP 49.6036123006769
mean w [ 0.01378708 -0.14882735  0.51446735 -0.15870207 -0.19184318 -0.01320775]
Mean policy from posterior
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
^	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	^	^	
<	<	<	<	^	>	^	^	
^	^	^	<	^	^	^	^	
^	^	^	<	^	^	^	.	
Mean rewards
-0.19	0.51	-0.16	-0.16	-0.15	-0.16	0.51	0.01	
0.51	-0.16	0.51	0.01	0.51	-0.16	0.01	-0.19	
0.51	0.01	0.51	-0.15	0.51	-0.19	-0.19	-0.16	
0.51	-0.15	0.01	0.51	0.01	0.01	-0.16	0.51	
0.01	-0.15	-0.16	-0.16	-0.16	-0.16	-0.15	-0.19	
0.51	0.01	-0.15	-0.16	-0.15	0.01	0.51	-0.16	
-0.15	-0.19	-0.15	-0.19	-0.15	-0.16	-0.16	0.01	
-0.15	-0.19	0.01	-0.16	-0.19	-0.19	-0.19	-0.01	
mean = 0.3483059262403656, map = 0.38912987022408574
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
v	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	^	^	
<	<	<	^	^	>	^	^	
^	^	^	^	^	^	^	^	
^	<	^	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
v	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	^	^	
<	<	<	^	^	>	^	^	
^	^	^	^	^	^	^	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	>	v	<	^	^	
^	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	^	^	
<	<	<	^	^	>	^	^	
^	^	^	^	^	^	^	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
<	<	v	<	v	<	^	^	
<	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	^	^	
<	<	<	^	^	>	^	^	
^	^	^	<	^	^	^	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
<	<	v	<	v	<	^	^	
<	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	^	^	
<	<	<	<	^	>	^	^	
^	^	^	<	^	^	^	^	
^	^	^	<	^	^	^	.	
cvar = , 0.35846545857240564, 0.35558818656537383, 0.35558729610441264, 0.35070805472419186, 0.34832618465311516
==========
iteration 44
==========
weights [-0.20169497 -0.5384595  -0.10666749 -0.08781692 -0.71497981  0.3729539 ]
expeced value MDP LP -1.6882638322738717
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.27822994  0.37801917 -0.08987394  0.67141263  0.42460003  0.37487622]
w_map [-0.58844275 -0.4708653  -0.39572544 -0.37607178 -0.35091766 -0.10415944] loglik 0.0
accepted/total = 2461/3000 = 0.8203333333333334
-------
true weights [-0.20169497 -0.5384595  -0.10666749 -0.08781692 -0.71497981  0.3729539 ]
features
0 	2 	3 	0 	0 	3 	0 	0 	
4 	4 	1 	0 	2 	3 	0 	1 	
1 	3 	3 	0 	4 	2 	4 	4 	
4 	3 	2 	0 	1 	4 	1 	1 	
2 	0 	4 	1 	4 	0 	3 	0 	
0 	3 	1 	2 	1 	0 	4 	1 	
0 	4 	1 	1 	0 	2 	4 	0 	
3 	0 	4 	1 	0 	4 	3 	5 	
optimal policy
>	>	>	>	>	v	<	<	
^	v	>	>	>	v	<	v	
>	v	v	^	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.48	-2.30	-2.22	-2.15	-1.97	-1.79	-1.97	-2.15	
-3.17	-2.96	-2.51	-1.99	-1.81	-1.72	-1.90	-2.33	
-2.78	-2.27	-2.22	-2.17	-2.34	-1.65	-1.89	-1.81	
-2.90	-2.20	-2.15	-2.07	-2.08	-1.55	-1.18	-1.10	
-2.22	-2.14	-2.58	-1.88	-1.55	-0.85	-0.65	-0.57	
-2.14	-1.95	-1.88	-1.36	-1.27	-0.73	-1.08	-0.37	
-2.30	-2.49	-1.79	-1.27	-0.73	-0.54	-0.44	0.17	
-2.12	-2.05	-1.87	-1.17	-0.63	-0.44	0.28	0.37	
map_weights [-0.58844275 -0.4708653  -0.39572544 -0.37607178 -0.35091766 -0.10415944]
MAP reward
-0.59	-0.40	-0.38	-0.59	-0.59	-0.38	-0.59	-0.59	
-0.35	-0.35	-0.47	-0.59	-0.40	-0.38	-0.59	-0.47	
-0.47	-0.38	-0.38	-0.59	-0.35	-0.40	-0.35	-0.35	
-0.35	-0.38	-0.40	-0.59	-0.47	-0.35	-0.47	-0.47	
-0.40	-0.59	-0.35	-0.47	-0.35	-0.59	-0.38	-0.59	
-0.59	-0.38	-0.47	-0.40	-0.47	-0.59	-0.35	-0.47	
-0.59	-0.35	-0.47	-0.47	-0.59	-0.40	-0.35	-0.59	
-0.38	-0.59	-0.35	-0.47	-0.59	-0.35	-0.38	-0.10	
Map policy
v	v	v	>	v	v	v	v	
>	v	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.297798643642241
mean w [-0.45235136 -0.38206109 -0.41314078 -0.28618939 -0.29375635 -0.04107436]
Mean policy from posterior
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.45	-0.41	-0.29	-0.45	-0.45	-0.29	-0.45	-0.45	
-0.29	-0.29	-0.38	-0.45	-0.41	-0.29	-0.45	-0.38	
-0.38	-0.29	-0.29	-0.45	-0.29	-0.41	-0.29	-0.29	
-0.29	-0.29	-0.41	-0.45	-0.38	-0.29	-0.38	-0.38	
-0.41	-0.45	-0.29	-0.38	-0.29	-0.45	-0.29	-0.45	
-0.45	-0.29	-0.38	-0.41	-0.38	-0.45	-0.29	-0.38	
-0.45	-0.29	-0.38	-0.38	-0.45	-0.41	-0.29	-0.45	
-0.29	-0.45	-0.29	-0.38	-0.45	-0.29	-0.29	-0.04	
mean = 0.8162950670636877, map = 0.7486797218832875
CVaR policy
>	>	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
>	v	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.20147656520006807, 0.3281190590794858, 0.6729446087788187, 0.7838303741299266, 0.8162950664961293
==========
iteration 45
==========
weights [-0.13132706 -0.72845255 -0.31441088 -0.54012427 -0.2334518   0.08379686]
expeced value MDP LP -2.3290516589831314
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.4328335  -0.13112424  0.14549898 -0.15531472  0.82324913  0.26912794]
w_map [-0.41719921 -0.66299595 -0.47135826 -0.31470784 -0.23065354 -0.10936402] loglik -5.967862648503797e-09
accepted/total = 1796/3000 = 0.5986666666666667
-------
true weights [-0.13132706 -0.72845255 -0.31441088 -0.54012427 -0.2334518   0.08379686]
features
3 	3 	1 	4 	1 	2 	2 	1 	
1 	2 	3 	4 	1 	4 	0 	4 	
3 	0 	2 	4 	3 	1 	1 	2 	
2 	2 	4 	1 	2 	1 	3 	1 	
2 	2 	3 	0 	2 	3 	1 	2 	
2 	4 	1 	3 	3 	3 	4 	2 	
1 	0 	3 	3 	3 	2 	4 	4 	
0 	2 	1 	1 	0 	1 	2 	5 	
optimal policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	v	v	>	v	^	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
optimal values
-4.49	-3.99	-4.09	-3.40	-3.33	-2.62	-2.41	-2.72	
-4.17	-3.48	-3.61	-3.19	-3.04	-2.33	-2.12	-2.01	
-3.71	-3.20	-3.10	-2.99	-2.78	-3.04	-2.50	-1.79	
-3.38	-3.10	-2.81	-2.79	-2.27	-2.39	-1.86	-1.49	
-3.17	-2.88	-2.60	-2.08	-1.97	-1.67	-1.33	-0.77	
-2.88	-2.59	-2.90	-2.20	-1.67	-1.15	-0.61	-0.46	
-3.09	-2.39	-2.28	-1.75	-1.23	-0.69	-0.38	-0.15	
-2.78	-2.68	-2.51	-1.80	-1.08	-0.96	-0.23	0.08	
map_weights [-0.41719921 -0.66299595 -0.47135826 -0.31470784 -0.23065354 -0.10936402]
MAP reward
-0.31	-0.31	-0.66	-0.23	-0.66	-0.47	-0.47	-0.66	
-0.66	-0.47	-0.31	-0.23	-0.66	-0.23	-0.42	-0.23	
-0.31	-0.42	-0.47	-0.23	-0.31	-0.66	-0.66	-0.47	
-0.47	-0.47	-0.23	-0.66	-0.47	-0.66	-0.31	-0.66	
-0.47	-0.47	-0.31	-0.42	-0.47	-0.31	-0.66	-0.47	
-0.47	-0.23	-0.66	-0.31	-0.31	-0.31	-0.23	-0.47	
-0.66	-0.42	-0.31	-0.31	-0.31	-0.47	-0.23	-0.23	
-0.42	-0.47	-0.66	-0.66	-0.42	-0.66	-0.47	-0.11	
Map policy
>	v	>	v	<	v	v	v	
v	>	>	v	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	^	>	>	>	.	
expeced value MDP LP -1.564034520668947
mean w [-0.26208239 -0.60630462 -0.30780359 -0.25404242 -0.14134934  0.10694616]
Mean policy from posterior
>	v	>	v	<	v	v	v	
>	>	>	v	v	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	^	^	>	>	.	
Mean rewards
-0.25	-0.25	-0.61	-0.14	-0.61	-0.31	-0.31	-0.61	
-0.61	-0.31	-0.25	-0.14	-0.61	-0.14	-0.26	-0.14	
-0.25	-0.26	-0.31	-0.14	-0.25	-0.61	-0.61	-0.31	
-0.31	-0.31	-0.14	-0.61	-0.31	-0.61	-0.25	-0.61	
-0.31	-0.31	-0.25	-0.26	-0.31	-0.25	-0.61	-0.31	
-0.31	-0.14	-0.61	-0.25	-0.25	-0.25	-0.14	-0.31	
-0.61	-0.26	-0.25	-0.25	-0.25	-0.31	-0.14	-0.14	
-0.26	-0.31	-0.61	-0.61	-0.26	-0.61	-0.31	0.11	
mean = 0.11293224878125452, map = 0.15502051578746467
CVaR policy
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	v	>	v	v	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	>	>	v	v	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	>	>	v	v	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	^	^	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
>	>	>	v	v	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	^	^	^	>	>	.	
cvar = , 0.11268332070141751, 0.09574517139184469, 0.09739194274216967, 0.11180437767842699, 0.11293224098256571
==========
iteration 46
==========
weights [-0.28003463 -0.43393688 -0.70423907 -0.3280981  -0.15792812  0.32363106]
expeced value MDP LP -1.7189552378788981
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.15938677  0.0689153  -0.4093443   0.13657295  0.13138167 -0.87542588]
w_map [-0.51512359 -0.5548272  -0.37443814 -0.39373281 -0.35770974 -0.06023916] loglik 0.0
accepted/total = 2381/3000 = 0.7936666666666666
-------
true weights [-0.28003463 -0.43393688 -0.70423907 -0.3280981  -0.15792812  0.32363106]
features
3 	1 	3 	1 	3 	2 	2 	1 	
0 	4 	4 	4 	4 	4 	4 	3 	
3 	0 	3 	0 	0 	0 	2 	0 	
3 	3 	2 	1 	0 	4 	4 	2 	
3 	2 	4 	1 	3 	0 	4 	3 	
3 	2 	3 	1 	1 	1 	0 	2 	
3 	1 	2 	0 	2 	3 	3 	3 	
0 	2 	1 	1 	1 	3 	3 	5 	
optimal policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.61	-2.45	-2.21	-2.18	-1.93	-2.17	-2.31	-2.35	
-2.30	-2.04	-1.90	-1.76	-1.62	-1.48	-1.62	-1.93	
-2.61	-2.30	-2.17	-1.86	-1.60	-1.33	-1.61	-1.87	
-2.91	-2.61	-2.44	-1.75	-1.33	-1.06	-0.91	-1.61	
-2.90	-2.60	-1.91	-1.77	-1.35	-1.04	-0.76	-1.03	
-3.16	-2.87	-2.19	-1.88	-1.46	-1.04	-0.61	-0.71	
-2.86	-2.56	-2.15	-1.46	-1.36	-0.66	-0.34	-0.01	
-2.56	-2.30	-1.61	-1.19	-0.77	-0.34	-0.01	0.32	
map_weights [-0.51512359 -0.5548272  -0.37443814 -0.39373281 -0.35770974 -0.06023916]
MAP reward
-0.39	-0.55	-0.39	-0.55	-0.39	-0.37	-0.37	-0.55	
-0.52	-0.36	-0.36	-0.36	-0.36	-0.36	-0.36	-0.39	
-0.39	-0.52	-0.39	-0.52	-0.52	-0.52	-0.37	-0.52	
-0.39	-0.39	-0.37	-0.55	-0.52	-0.36	-0.36	-0.37	
-0.39	-0.37	-0.36	-0.55	-0.39	-0.52	-0.36	-0.39	
-0.39	-0.37	-0.39	-0.55	-0.55	-0.55	-0.52	-0.37	
-0.39	-0.55	-0.37	-0.52	-0.37	-0.39	-0.39	-0.39	
-0.52	-0.37	-0.55	-0.55	-0.55	-0.39	-0.39	-0.06	
Map policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.126811593916762
mean w [-0.49640299 -0.22423001 -0.33794261 -0.25530058 -0.40580831 -0.29448754]
Mean policy from posterior
>	>	v	v	>	>	>	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
v	v	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.26	-0.22	-0.26	-0.22	-0.26	-0.34	-0.34	-0.22	
-0.50	-0.41	-0.41	-0.41	-0.41	-0.41	-0.41	-0.26	
-0.26	-0.50	-0.26	-0.50	-0.50	-0.50	-0.34	-0.50	
-0.26	-0.26	-0.34	-0.22	-0.50	-0.41	-0.41	-0.34	
-0.26	-0.34	-0.41	-0.22	-0.26	-0.50	-0.41	-0.26	
-0.26	-0.34	-0.26	-0.22	-0.22	-0.22	-0.50	-0.34	
-0.26	-0.22	-0.34	-0.50	-0.34	-0.26	-0.26	-0.26	
-0.50	-0.34	-0.22	-0.22	-0.22	-0.26	-0.26	-0.29	
mean = 0.6904003873044915, map = 0.4654498184893141
CVaR policy
>	v	>	v	>	>	v	v	
>	>	>	v	v	v	v	v	
>	v	v	>	>	>	v	v	
v	>	>	>	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	>	v	v	
v	>	v	v	>	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	v	>	v	v	v	>	v	
>	v	>	>	v	v	>	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	>	>	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	v	>	v	
v	v	>	v	v	v	>	v	
v	v	>	>	>	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	>	>	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	v	>	v	
v	v	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	>	>	v	
v	>	v	v	v	>	v	v	
v	>	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
v	v	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.6466911587665556, 0.8916508324670938, 0.7312072287196427, 0.7116959314515112, 0.6904003879835461
==========
iteration 47
==========
weights [-0.32654197 -0.65989334 -0.15157256 -0.37933918 -0.338548    0.42002847]
expeced value MDP LP -1.788102963274963
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.08522358 -0.60582493 -0.72694318  0.17666449 -0.0103236  -0.25680691]
w_map [-0.35085967 -0.341261   -0.54332568 -0.54515718 -0.38812354 -0.13190679] loglik 0.0
accepted/total = 2452/3000 = 0.8173333333333334
-------
true weights [-0.32654197 -0.65989334 -0.15157256 -0.37933918 -0.338548    0.42002847]
features
3 	1 	2 	4 	1 	2 	2 	4 	
0 	2 	0 	3 	0 	2 	1 	1 	
1 	4 	2 	1 	1 	1 	2 	4 	
0 	3 	3 	0 	1 	4 	0 	2 	
2 	0 	3 	3 	3 	4 	2 	0 	
4 	2 	4 	0 	0 	4 	4 	4 	
0 	3 	4 	0 	0 	1 	0 	3 	
3 	2 	2 	4 	1 	4 	0 	5 	
optimal policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.65	-3.63	-3.00	-2.94	-2.73	-2.09	-1.96	-2.07	
-3.30	-3.00	-2.88	-2.63	-2.27	-1.96	-1.83	-1.75	
-3.36	-2.89	-2.58	-2.73	-2.47	-1.83	-1.18	-1.10	
-2.73	-2.66	-2.45	-2.09	-2.01	-1.37	-1.04	-0.77	
-2.43	-2.30	-2.15	-1.79	-1.42	-1.05	-0.72	-0.63	
-2.31	-1.99	-1.86	-1.54	-1.22	-0.91	-0.57	-0.30	
-2.19	-1.88	-1.70	-1.53	-1.21	-0.90	-0.24	0.04	
-1.88	-1.51	-1.38	-1.24	-0.91	-0.25	0.09	0.42	
map_weights [-0.35085967 -0.341261   -0.54332568 -0.54515718 -0.38812354 -0.13190679]
MAP reward
-0.55	-0.34	-0.54	-0.39	-0.34	-0.54	-0.54	-0.39	
-0.35	-0.54	-0.35	-0.55	-0.35	-0.54	-0.34	-0.34	
-0.34	-0.39	-0.54	-0.34	-0.34	-0.34	-0.54	-0.39	
-0.35	-0.55	-0.55	-0.35	-0.34	-0.39	-0.35	-0.54	
-0.54	-0.35	-0.55	-0.55	-0.55	-0.39	-0.54	-0.35	
-0.39	-0.54	-0.39	-0.35	-0.35	-0.39	-0.39	-0.39	
-0.35	-0.55	-0.39	-0.35	-0.35	-0.34	-0.35	-0.55	
-0.55	-0.54	-0.54	-0.39	-0.34	-0.39	-0.35	-0.13	
Map policy
v	>	>	>	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	v	v	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.4477071160806587
mean w [-0.38109194 -0.32767417 -0.27261245 -0.44288461 -0.35666991 -0.14978995]
Mean policy from posterior
>	>	>	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.44	-0.33	-0.27	-0.36	-0.33	-0.27	-0.27	-0.36	
-0.38	-0.27	-0.38	-0.44	-0.38	-0.27	-0.33	-0.33	
-0.33	-0.36	-0.27	-0.33	-0.33	-0.33	-0.27	-0.36	
-0.38	-0.44	-0.44	-0.38	-0.33	-0.36	-0.38	-0.27	
-0.27	-0.38	-0.44	-0.44	-0.44	-0.36	-0.27	-0.38	
-0.36	-0.27	-0.36	-0.38	-0.38	-0.36	-0.36	-0.36	
-0.38	-0.44	-0.36	-0.38	-0.38	-0.33	-0.38	-0.44	
-0.44	-0.27	-0.27	-0.36	-0.33	-0.36	-0.38	-0.15	
mean = 0.13457512378450054, map = 0.4772234554482806
CVaR policy
>	>	>	v	>	>	v	v	
v	>	v	v	v	>	>	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	v	v	
>	v	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	v	>	>	>	>	v	v	
>	v	v	>	>	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.32384259107265856, 0.22554034733654027, 0.15305265670697366, 0.13367334940696218, 0.12748599274410144
==========
iteration 48
==========
weights [-0.38330247 -0.46518363 -0.7242588  -0.08270866 -0.18571323  0.26608733]
expeced value MDP LP -1.5625245722155583
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.30822725  0.44176778  0.75695452 -0.2107819   0.28925511 -0.09359235]
w_map [-0.42243048 -0.37583337 -0.35790551 -0.39784495 -0.62723733  0.02231925] loglik 0.0
accepted/total = 2322/3000 = 0.774
-------
true weights [-0.38330247 -0.46518363 -0.7242588  -0.08270866 -0.18571323  0.26608733]
features
0 	0 	0 	4 	2 	4 	1 	4 	
1 	2 	3 	1 	3 	0 	4 	2 	
0 	2 	4 	0 	3 	2 	3 	4 	
0 	2 	1 	0 	1 	2 	1 	3 	
0 	0 	3 	3 	2 	4 	3 	4 	
1 	1 	1 	2 	4 	0 	3 	4 	
3 	0 	0 	4 	4 	0 	0 	1 	
0 	1 	1 	4 	0 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	<	
>	>	>	>	>	>	v	v	
v	>	>	>	^	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.11	-2.75	-2.39	-2.13	-2.22	-1.62	-1.53	-1.70	
-3.17	-2.73	-2.03	-1.97	-1.52	-1.45	-1.07	-1.54	
-3.02	-2.82	-2.12	-1.95	-1.58	-1.61	-0.90	-0.82	
-2.67	-2.65	-2.03	-1.88	-1.89	-1.44	-1.00	-0.64	
-2.31	-1.94	-1.58	-1.51	-1.44	-0.72	-0.54	-0.57	
-2.37	-2.31	-1.94	-1.73	-1.02	-0.84	-0.46	-0.39	
-1.92	-1.86	-1.49	-1.12	-0.94	-0.76	-0.58	-0.20	
-2.21	-1.85	-1.40	-0.94	-0.76	-0.39	-0.20	0.27	
map_weights [-0.42243048 -0.37583337 -0.35790551 -0.39784495 -0.62723733  0.02231925]
MAP reward
-0.42	-0.42	-0.42	-0.63	-0.36	-0.63	-0.38	-0.63	
-0.38	-0.36	-0.40	-0.38	-0.40	-0.42	-0.63	-0.36	
-0.42	-0.36	-0.63	-0.42	-0.40	-0.36	-0.40	-0.63	
-0.42	-0.36	-0.38	-0.42	-0.38	-0.36	-0.38	-0.40	
-0.42	-0.42	-0.40	-0.40	-0.36	-0.63	-0.40	-0.63	
-0.38	-0.38	-0.38	-0.36	-0.63	-0.42	-0.40	-0.63	
-0.40	-0.42	-0.42	-0.63	-0.63	-0.42	-0.42	-0.38	
-0.42	-0.38	-0.38	-0.63	-0.42	-0.63	-0.38	0.02	
Map policy
v	v	v	>	v	v	v	v	
>	v	>	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.2020655234340567
mean w [-0.44172071 -0.25853819 -0.38056326 -0.32861387 -0.34378585 -0.02050444]
Mean policy from posterior
v	v	v	v	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.44	-0.44	-0.44	-0.34	-0.38	-0.34	-0.26	-0.34	
-0.26	-0.38	-0.33	-0.26	-0.33	-0.44	-0.34	-0.38	
-0.44	-0.38	-0.34	-0.44	-0.33	-0.38	-0.33	-0.34	
-0.44	-0.38	-0.26	-0.44	-0.26	-0.38	-0.26	-0.33	
-0.44	-0.44	-0.33	-0.33	-0.38	-0.34	-0.33	-0.34	
-0.26	-0.26	-0.26	-0.38	-0.34	-0.44	-0.33	-0.34	
-0.33	-0.44	-0.44	-0.34	-0.34	-0.44	-0.44	-0.26	
-0.44	-0.26	-0.26	-0.34	-0.44	-0.34	-0.26	-0.02	
mean = 0.35078968004650646, map = 0.677865194274653
CVaR policy
v	v	>	>	v	v	>	v	
>	v	>	>	v	v	>	v	
>	v	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	>	v	
>	v	>	>	v	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
>	v	>	>	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.5953173816960087, 0.6344669781243297, 0.5055653116108791, 0.3572053469072842, 0.3507896700345441
==========
iteration 49
==========
weights [-0.44520993 -0.27171281 -0.45007951 -0.36066169 -0.62605234  0.05805433]
expeced value MDP LP -2.4933815319654355
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.89673923 -0.1303581   0.08046862  0.19522308  0.04380108 -0.36381277]
w_map [-0.33699889 -0.7405455  -0.38195756 -0.40799958 -0.15755242 -0.02908797] loglik 0.0
accepted/total = 2481/3000 = 0.827
-------
true weights [-0.44520993 -0.27171281 -0.45007951 -0.36066169 -0.62605234  0.05805433]
features
3 	4 	0 	4 	1 	0 	4 	0 	
4 	0 	4 	3 	0 	4 	4 	4 	
4 	2 	2 	0 	1 	4 	2 	2 	
1 	3 	4 	3 	0 	2 	4 	0 	
3 	2 	2 	1 	4 	4 	1 	0 	
3 	3 	1 	3 	4 	1 	2 	1 	
1 	2 	3 	1 	1 	1 	3 	2 	
3 	1 	2 	0 	0 	0 	2 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.87	-4.64	-4.22	-3.82	-3.64	-3.93	-3.59	-2.99	
-4.56	-4.06	-3.82	-3.22	-3.40	-3.52	-3.00	-2.57	
-3.97	-3.65	-3.31	-2.89	-2.99	-2.92	-2.40	-1.97	
-3.38	-3.23	-3.07	-2.47	-2.74	-2.32	-1.97	-1.53	
-3.14	-2.90	-2.56	-2.13	-2.50	-1.89	-1.36	-1.10	
-2.81	-2.47	-2.13	-1.88	-1.89	-1.28	-1.10	-0.66	
-2.56	-2.31	-1.88	-1.53	-1.28	-1.01	-0.75	-0.39	
-2.72	-2.39	-2.14	-1.70	-1.27	-0.83	-0.39	0.06	
map_weights [-0.33699889 -0.7405455  -0.38195756 -0.40799958 -0.15755242 -0.02908797]
MAP reward
-0.41	-0.16	-0.34	-0.16	-0.74	-0.34	-0.16	-0.34	
-0.16	-0.34	-0.16	-0.41	-0.34	-0.16	-0.16	-0.16	
-0.16	-0.38	-0.38	-0.34	-0.74	-0.16	-0.38	-0.38	
-0.74	-0.41	-0.16	-0.41	-0.34	-0.38	-0.16	-0.34	
-0.41	-0.38	-0.38	-0.74	-0.16	-0.16	-0.74	-0.34	
-0.41	-0.41	-0.74	-0.41	-0.16	-0.74	-0.38	-0.74	
-0.74	-0.38	-0.41	-0.74	-0.74	-0.74	-0.41	-0.38	
-0.41	-0.74	-0.38	-0.34	-0.34	-0.34	-0.38	-0.03	
Map policy
>	>	>	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1140313356535443
mean w [-0.26050236 -0.45244561 -0.32288148 -0.32215986 -0.46156135  0.08587628]
Mean policy from posterior
v	v	v	v	v	>	>	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	>	>	v	
v	v	v	v	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.32	-0.46	-0.26	-0.46	-0.45	-0.26	-0.46	-0.26	
-0.46	-0.26	-0.46	-0.32	-0.26	-0.46	-0.46	-0.46	
-0.46	-0.32	-0.32	-0.26	-0.45	-0.46	-0.32	-0.32	
-0.45	-0.32	-0.46	-0.32	-0.26	-0.32	-0.46	-0.26	
-0.32	-0.32	-0.32	-0.45	-0.46	-0.46	-0.45	-0.26	
-0.32	-0.32	-0.45	-0.32	-0.46	-0.45	-0.32	-0.45	
-0.45	-0.32	-0.32	-0.45	-0.45	-0.45	-0.32	-0.32	
-0.32	-0.45	-0.32	-0.26	-0.26	-0.26	-0.32	0.09	
mean = 0.3122451998281548, map = 0.5288213567016045
CVaR policy
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	>	v	
v	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	>	>	v	
>	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	>	>	v	
>	v	>	v	v	>	v	v	
>	v	>	v	v	>	>	v	
v	v	v	v	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	>	>	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	>	>	v	
v	v	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	>	>	v	
>	v	>	v	v	>	v	v	
>	v	>	v	v	>	>	v	
v	v	v	v	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.36645922100102624, 0.3236910003766935, 0.31769799908156315, 0.31769798934900306, 0.3122451994658304
==========
iteration 50
==========
weights [-0.89793034 -0.16824879 -0.15900828 -0.23235499 -0.27964434  0.0891068 ]
expeced value MDP LP -1.471682357898469
demonstration
[(56, 2), (48, 1), (49, 2), (41, 1), (42, 1), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.3948691   0.09572414  0.50753625 -0.08293066 -0.69084106 -0.30525953]
w_map [-0.85672787 -0.15928576 -0.15081607 -0.29417446 -0.33580897 -0.13635846] loglik -1.2632496058984088e-05
accepted/total = 1265/3000 = 0.4216666666666667
-------
true weights [-0.89793034 -0.16824879 -0.15900828 -0.23235499 -0.27964434  0.0891068 ]
features
2 	1 	2 	3 	3 	0 	0 	0 	
1 	2 	4 	4 	2 	3 	1 	2 	
2 	0 	3 	3 	2 	4 	0 	4 	
3 	1 	0 	1 	2 	4 	0 	0 	
1 	4 	4 	2 	0 	2 	4 	3 	
0 	3 	2 	2 	0 	1 	0 	0 	
1 	3 	0 	1 	1 	4 	3 	4 	
4 	3 	3 	0 	2 	2 	2 	5 	
optimal policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	<	
v	>	>	v	v	v	<	^	
v	v	>	v	>	v	v	v	
>	v	v	v	>	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-2.30	-2.16	-2.02	-1.88	-1.75	-2.46	-2.61	-2.75	
-2.16	-2.02	-1.88	-1.66	-1.53	-1.58	-1.73	-1.87	
-2.01	-2.49	-1.61	-1.39	-1.39	-1.36	-2.25	-2.14	
-1.87	-1.66	-2.06	-1.17	-1.24	-1.09	-1.98	-2.19	
-1.66	-1.50	-1.28	-1.02	-1.71	-0.82	-1.09	-1.31	
-2.12	-1.24	-1.02	-0.86	-1.44	-0.67	-1.20	-1.09	
-1.61	-1.46	-1.60	-0.71	-0.55	-0.51	-0.30	-0.19	
-1.87	-1.68	-1.50	-1.28	-0.39	-0.23	-0.07	0.09	
map_weights [-0.85672787 -0.15928576 -0.15081607 -0.29417446 -0.33580897 -0.13635846]
MAP reward
-0.15	-0.16	-0.15	-0.29	-0.29	-0.86	-0.86	-0.86	
-0.16	-0.15	-0.34	-0.34	-0.15	-0.29	-0.16	-0.15	
-0.15	-0.86	-0.29	-0.29	-0.15	-0.34	-0.86	-0.34	
-0.29	-0.16	-0.86	-0.16	-0.15	-0.34	-0.86	-0.86	
-0.16	-0.34	-0.34	-0.15	-0.86	-0.15	-0.34	-0.29	
-0.86	-0.29	-0.15	-0.15	-0.86	-0.16	-0.86	-0.86	
-0.16	-0.29	-0.86	-0.16	-0.16	-0.34	-0.29	-0.34	
-0.34	-0.29	-0.29	-0.86	-0.15	-0.15	-0.15	-0.14	
Map policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	<	
v	>	>	v	v	v	<	^	
v	v	>	v	<	v	v	v	
>	v	v	v	>	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.0969885452615562
mean w [-0.68263211 -0.09041986 -0.09044615 -0.2495528  -0.43591198  0.06811227]
Mean policy from posterior
>	>	>	>	v	<	v	v	
>	^	v	v	v	<	<	<	
v	>	>	v	v	<	^	^	
>	v	>	v	<	v	<	v	
>	v	v	v	<	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.09	-0.09	-0.09	-0.25	-0.25	-0.68	-0.68	-0.68	
-0.09	-0.09	-0.44	-0.44	-0.09	-0.25	-0.09	-0.09	
-0.09	-0.68	-0.25	-0.25	-0.09	-0.44	-0.68	-0.44	
-0.25	-0.09	-0.68	-0.09	-0.09	-0.44	-0.68	-0.68	
-0.09	-0.44	-0.44	-0.09	-0.68	-0.09	-0.44	-0.25	
-0.68	-0.25	-0.09	-0.09	-0.68	-0.09	-0.68	-0.68	
-0.09	-0.25	-0.68	-0.09	-0.09	-0.44	-0.25	-0.44	
-0.44	-0.25	-0.25	-0.68	-0.09	-0.09	-0.09	0.07	
mean = 0.0737244295492494, map = 0.00552185799997762
CVaR policy
v	>	>	v	v	<	v	v	
v	>	v	v	v	<	<	<	
v	>	>	v	v	v	<	v	
>	v	>	v	<	v	<	v	
>	v	>	v	>	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	<	v	v	
v	>	v	v	v	<	<	<	
v	>	>	v	v	<	<	^	
>	v	>	v	<	v	<	v	
>	v	>	v	<	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	>	v	v	v	<	<	<	
v	>	>	v	v	<	<	^	
>	v	>	v	<	v	<	v	
>	v	v	v	<	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	>	v	v	v	<	<	<	
v	>	>	v	v	<	^	^	
v	v	>	v	<	v	<	v	
>	v	v	v	<	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	<	v	v	
v	^	v	v	v	<	<	<	
v	>	>	v	v	<	^	^	
>	v	>	v	<	v	<	v	
>	v	v	v	<	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.03318545100571124, 0.04580548203272805, 0.05603181641966848, 0.05983079888398324, 0.06679556145468624
==========
iteration 51
==========
weights [-0.08889064 -0.06545644 -0.44693923 -0.79932443 -0.37125299  0.10635271]
expeced value MDP LP -2.034645332989533
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.0860498  -0.79665662  0.42760823 -0.23778884  0.03653926  0.34235393]
w_map [-0.37595664 -0.67916861 -0.29524358 -0.42360245 -0.3474953   0.10012901] loglik 0.0
accepted/total = 2412/3000 = 0.804
-------
true weights [-0.08889064 -0.06545644 -0.44693923 -0.79932443 -0.37125299  0.10635271]
features
3 	2 	3 	0 	2 	4 	4 	0 	
3 	4 	4 	4 	4 	4 	3 	1 	
2 	2 	3 	4 	0 	2 	2 	1 	
2 	1 	3 	3 	1 	4 	4 	3 	
3 	2 	2 	1 	0 	3 	3 	4 	
1 	2 	1 	2 	2 	4 	2 	2 	
4 	4 	0 	2 	3 	3 	0 	2 	
3 	4 	0 	0 	4 	0 	3 	5 	
optimal policy
>	v	>	v	v	>	>	v	
>	v	>	v	v	<	>	v	
v	v	>	>	v	<	>	v	
>	v	v	v	v	<	v	v	
v	>	v	>	v	v	v	v	
>	>	v	<	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.27	-3.51	-3.40	-2.62	-2.64	-2.79	-2.45	-2.10	
-3.86	-3.09	-2.91	-2.56	-2.21	-2.56	-2.81	-2.03	
-3.17	-2.75	-2.99	-2.21	-1.86	-2.29	-2.41	-1.98	
-2.75	-2.33	-2.63	-2.57	-1.79	-2.14	-2.02	-1.94	
-2.68	-2.28	-1.85	-1.79	-1.74	-2.02	-1.66	-1.15	
-1.90	-1.85	-1.42	-1.85	-1.67	-1.23	-0.87	-0.79	
-2.08	-1.73	-1.37	-1.65	-1.93	-1.22	-0.43	-0.34	
-2.43	-1.65	-1.29	-1.22	-1.14	-0.78	-0.69	0.11	
map_weights [-0.37595664 -0.67916861 -0.29524358 -0.42360245 -0.3474953   0.10012901]
MAP reward
-0.42	-0.30	-0.42	-0.38	-0.30	-0.35	-0.35	-0.38	
-0.42	-0.35	-0.35	-0.35	-0.35	-0.35	-0.42	-0.68	
-0.30	-0.30	-0.42	-0.35	-0.38	-0.30	-0.30	-0.68	
-0.30	-0.68	-0.42	-0.42	-0.68	-0.35	-0.35	-0.42	
-0.42	-0.30	-0.30	-0.68	-0.38	-0.42	-0.42	-0.35	
-0.68	-0.30	-0.68	-0.30	-0.30	-0.35	-0.30	-0.30	
-0.35	-0.35	-0.38	-0.30	-0.42	-0.42	-0.38	-0.30	
-0.42	-0.35	-0.38	-0.38	-0.35	-0.38	-0.42	0.10	
Map policy
>	v	>	>	>	v	v	<	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8093422673574346
mean w [-0.21674686 -0.42377731 -0.44485903 -0.35269885 -0.29144255  0.29905173]
Mean policy from posterior
v	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.35	-0.44	-0.35	-0.22	-0.44	-0.29	-0.29	-0.22	
-0.35	-0.29	-0.29	-0.29	-0.29	-0.29	-0.35	-0.42	
-0.44	-0.44	-0.35	-0.29	-0.22	-0.44	-0.44	-0.42	
-0.44	-0.42	-0.35	-0.35	-0.42	-0.29	-0.29	-0.35	
-0.35	-0.44	-0.44	-0.42	-0.22	-0.35	-0.35	-0.29	
-0.42	-0.44	-0.42	-0.44	-0.44	-0.29	-0.44	-0.44	
-0.29	-0.29	-0.22	-0.44	-0.35	-0.35	-0.22	-0.44	
-0.35	-0.29	-0.22	-0.22	-0.29	-0.22	-0.35	0.30	
mean = 0.5188124876254232, map = 0.8271658776324129
CVaR policy
>	>	>	v	v	>	>	v	
>	>	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.3083846950268736, 0.46430829426528586, 0.48676551904811216, 0.5188124967562695, 0.5188124977156692
==========
iteration 52
==========
weights [-0.2303422  -0.27617581 -0.58030475 -0.3484603  -0.38072288  0.51724394]
expeced value MDP LP -1.4949577628705846
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.14411694 -0.4188177   0.35175339 -0.20882729 -0.0119028   0.7977099 ]
w_map [-0.30887089 -0.35310282 -0.7052794  -0.34601564 -0.39463828  0.08385669] loglik 0.0
accepted/total = 2018/3000 = 0.6726666666666666
-------
true weights [-0.2303422  -0.27617581 -0.58030475 -0.3484603  -0.38072288  0.51724394]
features
3 	1 	0 	0 	1 	0 	2 	0 	
3 	1 	1 	1 	0 	1 	3 	0 	
3 	1 	2 	4 	2 	2 	1 	1 	
2 	4 	0 	0 	4 	1 	1 	4 	
1 	1 	2 	1 	4 	0 	4 	3 	
3 	3 	4 	4 	1 	4 	0 	1 	
1 	0 	4 	0 	0 	2 	4 	0 	
1 	2 	2 	2 	1 	1 	3 	5 	
optimal policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	>	v	
^	^	>	>	>	>	>	.	
optimal values
-3.20	-2.88	-2.63	-2.42	-2.21	-1.95	-2.00	-1.43	
-3.04	-2.72	-2.47	-2.21	-1.95	-1.74	-1.48	-1.21	
-2.83	-2.51	-2.46	-2.05	-2.03	-1.67	-1.14	-0.99	
-2.82	-2.26	-1.90	-1.68	-1.47	-1.10	-0.88	-0.72	
-2.28	-2.02	-2.03	-1.47	-1.20	-0.83	-0.61	-0.35	
-2.02	-1.76	-1.58	-1.21	-0.88	-0.61	-0.23	0.00	
-1.69	-1.43	-1.21	-0.84	-0.62	-0.68	-0.10	0.28	
-1.95	-2.00	-1.54	-0.97	-0.39	-0.11	0.16	0.52	
map_weights [-0.30887089 -0.35310282 -0.7052794  -0.34601564 -0.39463828  0.08385669]
MAP reward
-0.35	-0.35	-0.31	-0.31	-0.35	-0.31	-0.71	-0.31	
-0.35	-0.35	-0.35	-0.35	-0.31	-0.35	-0.35	-0.31	
-0.35	-0.35	-0.71	-0.39	-0.71	-0.71	-0.35	-0.35	
-0.71	-0.39	-0.31	-0.31	-0.39	-0.35	-0.35	-0.39	
-0.35	-0.35	-0.71	-0.35	-0.39	-0.31	-0.39	-0.35	
-0.35	-0.35	-0.39	-0.39	-0.35	-0.39	-0.31	-0.35	
-0.35	-0.31	-0.39	-0.31	-0.31	-0.71	-0.39	-0.31	
-0.35	-0.71	-0.71	-0.71	-0.35	-0.35	-0.35	0.08	
Map policy
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	v	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.6575318688934888
mean w [-0.15032078 -0.20697479 -0.58170659 -0.2648191  -0.47467376 -0.03477042]
Mean policy from posterior
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.26	-0.21	-0.15	-0.15	-0.21	-0.15	-0.58	-0.15	
-0.26	-0.21	-0.21	-0.21	-0.15	-0.21	-0.26	-0.15	
-0.26	-0.21	-0.58	-0.47	-0.58	-0.58	-0.21	-0.21	
-0.58	-0.47	-0.15	-0.15	-0.47	-0.21	-0.21	-0.47	
-0.21	-0.21	-0.58	-0.21	-0.47	-0.15	-0.47	-0.26	
-0.26	-0.26	-0.47	-0.47	-0.21	-0.47	-0.15	-0.21	
-0.21	-0.15	-0.47	-0.15	-0.15	-0.58	-0.47	-0.15	
-0.21	-0.58	-0.58	-0.58	-0.21	-0.21	-0.26	-0.03	
mean = 0.0028039847848979615, map = 0.019576562320843705
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
cvar = , 0.012286133725660564, 0.009736811674130541, 0.0028039848647889443, 0.002803985907346762, 0.00280398481109434
==========
iteration 53
==========
weights [-0.31907605 -0.02261283 -0.25637322 -0.8144772  -0.35621239  0.20418503]
expeced value MDP LP -1.159931651599198
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.3375842   0.34092478  0.46094866  0.40924374 -0.2512001   0.5716219 ]
w_map [-0.52409275 -0.36984806 -0.46952416 -0.18767764 -0.52515064  0.23891456] loglik 0.0
accepted/total = 2421/3000 = 0.807
-------
true weights [-0.31907605 -0.02261283 -0.25637322 -0.8144772  -0.35621239  0.20418503]
features
0 	0 	3 	3 	3 	3 	0 	0 	
0 	0 	2 	1 	3 	3 	2 	3 	
0 	4 	4 	0 	4 	1 	1 	2 	
1 	2 	4 	4 	2 	3 	4 	1 	
4 	1 	3 	2 	0 	1 	1 	2 	
2 	1 	0 	2 	0 	3 	0 	3 	
3 	3 	2 	0 	0 	4 	2 	4 	
0 	1 	1 	1 	1 	1 	0 	5 	
optimal policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	v	v	>	>	v	v	
>	v	<	v	v	v	v	v	
>	v	v	v	>	>	v	<	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.98	-2.00	-2.69	-2.45	-3.01	-2.41	-1.62	-1.92	
-1.67	-1.70	-1.90	-1.66	-2.22	-1.88	-1.31	-2.01	
-1.37	-1.39	-1.74	-1.65	-1.42	-1.08	-1.06	-1.21	
-1.06	-1.05	-1.39	-1.34	-1.28	-1.53	-1.05	-0.97	
-1.15	-0.80	-1.58	-1.00	-1.03	-0.72	-0.70	-0.95	
-1.03	-0.79	-0.77	-0.75	-0.79	-1.30	-0.69	-0.97	
-1.35	-1.03	-0.46	-0.50	-0.48	-0.49	-0.37	-0.15	
-0.54	-0.22	-0.20	-0.18	-0.16	-0.14	-0.12	0.20	
map_weights [-0.52409275 -0.36984806 -0.46952416 -0.18767764 -0.52515064  0.23891456]
MAP reward
-0.52	-0.52	-0.19	-0.19	-0.19	-0.19	-0.52	-0.52	
-0.52	-0.52	-0.47	-0.37	-0.19	-0.19	-0.47	-0.19	
-0.52	-0.53	-0.53	-0.52	-0.53	-0.37	-0.37	-0.47	
-0.37	-0.47	-0.53	-0.53	-0.47	-0.19	-0.53	-0.37	
-0.53	-0.37	-0.19	-0.47	-0.52	-0.37	-0.37	-0.47	
-0.47	-0.37	-0.52	-0.47	-0.52	-0.19	-0.52	-0.19	
-0.19	-0.19	-0.47	-0.52	-0.52	-0.53	-0.47	-0.53	
-0.52	-0.37	-0.37	-0.37	-0.37	-0.37	-0.52	0.24	
Map policy
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
v	v	>	>	>	v	>	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0156090115280483
mean w [-0.38370768 -0.23595523 -0.42974757 -0.38402126 -0.39569886  0.14301673]
Mean policy from posterior
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.38	-0.38	-0.38	-0.38	-0.38	-0.38	-0.38	-0.38	
-0.38	-0.38	-0.43	-0.24	-0.38	-0.38	-0.43	-0.38	
-0.38	-0.40	-0.40	-0.38	-0.40	-0.24	-0.24	-0.43	
-0.24	-0.43	-0.40	-0.40	-0.43	-0.38	-0.40	-0.24	
-0.40	-0.24	-0.38	-0.43	-0.38	-0.24	-0.24	-0.43	
-0.43	-0.24	-0.38	-0.43	-0.38	-0.38	-0.38	-0.38	
-0.38	-0.38	-0.43	-0.38	-0.38	-0.40	-0.43	-0.40	
-0.38	-0.24	-0.24	-0.24	-0.24	-0.24	-0.38	0.14	
mean = 0.43248079137784545, map = 1.222453117351316
CVaR policy
>	>	>	>	>	>	v	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	v	>	v	
v	v	>	>	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
v	v	v	v	>	v	>	v	
>	v	v	v	>	>	v	v	
>	v	v	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.890698779111252, 0.3707539253505612, 0.41998134169721224, 0.4401440267601249, 0.43248081282222306
==========
iteration 54
==========
weights [-0.0212071  -0.22766981 -0.10325627 -0.04689663 -0.34228555  0.90426554]
expeced value MDP LP 0.24349806328469975
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.43017125  0.04833592 -0.29385743 -0.54440167  0.57690783  0.3115579 ]
w_map [-0.1818644  -0.34276028 -0.36704224 -0.36832854 -0.7441419  -0.15908378] loglik -4.242953366428992e-08
accepted/total = 1745/3000 = 0.5816666666666667
-------
true weights [-0.0212071  -0.22766981 -0.10325627 -0.04689663 -0.34228555  0.90426554]
features
3 	1 	4 	2 	3 	1 	1 	1 	
2 	2 	2 	3 	3 	4 	3 	2 	
4 	2 	1 	2 	0 	1 	4 	2 	
4 	3 	1 	1 	3 	4 	2 	3 	
0 	2 	1 	0 	1 	2 	2 	3 	
2 	4 	2 	1 	4 	0 	3 	3 	
3 	3 	2 	1 	2 	2 	3 	2 	
1 	4 	0 	4 	4 	0 	3 	5 	
optimal policy
v	v	>	>	v	<	v	v	
>	>	>	>	v	>	>	v	
>	^	>	>	v	<	v	v	
v	v	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
optimal values
-0.17	-0.25	-0.26	0.09	0.19	-0.04	0.13	0.18	
-0.12	-0.02	0.09	0.19	0.24	0.01	0.36	0.41	
-0.46	-0.12	-0.05	0.18	0.29	0.06	0.17	0.52	
-0.31	-0.04	-0.12	0.11	0.31	0.25	0.52	0.63	
0.03	0.01	0.11	0.34	0.37	0.60	0.63	0.68	
0.05	-0.14	0.15	0.13	0.36	0.71	0.74	0.74	
0.16	0.21	0.26	0.36	0.60	0.71	0.79	0.79	
-0.07	-0.11	0.23	0.12	0.47	0.82	0.85	0.90	
map_weights [-0.1818644  -0.34276028 -0.36704224 -0.36832854 -0.7441419  -0.15908378]
MAP reward
-0.37	-0.34	-0.74	-0.37	-0.37	-0.34	-0.34	-0.34	
-0.37	-0.37	-0.37	-0.37	-0.37	-0.74	-0.37	-0.37	
-0.74	-0.37	-0.34	-0.37	-0.18	-0.34	-0.74	-0.37	
-0.74	-0.37	-0.34	-0.34	-0.37	-0.74	-0.37	-0.37	
-0.18	-0.37	-0.34	-0.18	-0.34	-0.37	-0.37	-0.37	
-0.37	-0.74	-0.37	-0.34	-0.74	-0.18	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.34	-0.37	-0.37	-0.37	-0.37	
-0.34	-0.74	-0.18	-0.74	-0.74	-0.18	-0.37	-0.16	
Map policy
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	v	>	v	
^	>	^	>	>	>	>	.	
expeced value MDP LP -1.8904951113286574
mean w [-0.11039396 -0.29595774 -0.27907258 -0.25975091 -0.75588719 -0.18600319]
Mean policy from posterior
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	v	<	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
Mean rewards
-0.26	-0.30	-0.76	-0.28	-0.26	-0.30	-0.30	-0.30	
-0.28	-0.28	-0.28	-0.26	-0.26	-0.76	-0.26	-0.28	
-0.76	-0.28	-0.30	-0.28	-0.11	-0.30	-0.76	-0.28	
-0.76	-0.26	-0.30	-0.30	-0.26	-0.76	-0.28	-0.26	
-0.11	-0.28	-0.30	-0.11	-0.30	-0.28	-0.28	-0.26	
-0.28	-0.76	-0.28	-0.30	-0.76	-0.11	-0.26	-0.26	
-0.26	-0.26	-0.28	-0.30	-0.28	-0.28	-0.26	-0.28	
-0.30	-0.76	-0.11	-0.76	-0.76	-0.11	-0.26	-0.19	
mean = 0.01689328910142701, map = 0.11478061366042264
CVaR policy
>	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	>	>	v	
>	v	v	>	v	>	>	v	
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	>	v	
>	>	>	>	v	>	>	v	
>	>	v	>	v	<	v	v	
v	>	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	<	v	v	
v	>	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	v	<	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
cvar = , 0.10948675852183254, 0.09749451655738067, 0.0449586993890958, 0.02635536744217154, 0.01689329289784014
==========
iteration 55
==========
weights [-0.47728428 -0.33526691 -0.16233473 -0.18336267 -0.06880032  0.7714194 ]
expeced value MDP LP -0.3700508609062212
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.27039792  0.05378748 -0.20647477  0.12495865  0.89219843 -0.26405934]
w_map [-0.67132982 -0.40923749 -0.37919839 -0.37305742 -0.29579332  0.10669585] loglik 0.0
accepted/total = 2378/3000 = 0.7926666666666666
-------
true weights [-0.47728428 -0.33526691 -0.16233473 -0.18336267 -0.06880032  0.7714194 ]
features
4 	3 	0 	4 	0 	3 	2 	3 	
2 	4 	2 	4 	4 	2 	2 	1 	
4 	4 	3 	2 	3 	4 	1 	0 	
4 	4 	3 	4 	3 	2 	0 	4 	
4 	2 	4 	2 	0 	2 	0 	1 	
0 	4 	2 	0 	4 	3 	4 	3 	
1 	4 	4 	1 	3 	4 	0 	2 	
2 	1 	3 	1 	3 	3 	3 	5 	
optimal policy
v	v	>	v	v	v	v	<	
>	>	>	>	>	v	<	v	
v	v	>	v	>	v	<	v	
v	v	>	>	>	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-0.96	-0.92	-1.06	-0.59	-0.94	-0.58	-0.72	-0.89	
-0.90	-0.75	-0.69	-0.53	-0.47	-0.40	-0.56	-0.80	
-0.75	-0.69	-0.76	-0.58	-0.42	-0.24	-0.57	-0.47	
-0.69	-0.62	-0.60	-0.42	-0.36	-0.17	-0.47	0.00	
-0.62	-0.56	-0.50	-0.55	-0.40	-0.01	-0.14	0.07	
-0.88	-0.40	-0.43	-0.40	0.08	0.15	0.34	0.41	
-0.67	-0.34	-0.27	-0.20	0.13	0.32	0.12	0.60	
-0.80	-0.65	-0.32	-0.13	0.20	0.39	0.58	0.77	
map_weights [-0.67132982 -0.40923749 -0.37919839 -0.37305742 -0.29579332  0.10669585]
MAP reward
-0.30	-0.37	-0.67	-0.30	-0.67	-0.37	-0.38	-0.37	
-0.38	-0.30	-0.38	-0.30	-0.30	-0.38	-0.38	-0.41	
-0.30	-0.30	-0.37	-0.38	-0.37	-0.30	-0.41	-0.67	
-0.30	-0.30	-0.37	-0.30	-0.37	-0.38	-0.67	-0.30	
-0.30	-0.38	-0.30	-0.38	-0.67	-0.38	-0.67	-0.41	
-0.67	-0.30	-0.38	-0.67	-0.30	-0.37	-0.30	-0.37	
-0.41	-0.30	-0.30	-0.41	-0.37	-0.30	-0.67	-0.38	
-0.38	-0.41	-0.37	-0.41	-0.37	-0.37	-0.37	0.11	
Map policy
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	>	v	>	v	
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9970141038033096
mean w [-0.34452466 -0.33696704 -0.45581464 -0.24254727 -0.41496178  0.09637404]
Mean policy from posterior
>	>	v	>	v	>	>	v	
v	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.41	-0.24	-0.34	-0.41	-0.34	-0.24	-0.46	-0.24	
-0.46	-0.41	-0.46	-0.41	-0.41	-0.46	-0.46	-0.34	
-0.41	-0.41	-0.24	-0.46	-0.24	-0.41	-0.34	-0.34	
-0.41	-0.41	-0.24	-0.41	-0.24	-0.46	-0.34	-0.41	
-0.41	-0.46	-0.41	-0.46	-0.34	-0.46	-0.34	-0.34	
-0.34	-0.41	-0.46	-0.34	-0.41	-0.24	-0.41	-0.24	
-0.34	-0.41	-0.41	-0.34	-0.24	-0.41	-0.34	-0.46	
-0.46	-0.34	-0.24	-0.34	-0.24	-0.24	-0.24	0.10	
mean = 0.34305230744251314, map = 0.029100959346053723
CVaR policy
v	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	>	>	>	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	>	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	>	v	
v	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.5062511223492249, 0.4300287986044746, 0.35110249852975695, 0.3430523026006138, 0.3430523026057417
==========
iteration 56
==========
weights [-0.44050408 -0.11585265 -0.4575773  -0.53940492 -0.49653111  0.21367386]
expeced value MDP LP -1.8480005185521684
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.50475074  0.54695565 -0.2569031  -0.16883562 -0.25779731  0.53394952]
w_map [-0.40117593 -0.53660838 -0.33879125 -0.39213593 -0.53070085  0.03026258] loglik 0.0
accepted/total = 2459/3000 = 0.8196666666666667
-------
true weights [-0.44050408 -0.11585265 -0.4575773  -0.53940492 -0.49653111  0.21367386]
features
2 	0 	2 	4 	2 	1 	3 	0 	
3 	4 	3 	0 	0 	2 	0 	4 	
3 	1 	0 	1 	4 	3 	1 	3 	
2 	1 	2 	4 	2 	2 	1 	1 	
4 	3 	1 	3 	1 	0 	3 	2 	
0 	4 	4 	1 	0 	0 	3 	1 	
3 	4 	1 	0 	3 	0 	1 	0 	
3 	2 	4 	3 	3 	1 	1 	5 	
optimal policy
>	v	>	>	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.93	-3.51	-3.42	-2.99	-2.52	-2.08	-2.07	-2.34	
-3.61	-3.10	-3.15	-2.64	-2.41	-1.99	-1.55	-1.92	
-3.14	-2.63	-2.64	-2.22	-2.13	-1.65	-1.12	-1.43	
-2.97	-2.54	-2.45	-2.35	-1.87	-1.46	-1.01	-0.90	
-3.00	-2.53	-2.01	-1.96	-1.43	-1.33	-1.09	-0.80	
-2.81	-2.39	-1.91	-1.43	-1.33	-0.90	-0.56	-0.34	
-2.53	-2.01	-1.53	-1.43	-1.00	-0.46	-0.02	-0.23	
-2.54	-2.02	-1.58	-1.09	-0.56	-0.02	0.10	0.21	
map_weights [-0.40117593 -0.53660838 -0.33879125 -0.39213593 -0.53070085  0.03026258]
MAP reward
-0.34	-0.40	-0.34	-0.53	-0.34	-0.54	-0.39	-0.40	
-0.39	-0.53	-0.39	-0.40	-0.40	-0.34	-0.40	-0.53	
-0.39	-0.54	-0.40	-0.54	-0.53	-0.39	-0.54	-0.39	
-0.34	-0.54	-0.34	-0.53	-0.34	-0.34	-0.54	-0.54	
-0.53	-0.39	-0.54	-0.39	-0.54	-0.40	-0.39	-0.34	
-0.40	-0.53	-0.53	-0.54	-0.40	-0.40	-0.39	-0.54	
-0.39	-0.53	-0.54	-0.40	-0.39	-0.40	-0.54	-0.40	
-0.39	-0.34	-0.53	-0.39	-0.39	-0.54	-0.54	0.03	
Map policy
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	>	v	
v	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.232720015792714
mean w [-0.44680775 -0.34493513 -0.2750094  -0.3398635  -0.39122728  0.07124385]
Mean policy from posterior
v	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.28	-0.45	-0.28	-0.39	-0.28	-0.34	-0.34	-0.45	
-0.34	-0.39	-0.34	-0.45	-0.45	-0.28	-0.45	-0.39	
-0.34	-0.34	-0.45	-0.34	-0.39	-0.34	-0.34	-0.34	
-0.28	-0.34	-0.28	-0.39	-0.28	-0.28	-0.34	-0.34	
-0.39	-0.34	-0.34	-0.34	-0.34	-0.45	-0.34	-0.28	
-0.45	-0.39	-0.39	-0.34	-0.45	-0.45	-0.34	-0.34	
-0.34	-0.39	-0.34	-0.45	-0.34	-0.45	-0.34	-0.45	
-0.34	-0.28	-0.39	-0.34	-0.34	-0.34	-0.34	0.07	
mean = 0.3233305672280411, map = 0.6433326912532109
CVaR policy
v	>	>	>	v	v	>	v	
v	>	v	>	v	>	>	v	
v	v	v	v	v	v	>	v	
v	v	>	>	>	v	v	v	
v	v	v	>	>	>	>	v	
>	v	v	>	>	>	v	v	
>	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	v	>	v	
v	>	v	>	v	>	>	v	
v	>	v	>	v	v	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	v	v	>	v	v	
>	v	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	v	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	>	>	>	>	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.6272422477714639, 0.6318037668223722, 0.47268644357533174, 0.43482714154207813, 0.32333056710736563
==========
iteration 57
==========
weights [-0.2725645  -0.34215258 -0.8763279  -0.08535261 -0.12101183  0.13696965]
expeced value MDP LP -1.312996948050642
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.07941172  0.43503075 -0.61311885 -0.30935885  0.51936143  0.25117347]
w_map [-0.50790699 -0.3374033  -0.15789507 -0.35343683 -0.42027312  0.54928278] loglik 0.0
accepted/total = 2309/3000 = 0.7696666666666667
-------
true weights [-0.2725645  -0.34215258 -0.8763279  -0.08535261 -0.12101183  0.13696965]
features
3 	1 	0 	2 	3 	2 	0 	1 	
4 	2 	1 	1 	4 	0 	4 	4 	
1 	0 	3 	1 	4 	1 	0 	4 	
0 	3 	2 	3 	2 	1 	1 	3 	
3 	0 	0 	2 	0 	1 	4 	4 	
0 	2 	0 	1 	0 	1 	2 	1 	
0 	3 	1 	3 	4 	0 	4 	0 	
0 	3 	3 	3 	1 	3 	1 	5 	
optimal policy
v	<	v	>	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	>	>	^	>	>	v	
v	v	v	^	v	>	>	v	
v	<	v	v	v	>	>	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.28	-2.60	-2.29	-2.32	-1.46	-2.14	-1.28	-1.23	
-2.22	-2.89	-2.04	-1.71	-1.38	-1.28	-1.01	-0.90	
-2.12	-2.11	-1.89	-1.82	-1.49	-1.38	-1.05	-0.79	
-1.79	-1.86	-2.44	-1.89	-2.04	-1.34	-1.01	-0.67	
-1.54	-1.79	-1.58	-1.92	-1.17	-1.04	-0.71	-0.59	
-1.46	-1.81	-1.32	-1.06	-0.91	-0.86	-1.13	-0.48	
-1.20	-0.94	-1.06	-0.72	-0.64	-0.53	-0.26	-0.14	
-1.13	-0.86	-0.79	-0.71	-0.63	-0.29	-0.21	0.14	
map_weights [-0.50790699 -0.3374033  -0.15789507 -0.35343683 -0.42027312  0.54928278]
MAP reward
-0.35	-0.34	-0.51	-0.16	-0.35	-0.16	-0.51	-0.34	
-0.42	-0.16	-0.34	-0.34	-0.42	-0.51	-0.42	-0.42	
-0.34	-0.51	-0.35	-0.34	-0.42	-0.34	-0.51	-0.42	
-0.51	-0.35	-0.16	-0.35	-0.16	-0.34	-0.34	-0.35	
-0.35	-0.51	-0.51	-0.16	-0.51	-0.34	-0.42	-0.42	
-0.51	-0.16	-0.51	-0.34	-0.51	-0.34	-0.16	-0.34	
-0.51	-0.35	-0.34	-0.35	-0.42	-0.51	-0.42	-0.51	
-0.51	-0.35	-0.35	-0.35	-0.34	-0.35	-0.34	0.55	
Map policy
>	v	>	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5448126924755856
mean w [-0.4596782  -0.24763574 -0.34109477 -0.24339459 -0.3543527   0.36709295]
Mean policy from posterior
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.24	-0.25	-0.46	-0.34	-0.24	-0.34	-0.46	-0.25	
-0.35	-0.34	-0.25	-0.25	-0.35	-0.46	-0.35	-0.35	
-0.25	-0.46	-0.24	-0.25	-0.35	-0.25	-0.46	-0.35	
-0.46	-0.24	-0.34	-0.24	-0.34	-0.25	-0.25	-0.24	
-0.24	-0.46	-0.46	-0.34	-0.46	-0.25	-0.35	-0.35	
-0.46	-0.34	-0.46	-0.25	-0.46	-0.25	-0.34	-0.25	
-0.46	-0.24	-0.25	-0.24	-0.35	-0.46	-0.35	-0.46	
-0.46	-0.24	-0.24	-0.24	-0.25	-0.24	-0.25	0.37	
mean = 0.5090520506539695, map = 0.9096597319169928
CVaR policy
v	v	v	>	v	>	v	v	
>	v	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
v	v	>	v	>	>	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.6831718322711138, 0.6185086483456914, 0.5090520577800901, 0.5090520593529435, 0.509052058165036
==========
iteration 58
==========
weights [-0.69611314 -0.19058184 -0.24465214 -0.32533667 -0.22025134  0.51468028]
expeced value MDP LP -1.3038652479637114
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.3201241  -0.51331844  0.58908297 -0.00786448 -0.09553728  0.52708327]
w_map [-0.43034392 -0.47793378 -0.40031019 -0.47346968 -0.34465396  0.28840124] loglik 0.0
accepted/total = 2390/3000 = 0.7966666666666666
-------
true weights [-0.69611314 -0.19058184 -0.24465214 -0.32533667 -0.22025134  0.51468028]
features
4 	3 	0 	3 	1 	2 	1 	2 	
2 	1 	3 	4 	3 	0 	0 	0 	
4 	2 	1 	4 	1 	0 	1 	4 	
3 	3 	2 	2 	0 	1 	4 	3 	
4 	0 	1 	1 	4 	3 	2 	1 	
3 	3 	2 	4 	3 	4 	4 	2 	
3 	2 	4 	2 	0 	3 	0 	2 	
2 	0 	4 	0 	2 	1 	3 	5 	
optimal policy
v	v	v	v	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.54	-2.42	-2.70	-2.08	-2.08	-1.91	-1.68	-1.63	
-2.34	-2.12	-2.03	-1.78	-1.99	-2.19	-1.50	-1.40	
-2.15	-1.94	-1.72	-1.57	-1.68	-1.50	-0.82	-0.71	
-2.16	-1.85	-1.54	-1.36	-1.50	-0.82	-0.63	-0.50	
-2.02	-1.99	-1.31	-1.13	-0.95	-0.74	-0.42	-0.17	
-1.81	-1.50	-1.19	-0.96	-0.74	-0.42	-0.20	0.02	
-1.91	-1.60	-1.37	-1.18	-0.95	-0.33	-0.43	0.26	
-2.07	-1.84	-1.16	-0.95	-0.25	-0.01	0.18	0.51	
map_weights [-0.43034392 -0.47793378 -0.40031019 -0.47346968 -0.34465396  0.28840124]
MAP reward
-0.34	-0.47	-0.43	-0.47	-0.48	-0.40	-0.48	-0.40	
-0.40	-0.48	-0.47	-0.34	-0.47	-0.43	-0.43	-0.43	
-0.34	-0.40	-0.48	-0.34	-0.48	-0.43	-0.48	-0.34	
-0.47	-0.47	-0.40	-0.40	-0.43	-0.48	-0.34	-0.47	
-0.34	-0.43	-0.48	-0.48	-0.34	-0.47	-0.40	-0.48	
-0.47	-0.47	-0.40	-0.34	-0.47	-0.34	-0.34	-0.40	
-0.47	-0.40	-0.34	-0.40	-0.43	-0.47	-0.43	-0.40	
-0.40	-0.43	-0.34	-0.43	-0.40	-0.48	-0.47	0.29	
Map policy
v	>	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1906891519732086
mean w [-0.34579142 -0.28164291 -0.37334932 -0.46590905 -0.31813149  0.13508147]
Mean policy from posterior
v	v	v	v	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	v	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.32	-0.47	-0.35	-0.47	-0.28	-0.37	-0.28	-0.37	
-0.37	-0.28	-0.47	-0.32	-0.47	-0.35	-0.35	-0.35	
-0.32	-0.37	-0.28	-0.32	-0.28	-0.35	-0.28	-0.32	
-0.47	-0.47	-0.37	-0.37	-0.35	-0.28	-0.32	-0.47	
-0.32	-0.35	-0.28	-0.28	-0.32	-0.47	-0.37	-0.28	
-0.47	-0.47	-0.37	-0.32	-0.47	-0.32	-0.32	-0.37	
-0.47	-0.37	-0.32	-0.37	-0.35	-0.47	-0.35	-0.37	
-0.37	-0.35	-0.32	-0.35	-0.37	-0.28	-0.47	0.14	
mean = 0.19511885063697276, map = 0.2000890433228446
CVaR policy
>	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.4884861140607626, 0.43781785611163215, 0.4085323843650439, 0.37046751790133436, 0.37705184542824544
==========
iteration 59
==========
weights [-0.02402604 -0.14085619 -0.41371611 -0.05490963 -0.51419023  0.73553695]
expeced value MDP LP -0.3784134816102686
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.3130029   0.63295777 -0.22728891 -0.50577832 -0.32882918  0.29290452]
w_map [-0.44064058 -0.45344332 -0.28886153 -0.36934473 -0.36314409  0.49849262] loglik 0.0
accepted/total = 2504/3000 = 0.8346666666666667
-------
true weights [-0.02402604 -0.14085619 -0.41371611 -0.05490963 -0.51419023  0.73553695]
features
4 	1 	2 	3 	1 	4 	4 	0 	
2 	1 	3 	1 	0 	3 	2 	3 	
1 	0 	0 	2 	4 	2 	1 	4 	
0 	0 	1 	3 	1 	3 	4 	0 	
3 	2 	3 	2 	4 	2 	4 	3 	
1 	3 	2 	4 	2 	0 	1 	2 	
1 	0 	1 	4 	2 	2 	1 	3 	
3 	1 	1 	4 	4 	0 	0 	5 	
optimal policy
>	v	v	v	v	v	>	v	
v	v	v	<	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
^	<	^	>	>	>	v	v	
^	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.24	-0.73	-0.90	-0.68	-0.71	-1.06	-0.93	-0.42	
-1.00	-0.60	-0.49	-0.63	-0.57	-0.55	-0.81	-0.40	
-0.60	-0.46	-0.44	-0.69	-0.74	-0.50	-0.48	-0.35	
-0.46	-0.44	-0.42	-0.28	-0.23	-0.09	-0.35	0.17	
-0.51	-0.85	-0.47	-0.69	-0.55	-0.04	-0.11	0.20	
-0.65	-0.69	-0.88	-0.55	-0.04	0.38	0.41	0.25	
-0.78	-0.65	-0.64	-0.68	-0.16	0.25	0.56	0.67	
-0.69	-0.64	-0.50	-0.36	0.15	0.67	0.70	0.74	
map_weights [-0.44064058 -0.45344332 -0.28886153 -0.36934473 -0.36314409  0.49849262]
MAP reward
-0.36	-0.45	-0.29	-0.37	-0.45	-0.36	-0.36	-0.44	
-0.29	-0.45	-0.37	-0.45	-0.44	-0.37	-0.29	-0.37	
-0.45	-0.44	-0.44	-0.29	-0.36	-0.29	-0.45	-0.36	
-0.44	-0.44	-0.45	-0.37	-0.45	-0.37	-0.36	-0.44	
-0.37	-0.29	-0.37	-0.29	-0.36	-0.29	-0.36	-0.37	
-0.45	-0.37	-0.29	-0.36	-0.29	-0.44	-0.45	-0.29	
-0.45	-0.44	-0.45	-0.36	-0.29	-0.29	-0.45	-0.37	
-0.37	-0.45	-0.45	-0.36	-0.36	-0.44	-0.44	0.50	
Map policy
v	>	v	v	>	v	v	v	
>	>	v	v	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1225684140215666
mean w [-0.34081876 -0.42173996 -0.44384035 -0.30407055 -0.25774146  0.08207633]
Mean policy from posterior
>	v	v	>	>	>	>	v	
>	>	v	>	v	v	>	v	
v	v	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.26	-0.42	-0.44	-0.30	-0.42	-0.26	-0.26	-0.34	
-0.44	-0.42	-0.30	-0.42	-0.34	-0.30	-0.44	-0.30	
-0.42	-0.34	-0.34	-0.44	-0.26	-0.44	-0.42	-0.26	
-0.34	-0.34	-0.42	-0.30	-0.42	-0.30	-0.26	-0.34	
-0.30	-0.44	-0.30	-0.44	-0.26	-0.44	-0.26	-0.30	
-0.42	-0.30	-0.44	-0.26	-0.44	-0.34	-0.42	-0.44	
-0.42	-0.34	-0.42	-0.26	-0.44	-0.44	-0.42	-0.30	
-0.30	-0.42	-0.42	-0.26	-0.26	-0.34	-0.34	0.08	
mean = 0.675563164646787, map = 0.7346035946159271
CVaR policy
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	>	>	v	
v	v	>	>	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	>	v	
v	v	v	>	>	v	>	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	v	>	v	>	v	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	>	v	
>	>	v	>	v	v	>	v	
v	v	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	>	>	v	
>	>	v	>	v	v	>	v	
v	v	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	>	>	v	
>	>	v	>	v	v	>	v	
v	v	v	v	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.40555302324089776, 0.5386105237093495, 0.6669687837904861, 0.6696750771134257, 0.6696750727308938
==========
iteration 60
==========
weights [-0.24478746 -0.68042635 -0.21964141 -0.62989695 -0.14757166  0.10153406]
expeced value MDP LP -2.078471539219937
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.40034976  0.54300504  0.44376342 -0.30575317 -0.12305031 -0.48919652]
w_map [-0.38010616 -0.36024158 -0.49765665 -0.35710203 -0.58914307  0.05892157] loglik 0.0
accepted/total = 2412/3000 = 0.804
-------
true weights [-0.24478746 -0.68042635 -0.21964141 -0.62989695 -0.14757166  0.10153406]
features
1 	4 	3 	3 	0 	3 	3 	1 	
3 	1 	2 	0 	0 	2 	2 	1 	
1 	1 	0 	1 	1 	1 	4 	3 	
4 	4 	2 	2 	4 	4 	3 	4 	
0 	4 	1 	3 	3 	0 	0 	3 	
0 	1 	1 	0 	0 	0 	1 	1 	
4 	1 	1 	3 	1 	3 	4 	3 	
1 	0 	1 	3 	2 	1 	4 	5 	
optimal policy
>	>	v	v	v	v	v	<	
v	>	v	<	v	>	v	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	^	v	v	v	v	<	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.76	-3.11	-3.00	-3.21	-2.67	-2.87	-2.67	-3.32	
-3.48	-3.05	-2.39	-2.61	-2.45	-2.26	-2.06	-2.72	
-2.88	-2.75	-2.19	-2.43	-2.23	-2.09	-1.86	-2.47	
-2.22	-2.09	-1.97	-1.76	-1.56	-1.43	-1.73	-1.86	
-2.44	-2.22	-2.63	-2.14	-1.91	-1.29	-1.11	-1.73	
-2.66	-2.85	-2.19	-1.52	-1.29	-1.06	-0.87	-1.20	
-2.79	-3.10	-2.77	-2.11	-1.49	-0.82	-0.19	-0.53	
-3.10	-2.45	-2.22	-1.56	-0.94	-0.73	-0.05	0.10	
map_weights [-0.38010616 -0.36024158 -0.49765665 -0.35710203 -0.58914307  0.05892157]
MAP reward
-0.36	-0.59	-0.36	-0.36	-0.38	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.50	-0.38	-0.38	-0.50	-0.50	-0.36	
-0.36	-0.36	-0.38	-0.36	-0.36	-0.36	-0.59	-0.36	
-0.59	-0.59	-0.50	-0.50	-0.59	-0.59	-0.36	-0.59	
-0.38	-0.59	-0.36	-0.36	-0.36	-0.38	-0.38	-0.36	
-0.38	-0.36	-0.36	-0.38	-0.38	-0.38	-0.36	-0.36	
-0.59	-0.36	-0.36	-0.36	-0.36	-0.36	-0.59	-0.36	
-0.36	-0.38	-0.36	-0.36	-0.50	-0.36	-0.59	0.06	
Map policy
v	>	>	v	>	>	>	v	
v	v	>	v	v	v	>	v	
>	>	>	v	>	v	v	v	
v	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.473173682378051
mean w [-0.33096194 -0.40471776 -0.25974832 -0.38453764 -0.4701235   0.00447096]
Mean policy from posterior
v	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	v	
>	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.40	-0.47	-0.38	-0.38	-0.33	-0.38	-0.38	-0.40	
-0.38	-0.40	-0.26	-0.33	-0.33	-0.26	-0.26	-0.40	
-0.40	-0.40	-0.33	-0.40	-0.40	-0.40	-0.47	-0.38	
-0.47	-0.47	-0.26	-0.26	-0.47	-0.47	-0.38	-0.47	
-0.33	-0.47	-0.40	-0.38	-0.38	-0.33	-0.33	-0.38	
-0.33	-0.40	-0.40	-0.33	-0.33	-0.33	-0.40	-0.40	
-0.47	-0.40	-0.40	-0.38	-0.40	-0.38	-0.47	-0.38	
-0.40	-0.33	-0.40	-0.38	-0.26	-0.40	-0.47	0.00	
mean = 0.8719986439969878, map = 1.2498076571675245
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	v	>	v	
v	>	>	>	v	v	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	v	>	v	
>	>	>	>	v	v	v	v	
>	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	v	v	>	v	
>	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	v	>	>	v	
>	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	v	>	>	v	
>	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.5087868879405835, 0.5996233108954532, 0.6803981158931616, 0.70622600746132, 0.7482146585787595
==========
iteration 61
==========
weights [-0.48174809 -0.04079798 -0.24348761 -0.71022041 -0.00888151  0.44997352]
expeced value MDP LP -0.6802533721258843
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.6476762  -0.62151944 -0.33770232  0.15042111 -0.21263032  0.11112202]
w_map [-0.36221608 -0.18408033 -0.52852401 -0.70253855 -0.20197065  0.14568375] loglik -5.779899936442234e-08
accepted/total = 1729/3000 = 0.5763333333333334
-------
true weights [-0.48174809 -0.04079798 -0.24348761 -0.71022041 -0.00888151  0.44997352]
features
0 	1 	4 	0 	1 	3 	4 	2 	
1 	2 	4 	3 	2 	1 	0 	4 	
0 	1 	3 	3 	3 	2 	2 	3 	
0 	3 	1 	2 	0 	4 	4 	3 	
2 	0 	4 	3 	3 	2 	0 	1 	
0 	4 	2 	0 	1 	3 	1 	2 	
4 	4 	4 	0 	0 	2 	4 	0 	
2 	2 	3 	1 	0 	4 	4 	5 	
optimal policy
>	>	v	>	v	v	v	<	
>	>	^	>	>	v	v	<	
>	^	v	v	>	v	v	<	
v	>	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	<	
>	>	>	v	>	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.39	-0.92	-0.89	-1.15	-0.68	-1.11	-0.84	-1.07	
-1.15	-1.12	-0.89	-1.35	-0.64	-0.40	-0.84	-0.84	
-1.62	-1.15	-1.56	-1.54	-1.07	-0.37	-0.36	-1.06	
-1.77	-1.56	-0.86	-0.84	-0.60	-0.12	-0.12	-0.62	
-1.30	-1.07	-0.82	-1.53	-1.05	-0.35	-0.11	0.09	
-1.07	-0.59	-0.82	-0.82	-0.35	-0.34	0.38	0.13	
-0.59	-0.59	-0.59	-0.58	-0.31	0.18	0.42	-0.04	
-0.83	-0.83	-0.81	-0.10	-0.06	0.42	0.44	0.45	
map_weights [-0.36221608 -0.18408033 -0.52852401 -0.70253855 -0.20197065  0.14568375]
MAP reward
-0.36	-0.18	-0.20	-0.36	-0.18	-0.70	-0.20	-0.53	
-0.18	-0.53	-0.20	-0.70	-0.53	-0.18	-0.36	-0.20	
-0.36	-0.18	-0.70	-0.70	-0.70	-0.53	-0.53	-0.70	
-0.36	-0.70	-0.18	-0.53	-0.36	-0.20	-0.20	-0.70	
-0.53	-0.36	-0.20	-0.70	-0.70	-0.53	-0.36	-0.18	
-0.36	-0.20	-0.53	-0.36	-0.18	-0.70	-0.18	-0.53	
-0.20	-0.20	-0.20	-0.36	-0.36	-0.53	-0.20	-0.36	
-0.53	-0.53	-0.70	-0.18	-0.36	-0.20	-0.20	0.15	
Map policy
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
>	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.418339073851044
mean w [-0.24862166 -0.11363992 -0.49388418 -0.62218597 -0.14793867  0.07257196]
Mean policy from posterior
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
>	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	<	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.25	-0.11	-0.15	-0.25	-0.11	-0.62	-0.15	-0.49	
-0.11	-0.49	-0.15	-0.62	-0.49	-0.11	-0.25	-0.15	
-0.25	-0.11	-0.62	-0.62	-0.62	-0.49	-0.49	-0.62	
-0.25	-0.62	-0.11	-0.49	-0.25	-0.15	-0.15	-0.62	
-0.49	-0.25	-0.15	-0.62	-0.62	-0.49	-0.25	-0.11	
-0.25	-0.15	-0.49	-0.25	-0.11	-0.62	-0.11	-0.49	
-0.15	-0.15	-0.15	-0.25	-0.25	-0.49	-0.15	-0.25	
-0.49	-0.49	-0.62	-0.11	-0.25	-0.15	-0.15	0.07	
mean = 0.11262596073900766, map = 0.12136659710314435
CVaR policy
v	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	<	
>	v	v	>	v	v	v	v	
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
>	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	<	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
>	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	<	
v	v	v	>	v	>	v	v	
>	>	>	v	v	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
>	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	<	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	<	
>	v	v	v	v	v	v	<	
v	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	<	
v	v	v	>	v	>	v	v	
>	>	>	v	v	>	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.21087514720022327, 0.13301165044629193, 0.12957124060570435, 0.11262596066127617, 0.11262596086689325
==========
iteration 62
==========
weights [-0.1408033  -0.10463389 -0.60361841 -0.16941572 -0.70980778  0.26896509]
expeced value MDP LP -1.2165179906560697
demonstration
[(56, 2), (48, 2), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.27382527 -0.45606416 -0.09317008  0.67541955  0.47169837  0.17220226]
w_map [-0.20109791 -0.17862154 -0.60549738 -0.20323276 -0.71326892 -0.10474156] loglik -0.6931472636275338
accepted/total = 1603/3000 = 0.5343333333333333
-------
true weights [-0.1408033  -0.10463389 -0.60361841 -0.16941572 -0.70980778  0.26896509]
features
0 	4 	4 	0 	3 	3 	0 	2 	
2 	1 	4 	3 	0 	4 	4 	1 	
1 	1 	4 	4 	4 	3 	1 	4 	
3 	2 	2 	1 	2 	4 	2 	2 	
1 	2 	1 	1 	1 	4 	2 	1 	
0 	0 	3 	3 	4 	2 	3 	1 	
0 	4 	2 	1 	0 	0 	4 	1 	
3 	0 	2 	4 	0 	3 	3 	5 	
optimal policy
v	v	>	v	v	<	>	v	
v	v	<	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	>	v	v	
v	>	>	v	<	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	>	v	>	v	
^	<	>	>	>	>	>	.	
optimal values
-2.10	-2.26	-2.49	-1.79	-1.95	-2.10	-2.16	-2.04	
-1.98	-1.57	-2.26	-1.67	-1.79	-2.19	-2.04	-1.45	
-1.39	-1.48	-2.11	-1.52	-2.11	-1.50	-1.34	-1.36	
-1.30	-1.89	-1.41	-0.81	-1.41	-1.95	-1.25	-0.65	
-1.14	-1.41	-0.81	-0.72	-0.81	-1.36	-0.65	-0.05	
-1.05	-0.91	-0.78	-0.62	-1.06	-0.72	-0.11	0.06	
-1.18	-1.62	-1.05	-0.45	-0.35	-0.21	-0.55	0.16	
-1.33	-1.46	-1.52	-0.92	-0.21	-0.07	0.10	0.27	
map_weights [-0.20109791 -0.17862154 -0.60549738 -0.20323276 -0.71326892 -0.10474156]
MAP reward
-0.20	-0.71	-0.71	-0.20	-0.20	-0.20	-0.20	-0.61	
-0.61	-0.18	-0.71	-0.20	-0.20	-0.71	-0.71	-0.18	
-0.18	-0.18	-0.71	-0.71	-0.71	-0.20	-0.18	-0.71	
-0.20	-0.61	-0.61	-0.18	-0.61	-0.71	-0.61	-0.61	
-0.18	-0.61	-0.18	-0.18	-0.18	-0.71	-0.61	-0.18	
-0.20	-0.20	-0.20	-0.20	-0.71	-0.61	-0.20	-0.18	
-0.20	-0.71	-0.61	-0.18	-0.20	-0.20	-0.71	-0.18	
-0.20	-0.20	-0.61	-0.71	-0.20	-0.20	-0.20	-0.10	
Map policy
v	v	>	v	v	<	>	v	
v	v	>	v	<	v	v	v	
v	<	v	v	>	>	v	v	
v	<	v	v	v	>	v	v	
v	>	>	v	<	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.4534990888532704
mean w [-0.11833242 -0.18620584 -0.53701517 -0.13463922 -0.6053868  -0.02326882]
Mean policy from posterior
v	v	>	v	v	<	<	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	v	v	v	
v	v	v	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	v	v	v	v	
^	<	>	>	>	>	>	.	
Mean rewards
-0.12	-0.61	-0.61	-0.12	-0.13	-0.13	-0.12	-0.54	
-0.54	-0.19	-0.61	-0.13	-0.12	-0.61	-0.61	-0.19	
-0.19	-0.19	-0.61	-0.61	-0.61	-0.13	-0.19	-0.61	
-0.13	-0.54	-0.54	-0.19	-0.54	-0.61	-0.54	-0.54	
-0.19	-0.54	-0.19	-0.19	-0.19	-0.61	-0.54	-0.19	
-0.12	-0.12	-0.13	-0.13	-0.61	-0.54	-0.13	-0.19	
-0.12	-0.61	-0.54	-0.19	-0.12	-0.12	-0.61	-0.19	
-0.13	-0.12	-0.54	-0.61	-0.12	-0.13	-0.13	-0.02	
mean = 0.02203564541603731, map = 0.005717161953669336
CVaR policy
v	v	>	v	<	<	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	<	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	v	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	<	<	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	v	v	v	
v	v	v	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	v	v	v	v	
^	<	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	<	<	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	v	v	v	
v	v	v	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	v	v	v	v	
^	<	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	<	<	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	v	v	v	
v	v	v	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	v	v	v	
^	<	>	>	>	>	>	.	
cvar = , 0.043886795039779436, 0.040628615825416325, 0.02203564261168567, 0.022035641933154215, 0.022035641918539906
==========
iteration 63
==========
weights [-0.53480199 -0.71108202 -0.26845986 -0.03036318 -0.15956608  0.33150451]
expeced value MDP LP -1.3665223606979628
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.05026524 -0.12386213  0.04616675  0.90067577 -0.40283947 -0.08064571]
w_map [-0.68285919 -0.56138251 -0.29089599 -0.26731417 -0.15829067  0.1934418 ] loglik -0.693147180617828
accepted/total = 1803/3000 = 0.601
-------
true weights [-0.53480199 -0.71108202 -0.26845986 -0.03036318 -0.15956608  0.33150451]
features
1 	0 	4 	3 	2 	3 	2 	4 	
1 	1 	2 	4 	3 	0 	3 	0 	
2 	3 	0 	3 	0 	1 	3 	4 	
1 	2 	2 	3 	2 	1 	2 	4 	
4 	2 	2 	4 	1 	0 	3 	1 	
0 	0 	0 	2 	3 	4 	4 	3 	
1 	3 	2 	2 	1 	2 	1 	1 	
2 	3 	3 	0 	1 	0 	0 	5 	
optimal policy
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	<	
>	v	>	v	<	>	v	<	
>	>	>	v	<	>	v	<	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	^	^	v	v	
>	>	^	^	>	>	>	.	
optimal values
-2.71	-2.02	-1.50	-1.35	-1.43	-1.17	-1.16	-1.30	
-2.64	-2.29	-1.59	-1.34	-1.35	-1.42	-0.90	-1.42	
-1.95	-1.70	-1.71	-1.19	-1.71	-1.58	-0.87	-1.02	
-2.38	-1.68	-1.43	-1.17	-1.43	-1.55	-0.85	-1.00	
-1.81	-1.66	-1.41	-1.15	-1.45	-1.12	-0.59	-1.12	
-2.32	-2.05	-1.53	-1.00	-0.74	-0.72	-0.56	-0.41	
-2.23	-1.53	-1.52	-1.26	-1.45	-0.98	-0.92	-0.38	
-1.80	-1.55	-1.53	-1.78	-1.44	-0.74	-0.21	0.33	
map_weights [-0.68285919 -0.56138251 -0.29089599 -0.26731417 -0.15829067  0.1934418 ]
MAP reward
-0.56	-0.68	-0.16	-0.27	-0.29	-0.27	-0.29	-0.16	
-0.56	-0.56	-0.29	-0.16	-0.27	-0.68	-0.27	-0.68	
-0.29	-0.27	-0.68	-0.27	-0.68	-0.56	-0.27	-0.16	
-0.56	-0.29	-0.29	-0.27	-0.29	-0.56	-0.29	-0.16	
-0.16	-0.29	-0.29	-0.16	-0.56	-0.68	-0.27	-0.56	
-0.68	-0.68	-0.68	-0.29	-0.27	-0.16	-0.16	-0.27	
-0.56	-0.27	-0.29	-0.29	-0.56	-0.29	-0.56	-0.56	
-0.29	-0.27	-0.27	-0.68	-0.56	-0.68	-0.68	0.19	
Map policy
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	v	
>	v	>	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	^	>	>	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.6789439742296943
mean w [-0.66911572 -0.53307961 -0.22511255 -0.14428633 -0.16127574  0.09106102]
Mean policy from posterior
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	<	
>	v	>	v	<	>	v	<	
>	>	>	v	<	>	v	<	
>	>	>	v	v	>	v	v	
^	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	v	
>	>	^	^	>	>	>	.	
Mean rewards
-0.53	-0.67	-0.16	-0.14	-0.23	-0.14	-0.23	-0.16	
-0.53	-0.53	-0.23	-0.16	-0.14	-0.67	-0.14	-0.67	
-0.23	-0.14	-0.67	-0.14	-0.67	-0.53	-0.14	-0.16	
-0.53	-0.23	-0.23	-0.14	-0.23	-0.53	-0.23	-0.16	
-0.16	-0.23	-0.23	-0.16	-0.53	-0.67	-0.14	-0.53	
-0.67	-0.67	-0.67	-0.23	-0.14	-0.16	-0.16	-0.14	
-0.53	-0.14	-0.23	-0.23	-0.53	-0.23	-0.53	-0.53	
-0.23	-0.14	-0.14	-0.67	-0.53	-0.67	-0.67	0.09	
mean = 0.002799430895754851, map = 0.045444653829573856
CVaR policy
v	>	v	v	>	>	v	<	
v	v	>	v	>	>	v	v	
>	v	v	v	v	>	v	v	
v	>	v	v	>	>	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	v	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
v	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	v	
>	v	>	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	v	>	>	>	>	>	v	
>	>	>	^	>	>	>	v	
>	^	^	^	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	<	
>	v	>	v	<	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	v	
>	>	^	^	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	<	
>	v	>	v	<	>	v	<	
>	>	>	v	<	>	v	<	
>	>	>	v	v	>	v	v	
^	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	v	
>	>	^	^	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	<	
>	v	>	v	<	>	v	<	
>	>	>	v	<	>	v	<	
>	>	>	v	v	>	v	v	
^	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	v	
>	>	^	^	>	>	>	.	
cvar = , 0.09761191412233905, 0.05668564810038723, 0.01717431709599082, 0.0027994342731827437, 0.0027994303688723132
==========
iteration 64
==========
weights [-0.48701902 -0.16726154 -0.51772544 -0.39916227 -0.45686498  0.31422967]
expeced value MDP LP -2.404170670378075
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.61493735 -0.01210747 -0.10869632  0.08584156  0.43399313 -0.64356178]
w_map [-0.39258043 -0.19770687 -0.54758028 -0.42561566 -0.42534588 -0.38063188] loglik 0.0
accepted/total = 2485/3000 = 0.8283333333333334
-------
true weights [-0.48701902 -0.16726154 -0.51772544 -0.39916227 -0.45686498  0.31422967]
features
1 	3 	2 	1 	2 	0 	0 	3 	
1 	2 	2 	4 	2 	2 	2 	3 	
3 	1 	0 	4 	4 	0 	0 	0 	
4 	1 	4 	3 	1 	2 	2 	2 	
1 	1 	0 	0 	0 	4 	3 	2 	
1 	3 	4 	1 	0 	4 	4 	2 	
2 	3 	2 	4 	1 	2 	4 	0 	
3 	0 	2 	2 	2 	3 	4 	5 	
optimal policy
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.68	-3.89	-4.12	-3.64	-3.71	-3.79	-3.36	-2.93	
-3.54	-3.53	-3.98	-3.51	-3.23	-3.33	-2.91	-2.56	
-3.41	-3.04	-3.50	-3.08	-2.74	-2.85	-2.41	-2.18	
-3.33	-2.90	-3.08	-2.65	-2.30	-2.38	-1.94	-1.71	
-2.90	-2.76	-2.71	-2.28	-2.16	-1.88	-1.44	-1.20	
-2.76	-2.62	-2.25	-1.81	-1.69	-1.50	-1.05	-0.69	
-3.03	-2.54	-2.16	-1.66	-1.21	-1.06	-0.60	-0.18	
-2.91	-2.53	-2.07	-1.56	-1.06	-0.54	-0.15	0.31	
map_weights [-0.39258043 -0.19770687 -0.54758028 -0.42561566 -0.42534588 -0.38063188]
MAP reward
-0.20	-0.43	-0.55	-0.20	-0.55	-0.39	-0.39	-0.43	
-0.20	-0.55	-0.55	-0.43	-0.55	-0.55	-0.55	-0.43	
-0.43	-0.20	-0.39	-0.43	-0.43	-0.39	-0.39	-0.39	
-0.43	-0.20	-0.43	-0.43	-0.20	-0.55	-0.55	-0.55	
-0.20	-0.20	-0.39	-0.39	-0.39	-0.43	-0.43	-0.55	
-0.20	-0.43	-0.43	-0.20	-0.39	-0.43	-0.43	-0.55	
-0.55	-0.43	-0.55	-0.43	-0.20	-0.55	-0.43	-0.39	
-0.43	-0.39	-0.55	-0.55	-0.55	-0.43	-0.43	-0.38	
Map policy
v	v	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	v	v	>	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.3208030810181777
mean w [-0.33151673 -0.40944892 -0.33489824 -0.35448424 -0.45775763  0.05821481]
Mean policy from posterior
>	>	>	>	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	>	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
v	v	v	>	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.41	-0.35	-0.33	-0.41	-0.33	-0.33	-0.33	-0.35	
-0.41	-0.33	-0.33	-0.46	-0.33	-0.33	-0.33	-0.35	
-0.35	-0.41	-0.33	-0.46	-0.46	-0.33	-0.33	-0.33	
-0.46	-0.41	-0.46	-0.35	-0.41	-0.33	-0.33	-0.33	
-0.41	-0.41	-0.33	-0.33	-0.33	-0.46	-0.35	-0.33	
-0.41	-0.35	-0.46	-0.41	-0.33	-0.46	-0.46	-0.33	
-0.33	-0.35	-0.33	-0.46	-0.41	-0.33	-0.46	-0.33	
-0.35	-0.33	-0.33	-0.33	-0.33	-0.35	-0.46	0.06	
mean = 0.4736332955275757, map = 0.09253361312106767
CVaR policy
v	v	>	v	>	>	>	v	
v	v	v	v	v	v	>	v	
>	v	v	v	v	>	v	v	
v	v	>	>	v	v	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	>	>	v	
v	v	v	v	v	v	>	v	
>	v	v	v	v	>	v	v	
v	v	>	>	v	>	v	v	
v	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	>	v	
v	v	v	>	>	v	>	v	
>	v	v	v	>	>	>	v	
v	v	>	v	>	>	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	v	
>	>	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	v	v	>	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	>	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
v	v	v	>	v	v	>	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.03918747633697306, 0.06421125501203662, 0.28768455871970167, 0.44046756650807106, 0.473633295132887
==========
iteration 65
==========
weights [-0.50408971 -0.18216383 -0.26848375 -0.1616642  -0.43175195  0.65427924]
expeced value MDP LP -1.0195725831184053
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 6.89296026e-04  1.03972187e-01 -7.70661130e-01 -2.04338447e-01
 -5.75394711e-01 -1.49791378e-01]
w_map [-0.34489764 -0.43632427 -0.41767127 -0.36985856 -0.55030074 -0.27675115] loglik 0.0
accepted/total = 2391/3000 = 0.797
-------
true weights [-0.50408971 -0.18216383 -0.26848375 -0.1616642  -0.43175195  0.65427924]
features
0 	4 	3 	2 	3 	3 	4 	4 	
2 	3 	4 	4 	3 	0 	0 	4 	
4 	3 	0 	3 	4 	4 	4 	0 	
4 	2 	3 	1 	2 	3 	2 	0 	
3 	2 	1 	2 	4 	1 	3 	3 	
0 	1 	3 	1 	4 	4 	2 	2 	
2 	4 	2 	0 	0 	3 	1 	2 	
4 	2 	2 	1 	3 	3 	4 	5 	
optimal policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.43	-2.11	-1.93	-1.79	-1.54	-1.62	-1.82	-1.90	
-1.95	-1.69	-1.98	-1.56	-1.39	-1.48	-1.41	-1.48	
-1.96	-1.55	-1.64	-1.14	-1.24	-0.98	-0.91	-1.06	
-1.82	-1.40	-1.14	-0.99	-0.82	-0.55	-0.48	-0.56	
-1.55	-1.40	-1.15	-1.08	-0.82	-0.40	-0.22	-0.06	
-1.64	-1.15	-0.97	-0.97	-0.83	-0.40	-0.08	0.11	
-1.50	-1.24	-0.82	-0.79	-0.47	0.03	0.19	0.38	
-1.24	-0.82	-0.56	-0.29	-0.11	0.05	0.22	0.65	
map_weights [-0.34489764 -0.43632427 -0.41767127 -0.36985856 -0.55030074 -0.27675115]
MAP reward
-0.34	-0.55	-0.37	-0.42	-0.37	-0.37	-0.55	-0.55	
-0.42	-0.37	-0.55	-0.55	-0.37	-0.34	-0.34	-0.55	
-0.55	-0.37	-0.34	-0.37	-0.55	-0.55	-0.55	-0.34	
-0.55	-0.42	-0.37	-0.44	-0.42	-0.37	-0.42	-0.34	
-0.37	-0.42	-0.44	-0.42	-0.55	-0.44	-0.37	-0.37	
-0.34	-0.44	-0.37	-0.44	-0.55	-0.55	-0.42	-0.42	
-0.42	-0.55	-0.42	-0.34	-0.34	-0.37	-0.44	-0.42	
-0.55	-0.42	-0.42	-0.44	-0.37	-0.37	-0.55	-0.28	
Map policy
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	v	v	
>	>	v	v	v	v	>	v	
v	>	v	>	>	>	>	v	
v	>	v	v	>	>	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.898605399189233
mean w [-0.42757294 -0.31096309 -0.33842649 -0.23985021 -0.30153734  0.0489768 ]
Mean policy from posterior
>	>	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.43	-0.30	-0.24	-0.34	-0.24	-0.24	-0.30	-0.30	
-0.34	-0.24	-0.30	-0.30	-0.24	-0.43	-0.43	-0.30	
-0.30	-0.24	-0.43	-0.24	-0.30	-0.30	-0.30	-0.43	
-0.30	-0.34	-0.24	-0.31	-0.34	-0.24	-0.34	-0.43	
-0.24	-0.34	-0.31	-0.34	-0.30	-0.31	-0.24	-0.24	
-0.43	-0.31	-0.24	-0.31	-0.30	-0.30	-0.34	-0.34	
-0.34	-0.30	-0.34	-0.43	-0.43	-0.24	-0.31	-0.34	
-0.30	-0.34	-0.34	-0.31	-0.24	-0.24	-0.30	0.05	
mean = 0.2513916895542341, map = 0.38994556074417774
CVaR policy
v	>	v	v	>	v	v	v	
v	>	v	>	>	>	v	v	
v	>	v	v	v	>	>	v	
v	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	>	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.4922680875945531, 0.2417194300429184, 0.24595618387289164, 0.23220520374277354, 0.2513916897224766
==========
iteration 66
==========
weights [-0.30989402 -0.1210639  -0.0412376  -0.20132667 -0.15194379  0.90773859]
expeced value MDP LP 0.00484872204110065
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.09118785  0.88896518 -0.16293373  0.15913761  0.07150359 -0.3800536 ]
w_map [-0.52760982 -0.50665167 -0.34601704 -0.42418929 -0.17387513  0.36747117] loglik 0.0
accepted/total = 2350/3000 = 0.7833333333333333
-------
true weights [-0.30989402 -0.1210639  -0.0412376  -0.20132667 -0.15194379  0.90773859]
features
0 	4 	4 	1 	4 	0 	4 	2 	
4 	2 	0 	3 	2 	0 	1 	0 	
1 	4 	1 	1 	1 	3 	3 	0 	
2 	2 	1 	1 	0 	4 	3 	2 	
0 	4 	4 	4 	2 	1 	4 	3 	
1 	4 	4 	2 	3 	1 	2 	4 	
1 	4 	0 	1 	4 	3 	1 	1 	
3 	0 	2 	0 	1 	3 	1 	5 	
optimal policy
v	v	>	v	v	>	v	<	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-0.75	-0.47	-0.56	-0.41	-0.32	-0.55	-0.24	-0.28	
-0.44	-0.32	-0.52	-0.30	-0.17	-0.32	-0.09	-0.26	
-0.29	-0.29	-0.22	-0.10	-0.13	-0.01	0.03	0.05	
-0.18	-0.14	-0.10	0.03	-0.01	0.19	0.24	0.37	
-0.47	-0.16	-0.00	0.15	0.30	0.35	0.44	0.41	
-0.20	-0.08	0.07	0.22	0.27	0.47	0.60	0.62	
-0.32	-0.23	-0.15	0.16	0.29	0.44	0.65	0.78	
-0.42	-0.23	0.08	0.13	0.44	0.57	0.78	0.91	
map_weights [-0.52760982 -0.50665167 -0.34601704 -0.42418929 -0.17387513  0.36747117]
MAP reward
-0.53	-0.17	-0.17	-0.51	-0.17	-0.53	-0.17	-0.35	
-0.17	-0.35	-0.53	-0.42	-0.35	-0.53	-0.51	-0.53	
-0.51	-0.17	-0.51	-0.51	-0.51	-0.42	-0.42	-0.53	
-0.35	-0.35	-0.51	-0.51	-0.53	-0.17	-0.42	-0.35	
-0.53	-0.17	-0.17	-0.17	-0.35	-0.51	-0.17	-0.42	
-0.51	-0.17	-0.17	-0.35	-0.42	-0.51	-0.35	-0.17	
-0.51	-0.17	-0.53	-0.51	-0.17	-0.42	-0.51	-0.51	
-0.42	-0.53	-0.35	-0.53	-0.51	-0.42	-0.51	0.37	
Map policy
v	v	<	>	v	>	v	v	
>	v	<	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.336452739089826
mean w [-0.24713043 -0.39407729 -0.27532793 -0.33437168 -0.41373849 -0.10270165]
Mean policy from posterior
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
v	v	>	>	v	>	>	v	
v	>	>	>	v	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.25	-0.41	-0.41	-0.39	-0.41	-0.25	-0.41	-0.28	
-0.41	-0.28	-0.25	-0.33	-0.28	-0.25	-0.39	-0.25	
-0.39	-0.41	-0.39	-0.39	-0.39	-0.33	-0.33	-0.25	
-0.28	-0.28	-0.39	-0.39	-0.25	-0.41	-0.33	-0.28	
-0.25	-0.41	-0.41	-0.41	-0.28	-0.39	-0.41	-0.33	
-0.39	-0.41	-0.41	-0.28	-0.33	-0.39	-0.28	-0.41	
-0.39	-0.41	-0.25	-0.39	-0.41	-0.33	-0.39	-0.39	
-0.33	-0.25	-0.28	-0.25	-0.39	-0.33	-0.39	-0.10	
mean = 0.3135551274794586, map = 0.09376031315110189
CVaR policy
>	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	>	>	v	v	
>	v	v	v	v	>	v	v	
>	v	v	>	v	>	>	v	
>	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	>	>	v	
>	v	v	v	v	>	v	v	
>	v	v	>	v	>	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	>	v	>	>	v	
>	v	v	v	v	>	v	v	
>	v	v	>	v	>	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
v	v	v	>	v	>	>	v	
v	v	v	>	v	>	>	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	>	>	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
v	v	v	>	v	>	>	v	
v	v	v	>	v	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.29848515472003695, 0.3090318989517633, 0.3097793014476401, 0.3176896994465792, 0.3208850055762945
==========
iteration 67
==========
weights [-0.11013276 -0.67319968 -0.20101732 -0.12934998 -0.68243955  0.1086731 ]
expeced value MDP LP -1.7041313103468287
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.07831074 -0.7754687   0.59116003  0.17902544  0.09714412  0.03947721]
w_map [-0.18668298 -0.40584074 -0.20629603 -0.25367024 -0.73476999  0.39198109] loglik -1.0278673201469246e-07
accepted/total = 1796/3000 = 0.5986666666666667
-------
true weights [-0.11013276 -0.67319968 -0.20101732 -0.12934998 -0.68243955  0.1086731 ]
features
3 	1 	0 	4 	4 	0 	4 	1 	
4 	3 	2 	1 	1 	3 	3 	2 	
1 	2 	4 	1 	0 	3 	3 	3 	
2 	0 	3 	4 	1 	1 	1 	3 	
0 	4 	1 	2 	2 	3 	3 	4 	
1 	2 	1 	4 	4 	0 	1 	2 	
4 	0 	0 	0 	1 	3 	4 	0 	
0 	1 	4 	2 	3 	1 	2 	5 	
optimal policy
>	v	v	>	>	v	v	v	
>	v	<	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
optimal values
-3.29	-3.20	-2.81	-2.90	-2.24	-1.57	-2.03	-1.97	
-3.21	-2.55	-2.72	-2.77	-2.11	-1.47	-1.36	-1.31	
-3.09	-2.44	-2.78	-2.11	-1.46	-1.36	-1.24	-1.12	
-2.44	-2.27	-2.18	-2.08	-1.88	-1.69	-1.66	-1.00	
-2.31	-2.23	-2.07	-1.41	-1.22	-1.03	-1.00	-0.88	
-2.22	-1.56	-1.94	-1.85	-1.58	-0.91	-0.87	-0.20	
-2.04	-1.37	-1.28	-1.18	-1.47	-0.81	-0.68	-0.00	
-2.12	-2.03	-1.75	-1.08	-0.89	-0.77	-0.09	0.11	
map_weights [-0.18668298 -0.40584074 -0.20629603 -0.25367024 -0.73476999  0.39198109]
MAP reward
-0.25	-0.41	-0.19	-0.73	-0.73	-0.19	-0.73	-0.41	
-0.73	-0.25	-0.21	-0.41	-0.41	-0.25	-0.25	-0.21	
-0.41	-0.21	-0.73	-0.41	-0.19	-0.25	-0.25	-0.25	
-0.21	-0.19	-0.25	-0.73	-0.41	-0.41	-0.41	-0.25	
-0.19	-0.73	-0.41	-0.21	-0.21	-0.25	-0.25	-0.73	
-0.41	-0.21	-0.41	-0.73	-0.73	-0.19	-0.41	-0.21	
-0.73	-0.19	-0.19	-0.19	-0.41	-0.25	-0.73	-0.19	
-0.19	-0.41	-0.73	-0.21	-0.25	-0.41	-0.21	0.39	
Map policy
>	v	v	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.6219476825821542
mean w [-0.12548737 -0.36541649 -0.13455445 -0.32663822 -0.67441304  0.00864276]
Mean policy from posterior
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.33	-0.37	-0.13	-0.67	-0.67	-0.13	-0.67	-0.37	
-0.67	-0.33	-0.13	-0.37	-0.37	-0.33	-0.33	-0.13	
-0.37	-0.13	-0.67	-0.37	-0.13	-0.33	-0.33	-0.33	
-0.13	-0.13	-0.33	-0.67	-0.37	-0.37	-0.37	-0.33	
-0.13	-0.67	-0.37	-0.13	-0.13	-0.33	-0.33	-0.67	
-0.37	-0.13	-0.37	-0.67	-0.67	-0.13	-0.37	-0.13	
-0.67	-0.13	-0.13	-0.13	-0.37	-0.33	-0.67	-0.13	
-0.13	-0.37	-0.67	-0.13	-0.33	-0.37	-0.13	0.01	
mean = 0.17253219770274675, map = 0.1548603012525287
CVaR policy
>	v	v	v	>	v	v	v	
>	v	>	>	v	v	>	v	
v	v	>	>	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	^	>	>	>	>	>	.	
cvar = , 0.1375233537860181, 0.15755218092828716, 0.17253220522331159, 0.1725321976485299, 0.17253219771466122
==========
iteration 68
==========
weights [-0.4487679  -0.35687817 -0.53436972 -0.42553897 -0.44563112  0.07761331]
expeced value MDP LP -2.8139914937535053
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.53932495 -0.12307076  0.14599864 -0.028599   -0.63373234 -0.51983843]
w_map [-0.44763977 -0.30599903 -0.49531778 -0.49406497 -0.42426067 -0.19117063] loglik 0.0
accepted/total = 2447/3000 = 0.8156666666666667
-------
true weights [-0.4487679  -0.35687817 -0.53436972 -0.42553897 -0.44563112  0.07761331]
features
3 	4 	1 	1 	3 	3 	1 	4 	
4 	0 	4 	0 	4 	2 	1 	2 	
2 	3 	3 	2 	1 	4 	3 	3 	
3 	1 	4 	4 	0 	4 	4 	3 	
4 	2 	1 	0 	3 	2 	1 	2 	
3 	3 	4 	1 	4 	0 	2 	3 	
2 	0 	4 	3 	2 	0 	1 	0 	
2 	0 	3 	3 	2 	4 	2 	5 	
optimal policy
>	>	>	>	>	>	v	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-5.29	-4.91	-4.51	-4.20	-3.88	-3.49	-3.09	-3.07	
-5.05	-4.65	-4.33	-4.01	-3.59	-3.27	-2.76	-2.65	
-4.74	-4.24	-3.93	-3.68	-3.18	-2.85	-2.43	-2.14	
-4.24	-3.86	-3.54	-3.21	-2.86	-2.45	-2.03	-1.73	
-3.99	-3.62	-3.12	-2.79	-2.44	-2.11	-1.60	-1.32	
-3.58	-3.19	-2.79	-2.37	-2.03	-1.60	-1.25	-0.79	
-3.45	-2.95	-2.52	-2.10	-1.69	-1.17	-0.73	-0.37	
-3.18	-2.67	-2.24	-1.84	-1.42	-0.90	-0.46	0.08	
map_weights [-0.44763977 -0.30599903 -0.49531778 -0.49406497 -0.42426067 -0.19117063]
MAP reward
-0.49	-0.42	-0.31	-0.31	-0.49	-0.49	-0.31	-0.42	
-0.42	-0.45	-0.42	-0.45	-0.42	-0.50	-0.31	-0.50	
-0.50	-0.49	-0.49	-0.50	-0.31	-0.42	-0.49	-0.49	
-0.49	-0.31	-0.42	-0.42	-0.45	-0.42	-0.42	-0.49	
-0.42	-0.50	-0.31	-0.45	-0.49	-0.50	-0.31	-0.50	
-0.49	-0.49	-0.42	-0.31	-0.42	-0.45	-0.50	-0.49	
-0.50	-0.45	-0.42	-0.49	-0.50	-0.45	-0.31	-0.45	
-0.50	-0.45	-0.49	-0.49	-0.50	-0.42	-0.50	-0.19	
Map policy
>	>	>	v	v	>	v	v	
>	v	>	>	v	>	v	v	
v	v	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.114475687042801
mean w [-0.40826188 -0.45951412 -0.35478439 -0.25249774 -0.32564886  0.03687419]
Mean policy from posterior
v	v	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
v	>	>	>	>	>	>	v	
v	v	>	>	>	>	>	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.25	-0.33	-0.46	-0.46	-0.25	-0.25	-0.46	-0.33	
-0.33	-0.41	-0.33	-0.41	-0.33	-0.35	-0.46	-0.35	
-0.35	-0.25	-0.25	-0.35	-0.46	-0.33	-0.25	-0.25	
-0.25	-0.46	-0.33	-0.33	-0.41	-0.33	-0.33	-0.25	
-0.33	-0.35	-0.46	-0.41	-0.25	-0.35	-0.46	-0.35	
-0.25	-0.25	-0.33	-0.46	-0.33	-0.41	-0.35	-0.25	
-0.35	-0.41	-0.33	-0.25	-0.35	-0.41	-0.46	-0.41	
-0.35	-0.41	-0.25	-0.25	-0.35	-0.33	-0.35	0.04	
mean = 0.19700477918454062, map = 0.016212901043334504
CVaR policy
>	>	>	>	>	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	>	>	v	v	v	
v	>	v	>	>	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	>	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	>	>	v	
v	v	>	>	>	>	>	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
v	>	v	>	>	>	>	v	
v	v	>	>	>	>	>	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
v	>	>	>	>	>	>	v	
v	v	>	>	>	>	>	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
v	>	>	>	>	>	>	v	
v	v	>	>	>	>	>	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.12463426653694576, 0.20715716641600412, 0.1996170575707521, 0.19700477908950287, 0.19700477907579916
==========
iteration 69
==========
weights [-0.37050983 -0.25833116 -0.23439414 -0.08713256 -0.79646328  0.31480316]
expeced value MDP LP -1.8806386965914959
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.49780109  0.03271352 -0.25217932 -0.18922113  0.7464579   0.30744994]
w_map [-0.67680647 -0.32359738 -0.31442626 -0.23244507 -0.37718649 -0.37689986] loglik -0.6931471109696474
accepted/total = 1930/3000 = 0.6433333333333333
-------
true weights [-0.37050983 -0.25833116 -0.23439414 -0.08713256 -0.79646328  0.31480316]
features
3 	2 	2 	2 	1 	4 	0 	4 	
4 	0 	4 	4 	0 	4 	4 	3 	
2 	2 	1 	3 	3 	4 	3 	1 	
4 	2 	3 	2 	0 	1 	2 	0 	
3 	2 	1 	0 	1 	3 	1 	1 	
4 	1 	4 	2 	2 	4 	3 	2 	
3 	2 	2 	0 	1 	2 	1 	4 	
0 	0 	4 	1 	0 	0 	4 	5 	
optimal policy
>	v	>	>	v	<	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
optimal values
-2.93	-2.87	-2.80	-2.60	-2.38	-3.16	-2.48	-2.42	
-3.30	-2.66	-2.88	-2.64	-2.15	-2.90	-2.13	-1.64	
-2.53	-2.32	-2.10	-1.86	-1.80	-2.13	-1.34	-1.57	
-3.00	-2.22	-2.01	-1.94	-1.73	-1.37	-1.27	-1.33	
-2.25	-2.18	-1.97	-1.73	-1.37	-1.12	-1.04	-0.97	
-2.85	-2.25	-2.44	-1.66	-1.44	-1.58	-0.79	-0.71	
-2.07	-2.01	-1.79	-1.57	-1.21	-0.97	-0.74	-0.48	
-2.42	-2.36	-2.24	-1.46	-1.21	-0.85	-0.48	0.31	
map_weights [-0.67680647 -0.32359738 -0.31442626 -0.23244507 -0.37718649 -0.37689986]
MAP reward
-0.23	-0.31	-0.31	-0.31	-0.32	-0.38	-0.68	-0.38	
-0.38	-0.68	-0.38	-0.38	-0.68	-0.38	-0.38	-0.23	
-0.31	-0.31	-0.32	-0.23	-0.23	-0.38	-0.23	-0.32	
-0.38	-0.31	-0.23	-0.31	-0.68	-0.32	-0.31	-0.68	
-0.23	-0.31	-0.32	-0.68	-0.32	-0.23	-0.32	-0.32	
-0.38	-0.32	-0.38	-0.31	-0.31	-0.38	-0.23	-0.31	
-0.23	-0.31	-0.31	-0.68	-0.32	-0.31	-0.32	-0.38	
-0.68	-0.68	-0.38	-0.32	-0.68	-0.68	-0.38	-0.38	
Map policy
>	>	>	v	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
v	>	v	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -2.1188982173708375
mean w [-0.51740326 -0.18815639 -0.23391999 -0.23170302 -0.57242717 -0.01177749]
Mean policy from posterior
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.23	-0.23	-0.23	-0.23	-0.19	-0.57	-0.52	-0.57	
-0.57	-0.52	-0.57	-0.57	-0.52	-0.57	-0.57	-0.23	
-0.23	-0.23	-0.19	-0.23	-0.23	-0.57	-0.23	-0.19	
-0.57	-0.23	-0.23	-0.23	-0.52	-0.19	-0.23	-0.52	
-0.23	-0.23	-0.19	-0.52	-0.19	-0.23	-0.19	-0.19	
-0.57	-0.19	-0.57	-0.23	-0.23	-0.57	-0.23	-0.23	
-0.23	-0.23	-0.23	-0.52	-0.19	-0.23	-0.19	-0.57	
-0.52	-0.52	-0.57	-0.19	-0.52	-0.52	-0.57	-0.01	
mean = 0.039411219706852574, map = 0.2818709969646713
CVaR policy
>	>	>	>	v	>	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	>	>	v	v	
v	>	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	>	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
v	v	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.1032129130252899, 0.06328845586725795, 0.07786556910958597, 0.09307712943136215, 0.08960421928922524
==========
iteration 70
==========
weights [-0.22513359 -0.28905385 -0.03580942 -0.69498026 -0.53881297  0.30193286]
expeced value MDP LP -1.2175531702620517
demonstration
[(56, 2), (48, 2), (40, 1), (41, 1), (42, 2), (34, 1), (35, 1), (36, 1), (37, 3), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.70453797 -0.23644976 -0.0702577  -0.0061609  -0.19736508  0.63544526]
w_map [-0.1205385  -0.30967097 -0.12732479 -0.51254834 -0.77231407 -0.11911312] loglik -5.4313283023077474e-05
accepted/total = 1123/3000 = 0.37433333333333335
-------
true weights [-0.22513359 -0.28905385 -0.03580942 -0.69498026 -0.53881297  0.30193286]
features
2 	2 	1 	2 	1 	2 	1 	4 	
1 	4 	1 	4 	1 	4 	3 	2 	
0 	1 	1 	0 	1 	0 	3 	2 	
0 	0 	2 	0 	1 	2 	2 	1 	
0 	4 	2 	2 	0 	1 	3 	1 	
2 	2 	0 	1 	3 	0 	3 	1 	
0 	3 	0 	0 	4 	0 	2 	1 	
4 	0 	1 	0 	4 	4 	4 	5 	
optimal policy
>	>	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
optimal values
-1.91	-1.90	-1.88	-1.84	-1.83	-1.55	-1.71	-1.43	
-1.96	-2.07	-1.61	-1.95	-1.56	-1.53	-1.59	-0.90	
-1.69	-1.54	-1.33	-1.43	-1.28	-1.00	-1.50	-0.88	
-1.48	-1.27	-1.05	-1.22	-1.07	-0.79	-0.81	-0.85	
-1.50	-1.56	-1.03	-1.00	-0.98	-0.76	-1.25	-0.57	
-1.29	-1.27	-1.24	-1.28	-1.16	-0.47	-0.72	-0.28	
-1.50	-1.90	-1.22	-1.00	-0.79	-0.25	-0.03	0.01	
-2.02	-1.71	-1.50	-1.22	-1.31	-0.78	-0.24	0.30	
map_weights [-0.1205385  -0.30967097 -0.12732479 -0.51254834 -0.77231407 -0.11911312]
MAP reward
-0.13	-0.13	-0.31	-0.13	-0.31	-0.13	-0.31	-0.77	
-0.31	-0.77	-0.31	-0.77	-0.31	-0.77	-0.51	-0.13	
-0.12	-0.31	-0.31	-0.12	-0.31	-0.12	-0.51	-0.13	
-0.12	-0.12	-0.13	-0.12	-0.31	-0.13	-0.13	-0.31	
-0.12	-0.77	-0.13	-0.13	-0.12	-0.31	-0.51	-0.31	
-0.13	-0.13	-0.12	-0.31	-0.51	-0.12	-0.51	-0.31	
-0.12	-0.51	-0.12	-0.12	-0.77	-0.12	-0.13	-0.31	
-0.77	-0.12	-0.31	-0.12	-0.77	-0.77	-0.77	-0.12	
Map policy
v	<	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	>	v	>	v	<	v	
>	>	>	v	v	v	<	v	
^	>	>	>	>	v	>	v	
>	>	^	>	>	v	v	v	
^	>	^	>	>	>	>	v	
^	>	^	^	^	^	>	.	
expeced value MDP LP -0.9248654764729862
mean w [-0.11898592 -0.22029143 -0.05993877 -0.50977713 -0.61458262  0.2276705 ]
Mean policy from posterior
v	<	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	^	^	>	>	>	>	v	
^	>	^	^	^	^	>	.	
Mean rewards
-0.06	-0.06	-0.22	-0.06	-0.22	-0.06	-0.22	-0.61	
-0.22	-0.61	-0.22	-0.61	-0.22	-0.61	-0.51	-0.06	
-0.12	-0.22	-0.22	-0.12	-0.22	-0.12	-0.51	-0.06	
-0.12	-0.12	-0.06	-0.12	-0.22	-0.06	-0.06	-0.22	
-0.12	-0.61	-0.06	-0.06	-0.12	-0.22	-0.51	-0.22	
-0.06	-0.06	-0.12	-0.22	-0.51	-0.12	-0.51	-0.22	
-0.12	-0.51	-0.12	-0.12	-0.61	-0.12	-0.06	-0.22	
-0.61	-0.12	-0.22	-0.12	-0.61	-0.61	-0.61	0.23	
mean = 0.01710156933531115, map = 0.07711434424792452
CVaR policy
v	<	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	^	^	>	^	>	.	
CVaR policy
v	<	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
^	^	>	>	>	>	>	v	
^	>	>	^	>	^	>	.	
CVaR policy
v	<	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	^	>	>	>	>	>	v	
^	>	^	^	>	^	>	.	
CVaR policy
v	<	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	^	^	>	>	>	>	v	
^	>	^	^	^	^	>	.	
CVaR policy
v	<	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	^	^	>	>	>	>	v	
^	>	^	^	^	^	>	.	
cvar = , 0.012271544698630787, 0.011095145568106535, 0.006183343942028019, 0.017101584728781205, 0.017101580596604826
==========
iteration 71
==========
weights [-0.61278202 -0.25491264 -0.35505055 -0.54428494 -0.317345    0.19105732]
expeced value MDP LP -2.3003258457556166
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.0910687   0.70112103  0.25753285  0.37763206 -0.52872535  0.1079637 ]
w_map [-0.46551311 -0.11377382 -0.44703567 -0.35667601 -0.60451404  0.27902895] loglik 0.0
accepted/total = 2430/3000 = 0.81
-------
true weights [-0.61278202 -0.25491264 -0.35505055 -0.54428494 -0.317345    0.19105732]
features
3 	1 	2 	1 	1 	1 	3 	3 	
2 	4 	2 	1 	0 	2 	3 	4 	
3 	1 	1 	4 	1 	2 	4 	1 	
1 	2 	4 	2 	1 	4 	3 	1 	
2 	3 	0 	3 	3 	4 	0 	1 	
3 	3 	3 	0 	3 	2 	0 	0 	
0 	4 	3 	0 	4 	2 	3 	4 	
0 	0 	3 	4 	4 	4 	3 	5 	
optimal policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
^	^	>	>	>	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.11	-3.60	-3.42	-3.10	-2.91	-2.68	-2.83	-2.30	
-3.70	-3.38	-3.20	-2.87	-2.94	-2.45	-2.30	-1.78	
-3.61	-3.10	-2.87	-2.64	-2.35	-2.11	-1.78	-1.47	
-3.38	-3.15	-2.83	-2.53	-2.20	-1.97	-1.76	-1.23	
-3.70	-3.67	-3.30	-2.72	-2.19	-1.67	-1.59	-0.99	
-3.67	-3.16	-2.87	-2.41	-1.82	-1.36	-1.28	-0.74	
-3.22	-2.64	-2.34	-1.89	-1.29	-1.02	-0.67	-0.13	
-3.00	-2.41	-1.82	-1.29	-0.98	-0.67	-0.36	0.19	
map_weights [-0.46551311 -0.11377382 -0.44703567 -0.35667601 -0.60451404  0.27902895]
MAP reward
-0.36	-0.11	-0.45	-0.11	-0.11	-0.11	-0.36	-0.36	
-0.45	-0.60	-0.45	-0.11	-0.47	-0.45	-0.36	-0.60	
-0.36	-0.11	-0.11	-0.60	-0.11	-0.45	-0.60	-0.11	
-0.11	-0.45	-0.60	-0.45	-0.11	-0.60	-0.36	-0.11	
-0.45	-0.36	-0.47	-0.36	-0.36	-0.60	-0.47	-0.11	
-0.36	-0.36	-0.36	-0.47	-0.36	-0.45	-0.47	-0.47	
-0.47	-0.60	-0.36	-0.47	-0.60	-0.45	-0.36	-0.60	
-0.47	-0.47	-0.36	-0.60	-0.60	-0.60	-0.36	0.28	
Map policy
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
^	^	>	>	>	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.209292967134137
mean w [-0.40753614 -0.26888263 -0.41203852 -0.4407049  -0.28273187 -0.00644079]
Mean policy from posterior
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.44	-0.27	-0.41	-0.27	-0.27	-0.27	-0.44	-0.44	
-0.41	-0.28	-0.41	-0.27	-0.41	-0.41	-0.44	-0.28	
-0.44	-0.27	-0.27	-0.28	-0.27	-0.41	-0.28	-0.27	
-0.27	-0.41	-0.28	-0.41	-0.27	-0.28	-0.44	-0.27	
-0.41	-0.44	-0.41	-0.44	-0.44	-0.28	-0.41	-0.27	
-0.44	-0.44	-0.44	-0.41	-0.44	-0.41	-0.41	-0.41	
-0.41	-0.28	-0.44	-0.41	-0.28	-0.41	-0.44	-0.28	
-0.41	-0.41	-0.44	-0.28	-0.28	-0.28	-0.44	-0.01	
mean = 0.06161687992701337, map = 0.21935812805682708
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	>	v	
>	v	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.2624976198801323, 0.02069414741344744, 0.04938253557870631, 0.05462224086669032, 0.05761459747701636
==========
iteration 72
==========
weights [-0.17961962 -0.64037079 -0.66922549 -0.27033157 -0.18633629  0.04470917]
expeced value MDP LP -1.6678488754885077
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.69565473  0.44116187 -0.2611965  -0.14408981 -0.07630019 -0.47606038]
w_map [-0.1080561  -0.6746737  -0.36087923 -0.35390128 -0.48199773 -0.21292614] loglik 0.0
accepted/total = 2377/3000 = 0.7923333333333333
-------
true weights [-0.17961962 -0.64037079 -0.66922549 -0.27033157 -0.18633629  0.04470917]
features
0 	0 	0 	4 	2 	1 	0 	4 	
2 	2 	4 	4 	0 	3 	0 	4 	
2 	2 	4 	3 	1 	4 	2 	2 	
2 	3 	4 	3 	3 	3 	2 	3 	
3 	4 	1 	4 	0 	0 	3 	4 	
0 	4 	3 	2 	1 	0 	3 	1 	
1 	3 	3 	1 	3 	1 	4 	0 	
3 	4 	2 	1 	4 	3 	4 	5 	
optimal policy
>	>	v	v	v	v	v	<	
^	>	v	>	>	v	<	<	
>	>	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	^	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.52	-2.36	-2.20	-2.12	-2.44	-2.25	-1.95	-2.12	
-3.16	-2.69	-2.04	-1.96	-1.79	-1.63	-1.79	-1.96	
-3.17	-2.53	-1.88	-1.79	-1.99	-1.37	-2.02	-1.87	
-2.61	-1.96	-1.71	-1.54	-1.36	-1.19	-1.51	-1.21	
-2.32	-2.07	-1.91	-1.28	-1.10	-0.93	-0.85	-0.95	
-2.32	-2.16	-2.00	-1.94	-1.39	-0.76	-0.59	-0.77	
-2.62	-2.00	-1.74	-1.49	-0.86	-0.96	-0.32	-0.14	
-2.30	-2.05	-1.88	-1.23	-0.59	-0.41	-0.14	0.04	
map_weights [-0.1080561  -0.6746737  -0.36087923 -0.35390128 -0.48199773 -0.21292614]
MAP reward
-0.11	-0.11	-0.11	-0.48	-0.36	-0.67	-0.11	-0.48	
-0.36	-0.36	-0.48	-0.48	-0.11	-0.35	-0.11	-0.48	
-0.36	-0.36	-0.48	-0.35	-0.67	-0.48	-0.36	-0.36	
-0.36	-0.35	-0.48	-0.35	-0.35	-0.35	-0.36	-0.35	
-0.35	-0.48	-0.67	-0.48	-0.11	-0.11	-0.35	-0.48	
-0.11	-0.48	-0.35	-0.36	-0.67	-0.11	-0.35	-0.67	
-0.67	-0.35	-0.35	-0.67	-0.35	-0.67	-0.48	-0.11	
-0.35	-0.48	-0.36	-0.67	-0.48	-0.35	-0.48	-0.21	
Map policy
>	>	>	>	v	>	v	<	
^	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.19210730808764
mean w [-0.37910862 -0.43902631 -0.28850098 -0.39187235 -0.25050977 -0.03420383]
Mean policy from posterior
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.38	-0.38	-0.38	-0.25	-0.29	-0.44	-0.38	-0.25	
-0.29	-0.29	-0.25	-0.25	-0.38	-0.39	-0.38	-0.25	
-0.29	-0.29	-0.25	-0.39	-0.44	-0.25	-0.29	-0.29	
-0.29	-0.39	-0.25	-0.39	-0.39	-0.39	-0.29	-0.39	
-0.39	-0.25	-0.44	-0.25	-0.38	-0.38	-0.39	-0.25	
-0.38	-0.25	-0.39	-0.29	-0.44	-0.38	-0.39	-0.44	
-0.44	-0.39	-0.39	-0.44	-0.39	-0.44	-0.25	-0.38	
-0.39	-0.25	-0.29	-0.44	-0.25	-0.39	-0.25	-0.03	
mean = 0.47759246930920973, map = 0.18812986447551405
CVaR policy
v	v	>	>	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
v	v	>	v	v	>	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	>	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.8046877071960106, 0.6964155444974214, 0.4841412310810602, 0.4795503570681645, 0.47955035660711864
==========
iteration 73
==========
weights [-0.0065879  -0.31779271 -0.10417501 -0.6774825  -0.03263814  0.65426614]
expeced value MDP LP 0.05408183179890491
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.40669384  0.23659405 -0.08361558  0.34629509  0.70215021 -0.39836742]
w_map [-0.17373396 -0.36013153 -0.30740937 -0.8434891  -0.18416262 -0.01521623] loglik -7.588354904441985e-07
accepted/total = 1562/3000 = 0.5206666666666667
-------
true weights [-0.0065879  -0.31779271 -0.10417501 -0.6774825  -0.03263814  0.65426614]
features
3 	3 	0 	4 	1 	2 	2 	0 	
4 	2 	3 	0 	0 	4 	3 	2 	
4 	4 	0 	3 	4 	4 	2 	3 	
2 	2 	4 	2 	1 	0 	3 	2 	
4 	4 	4 	1 	1 	3 	0 	0 	
2 	2 	4 	4 	3 	1 	2 	4 	
1 	1 	1 	0 	0 	3 	3 	0 	
0 	4 	4 	3 	4 	4 	2 	5 	
optimal policy
v	v	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	<	v	v	v	v	
v	v	v	<	<	v	v	v	
>	>	v	v	<	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
optimal values
-0.50	-0.53	-0.20	-0.20	-0.48	-0.28	-0.38	-0.31	
0.18	0.15	-0.39	-0.17	-0.16	-0.17	-0.85	-0.31	
0.22	0.25	0.29	-0.39	-0.16	-0.14	-0.21	-0.20	
0.16	0.19	0.30	0.19	-0.13	-0.11	-0.11	0.48	
0.26	0.30	0.34	0.09	-0.23	-0.11	0.58	0.59	
0.16	0.26	0.37	0.41	-0.23	0.17	0.49	0.60	
-0.16	-0.06	0.12	0.45	0.46	-0.18	-0.04	0.64	
0.05	0.06	0.09	-0.21	0.47	0.51	0.54	0.65	
map_weights [-0.17373396 -0.36013153 -0.30740937 -0.8434891  -0.18416262 -0.01521623]
MAP reward
-0.84	-0.84	-0.17	-0.18	-0.36	-0.31	-0.31	-0.17	
-0.18	-0.31	-0.84	-0.17	-0.17	-0.18	-0.84	-0.31	
-0.18	-0.18	-0.17	-0.84	-0.18	-0.18	-0.31	-0.84	
-0.31	-0.31	-0.18	-0.31	-0.36	-0.17	-0.84	-0.31	
-0.18	-0.18	-0.18	-0.36	-0.36	-0.84	-0.17	-0.17	
-0.31	-0.31	-0.18	-0.18	-0.84	-0.36	-0.31	-0.18	
-0.36	-0.36	-0.36	-0.17	-0.17	-0.84	-0.84	-0.17	
-0.17	-0.18	-0.18	-0.84	-0.18	-0.18	-0.31	-0.02	
Map policy
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	<	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.5933215451392306
mean w [-0.11847404 -0.27794904 -0.41407816 -0.68953978 -0.12082867 -0.25233062]
Mean policy from posterior
v	>	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	<	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.69	-0.69	-0.12	-0.12	-0.28	-0.41	-0.41	-0.12	
-0.12	-0.41	-0.69	-0.12	-0.12	-0.12	-0.69	-0.41	
-0.12	-0.12	-0.12	-0.69	-0.12	-0.12	-0.41	-0.69	
-0.41	-0.41	-0.12	-0.41	-0.28	-0.12	-0.69	-0.41	
-0.12	-0.12	-0.12	-0.28	-0.28	-0.69	-0.12	-0.12	
-0.41	-0.41	-0.12	-0.12	-0.69	-0.28	-0.41	-0.12	
-0.28	-0.28	-0.28	-0.12	-0.12	-0.69	-0.69	-0.12	
-0.12	-0.12	-0.12	-0.69	-0.12	-0.12	-0.41	-0.25	
mean = 0.025698985535838326, map = 0.02284116248830656
CVaR policy
v	>	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
v	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	<	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	<	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
cvar = , 0.04650011010071253, 0.033905595324633525, 0.032878579416529, 0.02569898827336331, 0.02569898741827023
==========
iteration 74
==========
weights [-0.30557272 -0.31797187 -0.23414576 -0.67327985 -0.14898029  0.52458944]
expeced value MDP LP -1.2535152997443926
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.07102538 -0.82024626 -0.47242773  0.10071258 -0.10624749  0.27844562]
w_map [-0.39240185 -0.35402557 -0.36617134 -0.75960964 -0.09694809 -0.01412406] loglik 0.0
accepted/total = 2445/3000 = 0.815
-------
true weights [-0.30557272 -0.31797187 -0.23414576 -0.67327985 -0.14898029  0.52458944]
features
4 	0 	1 	4 	1 	4 	1 	3 	
2 	2 	2 	4 	3 	0 	1 	0 	
0 	2 	4 	0 	3 	1 	3 	4 	
1 	1 	0 	2 	2 	4 	4 	3 	
1 	0 	2 	2 	0 	2 	0 	0 	
4 	0 	2 	2 	2 	1 	1 	0 	
2 	0 	2 	2 	3 	3 	3 	4 	
0 	1 	1 	2 	0 	1 	2 	5 	
optimal policy
v	>	>	v	>	v	<	v	
>	>	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	>	v	
v	>	>	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.48	-2.42	-2.14	-1.84	-1.87	-1.57	-1.87	-2.01	
-2.35	-2.14	-1.93	-1.71	-2.10	-1.44	-1.66	-1.35	
-2.21	-1.93	-1.71	-1.58	-1.72	-1.14	-1.36	-1.06	
-2.18	-1.88	-1.58	-1.28	-1.06	-0.83	-0.69	-0.92	
-1.98	-1.76	-1.47	-1.25	-1.07	-0.78	-0.55	-0.25	
-1.68	-1.54	-1.25	-1.03	-0.80	-0.57	-0.26	0.06	
-1.54	-1.32	-1.03	-0.80	-1.01	-0.71	-0.31	0.37	
-1.49	-1.19	-0.88	-0.57	-0.34	-0.04	0.29	0.52	
map_weights [-0.39240185 -0.35402557 -0.36617134 -0.75960964 -0.09694809 -0.01412406]
MAP reward
-0.10	-0.39	-0.35	-0.10	-0.35	-0.10	-0.35	-0.76	
-0.37	-0.37	-0.37	-0.10	-0.76	-0.39	-0.35	-0.39	
-0.39	-0.37	-0.10	-0.39	-0.76	-0.35	-0.76	-0.10	
-0.35	-0.35	-0.39	-0.37	-0.37	-0.10	-0.10	-0.76	
-0.35	-0.39	-0.37	-0.37	-0.39	-0.37	-0.39	-0.39	
-0.10	-0.39	-0.37	-0.37	-0.37	-0.35	-0.35	-0.39	
-0.37	-0.39	-0.37	-0.37	-0.76	-0.76	-0.76	-0.10	
-0.39	-0.35	-0.35	-0.37	-0.39	-0.35	-0.37	-0.01	
Map policy
>	>	>	>	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0890459225325433
mean w [-0.35697373 -0.23787312 -0.36270913 -0.44690064 -0.35086683  0.08822843]
Mean policy from posterior
>	>	>	>	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	v	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.35	-0.36	-0.24	-0.35	-0.24	-0.35	-0.24	-0.45	
-0.36	-0.36	-0.36	-0.35	-0.45	-0.36	-0.24	-0.36	
-0.36	-0.36	-0.35	-0.36	-0.45	-0.24	-0.45	-0.35	
-0.24	-0.24	-0.36	-0.36	-0.36	-0.35	-0.35	-0.45	
-0.24	-0.36	-0.36	-0.36	-0.36	-0.36	-0.36	-0.36	
-0.35	-0.36	-0.36	-0.36	-0.36	-0.24	-0.24	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.45	-0.45	-0.45	-0.35	
-0.36	-0.24	-0.24	-0.36	-0.36	-0.24	-0.36	0.09	
mean = 0.1531041811600624, map = 0.02522520005256612
CVaR policy
>	>	>	>	>	>	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	v	>	>	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.4652182838262544, 0.14627885493518367, 0.14549043167996834, 0.14549040939844882, 0.1466664268020017
==========
iteration 75
==========
weights [-0.08194328 -0.39520843 -0.45167315 -0.29919102 -0.07673882  0.73326861]
expeced value MDP LP -0.7526650300552158
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.61366415 -0.11119382  0.3486061   0.2471486   0.16240346  0.63408889]
w_map [-0.25844954 -0.51236113 -0.51826733 -0.6193768  -0.0892744  -0.1024272 ] loglik -1.6760992593845003e-10
accepted/total = 1963/3000 = 0.6543333333333333
-------
true weights [-0.08194328 -0.39520843 -0.45167315 -0.29919102 -0.07673882  0.73326861]
features
4 	3 	2 	0 	1 	2 	3 	3 	
2 	3 	4 	2 	2 	4 	2 	4 	
2 	1 	1 	4 	4 	1 	4 	0 	
4 	0 	4 	0 	3 	1 	2 	0 	
1 	0 	3 	2 	2 	4 	1 	2 	
0 	2 	4 	2 	1 	0 	2 	1 	
0 	2 	0 	4 	3 	2 	0 	2 	
2 	1 	3 	1 	1 	3 	0 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
optimal values
-1.87	-1.82	-1.68	-1.40	-1.65	-1.36	-1.37	-1.09	
-1.82	-1.53	-1.25	-1.33	-1.26	-0.91	-1.24	-0.80	
-1.38	-1.25	-1.18	-0.89	-0.82	-0.85	-0.80	-0.73	
-0.94	-0.87	-0.79	-0.83	-0.75	-0.46	-0.75	-0.65	
-1.19	-0.80	-0.72	-0.96	-0.51	-0.06	-0.30	-0.57	
-0.95	-0.88	-0.43	-0.73	-0.38	0.02	0.10	-0.12	
-0.88	-0.80	-0.36	-0.28	-0.20	0.10	0.56	0.27	
-1.32	-1.04	-0.65	-0.45	-0.06	0.34	0.64	0.73	
map_weights [-0.25844954 -0.51236113 -0.51826733 -0.6193768  -0.0892744  -0.1024272 ]
MAP reward
-0.09	-0.62	-0.52	-0.26	-0.51	-0.52	-0.62	-0.62	
-0.52	-0.62	-0.09	-0.52	-0.52	-0.09	-0.52	-0.09	
-0.52	-0.51	-0.51	-0.09	-0.09	-0.51	-0.09	-0.26	
-0.09	-0.26	-0.09	-0.26	-0.62	-0.51	-0.52	-0.26	
-0.51	-0.26	-0.62	-0.52	-0.52	-0.09	-0.51	-0.52	
-0.26	-0.52	-0.09	-0.52	-0.51	-0.26	-0.52	-0.51	
-0.26	-0.52	-0.26	-0.09	-0.62	-0.52	-0.26	-0.52	
-0.52	-0.51	-0.62	-0.51	-0.51	-0.62	-0.26	-0.10	
Map policy
v	>	v	v	v	v	v	v	
v	>	v	v	>	v	v	v	
v	v	>	>	>	v	>	v	
>	>	>	v	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.6120253605100219
mean w [-0.12548357 -0.58716081 -0.32837369 -0.42018059 -0.29990102  0.18626741]
Mean policy from posterior
v	>	>	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.30	-0.42	-0.33	-0.13	-0.59	-0.33	-0.42	-0.42	
-0.33	-0.42	-0.30	-0.33	-0.33	-0.30	-0.33	-0.30	
-0.33	-0.59	-0.59	-0.30	-0.30	-0.59	-0.30	-0.13	
-0.30	-0.13	-0.30	-0.13	-0.42	-0.59	-0.33	-0.13	
-0.59	-0.13	-0.42	-0.33	-0.33	-0.30	-0.59	-0.33	
-0.13	-0.33	-0.30	-0.33	-0.59	-0.13	-0.33	-0.59	
-0.13	-0.33	-0.13	-0.30	-0.42	-0.33	-0.13	-0.33	
-0.33	-0.59	-0.42	-0.59	-0.59	-0.42	-0.13	0.19	
mean = 0.10938322255505184, map = 0.07222204223656037
CVaR policy
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
cvar = , 0.057625465220707106, 0.08750708086235637, 0.09958274931589661, 0.10778384209783332, 0.10938322307070814
==========
iteration 76
==========
weights [-0.79956774 -0.37369359 -0.32428528 -0.04562172 -0.14464077  0.30476431]
expeced value MDP LP -0.8024153454466554
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.70628281 -0.26676153 -0.21040201  0.18967874 -0.0608884   0.58825883]
w_map [-0.55853849 -0.42575662 -0.14673856 -0.21667916 -0.42693926  0.5059712 ] loglik -0.6931470500362646
accepted/total = 1899/3000 = 0.633
-------
true weights [-0.79956774 -0.37369359 -0.32428528 -0.04562172 -0.14464077  0.30476431]
features
4 	4 	3 	3 	2 	4 	4 	2 	
3 	3 	3 	3 	1 	4 	3 	1 	
4 	4 	3 	3 	3 	1 	1 	4 	
0 	4 	4 	1 	2 	3 	4 	0 	
1 	0 	2 	3 	2 	1 	3 	4 	
2 	4 	2 	3 	4 	2 	2 	0 	
1 	3 	3 	3 	4 	2 	4 	1 	
1 	1 	0 	0 	1 	3 	4 	5 	
optimal policy
v	>	>	v	<	v	v	<	
>	>	v	v	v	>	v	<	
^	>	>	v	v	v	v	<	
>	>	v	v	>	>	v	v	
v	v	>	v	<	>	v	<	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
optimal values
-1.17	-1.13	-1.00	-0.96	-1.28	-1.17	-1.04	-1.35	
-1.03	-1.00	-0.96	-0.93	-1.26	-1.04	-0.90	-1.27	
-1.17	-1.06	-0.93	-0.89	-0.89	-0.91	-0.87	-1.00	
-1.86	-1.07	-0.94	-0.85	-0.86	-0.54	-0.50	-1.29	
-1.31	-1.42	-0.80	-0.48	-0.80	-0.73	-0.36	-0.50	
-0.94	-0.62	-0.76	-0.44	-0.50	-0.54	-0.31	-0.87	
-0.85	-0.48	-0.44	-0.40	-0.36	-0.22	0.01	-0.07	
-1.22	-0.85	-1.24	-1.06	-0.26	0.11	0.16	0.30	
map_weights [-0.55853849 -0.42575662 -0.14673856 -0.21667916 -0.42693926  0.5059712 ]
MAP reward
-0.43	-0.43	-0.22	-0.22	-0.15	-0.43	-0.43	-0.15	
-0.22	-0.22	-0.22	-0.22	-0.43	-0.43	-0.22	-0.43	
-0.43	-0.43	-0.22	-0.22	-0.22	-0.43	-0.43	-0.43	
-0.56	-0.43	-0.43	-0.43	-0.15	-0.22	-0.43	-0.56	
-0.43	-0.56	-0.15	-0.22	-0.15	-0.43	-0.22	-0.43	
-0.15	-0.43	-0.15	-0.22	-0.43	-0.15	-0.15	-0.56	
-0.43	-0.22	-0.22	-0.22	-0.43	-0.15	-0.43	-0.43	
-0.43	-0.43	-0.56	-0.56	-0.43	-0.22	-0.43	0.51	
Map policy
v	>	>	v	v	<	v	v	
>	>	v	v	v	v	v	<	
>	>	>	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	>	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.4473456705155954
mean w [-0.36141986 -0.64165334 -0.21610776 -0.10160893 -0.337385    0.19880216]
Mean policy from posterior
v	>	v	v	<	<	v	<	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	v	v	<	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.34	-0.34	-0.10	-0.10	-0.22	-0.34	-0.34	-0.22	
-0.10	-0.10	-0.10	-0.10	-0.64	-0.34	-0.10	-0.64	
-0.34	-0.34	-0.10	-0.10	-0.10	-0.64	-0.64	-0.34	
-0.36	-0.34	-0.34	-0.64	-0.22	-0.10	-0.34	-0.36	
-0.64	-0.36	-0.22	-0.10	-0.22	-0.64	-0.10	-0.34	
-0.22	-0.34	-0.22	-0.10	-0.34	-0.22	-0.22	-0.36	
-0.64	-0.10	-0.10	-0.10	-0.34	-0.22	-0.34	-0.64	
-0.64	-0.64	-0.36	-0.36	-0.64	-0.10	-0.34	0.20	
mean = 0.038538029592005696, map = 0.42294515449762615
CVaR policy
v	>	>	v	<	v	v	v	
>	>	v	v	v	v	v	v	
^	>	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	>	v	v	<	<	v	<	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	v	v	<	<	v	<	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	v	v	<	<	v	<	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	>	v	v	<	<	v	<	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
cvar = , 0.2572146430345219, 0.21542420355264924, 0.18516059637553361, 0.18516059633969517, 0.18516059335871415
==========
iteration 77
==========
weights [-0.23061708 -0.30649039 -0.59327348 -0.4776827  -0.23756431  0.46506819]
expeced value MDP LP -1.6328825901316075
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.30165856  0.27811994  0.16792249 -0.04832952  0.68174765 -0.57994642]
w_map [-0.36441375 -0.34902674 -0.25912701 -0.7436494  -0.35046448  0.0489527 ] loglik 0.0
accepted/total = 2169/3000 = 0.723
-------
true weights [-0.23061708 -0.30649039 -0.59327348 -0.4776827  -0.23756431  0.46506819]
features
4 	2 	2 	4 	4 	3 	2 	2 	
3 	4 	0 	2 	0 	2 	2 	0 	
0 	4 	4 	4 	0 	2 	0 	3 	
0 	0 	2 	3 	3 	4 	3 	2 	
1 	0 	0 	1 	1 	0 	3 	3 	
3 	2 	4 	2 	0 	4 	0 	4 	
4 	2 	1 	1 	2 	4 	4 	1 	
0 	3 	3 	2 	3 	3 	3 	5 	
optimal policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	>	>	>	.	
optimal values
-3.09	-3.01	-2.91	-2.34	-2.12	-2.58	-2.63	-2.41	
-2.89	-2.45	-2.34	-2.48	-1.90	-2.16	-2.06	-1.83	
-2.43	-2.23	-2.13	-1.91	-1.69	-1.59	-1.48	-1.62	
-2.22	-2.01	-2.16	-1.83	-1.47	-1.00	-1.26	-1.15	
-2.09	-1.80	-1.59	-1.37	-1.07	-0.77	-0.79	-0.56	
-2.55	-2.16	-1.58	-1.36	-0.77	-0.55	-0.31	-0.09	
-2.30	-2.08	-1.50	-1.21	-0.91	-0.32	-0.09	0.15	
-2.51	-2.42	-1.97	-1.55	-0.97	-0.49	-0.02	0.47	
map_weights [-0.36441375 -0.34902674 -0.25912701 -0.7436494  -0.35046448  0.0489527 ]
MAP reward
-0.35	-0.26	-0.26	-0.35	-0.35	-0.74	-0.26	-0.26	
-0.74	-0.35	-0.36	-0.26	-0.36	-0.26	-0.26	-0.36	
-0.36	-0.35	-0.35	-0.35	-0.36	-0.26	-0.36	-0.74	
-0.36	-0.36	-0.26	-0.74	-0.74	-0.35	-0.74	-0.26	
-0.35	-0.36	-0.36	-0.35	-0.35	-0.36	-0.74	-0.74	
-0.74	-0.26	-0.35	-0.26	-0.36	-0.35	-0.36	-0.35	
-0.35	-0.26	-0.35	-0.35	-0.26	-0.35	-0.35	-0.35	
-0.36	-0.74	-0.74	-0.26	-0.74	-0.74	-0.74	0.05	
Map policy
>	>	>	v	v	v	v	v	
>	v	v	>	>	v	<	v	
>	>	v	>	>	v	<	v	
v	>	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
expeced value MDP LP -2.2585012368186783
mean w [-0.31977405 -0.19711745 -0.28387823 -0.62722581 -0.29743676 -0.27605632]
Mean policy from posterior
>	v	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	>	>	v	<	v	
v	>	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
Mean rewards
-0.30	-0.28	-0.28	-0.30	-0.30	-0.63	-0.28	-0.28	
-0.63	-0.30	-0.32	-0.28	-0.32	-0.28	-0.28	-0.32	
-0.32	-0.30	-0.30	-0.30	-0.32	-0.28	-0.32	-0.63	
-0.32	-0.32	-0.28	-0.63	-0.63	-0.30	-0.63	-0.28	
-0.20	-0.32	-0.32	-0.20	-0.20	-0.32	-0.63	-0.63	
-0.63	-0.28	-0.30	-0.28	-0.32	-0.30	-0.32	-0.30	
-0.30	-0.28	-0.20	-0.20	-0.28	-0.30	-0.30	-0.20	
-0.32	-0.63	-0.63	-0.28	-0.63	-0.63	-0.63	-0.28	
mean = 0.3822347244059894, map = 0.39502632063674614
CVaR policy
>	>	v	v	v	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	v	v	v	v	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	v	v	v	v	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	>	>	v	<	v	
v	>	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	>	>	v	<	v	
v	>	v	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
cvar = , 0.3128772879943096, 0.32472256975429903, 0.36328865686519607, 0.39284213654616273, 0.3929727360730264
==========
iteration 78
==========
weights [-0.56973416 -0.21635767 -0.19048689 -0.55703297 -0.51310729  0.13690242]
expeced value MDP LP -2.1029111882317597
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.39607947 -0.17696706 -0.40525318 -0.53034294 -0.17646595  0.57893843]
w_map [-0.34387508 -0.34991377 -0.47838975 -0.65388655 -0.2736907  -0.1672702 ] loglik 0.0
accepted/total = 2364/3000 = 0.788
-------
true weights [-0.56973416 -0.21635767 -0.19048689 -0.55703297 -0.51310729  0.13690242]
features
0 	0 	0 	4 	1 	2 	4 	2 	
0 	4 	0 	2 	3 	3 	4 	0 	
1 	4 	3 	0 	4 	3 	3 	1 	
1 	4 	3 	0 	4 	0 	1 	4 	
3 	4 	0 	3 	0 	2 	4 	1 	
4 	0 	2 	3 	0 	1 	4 	1 	
1 	4 	0 	1 	3 	3 	0 	0 	
2 	0 	2 	2 	2 	1 	1 	5 	
optimal policy
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.27	-4.61	-4.14	-3.61	-3.13	-2.94	-2.78	-2.29	
-3.73	-4.08	-3.75	-3.22	-3.31	-2.87	-2.60	-2.12	
-3.20	-3.60	-3.21	-3.06	-2.78	-2.33	-2.10	-1.56	
-3.01	-3.12	-2.68	-2.52	-2.29	-1.80	-1.56	-1.36	
-2.82	-2.64	-2.14	-1.97	-1.80	-1.24	-1.36	-0.86	
-2.29	-2.14	-1.59	-1.43	-1.60	-1.06	-1.15	-0.65	
-1.79	-1.91	-1.41	-0.88	-1.04	-0.85	-0.65	-0.43	
-1.59	-1.41	-0.85	-0.67	-0.48	-0.30	-0.08	0.14	
map_weights [-0.34387508 -0.34991377 -0.47838975 -0.65388655 -0.2736907  -0.1672702 ]
MAP reward
-0.34	-0.34	-0.34	-0.27	-0.35	-0.48	-0.27	-0.48	
-0.34	-0.27	-0.34	-0.48	-0.65	-0.65	-0.27	-0.34	
-0.35	-0.27	-0.65	-0.34	-0.27	-0.65	-0.65	-0.35	
-0.35	-0.27	-0.65	-0.34	-0.27	-0.34	-0.35	-0.27	
-0.65	-0.27	-0.34	-0.65	-0.34	-0.48	-0.27	-0.35	
-0.27	-0.34	-0.48	-0.65	-0.34	-0.35	-0.27	-0.35	
-0.35	-0.27	-0.34	-0.35	-0.65	-0.65	-0.34	-0.34	
-0.48	-0.34	-0.48	-0.48	-0.48	-0.35	-0.35	-0.17	
Map policy
>	v	>	v	>	>	v	v	
>	>	>	v	v	>	>	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	v	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8703387547693928
mean w [-0.39815374 -0.28587846 -0.21258843 -0.41373284 -0.3847627   0.20948937]
Mean policy from posterior
v	v	v	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.40	-0.40	-0.40	-0.38	-0.29	-0.21	-0.38	-0.21	
-0.40	-0.38	-0.40	-0.21	-0.41	-0.41	-0.38	-0.40	
-0.29	-0.38	-0.41	-0.40	-0.38	-0.41	-0.41	-0.29	
-0.29	-0.38	-0.41	-0.40	-0.38	-0.40	-0.29	-0.38	
-0.41	-0.38	-0.40	-0.41	-0.40	-0.21	-0.38	-0.29	
-0.38	-0.40	-0.21	-0.41	-0.40	-0.29	-0.38	-0.29	
-0.29	-0.38	-0.40	-0.29	-0.41	-0.41	-0.40	-0.40	
-0.21	-0.40	-0.21	-0.21	-0.21	-0.29	-0.29	0.21	
mean = 0.002355485568107163, map = 0.7093399056672167
CVaR policy
v	v	>	v	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	>	>	>	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	v	v	v	
v	>	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	>	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.5040635603639454, 0.14995370993242396, 0.0036729888540198985, 0.003869476027872132, 0.0038694659088562844
==========
iteration 79
==========
weights [-0.54965462 -0.14332739 -0.68734027 -0.28927885 -0.31835763  0.14094887]
expeced value MDP LP -1.958254054509639
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.15981782 -0.41362173 -0.11896312  0.06749702 -0.5349953  -0.70600799]
w_map [-0.56501828 -0.43352829 -0.42188864 -0.35974698 -0.41627138 -0.11008082] loglik 0.0
accepted/total = 2388/3000 = 0.796
-------
true weights [-0.54965462 -0.14332739 -0.68734027 -0.28927885 -0.31835763  0.14094887]
features
4 	0 	2 	3 	3 	4 	0 	4 	
0 	0 	4 	2 	0 	0 	2 	0 	
4 	1 	3 	2 	2 	4 	1 	4 	
0 	4 	1 	0 	4 	3 	3 	4 	
3 	4 	0 	4 	0 	2 	1 	2 	
1 	2 	0 	2 	0 	0 	1 	1 	
1 	2 	0 	1 	3 	0 	3 	0 	
3 	4 	1 	3 	1 	2 	4 	5 	
optimal policy
v	v	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	<	
v	<	>	>	>	>	v	v	
v	<	v	v	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.74	-3.69	-3.48	-2.83	-2.56	-2.30	-2.36	-2.30	
-3.46	-3.17	-2.82	-3.19	-2.53	-2.00	-1.83	-2.00	
-2.94	-2.65	-2.53	-2.80	-2.14	-1.46	-1.16	-1.46	
-3.02	-2.56	-2.26	-2.14	-1.61	-1.30	-1.02	-1.33	
-2.50	-2.79	-2.78	-2.26	-1.96	-1.42	-0.74	-1.23	
-2.23	-2.90	-2.47	-2.08	-1.69	-1.15	-0.60	-0.55	
-2.11	-2.38	-1.94	-1.41	-1.28	-1.01	-0.47	-0.41	
-1.99	-1.71	-1.41	-1.28	-1.00	-0.86	-0.18	0.14	
map_weights [-0.56501828 -0.43352829 -0.42188864 -0.35974698 -0.41627138 -0.11008082]
MAP reward
-0.42	-0.57	-0.42	-0.36	-0.36	-0.42	-0.57	-0.42	
-0.57	-0.57	-0.42	-0.42	-0.57	-0.57	-0.42	-0.57	
-0.42	-0.43	-0.36	-0.42	-0.42	-0.42	-0.43	-0.42	
-0.57	-0.42	-0.43	-0.57	-0.42	-0.36	-0.36	-0.42	
-0.36	-0.42	-0.57	-0.42	-0.57	-0.42	-0.43	-0.42	
-0.43	-0.42	-0.57	-0.42	-0.57	-0.57	-0.43	-0.43	
-0.43	-0.42	-0.57	-0.43	-0.36	-0.57	-0.36	-0.57	
-0.36	-0.42	-0.43	-0.36	-0.43	-0.42	-0.42	-0.11	
Map policy
>	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	v	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6896394947291387
mean w [-0.45674603 -0.23165763 -0.33663471 -0.34071066 -0.23305938  0.26046528]
Mean policy from posterior
v	>	v	v	>	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	v	v	v	>	v	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.23	-0.46	-0.34	-0.34	-0.34	-0.23	-0.46	-0.23	
-0.46	-0.46	-0.23	-0.34	-0.46	-0.46	-0.34	-0.46	
-0.23	-0.23	-0.34	-0.34	-0.34	-0.23	-0.23	-0.23	
-0.46	-0.23	-0.23	-0.46	-0.23	-0.34	-0.34	-0.23	
-0.34	-0.23	-0.46	-0.23	-0.46	-0.34	-0.23	-0.34	
-0.23	-0.34	-0.46	-0.34	-0.46	-0.46	-0.23	-0.23	
-0.23	-0.34	-0.46	-0.23	-0.34	-0.46	-0.34	-0.46	
-0.34	-0.23	-0.23	-0.34	-0.23	-0.34	-0.23	0.26	
mean = 0.2931044349991485, map = 0.2048826478785457
CVaR policy
v	>	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
>	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	v	>	v	>	v	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	v	v	v	>	v	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.22400935562570434, 0.2143769121097312, 0.21437682947481496, 0.26584485852810125, 0.2831360787431636
==========
iteration 80
==========
weights [-0.11326459 -0.49346822 -0.27584755 -0.3739412  -0.68835187  0.23218109]
expeced value MDP LP -1.878082627336454
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.19429995  0.5058288   0.34182357  0.70745284  0.24818563  0.16569783]
w_map [-0.216882   -0.32438729 -0.20957435 -0.23001548 -0.81719278 -0.28827507] loglik -2.079441554107987
accepted/total = 1727/3000 = 0.5756666666666667
-------
true weights [-0.11326459 -0.49346822 -0.27584755 -0.3739412  -0.68835187  0.23218109]
features
4 	0 	2 	1 	2 	3 	2 	0 	
1 	1 	4 	0 	0 	0 	2 	4 	
1 	3 	1 	0 	4 	4 	0 	1 	
2 	0 	4 	3 	4 	1 	4 	3 	
0 	3 	0 	1 	3 	0 	1 	0 	
3 	0 	4 	0 	1 	2 	2 	3 	
0 	2 	0 	2 	2 	1 	3 	4 	
0 	0 	2 	4 	1 	0 	3 	5 	
optimal policy
>	v	>	v	v	v	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-3.75	-3.09	-3.11	-2.87	-2.57	-2.57	-2.39	-2.47	
-3.38	-3.01	-3.06	-2.40	-2.31	-2.22	-2.13	-2.45	
-2.91	-2.54	-2.78	-2.31	-2.82	-2.27	-1.87	-1.78	
-2.44	-2.19	-2.62	-2.22	-2.15	-1.60	-1.95	-1.30	
-2.19	-2.10	-1.96	-1.86	-1.48	-1.12	-1.27	-0.93	
-2.10	-1.74	-2.06	-1.38	-1.50	-1.02	-0.79	-0.83	
-1.74	-1.64	-1.38	-1.28	-1.02	-0.75	-0.52	-0.46	
-1.84	-1.74	-1.64	-1.43	-0.75	-0.26	-0.14	0.23	
map_weights [-0.216882   -0.32438729 -0.20957435 -0.23001548 -0.81719278 -0.28827507]
MAP reward
-0.82	-0.22	-0.21	-0.32	-0.21	-0.23	-0.21	-0.22	
-0.32	-0.32	-0.82	-0.22	-0.22	-0.22	-0.21	-0.82	
-0.32	-0.23	-0.32	-0.22	-0.82	-0.82	-0.22	-0.32	
-0.21	-0.22	-0.82	-0.23	-0.82	-0.32	-0.82	-0.23	
-0.22	-0.23	-0.22	-0.32	-0.23	-0.22	-0.32	-0.22	
-0.23	-0.22	-0.82	-0.22	-0.32	-0.21	-0.21	-0.23	
-0.22	-0.21	-0.22	-0.21	-0.21	-0.32	-0.23	-0.82	
-0.22	-0.22	-0.21	-0.82	-0.32	-0.22	-0.23	-0.29	
Map policy
>	v	>	v	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	<	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.7694183486138204
mean w [-0.15470241 -0.30536786 -0.12262447 -0.43672525 -0.59944117  0.00215772]
Mean policy from posterior
v	>	>	v	v	v	v	<	
v	v	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.60	-0.15	-0.12	-0.31	-0.12	-0.44	-0.12	-0.15	
-0.31	-0.31	-0.60	-0.15	-0.15	-0.15	-0.12	-0.60	
-0.31	-0.44	-0.31	-0.15	-0.60	-0.60	-0.15	-0.31	
-0.12	-0.15	-0.60	-0.44	-0.60	-0.31	-0.60	-0.44	
-0.15	-0.44	-0.15	-0.31	-0.44	-0.15	-0.31	-0.15	
-0.44	-0.15	-0.60	-0.15	-0.31	-0.12	-0.12	-0.44	
-0.15	-0.12	-0.15	-0.12	-0.12	-0.31	-0.44	-0.60	
-0.15	-0.15	-0.12	-0.60	-0.31	-0.15	-0.44	0.00	
mean = 0.02881780645742671, map = 0.09537170920901006
CVaR policy
v	>	>	>	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	<	
v	v	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	<	
v	v	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	<	
v	v	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.13996125207185672, 0.032612488407517404, 0.03171178807449704, 0.028817812037042145, 0.02881780624065966
==========
iteration 81
==========
weights [-0.18275572 -0.13082037 -0.49748576 -0.01509867 -0.3286618   0.77055028]
expeced value MDP LP -0.13444942017680966
demonstration
[(56, 2), (48, 1), (49, 1), (50, 2), (42, 2), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.34605045 -0.33398391 -0.60477764 -0.13117818 -0.3146493  -0.5354773 ]
w_map [-0.33612664 -0.2186279  -0.58599388 -0.15167356 -0.65482982 -0.2098212 ] loglik -7.760112978871803e-07
accepted/total = 1495/3000 = 0.49833333333333335
-------
true weights [-0.18275572 -0.13082037 -0.49748576 -0.01509867 -0.3286618   0.77055028]
features
4 	4 	2 	3 	1 	1 	1 	2 	
2 	3 	1 	2 	4 	2 	0 	4 	
3 	4 	4 	3 	4 	2 	0 	0 	
4 	3 	1 	4 	4 	3 	0 	3 	
2 	2 	3 	1 	3 	4 	3 	0 	
0 	4 	3 	4 	3 	3 	0 	3 	
1 	4 	3 	4 	0 	1 	4 	0 	
0 	2 	2 	2 	3 	0 	4 	5 	
optimal policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
optimal values
-0.97	-0.65	-0.92	-0.59	-0.58	-0.46	-0.33	-0.66	
-0.82	-0.32	-0.43	-0.66	-0.67	-0.70	-0.20	-0.16	
-0.32	-0.31	-0.30	-0.16	-0.35	-0.35	-0.02	0.17	
-0.31	0.02	0.03	-0.15	-0.02	0.15	0.17	0.35	
-0.81	-0.34	0.16	0.18	0.31	0.02	0.35	0.37	
-0.36	-0.18	0.15	0.00	0.33	0.35	0.37	0.56	
-0.33	-0.20	0.13	-0.18	0.15	0.22	0.25	0.58	
-0.51	-0.69	-0.37	-0.27	0.23	0.25	0.43	0.77	
map_weights [-0.33612664 -0.2186279  -0.58599388 -0.15167356 -0.65482982 -0.2098212 ]
MAP reward
-0.65	-0.65	-0.59	-0.15	-0.22	-0.22	-0.22	-0.59	
-0.59	-0.15	-0.22	-0.59	-0.65	-0.59	-0.34	-0.65	
-0.15	-0.65	-0.65	-0.15	-0.65	-0.59	-0.34	-0.34	
-0.65	-0.15	-0.22	-0.65	-0.65	-0.15	-0.34	-0.15	
-0.59	-0.59	-0.15	-0.22	-0.15	-0.65	-0.15	-0.34	
-0.34	-0.65	-0.15	-0.65	-0.15	-0.15	-0.34	-0.15	
-0.22	-0.65	-0.15	-0.65	-0.34	-0.22	-0.65	-0.34	
-0.34	-0.59	-0.59	-0.59	-0.15	-0.34	-0.65	-0.21	
Map policy
v	v	v	>	>	>	v	v	
v	v	v	v	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.7015433278093228
mean w [-0.273172   -0.16873682 -0.61337589 -0.08032017 -0.54538707 -0.2611272 ]
Mean policy from posterior
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.55	-0.55	-0.61	-0.08	-0.17	-0.17	-0.17	-0.61	
-0.61	-0.08	-0.17	-0.61	-0.55	-0.61	-0.27	-0.55	
-0.08	-0.55	-0.55	-0.08	-0.55	-0.61	-0.27	-0.27	
-0.55	-0.08	-0.17	-0.55	-0.55	-0.08	-0.27	-0.08	
-0.61	-0.61	-0.08	-0.17	-0.08	-0.55	-0.08	-0.27	
-0.27	-0.55	-0.08	-0.55	-0.08	-0.08	-0.27	-0.08	
-0.17	-0.55	-0.08	-0.55	-0.27	-0.17	-0.55	-0.27	
-0.27	-0.61	-0.61	-0.61	-0.08	-0.27	-0.55	-0.26	
mean = 0.0003469056663961134, map = 0.02335089092865142
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
cvar = , 0.026726338229390134, 0.00264787584527465, 0.00034690592611610893, 0.0003469055940582555, 0.0003469056517070024
==========
iteration 82
==========
weights [-0.0970151  -0.30073936 -0.15961981 -0.07668966 -0.86338445  0.3512142 ]
expeced value MDP LP -1.1579534958900672
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.72568243 -0.17383951 -0.55487869 -0.20479462 -0.30400628  0.03022949]
w_map [-0.35064394 -0.25910649 -0.34482265 -0.32087918 -0.72582631  0.24743225] loglik 0.0
accepted/total = 2136/3000 = 0.712
-------
true weights [-0.0970151  -0.30073936 -0.15961981 -0.07668966 -0.86338445  0.3512142 ]
features
0 	1 	3 	2 	0 	3 	4 	0 	
2 	2 	2 	3 	3 	0 	4 	2 	
1 	4 	0 	0 	4 	0 	4 	2 	
0 	3 	1 	4 	1 	4 	4 	3 	
1 	4 	2 	1 	0 	3 	1 	1 	
3 	3 	4 	0 	2 	1 	3 	0 	
2 	1 	2 	2 	2 	1 	4 	4 	
0 	4 	4 	2 	4 	0 	0 	5 	
optimal policy
v	>	v	v	v	v	>	v	
>	>	v	v	<	v	>	v	
v	v	v	<	v	v	>	v	
v	>	v	>	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	<	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
optimal values
-1.89	-1.87	-1.59	-1.67	-1.67	-1.61	-2.20	-1.35	
-1.81	-1.67	-1.52	-1.52	-1.59	-1.55	-2.12	-1.27	
-1.71	-2.21	-1.38	-1.46	-1.71	-1.46	-1.97	-1.12	
-1.42	-1.36	-1.30	-1.71	-0.85	-1.38	-1.67	-0.97	
-1.34	-1.84	-1.00	-0.85	-0.56	-0.52	-0.82	-0.90	
-1.05	-0.98	-1.42	-0.56	-0.47	-0.45	-0.52	-0.61	
-1.07	-0.92	-0.62	-0.47	-0.31	-0.15	-0.62	-0.52	
-1.15	-1.77	-1.48	-0.62	-0.71	0.15	0.25	0.35	
map_weights [-0.35064394 -0.25910649 -0.34482265 -0.32087918 -0.72582631  0.24743225]
MAP reward
-0.35	-0.26	-0.32	-0.34	-0.35	-0.32	-0.73	-0.35	
-0.34	-0.34	-0.34	-0.32	-0.32	-0.35	-0.73	-0.34	
-0.26	-0.73	-0.35	-0.35	-0.73	-0.35	-0.73	-0.34	
-0.35	-0.32	-0.26	-0.73	-0.26	-0.73	-0.73	-0.32	
-0.26	-0.73	-0.34	-0.26	-0.35	-0.32	-0.26	-0.26	
-0.32	-0.32	-0.73	-0.35	-0.34	-0.26	-0.32	-0.35	
-0.34	-0.26	-0.34	-0.34	-0.34	-0.26	-0.73	-0.73	
-0.35	-0.73	-0.73	-0.34	-0.73	-0.35	-0.35	0.25	
Map policy
>	>	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	v	>	v	v	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.4708556677592886
mean w [-0.21299286 -0.24127113 -0.19874902 -0.372511   -0.6324511   0.31657551]
Mean policy from posterior
v	v	v	v	v	v	>	v	
>	>	v	<	v	v	>	v	
v	>	v	<	v	v	>	v	
v	>	v	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
Mean rewards
-0.21	-0.24	-0.37	-0.20	-0.21	-0.37	-0.63	-0.21	
-0.20	-0.20	-0.20	-0.37	-0.37	-0.21	-0.63	-0.20	
-0.24	-0.63	-0.21	-0.21	-0.63	-0.21	-0.63	-0.20	
-0.21	-0.37	-0.24	-0.63	-0.24	-0.63	-0.63	-0.37	
-0.24	-0.63	-0.20	-0.24	-0.21	-0.37	-0.24	-0.24	
-0.37	-0.37	-0.63	-0.21	-0.20	-0.24	-0.37	-0.21	
-0.20	-0.24	-0.20	-0.20	-0.20	-0.24	-0.63	-0.63	
-0.21	-0.63	-0.63	-0.20	-0.63	-0.21	-0.21	0.32	
mean = 0.02965696200316592, map = 0.0713806732324187
CVaR policy
v	>	v	v	v	v	>	v	
>	>	v	>	v	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	>	v	<	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
v	>	>	v	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	>	v	<	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
v	>	>	v	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
cvar = , 0.05911048029854227, 0.04355146915901664, 0.050483006592466406, 0.04238062981689117, 0.04238062978602053
==========
iteration 83
==========
weights [-0.56886229 -0.39226077 -0.38195774 -0.41244511 -0.31038352  0.33194363]
expeced value MDP LP -2.414623707571926
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.94462655 -0.16744978 -0.09858378 -0.0344392  -0.10071628  0.24205919]
w_map [-0.43166207 -0.55030939 -0.3561541  -0.35370321 -0.46204025  0.21305994] loglik 0.0
accepted/total = 2432/3000 = 0.8106666666666666
-------
true weights [-0.56886229 -0.39226077 -0.38195774 -0.41244511 -0.31038352  0.33194363]
features
1 	3 	1 	1 	0 	3 	3 	2 	
0 	4 	0 	0 	2 	4 	0 	3 	
0 	1 	2 	2 	2 	3 	4 	0 	
0 	4 	4 	4 	0 	1 	4 	4 	
3 	0 	3 	1 	3 	0 	1 	2 	
3 	2 	3 	3 	0 	2 	1 	0 	
1 	4 	0 	0 	3 	0 	3 	3 	
1 	0 	1 	3 	4 	0 	3 	5 	
optimal policy
>	v	>	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.67	-4.33	-4.15	-3.79	-3.44	-2.93	-2.80	-2.64	
-4.48	-3.95	-3.86	-3.44	-2.90	-2.54	-2.41	-2.28	
-4.21	-3.68	-3.32	-2.97	-2.61	-2.25	-1.86	-1.88	
-3.87	-3.34	-3.06	-2.78	-2.49	-1.94	-1.56	-1.33	
-3.69	-3.47	-2.96	-2.58	-2.21	-1.81	-1.27	-1.03	
-3.32	-2.93	-2.60	-2.21	-1.81	-1.26	-0.88	-0.65	
-2.94	-2.58	-2.29	-1.91	-1.36	-1.06	-0.50	-0.08	
-2.66	-2.29	-1.74	-1.36	-0.96	-0.65	-0.08	0.33	
map_weights [-0.43166207 -0.55030939 -0.3561541  -0.35370321 -0.46204025  0.21305994]
MAP reward
-0.55	-0.35	-0.55	-0.55	-0.43	-0.35	-0.35	-0.36	
-0.43	-0.46	-0.43	-0.43	-0.36	-0.46	-0.43	-0.35	
-0.43	-0.55	-0.36	-0.36	-0.36	-0.35	-0.46	-0.43	
-0.43	-0.46	-0.46	-0.46	-0.43	-0.55	-0.46	-0.46	
-0.35	-0.43	-0.35	-0.55	-0.35	-0.43	-0.55	-0.36	
-0.35	-0.36	-0.35	-0.35	-0.43	-0.36	-0.55	-0.43	
-0.55	-0.46	-0.43	-0.43	-0.35	-0.43	-0.35	-0.35	
-0.55	-0.43	-0.55	-0.35	-0.46	-0.43	-0.35	0.21	
Map policy
v	v	v	v	v	>	>	v	
v	>	v	>	v	>	>	v	
v	>	>	>	v	v	>	v	
v	v	v	>	v	v	>	v	
v	>	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.3977076035530307
mean w [-0.41181155 -0.33680454 -0.34447544 -0.340726   -0.31724308 -0.08000913]
Mean policy from posterior
>	v	>	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.34	-0.34	-0.34	-0.34	-0.41	-0.34	-0.34	-0.34	
-0.41	-0.32	-0.41	-0.41	-0.34	-0.32	-0.41	-0.34	
-0.41	-0.34	-0.34	-0.34	-0.34	-0.34	-0.32	-0.41	
-0.41	-0.32	-0.32	-0.32	-0.41	-0.34	-0.32	-0.32	
-0.34	-0.41	-0.34	-0.34	-0.34	-0.41	-0.34	-0.34	
-0.34	-0.34	-0.34	-0.34	-0.41	-0.34	-0.34	-0.41	
-0.34	-0.32	-0.41	-0.41	-0.34	-0.41	-0.34	-0.34	
-0.34	-0.41	-0.34	-0.34	-0.32	-0.41	-0.34	-0.08	
mean = 0.003607043589046288, map = 0.3254371796120856
CVaR policy
v	v	>	v	v	v	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	>	v	
>	v	v	>	>	>	v	v	
v	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	>	v	
>	v	v	>	>	>	v	v	
v	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.1047427732497499, 0.06955995998587294, 0.029769143473495685, 0.0023988723846724014, 0.0012627464274586053
==========
iteration 84
==========
weights [-0.05882344 -0.44639466 -0.61216112 -0.57223911 -0.20929647  0.22642385]
expeced value MDP LP -1.361321481278447
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.37758861  0.66243073  0.32204157 -0.09250901  0.48552974  0.26571518]
w_map [-0.24921621 -0.60805091 -0.59722561 -0.35585613 -0.2752998  -0.09520178] loglik -5.654783308273181e-10
accepted/total = 1861/3000 = 0.6203333333333333
-------
true weights [-0.05882344 -0.44639466 -0.61216112 -0.57223911 -0.20929647  0.22642385]
features
2 	0 	4 	0 	3 	2 	3 	4 	
3 	1 	0 	2 	0 	4 	2 	3 	
0 	2 	0 	4 	0 	2 	4 	3 	
1 	2 	4 	2 	1 	1 	4 	3 	
2 	4 	3 	1 	4 	4 	1 	4 	
2 	2 	2 	0 	3 	4 	4 	0 	
0 	0 	1 	1 	0 	2 	4 	0 	
2 	1 	1 	4 	2 	2 	1 	5 	
optimal policy
>	>	v	<	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
optimal values
-2.33	-1.73	-1.69	-1.73	-1.82	-2.05	-2.12	-1.99	
-2.48	-1.93	-1.50	-1.86	-1.26	-1.45	-1.56	-1.80	
-2.09	-2.05	-1.45	-1.41	-1.21	-1.56	-0.96	-1.24	
-2.51	-2.24	-1.65	-1.76	-1.16	-0.96	-0.75	-0.68	
-2.51	-1.92	-1.72	-1.16	-0.72	-0.52	-0.55	-0.11	
-2.26	-2.13	-1.54	-0.93	-0.88	-0.31	-0.11	0.10	
-1.67	-1.63	-1.58	-1.15	-0.71	-0.66	-0.05	0.17	
-2.26	-2.06	-1.78	-1.35	-1.31	-0.83	-0.22	0.23	
map_weights [-0.24921621 -0.60805091 -0.59722561 -0.35585613 -0.2752998  -0.09520178]
MAP reward
-0.60	-0.25	-0.28	-0.25	-0.36	-0.60	-0.36	-0.28	
-0.36	-0.61	-0.25	-0.60	-0.25	-0.28	-0.60	-0.36	
-0.25	-0.60	-0.25	-0.28	-0.25	-0.60	-0.28	-0.36	
-0.61	-0.60	-0.28	-0.60	-0.61	-0.61	-0.28	-0.36	
-0.60	-0.28	-0.36	-0.61	-0.28	-0.28	-0.61	-0.28	
-0.60	-0.60	-0.60	-0.25	-0.36	-0.28	-0.28	-0.25	
-0.25	-0.25	-0.61	-0.61	-0.25	-0.60	-0.28	-0.25	
-0.60	-0.61	-0.61	-0.28	-0.60	-0.60	-0.61	-0.10	
Map policy
>	>	v	>	v	>	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.8242163410785215
mean w [-0.13199926 -0.37322804 -0.47148    -0.43538897 -0.35151982  0.13794055]
Mean policy from posterior
>	>	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	>	v	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.47	-0.13	-0.35	-0.13	-0.44	-0.47	-0.44	-0.35	
-0.44	-0.37	-0.13	-0.47	-0.13	-0.35	-0.47	-0.44	
-0.13	-0.47	-0.13	-0.35	-0.13	-0.47	-0.35	-0.44	
-0.37	-0.47	-0.35	-0.47	-0.37	-0.37	-0.35	-0.44	
-0.47	-0.35	-0.44	-0.37	-0.35	-0.35	-0.37	-0.35	
-0.47	-0.47	-0.47	-0.13	-0.44	-0.35	-0.35	-0.13	
-0.13	-0.13	-0.37	-0.37	-0.13	-0.47	-0.35	-0.13	
-0.47	-0.37	-0.37	-0.35	-0.47	-0.47	-0.37	0.14	
mean = 0.0680836323847327, map = 0.08175963884910997
CVaR policy
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	>	v	
>	>	v	>	>	>	>	v	
v	v	v	v	>	>	>	v	
v	>	>	v	v	>	v	v	
v	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	>	>	v	
>	>	v	>	v	>	>	v	
>	>	v	>	v	>	>	v	
v	v	v	v	v	>	>	v	
v	>	>	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	>	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
v	>	v	v	v	v	v	v	
v	>	>	v	>	v	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
v	>	v	v	v	v	v	v	
v	>	>	v	>	v	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	>	v	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
cvar = , 0.47227076929921763, 0.3121766151905261, 0.09768868899701144, 0.06808363210907542, 0.06808363248030735
==========
iteration 85
==========
weights [-0.26584721 -0.30756227 -0.47704667 -0.36451169 -0.47507618  0.49858905]
expeced value MDP LP -1.85289582480823
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.44994261 -0.16846881 -0.03692096 -0.21536971 -0.84294225 -0.10426407]
w_map [-0.40817024 -0.35461226 -0.40610857 -0.42014737 -0.45068711  0.40383209] loglik 0.0
accepted/total = 2382/3000 = 0.794
-------
true weights [-0.26584721 -0.30756227 -0.47704667 -0.36451169 -0.47507618  0.49858905]
features
1 	4 	2 	4 	4 	3 	4 	2 	
3 	1 	4 	3 	0 	3 	3 	4 	
1 	2 	4 	3 	1 	2 	4 	0 	
1 	3 	1 	3 	3 	4 	1 	4 	
0 	1 	3 	2 	3 	2 	1 	1 	
4 	1 	4 	1 	2 	2 	4 	4 	
1 	2 	2 	1 	0 	1 	3 	0 	
1 	3 	3 	3 	3 	3 	2 	5 	
optimal policy
v	v	>	v	v	v	v	v	
v	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.75	-3.85	-3.64	-3.20	-2.86	-2.65	-2.42	-2.20	
-3.48	-3.41	-3.20	-2.75	-2.41	-2.31	-1.97	-1.74	
-3.15	-3.13	-2.85	-2.46	-2.17	-2.08	-1.62	-1.28	
-2.87	-2.68	-2.40	-2.12	-1.88	-1.62	-1.16	-1.02	
-2.59	-2.34	-2.11	-1.77	-1.53	-1.33	-0.86	-0.55	
-2.51	-2.06	-1.77	-1.30	-1.18	-0.92	-0.61	-0.25	
-2.22	-1.94	-1.47	-1.01	-0.71	-0.45	-0.14	0.23	
-2.06	-1.77	-1.42	-1.07	-0.71	-0.35	0.02	0.50	
map_weights [-0.40817024 -0.35461226 -0.40610857 -0.42014737 -0.45068711  0.40383209]
MAP reward
-0.35	-0.45	-0.41	-0.45	-0.45	-0.42	-0.45	-0.41	
-0.42	-0.35	-0.45	-0.42	-0.41	-0.42	-0.42	-0.45	
-0.35	-0.41	-0.45	-0.42	-0.35	-0.41	-0.45	-0.41	
-0.35	-0.42	-0.35	-0.42	-0.42	-0.45	-0.35	-0.45	
-0.41	-0.35	-0.42	-0.41	-0.42	-0.41	-0.35	-0.35	
-0.45	-0.35	-0.45	-0.35	-0.41	-0.41	-0.45	-0.45	
-0.35	-0.41	-0.41	-0.35	-0.41	-0.35	-0.42	-0.41	
-0.35	-0.42	-0.42	-0.42	-0.42	-0.42	-0.41	0.40	
Map policy
v	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	v	>	v	>	>	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9517882049683752
mean w [-0.35235022 -0.4519993  -0.31871836 -0.238645   -0.40299297  0.09811024]
Mean policy from posterior
v	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	v	v	>	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.45	-0.40	-0.32	-0.40	-0.40	-0.24	-0.40	-0.32	
-0.24	-0.45	-0.40	-0.24	-0.35	-0.24	-0.24	-0.40	
-0.45	-0.32	-0.40	-0.24	-0.45	-0.32	-0.40	-0.35	
-0.45	-0.24	-0.45	-0.24	-0.24	-0.40	-0.45	-0.40	
-0.35	-0.45	-0.24	-0.32	-0.24	-0.32	-0.45	-0.45	
-0.40	-0.45	-0.40	-0.45	-0.32	-0.32	-0.40	-0.40	
-0.45	-0.32	-0.32	-0.45	-0.35	-0.45	-0.24	-0.35	
-0.45	-0.24	-0.24	-0.24	-0.24	-0.24	-0.32	0.10	
mean = 0.3194187348798563, map = 0.12712309060404303
CVaR policy
>	>	>	>	v	v	>	v	
v	>	>	>	v	v	v	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	>	v	v	v	v	
v	>	v	>	v	v	v	v	
>	v	v	>	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	v	v	>	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	v	v	>	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	v	v	>	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.24829952003321143, 0.29617321074868563, 0.3056602646703013, 0.3194187349567055, 0.3194187349396591
==========
iteration 86
==========
weights [-0.75097374 -0.0187764  -0.15173761 -0.13237438 -0.48458289  0.40039735]
expeced value MDP LP -1.0553840498000273
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.51041362  0.12520145  0.38241907 -0.27194411 -0.70875137 -0.03572241]
w_map [-0.78001086 -0.25803741 -0.50631918 -0.09370132 -0.23607287  0.06426751] loglik -3.21865201158289e-10
accepted/total = 1881/3000 = 0.627
-------
true weights [-0.75097374 -0.0187764  -0.15173761 -0.13237438 -0.48458289  0.40039735]
features
3 	4 	3 	0 	0 	3 	3 	3 	
4 	3 	3 	2 	1 	3 	3 	0 	
2 	2 	2 	3 	1 	2 	1 	4 	
1 	2 	3 	0 	3 	2 	4 	4 	
1 	4 	4 	1 	4 	3 	1 	1 	
4 	2 	4 	2 	4 	0 	2 	0 	
0 	3 	1 	3 	1 	0 	0 	2 	
1 	2 	0 	2 	4 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	<	v	<	
>	>	>	>	v	v	<	<	
>	>	^	v	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-1.92	-1.81	-1.34	-1.83	-1.69	-1.20	-1.21	-1.33	
-1.81	-1.34	-1.21	-1.09	-0.95	-1.07	-1.08	-1.82	
-1.48	-1.35	-1.21	-1.06	-0.94	-0.95	-0.96	-1.44	
-1.47	-1.46	-1.33	-1.63	-0.93	-0.81	-1.02	-1.00	
-1.47	-1.49	-1.36	-0.89	-1.14	-0.66	-0.54	-0.52	
-1.49	-1.01	-1.22	-0.88	-1.09	-1.27	-0.53	-0.51	
-1.61	-0.87	-0.74	-0.73	-0.61	-0.86	-0.38	0.24	
-1.02	-1.01	-1.48	-0.74	-0.59	-0.11	0.38	0.40	
map_weights [-0.78001086 -0.25803741 -0.50631918 -0.09370132 -0.23607287  0.06426751]
MAP reward
-0.09	-0.24	-0.09	-0.78	-0.78	-0.09	-0.09	-0.09	
-0.24	-0.09	-0.09	-0.51	-0.26	-0.09	-0.09	-0.78	
-0.51	-0.51	-0.51	-0.09	-0.26	-0.51	-0.26	-0.24	
-0.26	-0.51	-0.09	-0.78	-0.09	-0.51	-0.24	-0.24	
-0.26	-0.24	-0.24	-0.26	-0.24	-0.09	-0.26	-0.26	
-0.24	-0.51	-0.24	-0.51	-0.24	-0.78	-0.51	-0.78	
-0.78	-0.09	-0.26	-0.09	-0.26	-0.78	-0.78	-0.51	
-0.26	-0.51	-0.78	-0.51	-0.24	-0.24	-0.26	0.06	
Map policy
>	>	v	v	v	v	v	<	
>	>	>	v	v	<	<	v	
v	>	>	>	v	<	v	v	
v	>	v	>	v	v	>	v	
>	>	v	>	v	<	<	v	
>	v	v	v	v	<	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.7261142775233438
mean w [-0.68871713 -0.15058239 -0.37131003 -0.16112536 -0.274198   -0.05135302]
Mean policy from posterior
>	>	v	v	v	v	v	<	
>	>	>	>	v	<	v	v	
v	>	>	>	v	<	v	v	
v	>	v	v	v	v	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	<	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.16	-0.27	-0.16	-0.69	-0.69	-0.16	-0.16	-0.16	
-0.27	-0.16	-0.16	-0.37	-0.15	-0.16	-0.16	-0.69	
-0.37	-0.37	-0.37	-0.16	-0.15	-0.37	-0.15	-0.27	
-0.15	-0.37	-0.16	-0.69	-0.16	-0.37	-0.27	-0.27	
-0.15	-0.27	-0.27	-0.15	-0.27	-0.16	-0.15	-0.15	
-0.27	-0.37	-0.27	-0.37	-0.27	-0.69	-0.37	-0.69	
-0.69	-0.16	-0.15	-0.16	-0.15	-0.69	-0.69	-0.37	
-0.15	-0.37	-0.69	-0.37	-0.27	-0.27	-0.15	-0.05	
mean = 0.25289442928554595, map = 0.4300947412038949
CVaR policy
>	>	v	v	>	v	v	<	
>	>	>	v	>	>	v	v	
v	>	>	>	v	v	v	v	
v	>	v	v	v	v	v	v	
>	v	v	v	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	v	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
>	v	>	v	v	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	<	
v	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	>	v	v	v	v	v	v	
>	v	>	v	v	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	>	v	v	
v	>	>	>	v	>	v	v	
v	>	v	v	v	v	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	<	v	v	
v	>	>	>	v	<	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
cvar = , 0.1351092408380925, 0.1410039462052124, 0.18967243609587614, 0.22285344630828652, 0.24941464511235645
==========
iteration 87
==========
weights [-0.02163477 -0.78721642 -0.11620583 -0.11232309 -0.39244965  0.44686155]
expeced value MDP LP -0.6133073751083885
demonstration
[(56, 1), (57, 2), (49, 2), (41, 1), (42, 2), (34, 1), (35, 1), (36, 3), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.09870436  0.25693665 -0.70445688  0.58292222 -0.17820689  0.23754053]
w_map [-0.04044327 -0.76461264 -0.1873002  -0.08420832 -0.48958805  0.36312944] loglik -0.752492413077281
accepted/total = 174/3000 = 0.058
-------
true weights [-0.02163477 -0.78721642 -0.11620583 -0.11232309 -0.39244965  0.44686155]
features
0 	2 	2 	4 	1 	4 	0 	1 	
4 	2 	0 	2 	4 	1 	4 	2 	
3 	4 	3 	0 	4 	2 	1 	0 	
4 	4 	0 	1 	1 	3 	0 	4 	
1 	2 	2 	2 	3 	2 	3 	4 	
1 	3 	3 	4 	0 	4 	2 	2 	
3 	0 	4 	4 	0 	4 	2 	1 	
2 	3 	1 	0 	0 	4 	2 	5 	
optimal policy
>	>	v	v	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	v	<	>	v	v	v	
>	>	v	v	>	>	v	<	
>	>	>	>	v	>	v	<	
>	>	^	>	v	>	v	<	
>	^	>	v	v	>	v	v	
>	^	>	>	>	>	>	.	
optimal values
-0.84	-0.83	-0.72	-1.10	-1.82	-1.35	-0.97	-1.35	
-1.10	-0.72	-0.61	-0.72	-1.05	-1.06	-0.95	-0.57	
-1.08	-0.98	-0.59	-0.61	-0.66	-0.27	-0.83	-0.46	
-1.26	-0.87	-0.49	-1.14	-0.94	-0.16	-0.05	-0.44	
-1.36	-0.58	-0.47	-0.36	-0.24	-0.14	-0.02	-0.42	
-1.46	-0.68	-0.58	-0.52	-0.13	-0.30	0.09	-0.03	
-0.80	-0.70	-0.89	-0.50	-0.11	-0.19	0.21	-0.34	
-0.91	-0.80	-0.90	-0.11	-0.09	-0.07	0.33	0.45	
map_weights [-0.04044327 -0.76461264 -0.1873002  -0.08420832 -0.48958805  0.36312944]
MAP reward
-0.04	-0.19	-0.19	-0.49	-0.76	-0.49	-0.04	-0.76	
-0.49	-0.19	-0.04	-0.19	-0.49	-0.76	-0.49	-0.19	
-0.08	-0.49	-0.08	-0.04	-0.49	-0.19	-0.76	-0.04	
-0.49	-0.49	-0.04	-0.76	-0.76	-0.08	-0.04	-0.49	
-0.76	-0.19	-0.19	-0.19	-0.08	-0.19	-0.08	-0.49	
-0.76	-0.08	-0.08	-0.49	-0.04	-0.49	-0.19	-0.19	
-0.08	-0.04	-0.49	-0.49	-0.04	-0.49	-0.19	-0.76	
-0.19	-0.08	-0.76	-0.04	-0.04	-0.49	-0.19	0.36	
Map policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	>	v	
>	>	v	<	>	v	v	v	
>	>	v	v	>	>	v	<	
>	>	>	>	v	>	v	<	
>	>	^	>	v	>	v	<	
>	^	>	v	v	>	v	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -0.48627264832950473
mean w [-0.02567595 -0.590507   -0.18961306 -0.04468936 -0.35667757  0.6388466 ]
Mean policy from posterior
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	>	v	
>	>	v	<	>	v	v	v	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	<	
>	>	>	>	v	>	v	<	
>	^	>	v	v	>	v	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.03	-0.19	-0.19	-0.36	-0.59	-0.36	-0.03	-0.59	
-0.36	-0.19	-0.03	-0.19	-0.36	-0.59	-0.36	-0.19	
-0.04	-0.36	-0.04	-0.03	-0.36	-0.19	-0.59	-0.03	
-0.36	-0.36	-0.03	-0.59	-0.59	-0.04	-0.03	-0.36	
-0.59	-0.19	-0.19	-0.19	-0.04	-0.19	-0.04	-0.36	
-0.59	-0.04	-0.04	-0.36	-0.03	-0.36	-0.19	-0.19	
-0.04	-0.03	-0.36	-0.36	-0.03	-0.36	-0.19	-0.59	
-0.19	-0.04	-0.59	-0.03	-0.03	-0.36	-0.19	0.64	
mean = 0.00841329328309659, map = 0.0014108756192067196
CVaR policy
v	v	v	>	v	>	v	<	
v	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	<	>	v	
v	v	>	>	v	<	v	v	
>	v	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	<	
v	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	<	>	v	
>	>	>	>	v	<	v	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
v	>	v	v	v	v	v	v	
>	>	v	<	v	v	v	v	
>	>	v	v	v	>	v	<	
>	v	v	>	v	<	v	v	
>	>	>	>	v	<	v	v	
>	^	>	v	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	<	>	v	v	v	
>	>	v	v	v	>	v	<	
>	v	>	>	v	<	v	v	
>	>	>	>	v	<	v	v	
>	^	>	v	v	>	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	<	>	v	v	v	
>	>	v	v	v	>	v	<	
>	v	>	>	v	<	v	<	
>	>	>	>	v	<	v	v	
>	^	>	v	v	>	v	v	
>	^	>	>	>	>	>	.	
cvar = , 0.7074002193503689, 0.5728186794041329, 0.16744222258268115, 0.04498546188516506, 0.03829027968945864
==========
iteration 88
==========
weights [-0.06630414 -0.07244412 -0.08869352 -0.20137503 -0.97022566  0.02448124]
expeced value MDP LP -0.928148503246208
demonstration
[(56, 2), (48, 1), (49, 2), (41, 2), (33, 2), (25, 1), (26, 1), (27, 2), (19, 1), (20, 1), (21, 1), (22, 3), (30, 1), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.44949028 -0.05753357  0.06716195 -0.3008924  -0.54311502 -0.63610336]
w_map [-0.17301179 -0.1113818  -0.03021865 -0.12181585 -0.96972846 -0.03918501] loglik -3.4659436159398957
accepted/total = 862/3000 = 0.28733333333333333
-------
true weights [-0.06630414 -0.07244412 -0.08869352 -0.20137503 -0.97022566  0.02448124]
features
0 	4 	4 	0 	4 	0 	1 	0 	
4 	1 	0 	4 	0 	2 	4 	3 	
0 	1 	2 	1 	2 	0 	1 	3 	
2 	1 	1 	2 	4 	4 	3 	1 	
0 	0 	4 	4 	1 	3 	1 	1 	
4 	0 	4 	3 	3 	4 	1 	0 	
1 	1 	0 	4 	0 	0 	0 	0 	
3 	1 	1 	4 	0 	4 	4 	5 	
optimal policy
v	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	>	^	v	v	v	v	
>	^	^	>	v	>	v	v	
^	^	>	>	v	v	v	v	
>	^	<	>	>	>	>	v	
^	^	^	>	^	^	>	.	
optimal values
-1.94	-1.89	-1.83	-1.73	-1.68	-0.72	-0.77	-0.71	
-1.89	-0.93	-0.87	-1.68	-0.72	-0.66	-1.48	-0.65	
-0.93	-0.87	-0.81	-0.73	-0.66	-0.58	-0.52	-0.45	
-1.01	-0.94	-0.87	-0.81	-1.47	-1.41	-0.45	-0.25	
-1.05	-0.99	-1.83	-1.47	-0.50	-0.45	-0.25	-0.18	
-2.01	-1.05	-1.60	-0.63	-0.44	-1.14	-0.18	-0.11	
-1.17	-1.11	-1.17	-1.21	-0.24	-0.17	-0.11	-0.04	
-1.36	-1.17	-1.23	-1.27	-0.30	-1.14	-0.95	0.02	
map_weights [-0.17301179 -0.1113818  -0.03021865 -0.12181585 -0.96972846 -0.03918501]
MAP reward
-0.17	-0.97	-0.97	-0.17	-0.97	-0.17	-0.11	-0.17	
-0.97	-0.11	-0.17	-0.97	-0.17	-0.03	-0.97	-0.12	
-0.17	-0.11	-0.03	-0.11	-0.03	-0.17	-0.11	-0.12	
-0.03	-0.11	-0.11	-0.03	-0.97	-0.97	-0.12	-0.11	
-0.17	-0.17	-0.97	-0.97	-0.11	-0.12	-0.11	-0.11	
-0.97	-0.17	-0.97	-0.12	-0.12	-0.97	-0.11	-0.17	
-0.11	-0.11	-0.17	-0.97	-0.17	-0.17	-0.17	-0.17	
-0.12	-0.11	-0.11	-0.97	-0.17	-0.97	-0.97	-0.04	
Map policy
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	v	>	v	v	
>	^	>	>	>	>	>	v	
^	^	<	>	^	^	>	.	
expeced value MDP LP -0.8265627990636308
mean w [-0.16444943 -0.0519108  -0.09024859 -0.17841012 -0.84834139  0.32843647]
Mean policy from posterior
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	v	>	v	v	
>	^	<	>	>	>	>	v	
^	^	<	>	^	^	>	.	
Mean rewards
-0.16	-0.85	-0.85	-0.16	-0.85	-0.16	-0.05	-0.16	
-0.85	-0.05	-0.16	-0.85	-0.16	-0.09	-0.85	-0.18	
-0.16	-0.05	-0.09	-0.05	-0.09	-0.16	-0.05	-0.18	
-0.09	-0.05	-0.05	-0.09	-0.85	-0.85	-0.18	-0.05	
-0.16	-0.16	-0.85	-0.85	-0.05	-0.18	-0.05	-0.05	
-0.85	-0.16	-0.85	-0.18	-0.18	-0.85	-0.05	-0.16	
-0.05	-0.05	-0.16	-0.85	-0.16	-0.16	-0.16	-0.16	
-0.18	-0.05	-0.05	-0.85	-0.16	-0.85	-0.85	0.33	
mean = 0.00209876414853416, map = 0.0035549835151050813
CVaR policy
>	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	^	>	v	v	
>	^	>	>	>	>	>	v	
^	^	<	>	^	>	>	.	
CVaR policy
>	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	>	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	^	>	v	v	
>	^	>	>	>	>	>	v	
^	^	<	>	^	^	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	^	>	v	v	
>	^	>	>	>	>	>	v	
^	^	<	>	^	^	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	^	>	v	v	
>	^	<	>	>	>	>	v	
^	^	<	>	^	^	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	v	v	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	^	>	v	v	
>	^	<	>	>	>	>	v	
^	^	<	>	^	^	>	.	
cvar = , 0.028330028575652477, 0.01637556581049271, 0.016375566973779176, 0.01491934702227038, 0.014919346521813592
==========
iteration 89
==========
weights [-0.58568471 -0.0204329  -0.03012558 -0.5176621  -0.36880953  0.50165112]
expeced value MDP LP -1.1214171522536454
demonstration
[(56, 2), (48, 2), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.37901718  0.52981378 -0.50414441  0.32991847  0.17735246 -0.42565438]
w_map [-0.910714   -0.11982497 -0.22533355 -0.15742053 -0.23663427 -0.15712981] loglik -8.430036498907612e-05
accepted/total = 856/3000 = 0.2853333333333333
-------
true weights [-0.58568471 -0.0204329  -0.03012558 -0.5176621  -0.36880953  0.50165112]
features
4 	1 	4 	0 	1 	1 	3 	1 	
4 	1 	2 	3 	3 	4 	0 	2 	
1 	4 	2 	1 	3 	1 	1 	2 	
4 	0 	1 	4 	2 	0 	0 	3 	
2 	3 	3 	1 	2 	4 	0 	3 	
2 	4 	4 	1 	4 	2 	4 	3 	
1 	0 	0 	1 	4 	1 	0 	4 	
0 	1 	4 	0 	4 	4 	4 	5 	
optimal policy
>	v	v	<	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	<	<	
v	>	>	v	v	v	^	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	v	<	v	
^	>	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
optimal values
-1.46	-1.10	-1.44	-2.01	-1.59	-1.58	-1.78	-1.27	
-1.45	-1.09	-1.08	-1.55	-1.72	-1.58	-1.80	-1.26	
-1.42	-1.42	-1.06	-1.04	-1.21	-1.22	-1.23	-1.25	
-1.76	-1.62	-1.04	-1.03	-0.70	-1.23	-1.80	-1.41	
-1.41	-1.69	-1.18	-0.67	-0.68	-0.65	-1.23	-0.90	
-1.39	-1.38	-1.02	-0.65	-0.65	-0.29	-0.65	-0.39	
-1.40	-1.79	-1.22	-0.64	-0.63	-0.26	-0.46	0.13	
-1.97	-1.55	-1.55	-1.19	-0.61	-0.24	0.13	0.50	
map_weights [-0.910714   -0.11982497 -0.22533355 -0.15742053 -0.23663427 -0.15712981]
MAP reward
-0.24	-0.12	-0.24	-0.91	-0.12	-0.12	-0.16	-0.12	
-0.24	-0.12	-0.23	-0.16	-0.16	-0.24	-0.91	-0.23	
-0.12	-0.24	-0.23	-0.12	-0.16	-0.12	-0.12	-0.23	
-0.24	-0.91	-0.12	-0.24	-0.23	-0.91	-0.91	-0.16	
-0.23	-0.16	-0.16	-0.12	-0.23	-0.24	-0.91	-0.16	
-0.23	-0.24	-0.24	-0.12	-0.24	-0.23	-0.24	-0.16	
-0.12	-0.91	-0.91	-0.12	-0.24	-0.12	-0.91	-0.24	
-0.91	-0.12	-0.24	-0.91	-0.24	-0.24	-0.24	-0.16	
Map policy
>	v	v	>	v	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
v	>	v	v	v	v	>	v	
>	>	>	v	v	v	>	v	
>	>	>	v	>	v	>	v	
^	^	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.371866752211181
mean w [-0.64264724 -0.04723198 -0.30270715 -0.39552644 -0.15181561 -0.28699484]
Mean policy from posterior
>	v	<	v	>	v	<	<	
v	v	v	v	<	v	v	v	
>	>	v	v	<	<	<	<	
^	>	>	v	<	v	^	v	
v	v	>	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.15	-0.05	-0.15	-0.64	-0.05	-0.05	-0.40	-0.05	
-0.15	-0.05	-0.30	-0.40	-0.40	-0.15	-0.64	-0.30	
-0.05	-0.15	-0.30	-0.05	-0.40	-0.05	-0.05	-0.30	
-0.15	-0.64	-0.05	-0.15	-0.30	-0.64	-0.64	-0.40	
-0.30	-0.40	-0.40	-0.05	-0.30	-0.15	-0.64	-0.40	
-0.30	-0.15	-0.15	-0.05	-0.15	-0.30	-0.15	-0.40	
-0.05	-0.64	-0.64	-0.05	-0.15	-0.05	-0.64	-0.15	
-0.64	-0.05	-0.15	-0.64	-0.15	-0.15	-0.15	-0.29	
mean = 0.1317425050224268, map = 0.30031308792582734
CVaR policy
>	v	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	>	>	v	
v	>	>	v	v	v	>	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
>	>	v	v	<	v	v	v	
>	>	v	v	<	<	>	v	
^	>	>	v	v	v	>	v	
v	>	>	v	<	v	v	v	
>	>	>	v	>	v	>	v	
^	^	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	<	v	
>	v	v	v	<	v	v	v	
>	>	v	v	<	<	<	v	
^	>	>	v	<	v	>	v	
v	v	>	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	>	v	<	<	
>	v	v	v	<	v	v	v	
>	>	v	v	<	<	<	<	
^	>	>	v	<	v	^	v	
v	v	>	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	>	v	<	<	
v	v	v	v	<	v	v	v	
>	>	v	v	<	<	<	<	
^	>	>	v	<	v	^	v	
v	v	>	v	<	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
cvar = , 0.03919595880066473, 0.05964560857776968, 0.10476597157535839, 0.1317425032256181, 0.13174250501492302
==========
iteration 90
==========
weights [-0.27930672 -0.15031688 -0.42450686 -0.63538588 -0.23820077  0.50865676]
expeced value MDP LP -1.4310444342424207
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.74504545  0.01120307 -0.24744624  0.36523447 -0.20275742  0.45721476]
w_map [-0.37052419 -0.6119488  -0.41118632 -0.36319813 -0.36008261  0.23996654] loglik 0.0
accepted/total = 2375/3000 = 0.7916666666666666
-------
true weights [-0.27930672 -0.15031688 -0.42450686 -0.63538588 -0.23820077  0.50865676]
features
1 	1 	3 	3 	4 	2 	1 	1 	
3 	4 	3 	0 	3 	1 	0 	4 	
4 	1 	0 	0 	2 	3 	1 	1 	
1 	2 	1 	0 	2 	1 	4 	2 	
2 	3 	2 	1 	1 	4 	0 	3 	
4 	1 	1 	3 	0 	0 	2 	3 	
4 	4 	3 	1 	1 	2 	0 	3 	
2 	0 	2 	0 	3 	4 	0 	5 	
optimal policy
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	>	>	>	v	v	v	<	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.42	-2.29	-2.90	-2.55	-2.16	-1.94	-1.53	-1.63	
-2.77	-2.16	-2.43	-1.94	-2.15	-1.53	-1.39	-1.49	
-2.16	-1.94	-1.81	-1.67	-1.83	-1.71	-1.13	-1.27	
-2.09	-1.96	-1.55	-1.41	-1.42	-1.09	-0.99	-1.40	
-2.26	-2.17	-1.55	-1.14	-1.00	-0.95	-0.76	-1.38	
-1.85	-1.63	-1.50	-1.36	-0.86	-0.72	-0.48	-0.77	
-1.81	-1.58	-1.36	-0.73	-0.59	-0.44	-0.06	-0.13	
-2.01	-1.61	-1.34	-0.92	-0.65	-0.02	0.22	0.51	
map_weights [-0.37052419 -0.6119488  -0.41118632 -0.36319813 -0.36008261  0.23996654]
MAP reward
-0.61	-0.61	-0.36	-0.36	-0.36	-0.41	-0.61	-0.61	
-0.36	-0.36	-0.36	-0.37	-0.36	-0.61	-0.37	-0.36	
-0.36	-0.61	-0.37	-0.37	-0.41	-0.36	-0.61	-0.61	
-0.61	-0.41	-0.61	-0.37	-0.41	-0.61	-0.36	-0.41	
-0.41	-0.36	-0.41	-0.61	-0.61	-0.36	-0.37	-0.36	
-0.36	-0.61	-0.61	-0.36	-0.37	-0.37	-0.41	-0.36	
-0.36	-0.36	-0.36	-0.61	-0.61	-0.41	-0.37	-0.36	
-0.41	-0.37	-0.41	-0.37	-0.36	-0.36	-0.37	0.24	
Map policy
v	>	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	v	>	>	>	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1406530370307606
mean w [-0.22944707 -0.43581267 -0.37788704 -0.32408539 -0.36449236 -0.11611724]
Mean policy from posterior
v	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	v	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.44	-0.44	-0.32	-0.32	-0.36	-0.38	-0.44	-0.44	
-0.32	-0.36	-0.32	-0.23	-0.32	-0.44	-0.23	-0.36	
-0.36	-0.44	-0.23	-0.23	-0.38	-0.32	-0.44	-0.44	
-0.44	-0.38	-0.44	-0.23	-0.38	-0.44	-0.36	-0.38	
-0.38	-0.32	-0.38	-0.44	-0.44	-0.36	-0.23	-0.32	
-0.36	-0.44	-0.44	-0.32	-0.23	-0.23	-0.38	-0.32	
-0.36	-0.36	-0.32	-0.44	-0.44	-0.38	-0.23	-0.32	
-0.38	-0.23	-0.38	-0.23	-0.32	-0.36	-0.23	-0.12	
mean = 0.3738249627866561, map = 0.9367432584527813
CVaR policy
v	v	>	>	>	>	v	v	
v	v	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	>	>	v	
v	v	>	>	>	v	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.3779425901359814, 0.47095743347414465, 0.35273826246412776, 0.3738249628991577, 0.37382496297251633
==========
iteration 91
==========
weights [-0.5155032  -0.0692184  -0.77005212 -0.15576571 -0.27468915  0.19174962]
expeced value MDP LP -1.6912166083940687
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.46897554 -0.12362596 -0.27902051 -0.12832688  0.7085643   0.41035957]
w_map [-0.38882118 -0.18870178 -0.7862043  -0.16360378 -0.40645616 -0.05585441] loglik -1.3862941696317392
accepted/total = 1708/3000 = 0.5693333333333334
-------
true weights [-0.5155032  -0.0692184  -0.77005212 -0.15576571 -0.27468915  0.19174962]
features
4 	3 	3 	4 	4 	2 	1 	1 	
2 	1 	1 	4 	0 	1 	0 	1 	
4 	3 	2 	2 	2 	1 	0 	1 	
0 	1 	2 	1 	0 	3 	2 	0 	
3 	2 	3 	0 	0 	0 	3 	2 	
2 	4 	3 	0 	3 	4 	2 	3 	
4 	3 	1 	0 	3 	1 	0 	2 	
4 	1 	3 	2 	1 	1 	0 	5 	
optimal policy
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	<	v	
>	v	v	v	>	v	<	<	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	<	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-2.72	-2.47	-2.42	-2.50	-2.24	-2.24	-2.04	-2.08	
-3.08	-2.34	-2.29	-2.24	-1.99	-1.49	-1.99	-2.03	
-2.65	-2.40	-2.97	-2.57	-2.19	-1.43	-1.94	-1.99	
-2.76	-2.26	-2.22	-1.82	-1.77	-1.38	-2.13	-1.99	
-2.35	-2.22	-1.46	-1.77	-1.27	-1.24	-1.38	-1.49	
-2.34	-1.58	-1.32	-1.27	-0.76	-0.73	-1.49	-0.73	
-1.58	-1.32	-1.18	-1.12	-0.61	-0.46	-0.84	-0.58	
-1.64	-1.38	-1.32	-1.22	-0.46	-0.39	-0.33	0.19	
map_weights [-0.38882118 -0.18870178 -0.7862043  -0.16360378 -0.40645616 -0.05585441]
MAP reward
-0.41	-0.16	-0.16	-0.41	-0.41	-0.79	-0.19	-0.19	
-0.79	-0.19	-0.19	-0.41	-0.39	-0.19	-0.39	-0.19	
-0.41	-0.16	-0.79	-0.79	-0.79	-0.19	-0.39	-0.19	
-0.39	-0.19	-0.79	-0.19	-0.39	-0.16	-0.79	-0.39	
-0.16	-0.79	-0.16	-0.39	-0.39	-0.39	-0.16	-0.79	
-0.79	-0.41	-0.16	-0.39	-0.16	-0.41	-0.79	-0.16	
-0.41	-0.16	-0.19	-0.39	-0.16	-0.19	-0.39	-0.79	
-0.41	-0.19	-0.16	-0.79	-0.19	-0.19	-0.39	-0.06	
Map policy
>	v	v	>	v	v	v	v	
>	v	>	>	>	v	<	v	
>	v	v	v	v	v	<	v	
v	>	v	v	v	v	v	v	
>	>	v	>	v	v	<	v	
>	>	>	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.370722889942476
mean w [-0.27708209 -0.09077182 -0.71127921 -0.16116945 -0.43378228  0.18712756]
Mean policy from posterior
>	v	v	>	v	v	v	v	
>	v	>	>	>	v	<	v	
>	v	v	v	v	v	<	<	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.43	-0.16	-0.16	-0.43	-0.43	-0.71	-0.09	-0.09	
-0.71	-0.09	-0.09	-0.43	-0.28	-0.09	-0.28	-0.09	
-0.43	-0.16	-0.71	-0.71	-0.71	-0.09	-0.28	-0.09	
-0.28	-0.09	-0.71	-0.09	-0.28	-0.16	-0.71	-0.28	
-0.16	-0.71	-0.16	-0.28	-0.28	-0.28	-0.16	-0.71	
-0.71	-0.43	-0.16	-0.28	-0.16	-0.43	-0.71	-0.16	
-0.43	-0.16	-0.09	-0.28	-0.16	-0.09	-0.28	-0.71	
-0.43	-0.09	-0.16	-0.71	-0.09	-0.09	-0.28	0.19	
mean = 0.013417253008910945, map = 0.03900154712669357
CVaR policy
>	v	v	>	v	v	v	v	
>	v	>	>	>	v	<	v	
>	v	v	v	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	v	v	v	v	v	<	<	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	>	>	>	v	<	v	
>	v	v	v	v	v	<	<	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	>	>	>	v	<	v	
>	v	v	v	v	v	<	<	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	v	>	>	>	v	<	v	
>	v	v	v	v	v	<	<	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.010259877503252968, 0.013417251150898535, 0.013417250525102453, 0.013417250720299423, 0.013417250535422198
==========
iteration 92
==========
weights [-0.3587393  -0.00138778 -0.51618354 -0.20340655 -0.72640482  0.18926319]
expeced value MDP LP -0.6299052228996818
demonstration
[(56, 2), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0), (48, 0)]
[-0.14089054  0.7545367  -0.33483996 -0.09884831 -0.09884567 -0.52836064]
w_map [-0.13987686  0.77856385 -0.35105172 -0.06096944 -0.00446374 -0.49729095] loglik 0.0
accepted/total = 2723/3000 = 0.9076666666666666
-------
true weights [-0.3587393  -0.00138778 -0.51618354 -0.20340655 -0.72640482  0.18926319]
features
2 	4 	4 	0 	1 	2 	3 	0 	
4 	2 	2 	3 	0 	3 	1 	2 	
1 	2 	2 	4 	4 	3 	1 	2 	
2 	1 	2 	4 	4 	3 	1 	3 	
1 	4 	2 	1 	4 	3 	1 	4 	
4 	1 	4 	1 	3 	2 	1 	3 	
1 	3 	3 	2 	4 	0 	2 	2 	
3 	3 	0 	4 	4 	0 	0 	5 	
optimal policy
v	v	>	>	^	<	v	<	
v	v	>	>	^	>	v	<	
<	<	<	^	>	>	v	<	
v	<	<	v	>	>	v	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	v	^	v	
^	<	<	^	>	>	>	.	
optimal values
-1.37	-1.88	-1.22	-0.50	-0.14	-0.65	-0.34	-0.70	
-0.86	-1.16	-1.20	-0.69	-0.50	-0.34	-0.14	-0.65	
-0.14	-0.65	-1.16	-1.41	-1.06	-0.34	-0.14	-0.65	
-0.65	-0.65	-1.16	-0.86	-1.06	-0.34	-0.14	-0.34	
-0.14	-0.86	-0.65	-0.14	-0.86	-0.34	-0.14	-0.86	
-0.86	-0.34	-0.86	-0.14	-0.34	-0.65	-0.14	-0.34	
-0.14	-0.34	-0.54	-0.65	-1.06	-0.88	-0.65	-0.33	
-0.34	-0.54	-0.89	-1.37	-1.25	-0.53	-0.17	0.19	
map_weights [-0.13987686  0.77856385 -0.35105172 -0.06096944 -0.00446374 -0.49729095]
MAP reward
-0.35	-0.00	-0.00	-0.14	0.78	-0.35	-0.06	-0.14	
-0.00	-0.35	-0.35	-0.06	-0.14	-0.06	0.78	-0.35	
0.78	-0.35	-0.35	-0.00	-0.00	-0.06	0.78	-0.35	
-0.35	0.78	-0.35	-0.00	-0.00	-0.06	0.78	-0.06	
0.78	-0.00	-0.35	0.78	-0.00	-0.06	0.78	-0.00	
-0.00	0.78	-0.00	0.78	-0.06	-0.35	0.78	-0.06	
0.78	-0.06	-0.06	-0.35	-0.00	-0.14	-0.35	-0.35	
-0.06	-0.06	-0.14	-0.00	-0.00	-0.14	-0.14	-0.50	
Map policy
v	>	>	>	^	<	v	<	
v	<	^	>	^	>	v	<	
<	<	<	v	>	>	v	<	
v	v	<	v	v	>	^	<	
<	<	>	v	<	>	^	<	
^	<	>	^	<	>	^	<	
<	<	^	^	^	^	^	^	
^	^	^	^	^	^	^	.	
expeced value MDP LP 46.77017664560751
mean w [-0.04760269  0.48289294 -0.27210455 -0.05232286 -0.17312851  0.1303842 ]
Mean policy from posterior
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	v	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	^	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
Mean rewards
-0.27	-0.17	-0.17	-0.05	0.48	-0.27	-0.05	-0.05	
-0.17	-0.27	-0.27	-0.05	-0.05	-0.05	0.48	-0.27	
0.48	-0.27	-0.27	-0.17	-0.17	-0.05	0.48	-0.27	
-0.27	0.48	-0.27	-0.17	-0.17	-0.05	0.48	-0.05	
0.48	-0.17	-0.27	0.48	-0.17	-0.05	0.48	-0.17	
-0.17	0.48	-0.17	0.48	-0.05	-0.27	0.48	-0.05	
0.48	-0.05	-0.05	-0.27	-0.17	-0.05	-0.27	-0.27	
-0.05	-0.05	-0.05	-0.17	-0.17	-0.05	-0.05	0.13	
mean = 0.0599467887432259, map = 0.09791384995364227
CVaR policy
v	v	>	>	^	<	v	v	
v	v	<	>	^	>	v	<	
<	<	<	v	>	>	v	<	
v	<	<	v	v	>	v	<	
<	<	>	v	<	>	^	<	
^	>	>	^	<	>	^	<	
<	<	>	^	<	^	^	<	
^	<	>	^	^	>	^	.	
CVaR policy
v	<	>	>	^	<	v	v	
v	<	<	>	^	>	v	<	
<	<	<	v	>	>	v	<	
^	v	<	v	v	>	^	<	
<	<	>	v	<	>	^	<	
v	<	>	^	<	>	^	<	
<	<	^	^	<	^	^	<	
^	<	>	^	^	^	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	>	>	^	<	
v	v	<	v	>	>	v	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	>	>	^	<	
v	v	<	v	>	>	v	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
cvar = , 0.10743286951742659, 0.12684929281677548, 0.05754564293704045, 0.058374944839260534, 0.05994683738933815
==========
iteration 93
==========
weights [-0.42813831 -0.01364042 -0.26249877 -0.12055687 -0.38381049  0.76535056]
expeced value MDP LP -0.6234165451809174
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.3344721  -0.41128882  0.09711965 -0.51265245  0.42183283 -0.51844212]
w_map [-0.3600442  -0.35144943 -0.32748174 -0.36502174 -0.71103559 -0.02819033] loglik -7.105427357601002e-15
accepted/total = 2135/3000 = 0.7116666666666667
-------
true weights [-0.42813831 -0.01364042 -0.26249877 -0.12055687 -0.38381049  0.76535056]
features
1 	2 	1 	0 	0 	1 	3 	4 	
1 	2 	4 	0 	4 	1 	1 	3 	
1 	2 	4 	4 	4 	0 	4 	3 	
2 	1 	2 	4 	2 	4 	2 	2 	
0 	2 	0 	2 	0 	0 	4 	3 	
0 	3 	1 	0 	3 	4 	0 	3 	
3 	1 	3 	0 	2 	2 	1 	2 	
1 	4 	0 	1 	4 	4 	4 	5 	
optimal policy
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	>	^	^	>	v	
>	v	<	v	v	>	>	v	
>	v	v	>	v	>	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
optimal values
-1.24	-1.39	-1.14	-1.14	-0.72	-0.29	-0.39	-0.64	
-1.24	-1.48	-1.46	-1.08	-0.66	-0.28	-0.27	-0.26	
-1.23	-1.23	-1.60	-1.41	-1.04	-0.71	-0.52	-0.14	
-1.23	-0.98	-1.23	-1.23	-0.86	-0.66	-0.28	-0.02	
-1.39	-0.98	-1.03	-0.86	-0.60	-0.57	-0.14	0.25	
-1.14	-0.72	-0.61	-0.60	-0.18	-0.18	0.04	0.37	
-0.72	-0.61	-0.60	-0.48	-0.06	0.21	0.48	0.50	
-0.73	-0.98	-0.83	-0.41	-0.40	-0.01	0.37	0.77	
map_weights [-0.3600442  -0.35144943 -0.32748174 -0.36502174 -0.71103559 -0.02819033]
MAP reward
-0.35	-0.33	-0.35	-0.36	-0.36	-0.35	-0.37	-0.71	
-0.35	-0.33	-0.71	-0.36	-0.71	-0.35	-0.35	-0.37	
-0.35	-0.33	-0.71	-0.71	-0.71	-0.36	-0.71	-0.37	
-0.33	-0.35	-0.33	-0.71	-0.33	-0.71	-0.33	-0.33	
-0.36	-0.33	-0.36	-0.33	-0.36	-0.36	-0.71	-0.37	
-0.36	-0.37	-0.35	-0.36	-0.37	-0.71	-0.36	-0.37	
-0.37	-0.35	-0.37	-0.36	-0.33	-0.33	-0.35	-0.33	
-0.35	-0.71	-0.36	-0.35	-0.71	-0.71	-0.71	-0.03	
Map policy
>	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	^	^	>	>	.	
expeced value MDP LP -1.963226890230289
mean w [-0.40222167 -0.27504985 -0.22442623 -0.21490305 -0.63999823 -0.11267505]
Mean policy from posterior
>	v	>	>	>	>	v	v	
>	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
>	v	>	v	v	>	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
Mean rewards
-0.28	-0.22	-0.28	-0.40	-0.40	-0.28	-0.21	-0.64	
-0.28	-0.22	-0.64	-0.40	-0.64	-0.28	-0.28	-0.21	
-0.28	-0.22	-0.64	-0.64	-0.64	-0.40	-0.64	-0.21	
-0.22	-0.28	-0.22	-0.64	-0.22	-0.64	-0.22	-0.22	
-0.40	-0.22	-0.40	-0.22	-0.40	-0.40	-0.64	-0.21	
-0.40	-0.21	-0.28	-0.40	-0.21	-0.64	-0.40	-0.21	
-0.21	-0.28	-0.21	-0.40	-0.22	-0.22	-0.28	-0.22	
-0.28	-0.64	-0.40	-0.28	-0.64	-0.64	-0.64	-0.11	
mean = 0.052739332001764616, map = 0.2486058617314847
CVaR policy
>	>	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
v	v	v	v	v	>	>	v	
v	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
CVaR policy
v	v	>	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
CVaR policy
>	v	>	>	>	>	v	v	
>	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
>	v	>	v	v	>	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
CVaR policy
>	v	>	>	>	>	v	v	
>	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
>	v	>	v	v	>	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
cvar = , 0.01879929030629912, 0.029936184649207775, 0.03073363120702788, 0.05273933281122456, 0.05273933340816761
==========
iteration 94
==========
weights [-0.2696454  -0.64597038 -0.38127187 -0.21671282 -0.12610708  0.5493432 ]
expeced value MDP LP -0.990944579120997
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.28623191  0.44659297  0.43550719  0.33826547 -0.64362088 -0.01697467]
w_map [-0.36225048 -0.38456132 -0.45349812 -0.434095   -0.52684037  0.22187247] loglik 0.0
accepted/total = 2329/3000 = 0.7763333333333333
-------
true weights [-0.2696454  -0.64597038 -0.38127187 -0.21671282 -0.12610708  0.5493432 ]
features
2 	2 	4 	0 	1 	1 	0 	3 	
2 	1 	0 	4 	3 	3 	1 	4 	
0 	0 	3 	1 	4 	4 	0 	4 	
4 	0 	1 	3 	3 	3 	1 	2 	
4 	0 	3 	3 	4 	4 	4 	2 	
2 	0 	4 	2 	0 	1 	0 	4 	
2 	2 	3 	1 	0 	3 	1 	3 	
3 	4 	4 	2 	0 	3 	1 	5 	
optimal policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	^	>	>	v	<	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	^	>	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.20	-1.84	-1.47	-1.36	-1.62	-1.50	-1.27	-1.01	
-1.98	-1.99	-1.36	-1.10	-0.99	-0.87	-1.44	-0.80	
-1.61	-1.64	-1.56	-1.42	-0.78	-0.66	-0.92	-0.69	
-1.36	-1.39	-1.51	-0.87	-0.66	-0.54	-0.84	-0.56	
-1.24	-1.13	-0.87	-0.66	-0.45	-0.32	-0.20	-0.19	
-1.61	-1.25	-0.99	-1.03	-0.71	-0.72	-0.07	0.20	
-1.76	-1.56	-1.19	-1.43	-0.80	-0.53	-0.32	0.33	
-1.40	-1.19	-1.08	-0.96	-0.58	-0.32	-0.10	0.55	
map_weights [-0.36225048 -0.38456132 -0.45349812 -0.434095   -0.52684037  0.22187247]
MAP reward
-0.45	-0.45	-0.53	-0.36	-0.38	-0.38	-0.36	-0.43	
-0.45	-0.38	-0.36	-0.53	-0.43	-0.43	-0.38	-0.53	
-0.36	-0.36	-0.43	-0.38	-0.53	-0.53	-0.36	-0.53	
-0.53	-0.36	-0.38	-0.43	-0.43	-0.43	-0.38	-0.45	
-0.53	-0.36	-0.43	-0.43	-0.53	-0.53	-0.53	-0.45	
-0.45	-0.36	-0.53	-0.45	-0.36	-0.38	-0.36	-0.53	
-0.45	-0.45	-0.43	-0.38	-0.36	-0.43	-0.38	-0.43	
-0.43	-0.53	-0.53	-0.45	-0.36	-0.43	-0.38	0.22	
Map policy
v	v	>	>	>	>	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.3971289325246965
mean w [-0.41533821 -0.33555248 -0.31086675 -0.42758713 -0.27900998 -0.15391767]
Mean policy from posterior
>	>	>	v	>	>	v	v	
>	>	>	v	v	>	>	v	
v	>	>	>	>	v	>	v	
v	>	>	>	v	v	v	v	
v	>	>	>	>	>	>	v	
v	v	>	v	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.31	-0.31	-0.28	-0.42	-0.34	-0.34	-0.42	-0.43	
-0.31	-0.34	-0.42	-0.28	-0.43	-0.43	-0.34	-0.28	
-0.42	-0.42	-0.43	-0.34	-0.28	-0.28	-0.42	-0.28	
-0.28	-0.42	-0.34	-0.43	-0.43	-0.43	-0.34	-0.31	
-0.28	-0.42	-0.43	-0.43	-0.28	-0.28	-0.28	-0.31	
-0.31	-0.42	-0.28	-0.31	-0.42	-0.34	-0.42	-0.28	
-0.31	-0.31	-0.43	-0.34	-0.42	-0.43	-0.34	-0.43	
-0.43	-0.28	-0.28	-0.31	-0.42	-0.43	-0.34	-0.15	
mean = 0.34171946461438507, map = 1.1074655699795433
CVaR policy
v	>	>	>	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	>	>	>	v	
v	v	v	v	>	>	>	v	
v	>	>	>	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	v	>	>	>	v	
v	v	>	>	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	>	>	v	v	
v	>	>	v	v	>	>	v	
v	>	>	>	>	>	>	v	
v	>	v	>	v	v	v	v	
v	v	v	>	>	>	v	v	
v	>	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
>	>	>	v	v	>	>	v	
v	>	>	>	>	>	>	v	
v	>	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
v	v	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
>	>	>	v	v	>	>	v	
v	>	>	>	>	v	>	v	
v	>	>	>	v	v	v	v	
v	>	>	>	>	>	>	v	
v	v	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.5784963950811839, 0.6558611200959696, 0.6431746061962096, 0.42725017174498126, 0.3510231493692135
==========
iteration 95
==========
weights [-0.49978479 -0.13993913 -0.09484483 -0.68582209 -0.12961334  0.48423663]
expeced value MDP LP -1.2930192472989095
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.47297857 -0.03915114  0.47083053 -0.39199359 -0.0820722   0.62664364]
w_map [-0.71200066 -0.19828253 -0.22079487 -0.58167998 -0.25391275 -0.04653426] loglik -0.6931471791512926
accepted/total = 1575/3000 = 0.525
-------
true weights [-0.49978479 -0.13993913 -0.09484483 -0.68582209 -0.12961334  0.48423663]
features
1 	4 	1 	1 	0 	1 	0 	4 	
4 	4 	0 	1 	4 	0 	4 	4 	
1 	0 	1 	0 	4 	3 	2 	3 	
3 	0 	4 	2 	4 	0 	4 	3 	
0 	0 	1 	1 	2 	0 	3 	3 	
4 	3 	4 	1 	3 	0 	1 	0 	
2 	0 	0 	3 	1 	0 	4 	1 	
4 	2 	1 	0 	3 	0 	0 	5 	
optimal policy
>	>	>	v	v	v	v	v	
>	^	v	>	v	>	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	<	
>	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
optimal values
-1.98	-1.86	-1.75	-1.62	-1.86	-1.57	-1.44	-1.19	
-2.08	-1.97	-1.95	-1.50	-1.37	-1.44	-0.95	-1.07	
-2.07	-1.95	-1.46	-1.71	-1.25	-1.51	-0.83	-1.51	
-2.49	-1.82	-1.34	-1.22	-1.14	-1.24	-0.75	-1.42	
-2.24	-1.76	-1.28	-1.15	-1.02	-0.93	-0.62	-0.85	
-2.10	-2.03	-1.36	-1.24	-1.11	-0.44	0.06	-0.16	
-1.99	-2.09	-1.60	-1.11	-0.43	-0.30	0.21	0.34	
-1.92	-1.80	-1.73	-1.60	-1.11	-0.52	-0.02	0.48	
map_weights [-0.71200066 -0.19828253 -0.22079487 -0.58167998 -0.25391275 -0.04653426]
MAP reward
-0.20	-0.25	-0.20	-0.20	-0.71	-0.20	-0.71	-0.25	
-0.25	-0.25	-0.71	-0.20	-0.25	-0.71	-0.25	-0.25	
-0.20	-0.71	-0.20	-0.71	-0.25	-0.58	-0.22	-0.58	
-0.58	-0.71	-0.25	-0.22	-0.25	-0.71	-0.25	-0.58	
-0.71	-0.71	-0.20	-0.20	-0.22	-0.71	-0.58	-0.58	
-0.25	-0.58	-0.25	-0.20	-0.58	-0.71	-0.20	-0.71	
-0.22	-0.71	-0.71	-0.58	-0.20	-0.71	-0.25	-0.20	
-0.25	-0.22	-0.20	-0.71	-0.58	-0.71	-0.71	-0.05	
Map policy
>	>	>	v	v	>	v	v	
^	^	v	>	v	>	v	<	
>	>	v	>	>	>	v	<	
>	>	v	v	v	>	v	<	
v	>	>	v	v	>	v	v	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
expeced value MDP LP -1.9325517157359662
mean w [-0.52037446 -0.09541494 -0.43561396 -0.46828248 -0.21701959 -0.15184414]
Mean policy from posterior
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	<	v	>	v	<	
>	>	v	v	>	>	v	<	
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
Mean rewards
-0.10	-0.22	-0.10	-0.10	-0.52	-0.10	-0.52	-0.22	
-0.22	-0.22	-0.52	-0.10	-0.22	-0.52	-0.22	-0.22	
-0.10	-0.52	-0.10	-0.52	-0.22	-0.47	-0.44	-0.47	
-0.47	-0.52	-0.22	-0.44	-0.22	-0.52	-0.22	-0.47	
-0.52	-0.52	-0.10	-0.10	-0.44	-0.52	-0.47	-0.47	
-0.22	-0.47	-0.22	-0.10	-0.47	-0.52	-0.10	-0.52	
-0.44	-0.52	-0.52	-0.47	-0.10	-0.52	-0.22	-0.10	
-0.22	-0.44	-0.10	-0.52	-0.47	-0.52	-0.52	-0.15	
mean = 0.10554723243026443, map = 0.1241742780721351
CVaR policy
>	>	>	v	v	v	v	v	
v	>	v	>	v	>	v	v	
>	>	v	v	>	>	v	<	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	v	>	>	v	<	
>	>	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	v	>	>	v	<	
>	>	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	<	v	>	v	<	
>	>	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
v	>	v	>	v	>	v	<	
>	>	v	<	v	>	v	<	
>	>	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
cvar = , 0.139032030588091, 0.12142277381536548, 0.11881869146459056, 0.10713267344220112, 0.10713267280261962
==========
iteration 96
==========
weights [-0.12573145 -0.02573221 -0.16504818 -0.023658   -0.77151134  0.60041578]
expeced value MDP LP -0.11446347967932347
demonstration
[(56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[ 0.05198623  0.09224074  0.65511978  0.60474491 -0.38002532 -0.2224219 ]
w_map [-0.31140211 -0.13719871 -0.25097362 -0.23404088 -0.83769587 -0.25437764] loglik -2.82687240584778e-05
accepted/total = 1349/3000 = 0.44966666666666666
-------
true weights [-0.12573145 -0.02573221 -0.16504818 -0.023658   -0.77151134  0.60041578]
features
3 	3 	3 	3 	3 	4 	1 	0 	
4 	4 	2 	0 	4 	2 	3 	1 	
4 	2 	0 	3 	4 	1 	2 	0 	
0 	2 	2 	3 	0 	1 	2 	1 	
2 	4 	0 	4 	4 	0 	4 	1 	
4 	2 	3 	0 	2 	0 	4 	1 	
3 	1 	2 	0 	2 	1 	2 	2 	
2 	1 	1 	3 	1 	4 	1 	5 	
optimal policy
>	>	>	v	<	>	v	v	
^	^	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	v	
^	v	v	^	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	^	>	>	.	
optimal values
-0.24	-0.22	-0.20	-0.18	-0.20	-0.64	0.13	0.06	
-1.01	-0.99	-0.32	-0.16	-0.78	-0.01	0.16	0.18	
-1.09	-0.32	-0.16	-0.03	-0.66	0.12	0.04	0.21	
-0.46	-0.34	-0.17	-0.01	0.02	0.14	0.17	0.34	
-0.62	-0.87	-0.20	-0.78	-0.66	0.11	-0.41	0.37	
-0.73	-0.10	-0.07	-0.05	0.07	0.24	-0.38	0.40	
0.04	0.07	-0.05	0.07	0.20	0.37	0.40	0.43	
-0.07	0.09	0.12	0.15	0.17	-0.21	0.57	0.60	
map_weights [-0.31140211 -0.13719871 -0.25097362 -0.23404088 -0.83769587 -0.25437764]
MAP reward
-0.23	-0.23	-0.23	-0.23	-0.23	-0.84	-0.14	-0.31	
-0.84	-0.84	-0.25	-0.31	-0.84	-0.25	-0.23	-0.14	
-0.84	-0.25	-0.31	-0.23	-0.84	-0.14	-0.25	-0.31	
-0.31	-0.25	-0.25	-0.23	-0.31	-0.14	-0.25	-0.14	
-0.25	-0.84	-0.31	-0.84	-0.84	-0.31	-0.84	-0.14	
-0.84	-0.25	-0.23	-0.31	-0.25	-0.31	-0.84	-0.14	
-0.23	-0.14	-0.25	-0.31	-0.25	-0.14	-0.25	-0.25	
-0.25	-0.14	-0.14	-0.23	-0.14	-0.84	-0.14	-0.25	
Map policy
>	>	>	v	>	v	v	v	
^	v	v	v	>	v	>	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	v	v	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
expeced value MDP LP -1.600340142215818
mean w [-0.44983692 -0.08741041 -0.1899355  -0.19668382 -0.65036387 -0.35883813]
Mean policy from posterior
>	>	>	>	>	v	v	v	
^	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	^	>	v	
>	v	v	>	v	v	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	^	>	>	.	
Mean rewards
-0.20	-0.20	-0.20	-0.20	-0.20	-0.65	-0.09	-0.45	
-0.65	-0.65	-0.19	-0.45	-0.65	-0.19	-0.20	-0.09	
-0.65	-0.19	-0.45	-0.20	-0.65	-0.09	-0.19	-0.45	
-0.45	-0.19	-0.19	-0.20	-0.45	-0.09	-0.19	-0.09	
-0.19	-0.65	-0.45	-0.65	-0.65	-0.45	-0.65	-0.09	
-0.65	-0.19	-0.20	-0.45	-0.19	-0.45	-0.65	-0.09	
-0.20	-0.09	-0.19	-0.45	-0.19	-0.09	-0.19	-0.19	
-0.19	-0.09	-0.09	-0.20	-0.09	-0.65	-0.09	-0.36	
mean = 0.09954038425563239, map = 0.044533011676006845
CVaR policy
>	>	>	v	>	v	v	v	
^	v	v	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	^	>	v	
>	v	v	>	v	v	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	^	>	v	
>	v	v	>	v	v	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	^	>	>	.	
cvar = , 0.05704159452079949, 0.07209064321216996, 0.09505633326329199, 0.10522343352684804, 0.10522344394734257
==========
iteration 97
==========
weights [-0.1466662  -0.34992279 -0.23201547 -0.79428403 -0.39089502  0.13610969]
expeced value MDP LP -1.9597827850887861
demonstration
[(56, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.17121493  0.29909125 -0.61735844 -0.08407327  0.64063459 -0.28743249]
w_map [-0.10938605 -0.4977207  -0.25197957 -0.80430006 -0.16288143 -0.0581907 ] loglik -7.451816941284051e-11
accepted/total = 1965/3000 = 0.655
-------
true weights [-0.1466662  -0.34992279 -0.23201547 -0.79428403 -0.39089502  0.13610969]
features
2 	0 	2 	4 	1 	2 	4 	0 	
1 	2 	4 	0 	2 	2 	1 	0 	
3 	3 	0 	2 	3 	3 	0 	4 	
2 	2 	2 	3 	3 	4 	4 	0 	
0 	2 	3 	4 	0 	4 	3 	3 	
3 	2 	2 	3 	2 	4 	1 	2 	
1 	4 	1 	0 	4 	1 	0 	2 	
3 	3 	3 	4 	4 	3 	1 	5 	
optimal policy
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
optimal values
-3.52	-3.32	-3.20	-3.00	-2.80	-2.47	-2.26	-1.89	
-3.52	-3.20	-3.00	-2.64	-2.52	-2.31	-2.10	-1.76	
-3.20	-3.08	-2.64	-2.69	-2.89	-2.51	-1.76	-1.63	
-2.43	-2.31	-2.52	-2.49	-2.11	-1.73	-1.63	-1.25	
-2.22	-2.10	-2.45	-1.71	-1.33	-1.36	-1.38	-1.12	
-2.66	-1.88	-1.67	-1.90	-1.20	-0.98	-0.59	-0.33	
-2.16	-1.83	-1.45	-1.11	-0.98	-0.59	-0.24	-0.10	
-2.93	-2.60	-2.23	-1.49	-1.36	-1.01	-0.22	0.14	
map_weights [-0.10938605 -0.4977207  -0.25197957 -0.80430006 -0.16288143 -0.0581907 ]
MAP reward
-0.25	-0.11	-0.25	-0.16	-0.50	-0.25	-0.16	-0.11	
-0.50	-0.25	-0.16	-0.11	-0.25	-0.25	-0.50	-0.11	
-0.80	-0.80	-0.11	-0.25	-0.80	-0.80	-0.11	-0.16	
-0.25	-0.25	-0.25	-0.80	-0.80	-0.16	-0.16	-0.11	
-0.11	-0.25	-0.80	-0.16	-0.11	-0.16	-0.80	-0.80	
-0.80	-0.25	-0.25	-0.80	-0.25	-0.16	-0.50	-0.25	
-0.50	-0.16	-0.50	-0.11	-0.16	-0.50	-0.11	-0.25	
-0.80	-0.80	-0.80	-0.16	-0.16	-0.80	-0.50	-0.06	
Map policy
>	>	v	v	>	>	>	v	
>	>	v	v	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	v	>	>	>	v	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
expeced value MDP LP -1.974790044475955
mean w [-0.17447743 -0.36816229 -0.2192673  -0.68392141 -0.31861248  0.00416224]
Mean policy from posterior
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	<	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
Mean rewards
-0.22	-0.17	-0.22	-0.32	-0.37	-0.22	-0.32	-0.17	
-0.37	-0.22	-0.32	-0.17	-0.22	-0.22	-0.37	-0.17	
-0.68	-0.68	-0.17	-0.22	-0.68	-0.68	-0.17	-0.32	
-0.22	-0.22	-0.22	-0.68	-0.68	-0.32	-0.32	-0.17	
-0.17	-0.22	-0.68	-0.32	-0.17	-0.32	-0.68	-0.68	
-0.68	-0.22	-0.22	-0.68	-0.22	-0.32	-0.37	-0.22	
-0.37	-0.32	-0.37	-0.17	-0.32	-0.37	-0.17	-0.22	
-0.68	-0.68	-0.68	-0.32	-0.32	-0.68	-0.37	0.00	
mean = 0.0017839523938343316, map = 0.12424302827863398
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	>	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	>	v	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	>	v	
>	v	>	>	>	v	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
cvar = , 0.08151067785953936, 0.05849626500488547, 0.007292303439005554, 0.007292303685191515, 0.007292303448708237
==========
iteration 98
==========
weights [-0.17623077 -0.53731002 -0.07272226 -0.1766744  -0.2995494   0.74431744]
expeced value MDP LP -0.31006315109263427
demonstration
[(56, 1), (57, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.40579341  0.20531456  0.14233956  0.25117884  0.43678682 -0.72044677]
w_map [-0.36096305 -0.23888521 -0.16804169 -0.56092624 -0.67241407  0.13275015] loglik -5.848713113820736e-08
accepted/total = 1625/3000 = 0.5416666666666666
-------
true weights [-0.17623077 -0.53731002 -0.07272226 -0.1766744  -0.2995494   0.74431744]
features
2 	3 	4 	3 	0 	1 	2 	0 	
0 	2 	0 	0 	2 	1 	2 	2 	
1 	0 	2 	1 	0 	1 	4 	3 	
3 	2 	2 	4 	4 	1 	0 	0 	
0 	1 	3 	3 	3 	3 	2 	2 	
2 	3 	3 	4 	4 	3 	0 	3 	
3 	2 	4 	2 	0 	3 	3 	2 	
0 	0 	4 	0 	4 	4 	4 	5 	
optimal policy
v	v	v	>	v	>	v	v	
>	v	v	>	v	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
optimal values
-0.99	-0.93	-0.98	-0.91	-0.74	-0.71	-0.17	-0.20	
-0.93	-0.76	-0.69	-0.74	-0.57	-0.64	-0.10	-0.03	
-1.22	-0.69	-0.52	-1.04	-0.50	-0.69	-0.15	0.04	
-0.69	-0.52	-0.45	-0.51	-0.33	-0.39	0.15	0.22	
-0.74	-0.92	-0.38	-0.21	-0.03	0.15	0.33	0.40	
-0.57	-0.50	-0.43	-0.25	-0.18	0.12	0.30	0.48	
-0.50	-0.32	-0.25	0.05	0.12	0.30	0.48	0.66	
-0.67	-0.50	-0.43	-0.13	-0.17	0.13	0.44	0.74	
map_weights [-0.36096305 -0.23888521 -0.16804169 -0.56092624 -0.67241407  0.13275015]
MAP reward
-0.17	-0.56	-0.67	-0.56	-0.36	-0.24	-0.17	-0.36	
-0.36	-0.17	-0.36	-0.36	-0.17	-0.24	-0.17	-0.17	
-0.24	-0.36	-0.17	-0.24	-0.36	-0.24	-0.67	-0.56	
-0.56	-0.17	-0.17	-0.67	-0.67	-0.24	-0.36	-0.36	
-0.36	-0.24	-0.56	-0.56	-0.56	-0.56	-0.17	-0.17	
-0.17	-0.56	-0.56	-0.67	-0.67	-0.56	-0.36	-0.56	
-0.56	-0.17	-0.67	-0.17	-0.36	-0.56	-0.56	-0.17	
-0.36	-0.36	-0.67	-0.36	-0.67	-0.67	-0.67	0.13	
Map policy
v	v	v	>	v	v	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
>	^	>	>	>	>	>	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
expeced value MDP LP -1.5764253089745766
mean w [-0.22819429 -0.36886248 -0.09225911 -0.41587146 -0.54810427  0.139618  ]
Mean policy from posterior
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
Mean rewards
-0.09	-0.42	-0.55	-0.42	-0.23	-0.37	-0.09	-0.23	
-0.23	-0.09	-0.23	-0.23	-0.09	-0.37	-0.09	-0.09	
-0.37	-0.23	-0.09	-0.37	-0.23	-0.37	-0.55	-0.42	
-0.42	-0.09	-0.09	-0.55	-0.55	-0.37	-0.23	-0.23	
-0.23	-0.37	-0.42	-0.42	-0.42	-0.42	-0.09	-0.09	
-0.09	-0.42	-0.42	-0.55	-0.55	-0.42	-0.23	-0.42	
-0.42	-0.09	-0.55	-0.09	-0.23	-0.42	-0.42	-0.09	
-0.23	-0.23	-0.55	-0.23	-0.55	-0.55	-0.55	0.14	
mean = 0.19029622779134658, map = 0.3929548755426978
CVaR policy
v	v	v	v	v	>	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	v	v	
v	v	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
cvar = , 0.32647221684999084, 0.19029623184146505, 0.19029622778804023, 0.19029623562729214, 0.1902962298918549
==========
iteration 99
==========
weights [-0.83363091 -0.26301518 -0.31569054 -0.05727325 -0.36344545  0.02914067]
expeced value MDP LP -1.533172126056173
demonstration
[(56, 1), (57, 1), (58, 2), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.65947404 -0.07011811 -0.19484391  0.35030978 -0.51646109 -0.36436835]
w_map [-0.42244677 -0.39705119 -0.15090653 -0.18719503 -0.67359449  0.3903137 ] loglik -1.2261151738357512e-07
accepted/total = 1755/3000 = 0.585
-------
true weights [-0.83363091 -0.26301518 -0.31569054 -0.05727325 -0.36344545  0.02914067]
features
2 	2 	2 	0 	1 	1 	3 	4 	
3 	2 	0 	1 	0 	1 	0 	4 	
1 	3 	2 	1 	1 	0 	3 	4 	
1 	2 	4 	2 	0 	1 	4 	3 	
4 	0 	0 	4 	3 	1 	4 	3 	
3 	0 	4 	0 	4 	3 	4 	2 	
2 	1 	2 	3 	2 	1 	1 	3 	
0 	3 	3 	0 	1 	2 	4 	5 	
optimal policy
v	v	<	>	>	>	>	v	
v	v	>	v	>	>	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	v	>	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
optimal values
-2.71	-2.73	-3.02	-2.86	-2.05	-1.81	-1.56	-1.52	
-2.42	-2.44	-2.87	-2.06	-2.74	-1.93	-1.68	-1.17	
-2.39	-2.15	-2.11	-1.81	-1.93	-1.68	-0.86	-0.81	
-2.39	-2.21	-1.91	-1.57	-1.73	-1.06	-0.81	-0.45	
-2.15	-2.90	-2.08	-1.26	-0.91	-0.86	-0.76	-0.40	
-1.81	-2.29	-1.57	-1.73	-0.96	-0.60	-0.65	-0.34	
-1.77	-1.47	-1.22	-0.91	-0.86	-0.55	-0.29	-0.03	
-2.13	-1.31	-1.26	-1.73	-0.90	-0.65	-0.33	0.03	
map_weights [-0.42244677 -0.39705119 -0.15090653 -0.18719503 -0.67359449  0.3903137 ]
MAP reward
-0.15	-0.15	-0.15	-0.42	-0.40	-0.40	-0.19	-0.67	
-0.19	-0.15	-0.42	-0.40	-0.42	-0.40	-0.42	-0.67	
-0.40	-0.19	-0.15	-0.40	-0.40	-0.42	-0.19	-0.67	
-0.40	-0.15	-0.67	-0.15	-0.42	-0.40	-0.67	-0.19	
-0.67	-0.42	-0.42	-0.67	-0.19	-0.40	-0.67	-0.19	
-0.19	-0.42	-0.67	-0.42	-0.67	-0.19	-0.67	-0.15	
-0.15	-0.40	-0.15	-0.19	-0.15	-0.40	-0.40	-0.19	
-0.42	-0.19	-0.19	-0.42	-0.40	-0.15	-0.67	0.39	
Map policy
>	v	v	>	>	>	v	v	
>	v	v	v	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	>	v	>	>	v	
v	v	v	v	>	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.68741976479583
mean w [-0.47103026 -0.28998832 -0.22189384 -0.09863084 -0.62078772 -0.04755047]
Mean policy from posterior
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	>	v	v	
v	^	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	^	>	>	>	.	
Mean rewards
-0.22	-0.22	-0.22	-0.47	-0.29	-0.29	-0.10	-0.62	
-0.10	-0.22	-0.47	-0.29	-0.47	-0.29	-0.47	-0.62	
-0.29	-0.10	-0.22	-0.29	-0.29	-0.47	-0.10	-0.62	
-0.29	-0.22	-0.62	-0.22	-0.47	-0.29	-0.62	-0.10	
-0.62	-0.47	-0.47	-0.62	-0.10	-0.29	-0.62	-0.10	
-0.10	-0.47	-0.62	-0.47	-0.62	-0.10	-0.62	-0.22	
-0.22	-0.29	-0.22	-0.10	-0.22	-0.29	-0.29	-0.10	
-0.47	-0.10	-0.10	-0.47	-0.29	-0.22	-0.62	-0.05	
mean = 0.12418933084531014, map = 0.24849188961430646
CVaR policy
v	v	>	>	>	>	v	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
v	v	>	>	v	>	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	v	>	>	v	>	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	^	>	>	>	.	
CVaR policy
v	v	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	>	v	v	
v	v	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	^	>	>	>	.	
CVaR policy
v	v	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	>	v	v	
v	^	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	^	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	v	>	v	v	
v	^	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	^	>	>	>	.	
cvar = , 0.11596042697719833, 0.12419749241956657, 0.12629980309814837, 0.11877427439540966, 0.12418934633847689
