==========
iteration 0
==========
weights [-0.08410756 -0.31708325 -0.40439986 -0.52150567 -0.58915789  0.3313151 ]
expeced value MDP LP -1.640679819826811
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 3), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.24492719 -0.4553005  -0.44347413  0.59901609  0.40064433 -0.1292543 ]
w_map [-0.18918192 -0.38459913 -0.39386622 -0.54636302 -0.55878581 -0.22451943] loglik -1.386295948844655
accepted/total = 1560/3000 = 0.52
-------
true weights [-0.08410756 -0.31708325 -0.40439986 -0.52150567 -0.58915789  0.3313151 ]
features
0 	1 	0 	4 	3 	4 	3 	2 	
4 	3 	3 	0 	4 	1 	3 	3 	
3 	2 	1 	4 	0 	3 	3 	2 	
4 	3 	1 	1 	4 	0 	3 	2 	
0 	4 	0 	0 	4 	2 	4 	3 	
2 	0 	1 	4 	0 	0 	1 	3 	
1 	1 	0 	2 	4 	1 	2 	0 	
2 	0 	4 	2 	0 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	<	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.88	-2.82	-2.53	-2.80	-2.68	-2.41	-2.85	-2.48	
-3.41	-2.85	-2.47	-2.23	-2.18	-1.84	-2.35	-2.09	
-2.85	-2.35	-1.97	-2.17	-1.61	-1.54	-2.05	-1.59	
-2.57	-2.17	-1.67	-1.60	-1.61	-1.03	-1.54	-1.20	
-2.00	-1.94	-1.36	-1.29	-1.22	-0.96	-1.06	-0.80	
-1.98	-1.59	-1.52	-1.22	-0.64	-0.56	-0.48	-0.28	
-2.12	-1.82	-1.52	-1.45	-1.06	-0.48	-0.16	0.24	
-2.09	-1.70	-1.63	-1.05	-0.66	-0.58	0.01	0.33	
map_weights [-0.18918192 -0.38459913 -0.39386622 -0.54636302 -0.55878581 -0.22451943]
MAP reward
-0.19	-0.38	-0.19	-0.56	-0.55	-0.56	-0.55	-0.39	
-0.56	-0.55	-0.55	-0.19	-0.56	-0.38	-0.55	-0.55	
-0.55	-0.39	-0.38	-0.56	-0.19	-0.55	-0.55	-0.39	
-0.56	-0.55	-0.38	-0.38	-0.56	-0.19	-0.55	-0.39	
-0.19	-0.56	-0.19	-0.19	-0.56	-0.39	-0.56	-0.55	
-0.39	-0.19	-0.38	-0.56	-0.19	-0.19	-0.38	-0.55	
-0.38	-0.38	-0.19	-0.39	-0.56	-0.38	-0.39	-0.19	
-0.39	-0.19	-0.56	-0.39	-0.19	-0.56	-0.38	-0.22	
Map policy
>	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	v	>	v	
v	>	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7517367922507203
mean w [-0.10805425 -0.23479668 -0.28689035 -0.5212312  -0.6663237   0.05767483]
Mean policy from posterior
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	<	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.11	-0.23	-0.11	-0.67	-0.52	-0.67	-0.52	-0.29	
-0.67	-0.52	-0.52	-0.11	-0.67	-0.23	-0.52	-0.52	
-0.52	-0.29	-0.23	-0.67	-0.11	-0.52	-0.52	-0.29	
-0.67	-0.52	-0.23	-0.23	-0.67	-0.11	-0.52	-0.29	
-0.11	-0.67	-0.11	-0.11	-0.67	-0.29	-0.67	-0.52	
-0.29	-0.11	-0.23	-0.67	-0.11	-0.11	-0.23	-0.52	
-0.23	-0.23	-0.11	-0.29	-0.67	-0.23	-0.29	-0.11	
-0.29	-0.11	-0.67	-0.29	-0.11	-0.67	-0.23	0.06	
mean = 0.007818039996415171, map = 0.007781383497529992
CVaR policy
>	>	v	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	>	v	
>	v	v	v	v	v	<	v	
>	>	v	>	>	v	>	v	
v	>	v	v	>	v	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	<	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	<	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	v	v	v	v	v	<	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	v	<	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
cvar = , 0.007965858230921219, 0.005266528291345773, 0.0020687532006675724, 0.00781803881522225, 0.007818038768888647
==========
iteration 1
==========
weights [-0.33233128 -0.19253504 -0.18998862 -0.66592242 -0.41271132  0.4501191 ]
expeced value MDP LP -1.6399212618541847
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 1), (33, 1), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[ 0.20753677 -0.81256088  0.33631335  0.14867233  0.17679047 -0.36084388]
w_map [-0.51379596 -0.25312872 -0.27612052 -0.50812893 -0.57861303 -0.05204772] loglik -0.6931471821981781
accepted/total = 1988/3000 = 0.6626666666666666
-------
true weights [-0.33233128 -0.19253504 -0.18998862 -0.66592242 -0.41271132  0.4501191 ]
features
1 	3 	4 	1 	4 	2 	0 	2 	
2 	0 	0 	4 	0 	4 	2 	0 	
2 	4 	2 	1 	0 	4 	4 	3 	
1 	0 	1 	3 	3 	4 	2 	0 	
1 	2 	2 	2 	1 	3 	2 	4 	
1 	3 	1 	3 	1 	2 	2 	3 	
2 	3 	0 	3 	1 	4 	3 	1 	
2 	3 	2 	3 	0 	3 	0 	5 	
optimal policy
v	v	v	v	>	>	v	v	
v	>	v	v	>	>	v	<	
v	>	v	<	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
optimal values
-2.54	-3.12	-2.56	-2.59	-2.43	-2.03	-1.86	-2.03	
-2.37	-2.48	-2.17	-2.42	-2.26	-1.94	-1.55	-1.86	
-2.20	-2.25	-1.86	-2.03	-2.08	-1.77	-1.37	-1.80	
-2.03	-2.00	-1.69	-1.98	-1.81	-1.37	-0.97	-1.15	
-1.86	-1.68	-1.51	-1.33	-1.15	-1.44	-0.79	-0.82	
-2.03	-2.33	-1.69	-1.63	-0.97	-0.79	-0.60	-0.42	
-2.20	-2.63	-1.98	-1.66	-1.01	-0.82	-0.42	0.25	
-2.37	-2.36	-1.71	-1.54	-0.88	-0.55	0.11	0.45	
map_weights [-0.51379596 -0.25312872 -0.27612052 -0.50812893 -0.57861303 -0.05204772]
MAP reward
-0.25	-0.51	-0.58	-0.25	-0.58	-0.28	-0.51	-0.28	
-0.28	-0.51	-0.51	-0.58	-0.51	-0.58	-0.28	-0.51	
-0.28	-0.58	-0.28	-0.25	-0.51	-0.58	-0.58	-0.51	
-0.25	-0.51	-0.25	-0.51	-0.51	-0.58	-0.28	-0.51	
-0.25	-0.28	-0.28	-0.28	-0.25	-0.51	-0.28	-0.58	
-0.25	-0.51	-0.25	-0.51	-0.25	-0.28	-0.28	-0.51	
-0.28	-0.51	-0.51	-0.51	-0.25	-0.58	-0.51	-0.25	
-0.28	-0.51	-0.28	-0.51	-0.51	-0.51	-0.51	-0.05	
Map policy
v	<	v	v	>	>	v	v	
v	>	v	v	v	>	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7927749714749122
mean w [-0.52594118 -0.18854042 -0.14439584 -0.49592331 -0.44074334  0.0113159 ]
Mean policy from posterior
v	<	v	v	>	v	v	v	
v	<	v	v	>	>	v	<	
v	>	v	<	v	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	>	v	
^	<	^	>	>	>	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.19	-0.50	-0.44	-0.19	-0.44	-0.14	-0.53	-0.14	
-0.14	-0.53	-0.53	-0.44	-0.53	-0.44	-0.14	-0.53	
-0.14	-0.44	-0.14	-0.19	-0.53	-0.44	-0.44	-0.50	
-0.19	-0.53	-0.19	-0.50	-0.50	-0.44	-0.14	-0.53	
-0.19	-0.14	-0.14	-0.14	-0.19	-0.50	-0.14	-0.44	
-0.19	-0.50	-0.19	-0.50	-0.19	-0.14	-0.14	-0.50	
-0.14	-0.50	-0.53	-0.50	-0.19	-0.44	-0.50	-0.19	
-0.14	-0.50	-0.14	-0.50	-0.53	-0.50	-0.53	0.01	
mean = 0.016411027560462976, map = 0.04222603348025
CVaR policy
v	>	v	>	>	v	v	v	
v	v	v	v	>	>	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	v	
v	v	v	v	>	>	v	<	
v	>	v	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
^	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	v	v	v	v	<	
v	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	<	v	v	v	<	
v	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	>	v	v	<	
v	v	v	v	>	>	v	<	
v	>	v	<	v	v	v	<	
v	v	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
^	^	^	>	>	>	v	v	
^	^	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.04148620284368176, 0.017066840229130964, 0.01804630873323343, 0.014054634669455002, 0.01627270914778789
==========
iteration 2
==========
weights [-0.17656382 -0.09591669 -0.40970874 -0.69359356 -0.55654522  0.03081159]
expeced value MDP LP -2.1813044872200917
demonstration
[(0, 3), (8, 1), (9, 3), (17, 1), (18, 3), (26, 3), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.64662232  0.0286945   0.55358268  0.04215662 -0.44306747  0.27661609]
w_map [-0.36553969 -0.11833403 -0.62637816 -0.58300812 -0.34430046  0.03983621] loglik -1.3862943638854546
accepted/total = 1770/3000 = 0.59
-------
true weights [-0.17656382 -0.09591669 -0.40970874 -0.69359356 -0.55654522  0.03081159]
features
3 	4 	4 	2 	3 	2 	2 	2 	
4 	4 	3 	3 	2 	1 	4 	0 	
3 	0 	1 	4 	3 	4 	3 	3 	
3 	0 	4 	4 	2 	0 	0 	4 	
1 	4 	1 	1 	0 	3 	4 	3 	
2 	1 	3 	0 	4 	0 	1 	0 	
1 	4 	2 	3 	2 	2 	2 	4 	
3 	0 	4 	4 	0 	1 	4 	5 	
optimal policy
v	v	v	>	>	v	<	v	
>	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.30	-3.64	-3.63	-3.70	-3.33	-2.66	-3.04	-3.13	
-3.64	-3.11	-3.10	-3.33	-2.66	-2.27	-2.71	-2.75	
-3.25	-2.58	-2.43	-2.82	-2.73	-2.20	-2.18	-2.60	
-3.10	-2.51	-2.36	-2.28	-2.05	-1.66	-1.50	-1.93	
-2.43	-2.36	-1.82	-1.74	-1.66	-1.64	-1.33	-1.38	
-2.80	-2.41	-2.34	-1.66	-1.50	-0.95	-0.79	-0.70	
-2.64	-2.57	-2.26	-1.87	-1.19	-1.02	-0.93	-0.53	
-2.71	-2.04	-1.88	-1.34	-0.79	-0.62	-0.53	0.03	
map_weights [-0.36553969 -0.11833403 -0.62637816 -0.58300812 -0.34430046  0.03983621]
MAP reward
-0.58	-0.34	-0.34	-0.63	-0.58	-0.63	-0.63	-0.63	
-0.34	-0.34	-0.58	-0.58	-0.63	-0.12	-0.34	-0.37	
-0.58	-0.37	-0.12	-0.34	-0.58	-0.34	-0.58	-0.58	
-0.58	-0.37	-0.34	-0.34	-0.63	-0.37	-0.37	-0.34	
-0.12	-0.34	-0.12	-0.12	-0.37	-0.58	-0.34	-0.58	
-0.63	-0.12	-0.58	-0.37	-0.34	-0.37	-0.12	-0.37	
-0.12	-0.34	-0.63	-0.58	-0.63	-0.63	-0.63	-0.34	
-0.58	-0.37	-0.34	-0.34	-0.37	-0.12	-0.34	0.04	
Map policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8544410286685338
mean w [-0.23631047 -0.10246735 -0.57542829 -0.58033976 -0.29359842 -0.15477571]
Mean policy from posterior
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.58	-0.29	-0.29	-0.58	-0.58	-0.58	-0.58	-0.58	
-0.29	-0.29	-0.58	-0.58	-0.58	-0.10	-0.29	-0.24	
-0.58	-0.24	-0.10	-0.29	-0.58	-0.29	-0.58	-0.58	
-0.58	-0.24	-0.29	-0.29	-0.58	-0.24	-0.24	-0.29	
-0.10	-0.29	-0.10	-0.10	-0.24	-0.58	-0.29	-0.58	
-0.58	-0.10	-0.58	-0.24	-0.29	-0.24	-0.10	-0.24	
-0.10	-0.29	-0.58	-0.58	-0.58	-0.58	-0.58	-0.29	
-0.58	-0.24	-0.29	-0.29	-0.24	-0.10	-0.29	-0.15	
mean = 0.0173872285820158, map = 0.023323256104845314
CVaR policy
>	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	v	v	
>	v	v	v	>	v	v	v	
>	>	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	>	>	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.005459851960583073, 0.017387228502036223, 0.017387228517999453, 0.017387230591345393, 0.017387228911696084
==========
iteration 3
==========
weights [-0.19171245 -0.31367861 -0.54062804 -0.33488767 -0.23603901  0.63616761]
expeced value MDP LP -1.1948997534909562
demonstration
[(0, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 3), (20, 1), (21, 1), (22, 1), (23, 3), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.42090411  0.0892561   0.44525573 -0.58333554  0.06814966  0.52124436]
w_map [-0.24791824 -0.48539755 -0.59640022 -0.40311587 -0.33983271 -0.26314227] loglik -1.386294361054226
accepted/total = 1614/3000 = 0.538
-------
true weights [-0.19171245 -0.31367861 -0.54062804 -0.33488767 -0.23603901  0.63616761]
features
3 	2 	4 	2 	0 	4 	3 	4 	
0 	0 	1 	0 	0 	0 	3 	0 	
4 	3 	4 	2 	0 	4 	0 	0 	
4 	3 	4 	4 	1 	1 	0 	1 	
3 	3 	1 	1 	3 	1 	2 	4 	
3 	3 	1 	2 	1 	4 	0 	0 	
1 	3 	0 	1 	3 	2 	0 	3 	
4 	1 	3 	2 	1 	4 	3 	5 	
optimal policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.54	-2.58	-2.10	-2.11	-1.59	-1.45	-1.37	-1.05	
-2.23	-2.06	-1.89	-1.59	-1.41	-1.23	-1.15	-0.82	
-2.44	-2.23	-1.91	-1.76	-1.23	-1.05	-0.82	-0.64	
-2.23	-2.01	-1.69	-1.47	-1.25	-0.94	-0.64	-0.45	
-2.21	-1.89	-1.57	-1.27	-0.97	-0.64	-0.63	-0.14	
-2.02	-1.71	-1.38	-1.17	-0.64	-0.33	-0.09	0.10	
-1.70	-1.41	-1.08	-0.90	-0.59	-0.44	0.10	0.29	
-1.65	-1.43	-1.12	-0.80	-0.26	0.06	0.29	0.64	
map_weights [-0.24791824 -0.48539755 -0.59640022 -0.40311587 -0.33983271 -0.26314227]
MAP reward
-0.40	-0.60	-0.34	-0.60	-0.25	-0.34	-0.40	-0.34	
-0.25	-0.25	-0.49	-0.25	-0.25	-0.25	-0.40	-0.25	
-0.34	-0.40	-0.34	-0.60	-0.25	-0.34	-0.25	-0.25	
-0.34	-0.40	-0.34	-0.34	-0.49	-0.49	-0.25	-0.49	
-0.40	-0.40	-0.49	-0.49	-0.40	-0.49	-0.60	-0.34	
-0.40	-0.40	-0.49	-0.60	-0.49	-0.34	-0.25	-0.25	
-0.49	-0.40	-0.25	-0.49	-0.40	-0.60	-0.25	-0.40	
-0.34	-0.49	-0.40	-0.60	-0.49	-0.34	-0.40	-0.26	
Map policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4786990960844415
mean w [-0.12418297 -0.32156366 -0.59883337 -0.46746326 -0.1558764   0.25075163]
Mean policy from posterior
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	>	v	
^	^	^	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.47	-0.60	-0.16	-0.60	-0.12	-0.16	-0.47	-0.16	
-0.12	-0.12	-0.32	-0.12	-0.12	-0.12	-0.47	-0.12	
-0.16	-0.47	-0.16	-0.60	-0.12	-0.16	-0.12	-0.12	
-0.16	-0.47	-0.16	-0.16	-0.32	-0.32	-0.12	-0.32	
-0.47	-0.47	-0.32	-0.32	-0.47	-0.32	-0.60	-0.16	
-0.47	-0.47	-0.32	-0.60	-0.32	-0.16	-0.12	-0.12	
-0.32	-0.47	-0.12	-0.32	-0.47	-0.60	-0.12	-0.47	
-0.16	-0.32	-0.47	-0.60	-0.32	-0.16	-0.47	0.25	
mean = 0.02799387439307366, map = 0.006857770342478364
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	v	v	>	v	
^	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	v	v	>	v	
^	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	>	v	
^	>	v	>	>	>	>	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	>	>	v	v	v	v	
^	^	v	>	>	>	v	v	
^	>	>	>	>	>	>	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.007249103186420136, 0.004233454094209943, 0.004233453684346911, 0.02274570904492723, 0.02501203058494661
==========
iteration 4
==========
weights [-0.28507528 -0.44883582 -0.47654705 -0.19042536 -0.65668186  0.15062708]
expeced value MDP LP -2.4888440735753576
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 1), (33, 1), (34, 3), (42, 3), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.3944831  -0.26570365 -0.31672596 -0.64372304  0.50327833  0.07616356]
w_map [-0.24167466 -0.5471858  -0.43543762 -0.15769405 -0.65346897  0.0261922 ] loglik -0.693147197101311
accepted/total = 1900/3000 = 0.6333333333333333
-------
true weights [-0.28507528 -0.44883582 -0.47654705 -0.19042536 -0.65668186  0.15062708]
features
1 	1 	4 	0 	2 	3 	4 	2 	
2 	4 	2 	4 	3 	4 	1 	4 	
1 	1 	2 	0 	1 	2 	1 	4 	
2 	4 	1 	1 	3 	3 	2 	0 	
0 	2 	0 	4 	1 	4 	2 	2 	
1 	2 	0 	4 	2 	1 	0 	1 	
2 	0 	2 	3 	2 	1 	1 	4 	
3 	2 	3 	0 	0 	2 	1 	5 	
optimal policy
v	>	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
v	v	v	v	>	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.90	-4.61	-4.20	-3.58	-3.33	-3.37	-3.43	-3.41	
-4.49	-4.40	-3.81	-3.51	-2.88	-3.21	-2.81	-2.96	
-4.06	-3.78	-3.36	-2.98	-2.72	-2.58	-2.38	-2.33	
-3.64	-3.54	-2.92	-2.72	-2.29	-2.12	-1.95	-1.69	
-3.20	-2.94	-2.49	-2.78	-2.35	-2.10	-1.49	-1.42	
-3.01	-2.68	-2.23	-2.14	-1.92	-1.46	-1.02	-0.95	
-2.59	-2.23	-1.96	-1.50	-1.52	-1.19	-0.75	-0.51	
-2.13	-1.96	-1.50	-1.33	-1.05	-0.77	-0.30	0.15	
map_weights [-0.24167466 -0.5471858  -0.43543762 -0.15769405 -0.65346897  0.0261922 ]
MAP reward
-0.55	-0.55	-0.65	-0.24	-0.44	-0.16	-0.65	-0.44	
-0.44	-0.65	-0.44	-0.65	-0.16	-0.65	-0.55	-0.65	
-0.55	-0.55	-0.44	-0.24	-0.55	-0.44	-0.55	-0.65	
-0.44	-0.65	-0.55	-0.55	-0.16	-0.16	-0.44	-0.24	
-0.24	-0.44	-0.24	-0.65	-0.55	-0.65	-0.44	-0.44	
-0.55	-0.44	-0.24	-0.65	-0.44	-0.55	-0.24	-0.55	
-0.44	-0.24	-0.44	-0.16	-0.44	-0.55	-0.55	-0.65	
-0.16	-0.44	-0.16	-0.24	-0.24	-0.44	-0.55	0.03	
Map policy
v	>	>	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.924689658118513
mean w [-0.1346987  -0.4936443  -0.29695863 -0.25283837 -0.55963418  0.1694949 ]
Mean policy from posterior
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	<	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	<	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.49	-0.49	-0.56	-0.13	-0.30	-0.25	-0.56	-0.30	
-0.30	-0.56	-0.30	-0.56	-0.25	-0.56	-0.49	-0.56	
-0.49	-0.49	-0.30	-0.13	-0.49	-0.30	-0.49	-0.56	
-0.30	-0.56	-0.49	-0.49	-0.25	-0.25	-0.30	-0.13	
-0.13	-0.30	-0.13	-0.56	-0.49	-0.56	-0.30	-0.30	
-0.49	-0.30	-0.13	-0.56	-0.30	-0.49	-0.13	-0.49	
-0.30	-0.13	-0.30	-0.25	-0.30	-0.49	-0.49	-0.56	
-0.25	-0.30	-0.25	-0.13	-0.13	-0.30	-0.49	0.17	
mean = 0.04002526208049506, map = 0.0034037012791197085
CVaR policy
v	>	v	>	v	v	>	v	
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	>	>	v	
>	>	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	<	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
v	>	v	<	v	v	v	v	
v	v	v	>	>	>	v	v	
>	>	v	<	v	>	v	v	
>	v	v	v	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.0675934884397762, 0.018646004218450862, 0.014748645198109589, 0.03464083913316074, 0.04002526189633926
==========
iteration 5
==========
weights [-0.29346291 -0.17950379 -0.77916458 -0.47179135 -0.1487552   0.17275788]
expeced value MDP LP -1.6481470730449301
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 3), (34, 0), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.2828946   0.18637476  0.68916833  0.28601287  0.35954916 -0.4463218 ]
w_map [-0.32964627 -0.20879249 -0.68816943 -0.60241474 -0.10524872  0.0134556 ] loglik -0.6931473195298281
accepted/total = 1309/3000 = 0.43633333333333335
-------
true weights [-0.29346291 -0.17950379 -0.77916458 -0.47179135 -0.1487552   0.17275788]
features
1 	1 	0 	0 	0 	3 	3 	4 	
2 	3 	4 	2 	2 	4 	3 	2 	
4 	0 	0 	2 	4 	1 	0 	3 	
0 	2 	1 	1 	0 	1 	2 	0 	
0 	4 	4 	4 	2 	3 	2 	1 	
0 	4 	2 	3 	3 	2 	3 	0 	
4 	4 	1 	4 	0 	1 	1 	0 	
1 	2 	0 	2 	2 	1 	0 	5 	
optimal policy
>	>	v	<	>	v	v	v	
v	>	v	<	v	v	v	v	
v	>	v	v	>	>	>	v	
v	v	v	v	<	>	>	v	
>	v	<	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
optimal values
-2.77	-2.61	-2.46	-2.73	-2.64	-2.37	-2.53	-2.23	
-2.93	-2.64	-2.19	-2.95	-2.67	-1.91	-2.08	-2.11	
-2.17	-2.33	-2.06	-2.43	-1.91	-1.78	-1.62	-1.34	
-2.04	-2.25	-1.78	-1.67	-1.95	-1.81	-1.65	-0.88	
-1.77	-1.49	-1.62	-1.50	-2.00	-1.71	-1.36	-0.59	
-1.63	-1.35	-1.85	-1.37	-1.23	-1.25	-0.77	-0.41	
-1.35	-1.22	-1.08	-0.91	-0.77	-0.48	-0.30	-0.12	
-1.52	-1.98	-1.36	-1.68	-1.08	-0.30	-0.12	0.17	
map_weights [-0.32964627 -0.20879249 -0.68816943 -0.60241474 -0.10524872  0.0134556 ]
MAP reward
-0.21	-0.21	-0.33	-0.33	-0.33	-0.60	-0.60	-0.11	
-0.69	-0.60	-0.11	-0.69	-0.69	-0.11	-0.60	-0.69	
-0.11	-0.33	-0.33	-0.69	-0.11	-0.21	-0.33	-0.60	
-0.33	-0.69	-0.21	-0.21	-0.33	-0.21	-0.69	-0.33	
-0.33	-0.11	-0.11	-0.11	-0.69	-0.60	-0.69	-0.21	
-0.33	-0.11	-0.69	-0.60	-0.60	-0.69	-0.60	-0.33	
-0.11	-0.11	-0.21	-0.11	-0.33	-0.21	-0.21	-0.33	
-0.21	-0.69	-0.33	-0.69	-0.69	-0.21	-0.33	0.01	
Map policy
>	>	v	<	>	v	>	v	
v	>	v	<	v	v	v	v	
v	>	v	v	>	v	>	v	
v	v	v	v	<	>	>	v	
>	v	<	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
expeced value MDP LP -1.079927442714448
mean w [-0.1937907  -0.10620918 -0.58355286 -0.47518887 -0.06458814  0.10104397]
Mean policy from posterior
>	>	v	<	<	v	>	v	
v	>	v	<	v	v	v	v	
v	>	v	v	v	>	>	v	
v	v	v	v	<	>	>	v	
>	v	<	<	<	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
Mean rewards
-0.11	-0.11	-0.19	-0.19	-0.19	-0.48	-0.48	-0.06	
-0.58	-0.48	-0.06	-0.58	-0.58	-0.06	-0.48	-0.58	
-0.06	-0.19	-0.19	-0.58	-0.06	-0.11	-0.19	-0.48	
-0.19	-0.58	-0.11	-0.11	-0.19	-0.11	-0.58	-0.19	
-0.19	-0.06	-0.06	-0.06	-0.58	-0.48	-0.58	-0.11	
-0.19	-0.06	-0.58	-0.48	-0.48	-0.58	-0.48	-0.19	
-0.06	-0.06	-0.11	-0.06	-0.19	-0.11	-0.11	-0.19	
-0.11	-0.58	-0.19	-0.58	-0.58	-0.11	-0.19	0.10	
mean = 0.0439871680046624, map = 0.035206007037894116
CVaR policy
>	>	v	<	>	v	>	v	
v	>	v	<	v	v	v	v	
v	>	v	v	>	v	>	v	
v	v	v	v	<	>	>	v	
>	v	<	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	>	v	<	<	v	>	v	
v	>	v	<	v	v	v	v	
v	>	v	v	>	v	>	v	
v	v	v	v	<	>	>	v	
>	v	<	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	>	v	<	<	v	>	v	
v	>	v	<	v	v	v	v	
v	>	v	v	v	v	>	v	
v	v	v	v	<	>	>	v	
>	v	<	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	>	v	<	<	v	>	v	
v	>	v	<	v	v	v	v	
v	>	v	v	v	>	>	v	
v	v	v	v	<	>	>	v	
>	v	<	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
CVaR policy
>	>	v	<	<	v	>	v	
v	>	v	<	v	v	v	v	
v	>	v	v	v	>	>	v	
v	v	v	v	<	>	>	v	
>	v	<	<	<	v	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
cvar = , 0.019843548301884395, 0.037935250747207494, 0.0446320367670523, 0.03588708954387276, 0.043987167118423764
==========
iteration 6
==========
weights [-0.41968772 -0.19150613 -0.43021903 -0.38564015 -0.19455769  0.64461476]
expeced value MDP LP -1.1838108390563458
demonstration
[(0, 1), (1, 1), (2, 3), (10, 1), (11, 3), (19, 1), (20, 1), (21, 3), (29, 3), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.12397259  0.06819702 -0.78609837  0.01043411  0.01045488 -0.60150739]
w_map [-0.50659792 -0.14938118 -0.6876858  -0.35473811 -0.34051401  0.07964378] loglik -0.6931482138736156
accepted/total = 1465/3000 = 0.48833333333333334
-------
true weights [-0.41968772 -0.19150613 -0.43021903 -0.38564015 -0.19455769  0.64461476]
features
2 	3 	0 	0 	4 	4 	3 	4 	
0 	2 	4 	2 	2 	0 	4 	2 	
2 	3 	2 	4 	1 	1 	0 	0 	
2 	2 	1 	0 	4 	0 	0 	1 	
4 	2 	2 	2 	0 	1 	4 	4 	
1 	4 	1 	2 	1 	4 	2 	1 	
0 	4 	4 	3 	2 	0 	0 	1 	
2 	4 	4 	0 	2 	0 	2 	5 	
optimal policy
>	>	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.06	-2.66	-2.29	-2.11	-1.70	-1.52	-1.53	-1.17	
-2.70	-2.30	-1.89	-1.71	-1.53	-1.34	-1.16	-0.98	
-2.49	-2.08	-1.71	-1.30	-1.11	-0.93	-0.97	-0.56	
-2.12	-1.94	-1.52	-1.35	-0.94	-0.75	-0.56	-0.14	
-1.71	-1.77	-1.59	-1.17	-0.75	-0.33	-0.14	0.05	
-1.53	-1.35	-1.17	-0.99	-0.56	-0.37	-0.18	0.25	
-1.93	-1.53	-1.35	-1.20	-0.82	-0.40	0.02	0.45	
-1.84	-1.42	-1.24	-1.06	-0.64	-0.21	0.21	0.64	
map_weights [-0.50659792 -0.14938118 -0.6876858  -0.35473811 -0.34051401  0.07964378]
MAP reward
-0.69	-0.35	-0.51	-0.51	-0.34	-0.34	-0.35	-0.34	
-0.51	-0.69	-0.34	-0.69	-0.69	-0.51	-0.34	-0.69	
-0.69	-0.35	-0.69	-0.34	-0.15	-0.15	-0.51	-0.51	
-0.69	-0.69	-0.15	-0.51	-0.34	-0.51	-0.51	-0.15	
-0.34	-0.69	-0.69	-0.69	-0.51	-0.15	-0.34	-0.34	
-0.15	-0.34	-0.15	-0.69	-0.15	-0.34	-0.69	-0.15	
-0.51	-0.34	-0.34	-0.35	-0.69	-0.51	-0.51	-0.15	
-0.69	-0.34	-0.34	-0.51	-0.69	-0.51	-0.69	0.08	
Map policy
>	>	v	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0219203655765257
mean w [-0.46061803 -0.10512985 -0.61215929 -0.31773276 -0.2469882  -0.21656018]
Mean policy from posterior
>	>	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	^	>	v	
^	>	^	>	>	>	>	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.61	-0.32	-0.46	-0.46	-0.25	-0.25	-0.32	-0.25	
-0.46	-0.61	-0.25	-0.61	-0.61	-0.46	-0.25	-0.61	
-0.61	-0.32	-0.61	-0.25	-0.11	-0.11	-0.46	-0.46	
-0.61	-0.61	-0.11	-0.46	-0.25	-0.46	-0.46	-0.11	
-0.25	-0.61	-0.61	-0.61	-0.46	-0.11	-0.25	-0.25	
-0.11	-0.25	-0.11	-0.61	-0.11	-0.25	-0.61	-0.11	
-0.46	-0.25	-0.25	-0.32	-0.61	-0.46	-0.46	-0.11	
-0.61	-0.25	-0.25	-0.46	-0.61	-0.46	-0.61	-0.22	
mean = 0.04904281188469772, map = 0.001294551407445832
CVaR policy
>	>	v	>	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	>	v	v	>	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	^	>	v	
^	>	^	>	>	>	>	v	
>	>	^	>	>	>	>	.	
cvar = , 0.007713396065342604, 0.0012945477786552306, 0.0009940601351610123, -2.9929532185946073e-09, 0.049042811844976164
==========
iteration 7
==========
weights [-0.25808208 -0.27387159 -0.54798065 -0.11659288 -0.73758758  0.02181429]
expeced value MDP LP -1.7243150469732487
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.57741193 -0.11874914 -0.42947639 -0.40010865 -0.10057445 -0.545749  ]
w_map [-0.20592906 -0.43134035 -0.6056818  -0.19467986 -0.60174573  0.06848441] loglik -3.4821360728187756e-07
accepted/total = 1570/3000 = 0.5233333333333333
-------
true weights [-0.25808208 -0.27387159 -0.54798065 -0.11659288 -0.73758758  0.02181429]
features
0 	3 	1 	1 	4 	0 	0 	4 	
3 	2 	1 	4 	1 	2 	1 	2 	
2 	2 	3 	1 	1 	4 	0 	1 	
3 	4 	0 	4 	4 	3 	1 	2 	
1 	1 	4 	4 	4 	3 	3 	1 	
0 	3 	3 	1 	3 	1 	4 	1 	
4 	0 	4 	1 	2 	1 	1 	3 	
2 	2 	2 	1 	4 	0 	3 	5 	
optimal policy
v	>	v	>	>	>	v	<	
v	>	v	>	v	>	v	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	^	>	>	v	>	v	
>	^	>	>	>	>	>	.	
optimal values
-2.91	-2.89	-2.80	-2.97	-2.72	-2.01	-1.77	-2.49	
-2.68	-3.07	-2.55	-2.92	-2.20	-2.06	-1.52	-1.98	
-2.59	-2.82	-2.30	-2.20	-1.95	-1.69	-1.26	-1.44	
-2.07	-2.45	-2.33	-2.41	-1.69	-0.97	-1.01	-1.18	
-1.97	-1.73	-2.09	-1.99	-1.59	-0.86	-0.75	-0.64	
-1.71	-1.47	-1.37	-1.26	-1.00	-0.89	-1.10	-0.37	
-2.43	-1.71	-2.09	-1.43	-1.16	-0.62	-0.37	-0.09	
-2.77	-2.24	-1.88	-1.35	-1.09	-0.35	-0.09	0.02	
map_weights [-0.20592906 -0.43134035 -0.6056818  -0.19467986 -0.60174573  0.06848441]
MAP reward
-0.21	-0.19	-0.43	-0.43	-0.60	-0.21	-0.21	-0.60	
-0.19	-0.61	-0.43	-0.60	-0.43	-0.61	-0.43	-0.61	
-0.61	-0.61	-0.19	-0.43	-0.43	-0.60	-0.21	-0.43	
-0.19	-0.60	-0.21	-0.60	-0.60	-0.19	-0.43	-0.61	
-0.43	-0.43	-0.60	-0.60	-0.60	-0.19	-0.19	-0.43	
-0.21	-0.19	-0.19	-0.43	-0.19	-0.43	-0.60	-0.43	
-0.60	-0.21	-0.60	-0.43	-0.61	-0.43	-0.43	-0.19	
-0.61	-0.61	-0.61	-0.43	-0.60	-0.21	-0.19	0.07	
Map policy
v	>	v	>	>	>	v	<	
v	>	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7378534851143743
mean w [-0.14838086 -0.31027247 -0.47109108 -0.15356994 -0.64720093  0.03594818]
Mean policy from posterior
v	<	v	>	>	>	v	<	
v	>	v	>	>	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.15	-0.15	-0.31	-0.31	-0.65	-0.15	-0.15	-0.65	
-0.15	-0.47	-0.31	-0.65	-0.31	-0.47	-0.31	-0.47	
-0.47	-0.47	-0.15	-0.31	-0.31	-0.65	-0.15	-0.31	
-0.15	-0.65	-0.15	-0.65	-0.65	-0.15	-0.31	-0.47	
-0.31	-0.31	-0.65	-0.65	-0.65	-0.15	-0.15	-0.31	
-0.15	-0.15	-0.15	-0.31	-0.15	-0.31	-0.65	-0.31	
-0.65	-0.15	-0.65	-0.31	-0.47	-0.31	-0.31	-0.15	
-0.47	-0.47	-0.47	-0.31	-0.65	-0.15	-0.15	0.04	
mean = 0.03783025660724326, map = 0.025705103760926606
CVaR policy
v	>	v	>	>	>	v	v	
v	>	v	v	v	v	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	v	<	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	>	v	<	
v	>	v	v	v	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	>	v	<	
v	>	v	>	>	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	>	>	>	v	<	
v	>	v	>	>	>	v	v	
v	>	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.06772810275982133, 0.03878274316714969, 0.03878274355159217, 0.03783025696197084, 0.037830257191051375
==========
iteration 8
==========
weights [-0.26192152 -0.1579791  -0.15035918 -0.61724254 -0.70747556  0.04818527]
expeced value MDP LP -1.9647020575994665
demonstration
[(0, 3), (8, 1), (9, 1), (10, 3), (18, 1), (19, 3), (27, 3), (35, 3), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.43416925  0.06146163 -0.03087556  0.67236082  0.00151726 -0.59556264]
w_map [-0.3679027  -0.27397831 -0.04203502 -0.58979418 -0.65944235 -0.07138041] loglik -0.8446824992321158
accepted/total = 1562/3000 = 0.5206666666666667
-------
true weights [-0.26192152 -0.1579791  -0.15035918 -0.61724254 -0.70747556  0.04818527]
features
3 	4 	4 	4 	1 	3 	2 	4 	
1 	2 	0 	4 	0 	1 	0 	3 	
0 	0 	2 	0 	4 	3 	3 	3 	
1 	0 	3 	1 	4 	0 	3 	3 	
0 	3 	4 	0 	0 	0 	4 	4 	
0 	2 	2 	1 	3 	4 	0 	3 	
2 	1 	3 	0 	3 	1 	2 	2 	
0 	3 	3 	1 	2 	3 	4 	5 	
optimal policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	<	<	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	^	>	>	>	^	>	.	
optimal values
-3.27	-3.23	-3.10	-3.40	-2.72	-2.94	-2.71	-3.39	
-2.68	-2.55	-2.42	-2.74	-2.59	-2.35	-2.59	-3.18	
-2.55	-2.42	-2.18	-2.05	-2.74	-2.21	-2.42	-2.62	
-2.31	-2.53	-2.41	-1.81	-2.30	-1.61	-1.82	-2.02	
-2.18	-2.29	-2.25	-1.67	-1.61	-1.36	-1.21	-1.42	
-1.93	-1.69	-1.55	-1.42	-1.63	-1.11	-0.51	-0.72	
-1.96	-1.83	-1.88	-1.27	-1.02	-0.41	-0.25	-0.10	
-2.20	-2.43	-1.91	-1.31	-1.16	-1.02	-0.66	0.05	
map_weights [-0.3679027  -0.27397831 -0.04203502 -0.58979418 -0.65944235 -0.07138041]
MAP reward
-0.59	-0.66	-0.66	-0.66	-0.27	-0.59	-0.04	-0.66	
-0.27	-0.04	-0.37	-0.66	-0.37	-0.27	-0.37	-0.59	
-0.37	-0.37	-0.04	-0.37	-0.66	-0.59	-0.59	-0.59	
-0.27	-0.37	-0.59	-0.27	-0.66	-0.37	-0.59	-0.59	
-0.37	-0.59	-0.66	-0.37	-0.37	-0.37	-0.66	-0.66	
-0.37	-0.04	-0.04	-0.27	-0.59	-0.66	-0.37	-0.59	
-0.04	-0.27	-0.59	-0.37	-0.59	-0.27	-0.04	-0.04	
-0.37	-0.59	-0.59	-0.27	-0.04	-0.59	-0.66	-0.07	
Map policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	>	>	>	>	^	>	.	
expeced value MDP LP -1.979596771654259
mean w [-0.2648574  -0.15405654 -0.12893438 -0.45348707 -0.65115313 -0.23933106]
Mean policy from posterior
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	>	>	>	>	^	>	.	
Mean rewards
-0.45	-0.65	-0.65	-0.65	-0.15	-0.45	-0.13	-0.65	
-0.15	-0.13	-0.26	-0.65	-0.26	-0.15	-0.26	-0.45	
-0.26	-0.26	-0.13	-0.26	-0.65	-0.45	-0.45	-0.45	
-0.15	-0.26	-0.45	-0.15	-0.65	-0.26	-0.45	-0.45	
-0.26	-0.45	-0.65	-0.26	-0.26	-0.26	-0.65	-0.65	
-0.26	-0.13	-0.13	-0.15	-0.45	-0.65	-0.26	-0.45	
-0.13	-0.15	-0.45	-0.26	-0.45	-0.15	-0.13	-0.13	
-0.26	-0.45	-0.45	-0.15	-0.13	-0.45	-0.65	-0.24	
mean = 0.013232378680663626, map = 0.007528672584979468
CVaR policy
v	v	v	>	v	v	v	v	
>	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
>	v	v	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	^	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	>	>	>	>	^	>	.	
CVaR policy
v	v	v	>	v	v	v	<	
>	v	v	v	>	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	>	>	>	v	
^	>	>	>	>	^	>	.	
cvar = , 0.051301455373528704, 0.04197158353555386, 0.013693753650776186, 0.005044448376976929, 0.013232378400111822
==========
iteration 9
==========
weights [-0.5868067  -0.02628692 -0.22383965 -0.35914696 -0.68844222  0.04385757]
expeced value MDP LP -1.8970149022680243
demonstration
[(0, 1), (1, 3), (9, 1), (10, 3), (18, 1), (19, 1), (20, 1), (21, 3), (29, 3), (37, 1), (38, 3), (46, 3), (54, 3), (62, 1), (63, None)]
[ 0.77981719 -0.1684267  -0.21690332  0.06899663 -0.01194947  0.55818207]
w_map [-0.64249309 -0.13419901 -0.25510551 -0.35480389 -0.57273898  0.22405063] loglik -0.00015010141925841936
accepted/total = 946/3000 = 0.31533333333333335
-------
true weights [-0.5868067  -0.02628692 -0.22383965 -0.35914696 -0.68844222  0.04385757]
features
2 	1 	3 	2 	4 	2 	0 	0 	
2 	2 	1 	4 	1 	1 	2 	0 	
2 	3 	3 	1 	1 	2 	3 	4 	
0 	2 	1 	1 	4 	3 	4 	1 	
4 	2 	1 	4 	4 	2 	2 	2 	
2 	2 	2 	0 	3 	3 	3 	3 	
3 	2 	4 	0 	2 	0 	3 	4 	
4 	1 	3 	3 	2 	2 	3 	5 	
optimal policy
>	v	v	v	v	v	<	v	
>	>	v	>	v	v	<	v	
>	v	>	>	>	v	v	v	
>	>	>	^	>	v	>	v	
>	v	^	v	v	>	v	v	
>	v	<	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.75	-2.56	-2.69	-2.88	-2.68	-2.21	-2.77	-3.03	
-2.75	-2.55	-2.35	-2.68	-2.01	-2.01	-2.21	-2.47	
-2.76	-2.57	-2.35	-2.01	-2.01	-2.00	-2.24	-1.90	
-2.79	-2.23	-2.02	-2.02	-2.46	-1.79	-1.90	-1.23	
-2.76	-2.09	-2.03	-2.56	-2.00	-1.45	-1.24	-1.21	
-2.09	-1.88	-2.09	-1.89	-1.32	-1.37	-1.02	-1.00	
-2.02	-1.68	-2.13	-1.55	-0.97	-1.12	-0.67	-0.65	
-2.14	-1.47	-1.45	-1.11	-0.75	-0.54	-0.32	0.04	
map_weights [-0.64249309 -0.13419901 -0.25510551 -0.35480389 -0.57273898  0.22405063]
MAP reward
-0.26	-0.13	-0.35	-0.26	-0.57	-0.26	-0.64	-0.64	
-0.26	-0.26	-0.13	-0.57	-0.13	-0.13	-0.26	-0.64	
-0.26	-0.35	-0.35	-0.13	-0.13	-0.26	-0.35	-0.57	
-0.64	-0.26	-0.13	-0.13	-0.57	-0.35	-0.57	-0.13	
-0.57	-0.26	-0.13	-0.57	-0.57	-0.26	-0.26	-0.26	
-0.26	-0.26	-0.26	-0.64	-0.35	-0.35	-0.35	-0.35	
-0.35	-0.26	-0.57	-0.64	-0.26	-0.64	-0.35	-0.57	
-0.57	-0.13	-0.35	-0.35	-0.26	-0.26	-0.35	0.22	
Map policy
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	v	>	>	>	v	v	v	
>	v	v	^	>	v	>	v	
v	v	v	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5364038609268487
mean w [-0.56518324 -0.10504823 -0.2179661  -0.27187065 -0.594217    0.19922674]
Mean policy from posterior
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	v	>	^	>	v	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.22	-0.11	-0.27	-0.22	-0.59	-0.22	-0.57	-0.57	
-0.22	-0.22	-0.11	-0.59	-0.11	-0.11	-0.22	-0.57	
-0.22	-0.27	-0.27	-0.11	-0.11	-0.22	-0.27	-0.59	
-0.57	-0.22	-0.11	-0.11	-0.59	-0.27	-0.59	-0.11	
-0.59	-0.22	-0.11	-0.59	-0.59	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.57	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.22	-0.59	-0.57	-0.22	-0.57	-0.27	-0.59	
-0.59	-0.11	-0.27	-0.27	-0.22	-0.22	-0.27	0.20	
mean = 0.007434440002522802, map = 0.015362056672247437
CVaR policy
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	^	>	v	v	v	
>	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	v	v	^	>	v	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	<	v	
>	>	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	v	v	^	>	v	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	v	>	^	>	v	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	v	>	^	>	v	v	v	
v	v	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.017070361198321393, 0.016391109677340587, 0.00876417400924745, 0.0074344383634252775, 0.007434438431361379
==========
iteration 10
==========
weights [-0.34467623 -0.45326995 -0.65711793 -0.41425282 -0.26255775  0.05829845]
expeced value MDP LP -2.4697658965533784
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 1), (19, 3), (27, 3), (35, 3), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.12885568  0.73428346 -0.64999146  0.04372387 -0.11160627  0.08583339]
w_map [-0.34952692 -0.43875796 -0.68143765 -0.45039252 -0.12839773  0.04032002] loglik -7.88406588583257e-05
accepted/total = 820/3000 = 0.2733333333333333
-------
true weights [-0.34467623 -0.45326995 -0.65711793 -0.41425282 -0.26255775  0.05829845]
features
0 	1 	1 	4 	4 	2 	2 	1 	
2 	4 	2 	1 	2 	4 	1 	4 	
4 	4 	0 	0 	4 	4 	1 	2 	
3 	1 	3 	4 	1 	1 	1 	0 	
3 	2 	1 	4 	2 	3 	4 	3 	
1 	3 	1 	3 	1 	1 	0 	1 	
4 	0 	1 	4 	3 	1 	0 	2 	
1 	4 	3 	2 	4 	1 	3 	5 	
optimal policy
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.40	-4.10	-3.98	-3.56	-3.47	-3.24	-3.23	-3.08	
-4.30	-3.68	-3.85	-3.33	-3.24	-2.61	-2.60	-2.66	
-3.68	-3.45	-3.22	-2.91	-2.61	-2.37	-2.16	-2.42	
-3.78	-3.40	-2.98	-2.59	-2.56	-2.13	-1.73	-1.78	
-3.52	-3.41	-2.78	-2.35	-2.33	-1.69	-1.29	-1.45	
-3.14	-2.86	-2.54	-2.11	-1.90	-1.48	-1.04	-1.05	
-2.71	-2.47	-2.15	-1.71	-1.46	-1.14	-0.70	-0.60	
-2.78	-2.35	-2.10	-1.71	-1.06	-0.81	-0.36	0.06	
map_weights [-0.34952692 -0.43875796 -0.68143765 -0.45039252 -0.12839773  0.04032002]
MAP reward
-0.35	-0.44	-0.44	-0.13	-0.13	-0.68	-0.68	-0.44	
-0.68	-0.13	-0.68	-0.44	-0.68	-0.13	-0.44	-0.13	
-0.13	-0.13	-0.35	-0.35	-0.13	-0.13	-0.44	-0.68	
-0.45	-0.44	-0.45	-0.13	-0.44	-0.44	-0.44	-0.35	
-0.45	-0.68	-0.44	-0.13	-0.68	-0.45	-0.13	-0.45	
-0.44	-0.45	-0.44	-0.45	-0.44	-0.44	-0.35	-0.44	
-0.13	-0.35	-0.44	-0.13	-0.45	-0.44	-0.35	-0.68	
-0.44	-0.13	-0.45	-0.68	-0.13	-0.44	-0.45	0.04	
Map policy
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	<	
>	>	>	v	>	v	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.2442940178910211
mean w [-0.19338722 -0.34943858 -0.70780852 -0.23320997 -0.07860863  0.23520216]
Mean policy from posterior
>	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	<	
>	>	>	v	>	v	v	v	
^	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	>	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.19	-0.35	-0.35	-0.08	-0.08	-0.71	-0.71	-0.35	
-0.71	-0.08	-0.71	-0.35	-0.71	-0.08	-0.35	-0.08	
-0.08	-0.08	-0.19	-0.19	-0.08	-0.08	-0.35	-0.71	
-0.23	-0.35	-0.23	-0.08	-0.35	-0.35	-0.35	-0.19	
-0.23	-0.71	-0.35	-0.08	-0.71	-0.23	-0.08	-0.23	
-0.35	-0.23	-0.35	-0.23	-0.35	-0.35	-0.19	-0.35	
-0.08	-0.19	-0.35	-0.08	-0.23	-0.35	-0.19	-0.71	
-0.35	-0.08	-0.23	-0.71	-0.08	-0.35	-0.23	0.24	
mean = 0.05072506684640343, map = 0.01074226280697621
CVaR policy
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	<	
>	>	>	v	>	v	v	v	
^	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	<	
>	>	>	v	>	v	v	v	
^	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	>	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	<	
>	>	>	v	>	v	v	v	
^	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	>	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	<	
>	>	>	v	>	v	v	v	
^	>	>	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	>	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	^	^	>	>	>	>	.	
cvar = , -9.722719518379108e-09, 0.025853115387047154, 0.025853114753331408, 0.05072506599734217, 0.05072506672093491
==========
iteration 11
==========
weights [-0.58041721 -0.34975756 -0.46086771 -0.14345249 -0.29632741  0.46903935]
expeced value MDP LP -1.45901076468911
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 3), (48, 3), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.23769885 -0.38254703  0.82785067 -0.16770675  0.24000628 -0.16152932]
w_map [-0.60530166 -0.36071515 -0.50872978 -0.21866653 -0.36847559 -0.2471825 ] loglik -3.511843281955862e-06
accepted/total = 962/3000 = 0.32066666666666666
-------
true weights [-0.58041721 -0.34975756 -0.46086771 -0.14345249 -0.29632741  0.46903935]
features
0 	3 	2 	2 	3 	2 	1 	0 	
3 	0 	2 	0 	3 	1 	3 	1 	
4 	3 	0 	2 	3 	0 	4 	2 	
1 	2 	0 	1 	1 	0 	4 	4 	
4 	0 	1 	0 	4 	3 	2 	1 	
4 	4 	3 	1 	1 	2 	2 	0 	
3 	4 	4 	4 	4 	3 	0 	2 	
3 	3 	4 	2 	3 	4 	3 	5 	
optimal policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.12	-2.73	-2.62	-2.18	-1.73	-2.18	-2.20	-2.55	
-2.56	-3.10	-2.61	-2.17	-1.61	-1.94	-1.87	-1.99	
-2.44	-2.54	-2.49	-1.92	-1.48	-1.86	-1.74	-1.66	
-2.17	-2.42	-2.04	-1.68	-1.35	-1.29	-1.46	-1.21	
-1.84	-1.98	-1.47	-1.58	-1.01	-0.72	-1.17	-0.92	
-1.56	-1.42	-1.13	-1.05	-0.76	-0.58	-0.72	-0.58	
-1.27	-1.29	-1.00	-0.71	-0.42	-0.12	-0.26	0.00	
-1.14	-1.01	-0.87	-0.58	-0.12	0.02	0.32	0.47	
map_weights [-0.60530166 -0.36071515 -0.50872978 -0.21866653 -0.36847559 -0.2471825 ]
MAP reward
-0.61	-0.22	-0.51	-0.51	-0.22	-0.51	-0.36	-0.61	
-0.22	-0.61	-0.51	-0.61	-0.22	-0.36	-0.22	-0.36	
-0.37	-0.22	-0.61	-0.51	-0.22	-0.61	-0.37	-0.51	
-0.36	-0.51	-0.61	-0.36	-0.36	-0.61	-0.37	-0.37	
-0.37	-0.61	-0.36	-0.61	-0.37	-0.22	-0.51	-0.36	
-0.37	-0.37	-0.22	-0.36	-0.36	-0.51	-0.51	-0.61	
-0.22	-0.37	-0.37	-0.37	-0.37	-0.22	-0.61	-0.51	
-0.22	-0.22	-0.37	-0.51	-0.22	-0.37	-0.22	-0.25	
Map policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	>	v	
v	>	v	v	>	v	v	v	
v	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.00153235617039
mean w [-0.54596927 -0.40143914 -0.41072358 -0.09918866 -0.25172636 -0.40011151]
Mean policy from posterior
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	<	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.55	-0.10	-0.41	-0.41	-0.10	-0.41	-0.40	-0.55	
-0.10	-0.55	-0.41	-0.55	-0.10	-0.40	-0.10	-0.40	
-0.25	-0.10	-0.55	-0.41	-0.10	-0.55	-0.25	-0.41	
-0.40	-0.41	-0.55	-0.40	-0.40	-0.55	-0.25	-0.25	
-0.25	-0.55	-0.40	-0.55	-0.25	-0.10	-0.41	-0.40	
-0.25	-0.25	-0.10	-0.40	-0.40	-0.41	-0.41	-0.55	
-0.10	-0.25	-0.25	-0.25	-0.25	-0.10	-0.55	-0.41	
-0.10	-0.10	-0.25	-0.41	-0.10	-0.25	-0.10	-0.40	
mean = 0.006427214585015495, map = 0.020713424987158335
CVaR policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	<	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	>	v	<	v	v	
v	v	>	>	v	<	v	<	
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	<	v	
v	>	v	v	v	v	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.004152696428150193, 0.0002349113269812797, 0.00012979753959485585, 0.006427214353060595, 0.006427218552984781
==========
iteration 12
==========
weights [-0.52224542 -0.36797392 -0.51471427 -0.42126538 -0.30986084  0.23118365]
expeced value MDP LP -2.5537975529811376
demonstration
[(0, 3), (8, 1), (9, 1), (10, 1), (11, 3), (19, 1), (20, 3), (28, 3), (36, 3), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.08983181  0.47897117  0.32531273  0.71103612 -0.22855786  0.31444783]
w_map [-0.64007712 -0.23481569 -0.61160473 -0.32961504 -0.14418474 -0.17795282] loglik -1.3862943794760554
accepted/total = 1176/3000 = 0.392
-------
true weights [-0.52224542 -0.36797392 -0.51471427 -0.42126538 -0.30986084  0.23118365]
features
2 	1 	3 	1 	0 	4 	0 	3 	
1 	3 	4 	4 	4 	2 	0 	2 	
0 	2 	0 	4 	4 	0 	1 	0 	
2 	4 	4 	3 	1 	2 	3 	4 	
2 	0 	4 	3 	4 	0 	0 	2 	
1 	2 	0 	4 	3 	1 	2 	0 	
1 	0 	1 	0 	3 	0 	4 	0 	
0 	1 	2 	3 	1 	1 	2 	5 	
optimal policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.78	-4.31	-3.98	-3.65	-3.53	-3.63	-3.36	-3.01	
-4.31	-3.98	-3.59	-3.32	-3.04	-3.35	-2.86	-2.62	
-4.33	-3.85	-3.53	-3.04	-2.75	-2.86	-2.37	-2.12	
-3.85	-3.37	-3.09	-2.87	-2.47	-2.46	-2.02	-1.62	
-3.78	-3.30	-2.81	-2.52	-2.12	-1.97	-1.61	-1.32	
-3.45	-3.11	-2.62	-2.12	-1.83	-1.46	-1.10	-0.81	
-3.11	-2.77	-2.27	-1.93	-1.42	-1.11	-0.59	-0.29	
-2.77	-2.27	-1.92	-1.42	-1.01	-0.65	-0.29	0.23	
map_weights [-0.64007712 -0.23481569 -0.61160473 -0.32961504 -0.14418474 -0.17795282]
MAP reward
-0.61	-0.23	-0.33	-0.23	-0.64	-0.14	-0.64	-0.33	
-0.23	-0.33	-0.14	-0.14	-0.14	-0.61	-0.64	-0.61	
-0.64	-0.61	-0.64	-0.14	-0.14	-0.64	-0.23	-0.64	
-0.61	-0.14	-0.14	-0.33	-0.23	-0.61	-0.33	-0.14	
-0.61	-0.64	-0.14	-0.33	-0.14	-0.64	-0.64	-0.61	
-0.23	-0.61	-0.64	-0.14	-0.33	-0.23	-0.61	-0.64	
-0.23	-0.64	-0.23	-0.64	-0.33	-0.64	-0.14	-0.64	
-0.64	-0.23	-0.61	-0.33	-0.23	-0.23	-0.61	-0.18	
Map policy
>	>	v	v	v	v	<	v	
>	>	>	v	v	<	v	v	
^	v	>	>	v	<	v	v	
>	>	v	>	v	<	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5024883502751636
mean w [-0.68037804 -0.15463956 -0.47983898 -0.23232248 -0.10028699  0.32760404]
Mean policy from posterior
>	>	v	v	v	v	<	<	
>	>	>	v	v	<	v	v	
^	v	>	>	v	<	v	v	
>	>	v	>	v	<	v	v	
^	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.48	-0.15	-0.23	-0.15	-0.68	-0.10	-0.68	-0.23	
-0.15	-0.23	-0.10	-0.10	-0.10	-0.48	-0.68	-0.48	
-0.68	-0.48	-0.68	-0.10	-0.10	-0.68	-0.15	-0.68	
-0.48	-0.10	-0.10	-0.23	-0.15	-0.48	-0.23	-0.10	
-0.48	-0.68	-0.10	-0.23	-0.10	-0.68	-0.68	-0.48	
-0.15	-0.48	-0.68	-0.10	-0.23	-0.15	-0.48	-0.68	
-0.15	-0.68	-0.15	-0.68	-0.23	-0.68	-0.10	-0.68	
-0.68	-0.15	-0.48	-0.23	-0.15	-0.15	-0.48	0.33	
mean = 0.07478266893129915, map = 0.04056895797436422
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	v	v	
v	v	>	>	v	<	v	v	
>	>	v	>	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	v	
>	>	>	v	v	<	v	v	
v	v	>	>	v	<	v	v	
>	>	v	>	v	<	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	v	
>	>	>	v	v	<	v	v	
^	v	>	>	v	<	v	v	
>	>	v	>	v	<	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	<	
>	>	>	v	v	<	v	v	
^	v	>	>	v	<	v	v	
>	>	v	>	v	<	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	<	<	
>	>	>	v	v	<	v	v	
^	v	>	>	v	<	v	v	
>	>	v	>	v	<	v	v	
^	>	>	v	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.011315077412967334, 0.03349217358307044, 0.040568957786702775, 0.06631153944665247, 0.07478267587891896
==========
iteration 13
==========
weights [-0.16465125 -0.39456712 -0.30432771 -0.20456629 -0.82626103  0.00606123]
expeced value MDP LP -1.792204919258439
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.79365557 -0.5405744   0.24215491  0.13593594  0.01792059 -0.02124693]
w_map [-0.21518773 -0.52808179 -0.35445242 -0.18733237 -0.71474379 -0.05687923] loglik -8.333386922743102e-07
accepted/total = 1430/3000 = 0.4766666666666667
-------
true weights [-0.16465125 -0.39456712 -0.30432771 -0.20456629 -0.82626103  0.00606123]
features
3 	3 	1 	2 	1 	3 	0 	0 	
2 	1 	3 	1 	2 	2 	1 	0 	
3 	4 	3 	3 	4 	3 	2 	1 	
0 	0 	2 	4 	2 	0 	1 	4 	
0 	4 	4 	1 	0 	4 	0 	2 	
2 	0 	2 	1 	2 	2 	0 	3 	
0 	4 	4 	3 	2 	0 	3 	3 	
3 	3 	0 	1 	3 	1 	4 	5 	
optimal policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	<	
v	<	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
optimal values
-3.08	-3.13	-2.95	-2.59	-2.31	-1.93	-1.93	-2.07	
-2.91	-2.95	-2.59	-2.40	-2.03	-1.74	-1.78	-1.93	
-2.63	-3.39	-2.63	-2.45	-2.27	-1.45	-1.40	-1.78	
-2.45	-2.59	-2.64	-2.36	-1.55	-1.26	-1.11	-1.52	
-2.31	-2.69	-2.50	-1.69	-1.31	-1.54	-0.72	-0.70	
-2.16	-1.88	-1.73	-1.44	-1.16	-0.86	-0.56	-0.40	
-2.11	-2.59	-1.87	-1.06	-0.86	-0.56	-0.40	-0.20	
-1.97	-1.78	-1.59	-1.44	-1.06	-0.95	-0.82	0.01	
map_weights [-0.21518773 -0.52808179 -0.35445242 -0.18733237 -0.71474379 -0.05687923]
MAP reward
-0.19	-0.19	-0.53	-0.35	-0.53	-0.19	-0.22	-0.22	
-0.35	-0.53	-0.19	-0.53	-0.35	-0.35	-0.53	-0.22	
-0.19	-0.71	-0.19	-0.19	-0.71	-0.19	-0.35	-0.53	
-0.22	-0.22	-0.35	-0.71	-0.35	-0.22	-0.53	-0.71	
-0.22	-0.71	-0.71	-0.53	-0.22	-0.71	-0.22	-0.35	
-0.35	-0.22	-0.35	-0.53	-0.35	-0.35	-0.22	-0.19	
-0.22	-0.71	-0.71	-0.19	-0.35	-0.22	-0.19	-0.19	
-0.19	-0.19	-0.22	-0.53	-0.19	-0.53	-0.71	-0.06	
Map policy
v	>	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
expeced value MDP LP -1.6116584486201184
mean w [-0.1704937  -0.42913605 -0.25371216 -0.17566229 -0.69736461  0.06005763]
Mean policy from posterior
v	<	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	v	v	v	
v	<	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
Mean rewards
-0.18	-0.18	-0.43	-0.25	-0.43	-0.18	-0.17	-0.17	
-0.25	-0.43	-0.18	-0.43	-0.25	-0.25	-0.43	-0.17	
-0.18	-0.70	-0.18	-0.18	-0.70	-0.18	-0.25	-0.43	
-0.17	-0.17	-0.25	-0.70	-0.25	-0.17	-0.43	-0.70	
-0.17	-0.70	-0.70	-0.43	-0.17	-0.70	-0.17	-0.25	
-0.25	-0.17	-0.25	-0.43	-0.25	-0.25	-0.17	-0.18	
-0.17	-0.70	-0.70	-0.18	-0.25	-0.17	-0.18	-0.18	
-0.18	-0.18	-0.17	-0.43	-0.18	-0.43	-0.70	0.06	
mean = 0.020634996581420317, map = 0.02773036483230773
CVaR policy
v	>	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
v	v	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
CVaR policy
v	<	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	>	v	v	v	v	
v	<	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
CVaR policy
v	<	v	>	>	v	v	v	
v	>	>	>	>	v	v	v	
v	>	>	v	v	v	v	v	
v	<	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
cvar = , 0.04590260165998927, 0.017468911479049165, 0.013470636922625623, 0.020634993005113067, 0.02063499330077878
==========
iteration 14
==========
weights [-0.67693597 -0.56792764 -0.00178009 -0.1635496  -0.26157735  0.35219531]
expeced value MDP LP -0.6461380456201834
demonstration
[(0, 3), (8, 1), (9, 0), (8, 1), (9, 0), (8, 0), (8, 0), (8, 0), (8, 1), (9, 0), (8, 1), (9, 0), (8, 1), (9, 0), (8, 0), (8, 0), (8, 1), (9, 0), (8, 1), (9, 0)]
[ 0.26245201 -0.24200151 -0.21350556  0.89831641 -0.01164814  0.14093096]
w_map [-0.48347337 -0.25314218  0.06454104 -0.5638204  -0.36157461 -0.49937696] loglik -8.317766166720162
accepted/total = 2801/3000 = 0.9336666666666666
-------
true weights [-0.67693597 -0.56792764 -0.00178009 -0.1635496  -0.26157735  0.35219531]
features
0 	4 	1 	1 	2 	0 	4 	3 	
2 	2 	3 	1 	4 	4 	2 	1 	
1 	1 	3 	0 	3 	3 	1 	0 	
4 	4 	1 	3 	3 	3 	1 	1 	
3 	4 	1 	1 	3 	0 	3 	1 	
3 	4 	2 	0 	2 	2 	0 	2 	
2 	4 	2 	1 	0 	3 	2 	1 	
3 	2 	2 	0 	0 	0 	1 	5 	
optimal policy
v	v	v	>	^	<	v	<	
>	<	<	<	^	<	<	<	
^	^	^	<	^	<	^	^	
v	v	^	>	v	<	<	v	
v	v	v	>	v	v	>	v	
v	>	v	>	>	<	>	>	
<	v	v	<	^	^	>	v	
>	v	<	<	<	>	>	.	
optimal values
-0.85	-0.44	-0.90	-0.74	-0.18	-0.85	-0.94	-1.10	
-0.18	-0.18	-0.34	-0.90	-0.44	-0.70	-0.69	-1.25	
-0.74	-0.74	-0.50	-1.17	-0.60	-0.75	-1.25	-1.92	
-0.76	-0.95	-1.06	-0.66	-0.50	-0.66	-1.22	-1.30	
-0.50	-0.70	-0.74	-0.90	-0.34	-0.85	-0.90	-0.74	
-0.34	-0.44	-0.18	-0.85	-0.18	-0.18	-0.85	-0.18	
-0.18	-0.44	-0.18	-0.74	-0.85	-0.34	-0.22	-0.22	
-0.34	-0.18	-0.18	-0.85	-1.52	-0.89	-0.22	0.35	
map_weights [-0.48347337 -0.25314218  0.06454104 -0.5638204  -0.36157461 -0.49937696]
MAP reward
-0.48	-0.36	-0.25	-0.25	0.06	-0.48	-0.36	-0.56	
0.06	0.06	-0.56	-0.25	-0.36	-0.36	0.06	-0.25	
-0.25	-0.25	-0.56	-0.48	-0.56	-0.56	-0.25	-0.48	
-0.36	-0.36	-0.25	-0.56	-0.56	-0.56	-0.25	-0.25	
-0.56	-0.36	-0.25	-0.25	-0.56	-0.48	-0.56	-0.25	
-0.56	-0.36	0.06	-0.48	0.06	0.06	-0.48	0.06	
0.06	-0.36	0.06	-0.25	-0.48	-0.56	0.06	-0.25	
-0.56	0.06	0.06	-0.48	-0.48	-0.48	-0.25	-0.50	
Map policy
v	v	>	>	^	<	<	<	
<	<	<	^	^	<	<	<	
^	^	<	^	^	^	^	v	
^	^	v	v	v	v	>	v	
v	>	v	<	v	v	>	v	
v	>	v	>	>	<	>	>	
<	>	v	<	^	^	>	^	
>	v	<	<	^	^	^	.	
expeced value MDP LP 42.53980299946585
mean w [-0.2050718  -0.27556638  0.4411729  -0.14009946 -0.2512671   0.05528507]
Mean policy from posterior
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	>	v	<	^	^	<	^	
>	v	<	<	<	^	^	.	
Mean rewards
-0.21	-0.25	-0.28	-0.28	0.44	-0.21	-0.25	-0.14	
0.44	0.44	-0.14	-0.28	-0.25	-0.25	0.44	-0.28	
-0.28	-0.28	-0.14	-0.21	-0.14	-0.14	-0.28	-0.21	
-0.25	-0.25	-0.28	-0.14	-0.14	-0.14	-0.28	-0.28	
-0.14	-0.25	-0.28	-0.28	-0.14	-0.21	-0.14	-0.28	
-0.14	-0.25	0.44	-0.21	0.44	0.44	-0.21	0.44	
0.44	-0.25	0.44	-0.28	-0.21	-0.14	0.44	-0.28	
-0.14	0.44	0.44	-0.21	-0.21	-0.21	-0.28	0.06	
mean = 0.081085854709113, map = 0.12086410697777705
CVaR policy
v	v	v	>	^	<	<	<	
>	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	v	<	>	<	>	>	
<	>	v	<	^	^	^	^	
>	>	^	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	v	<	>	<	>	>	
<	v	^	<	^	^	^	^	
>	v	v	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	>	v	<	^	^	<	^	
>	v	v	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	<	>	<	>	>	
<	v	v	<	^	^	<	^	
>	v	v	<	<	^	^	.	
CVaR policy
v	v	v	>	^	<	<	<	
<	<	<	<	^	^	<	<	
^	^	^	<	^	v	^	v	
^	^	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	>	>	<	>	>	
<	v	v	<	^	^	<	^	
>	v	v	<	^	^	^	.	
cvar = , 0.10483951009733061, 0.10483076812650294, 0.08112216901072611, 0.08108798266172113, 0.08109616432396016
==========
iteration 15
==========
weights [-0.17112211 -0.63055689 -0.18926621 -0.15706279 -0.03268814  0.71523164]
expeced value MDP LP -0.3725553197038518
demonstration
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (12, 3), (20, 1), (21, 1), (22, 1), (23, 3), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.4926968   0.35866295 -0.27491476 -0.06616033  0.60135796  0.43246276]
w_map [-0.3476177  -0.69854015 -0.32692415 -0.29878026 -0.33294025  0.29018157] loglik -6.135536523288465e-12
accepted/total = 1866/3000 = 0.622
-------
true weights [-0.17112211 -0.63055689 -0.18926621 -0.15706279 -0.03268814  0.71523164]
features
1 	4 	3 	3 	4 	4 	1 	2 	
2 	3 	0 	1 	2 	1 	2 	2 	
4 	1 	1 	2 	4 	0 	3 	0 	
4 	3 	1 	0 	4 	1 	1 	2 	
1 	1 	1 	1 	1 	4 	0 	0 	
0 	0 	4 	2 	4 	1 	2 	3 	
0 	4 	1 	3 	3 	4 	2 	4 	
1 	2 	1 	2 	0 	0 	4 	5 	
optimal policy
>	>	>	>	v	<	v	v	
v	^	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	>	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	^	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.54	-0.92	-0.90	-0.75	-0.60	-0.62	-1.00	-0.41	
-1.14	-1.07	-1.06	-1.20	-0.57	-0.98	-0.37	-0.22	
-0.96	-1.53	-1.20	-0.57	-0.38	-0.36	-0.19	-0.03	
-0.94	-0.91	-1.19	-0.57	-0.40	-0.51	-0.47	0.14	
-0.93	-0.76	-0.59	-0.56	-0.37	0.13	0.16	0.34	
-0.30	-0.13	0.04	0.07	0.26	-0.18	0.32	0.51	
-0.34	-0.17	-0.49	0.14	0.30	0.46	0.48	0.68	
-0.96	-0.35	-0.50	0.13	0.32	0.50	0.68	0.72	
map_weights [-0.3476177  -0.69854015 -0.32692415 -0.29878026 -0.33294025  0.29018157]
MAP reward
-0.70	-0.33	-0.30	-0.30	-0.33	-0.33	-0.70	-0.33	
-0.33	-0.30	-0.35	-0.70	-0.33	-0.70	-0.33	-0.33	
-0.33	-0.70	-0.70	-0.33	-0.33	-0.35	-0.30	-0.35	
-0.33	-0.30	-0.70	-0.35	-0.33	-0.70	-0.70	-0.33	
-0.70	-0.70	-0.70	-0.70	-0.70	-0.33	-0.35	-0.35	
-0.35	-0.35	-0.33	-0.33	-0.33	-0.70	-0.33	-0.30	
-0.35	-0.33	-0.70	-0.30	-0.30	-0.33	-0.33	-0.33	
-0.70	-0.33	-0.70	-0.33	-0.35	-0.35	-0.33	0.29	
Map policy
>	>	>	>	v	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	>	>	>	>	v	
>	v	v	v	v	v	>	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	>	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.2299996340710857
mean w [-0.19840235 -0.67951538 -0.25603681 -0.16660294 -0.23617059  0.44103055]
Mean policy from posterior
>	>	>	>	v	v	v	v	
>	>	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	>	^	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.68	-0.24	-0.17	-0.17	-0.24	-0.24	-0.68	-0.26	
-0.26	-0.17	-0.20	-0.68	-0.26	-0.68	-0.26	-0.26	
-0.24	-0.68	-0.68	-0.26	-0.24	-0.20	-0.17	-0.20	
-0.24	-0.17	-0.68	-0.20	-0.24	-0.68	-0.68	-0.26	
-0.68	-0.68	-0.68	-0.68	-0.68	-0.24	-0.20	-0.20	
-0.20	-0.20	-0.24	-0.26	-0.24	-0.68	-0.26	-0.17	
-0.20	-0.24	-0.68	-0.17	-0.17	-0.24	-0.26	-0.24	
-0.68	-0.26	-0.68	-0.26	-0.20	-0.20	-0.24	0.44	
mean = 0.09278537943211845, map = 0.08013687742838516
CVaR policy
>	>	>	>	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	>	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	>	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	v	^	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	>	^	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	v	>	>	^	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	v	v	>	>	v	
^	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.12137968436170832, 0.12595484564130843, 0.11230028463751118, 0.11044401196092413, 0.09652147101459607
==========
iteration 16
==========
weights [-0.2406627  -0.2448106  -0.93441745 -0.05926605 -0.02083348  0.07118127]
expeced value MDP LP -0.8498426005386726
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 1), (26, 1), (27, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.25448677  0.49532468  0.18018773 -0.12810203 -0.48373779  0.63797331]
w_map [-0.51389325 -0.41570262 -0.65227671 -0.22337885 -0.22586407 -0.19164409] loglik -0.6931472289932117
accepted/total = 1699/3000 = 0.5663333333333334
-------
true weights [-0.2406627  -0.2448106  -0.93441745 -0.05926605 -0.02083348  0.07118127]
features
4 	1 	1 	2 	2 	4 	3 	0 	
1 	4 	1 	3 	2 	4 	3 	1 	
3 	1 	3 	0 	3 	1 	4 	1 	
3 	4 	3 	1 	3 	1 	2 	3 	
2 	2 	2 	3 	1 	4 	3 	3 	
3 	4 	2 	3 	0 	0 	2 	4 	
4 	2 	3 	0 	0 	1 	1 	1 	
3 	1 	1 	0 	4 	4 	1 	5 	
optimal policy
v	v	v	v	>	v	v	<	
v	v	v	v	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
v	<	>	^	v	^	>	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.26	-1.40	-1.43	-1.88	-1.58	-0.65	-0.67	-0.91	
-1.26	-1.17	-1.19	-0.96	-1.56	-0.63	-0.62	-0.79	
-1.02	-1.16	-0.96	-0.91	-0.67	-0.80	-0.56	-0.55	
-0.97	-0.92	-0.91	-0.86	-0.62	-0.57	-1.24	-0.31	
-1.90	-1.85	-1.55	-0.62	-0.57	-0.33	-0.31	-0.25	
-1.04	-1.05	-1.60	-0.67	-0.69	-0.56	-1.13	-0.19	
-0.99	-1.67	-0.74	-0.69	-0.45	-0.44	-0.42	-0.17	
-0.98	-0.93	-0.69	-0.45	-0.21	-0.19	-0.17	0.07	
map_weights [-0.51389325 -0.41570262 -0.65227671 -0.22337885 -0.22586407 -0.19164409]
MAP reward
-0.23	-0.42	-0.42	-0.65	-0.65	-0.23	-0.22	-0.51	
-0.42	-0.23	-0.42	-0.22	-0.65	-0.23	-0.22	-0.42	
-0.22	-0.42	-0.22	-0.51	-0.22	-0.42	-0.23	-0.42	
-0.22	-0.23	-0.22	-0.42	-0.22	-0.42	-0.65	-0.22	
-0.65	-0.65	-0.65	-0.22	-0.42	-0.23	-0.22	-0.22	
-0.22	-0.23	-0.65	-0.22	-0.51	-0.51	-0.65	-0.23	
-0.23	-0.65	-0.22	-0.51	-0.51	-0.42	-0.42	-0.42	
-0.22	-0.42	-0.42	-0.51	-0.23	-0.23	-0.42	-0.19	
Map policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	>	v	
v	>	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.3730372700503832
mean w [-0.52858664 -0.31138137 -0.5489676  -0.12213837 -0.16932747  0.15468886]
Mean policy from posterior
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
v	>	>	^	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.17	-0.31	-0.31	-0.55	-0.55	-0.17	-0.12	-0.53	
-0.31	-0.17	-0.31	-0.12	-0.55	-0.17	-0.12	-0.31	
-0.12	-0.31	-0.12	-0.53	-0.12	-0.31	-0.17	-0.31	
-0.12	-0.17	-0.12	-0.31	-0.12	-0.31	-0.55	-0.12	
-0.55	-0.55	-0.55	-0.12	-0.31	-0.17	-0.12	-0.12	
-0.12	-0.17	-0.55	-0.12	-0.53	-0.53	-0.55	-0.17	
-0.17	-0.55	-0.12	-0.53	-0.53	-0.31	-0.31	-0.31	
-0.12	-0.31	-0.31	-0.53	-0.17	-0.17	-0.31	0.15	
mean = 0.02573290597779898, map = 0.02996973545547299
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
v	>	>	^	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
v	>	>	^	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
v	>	>	^	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
v	>	>	^	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.02996981998120618, 0.025732907663267413, 0.025732905762523073, 0.025732905791637672, 0.025732905795459837
==========
iteration 17
==========
weights [-0.79093218 -0.13423246 -0.04159454 -0.24763618 -0.37166243  0.3939812 ]
expeced value MDP LP -0.9781668881702649
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 3), (33, 1), (34, 1), (35, 3), (43, 3), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.08353974  0.2685571   0.41423592 -0.57056118  0.58448617 -0.2866054 ]
w_map [-0.66190141 -0.32525488 -0.17754181 -0.46102613 -0.44559688  0.11607336] loglik -6.131846074453051e-06
accepted/total = 1410/3000 = 0.47
-------
true weights [-0.79093218 -0.13423246 -0.04159454 -0.24763618 -0.37166243  0.3939812 ]
features
4 	2 	4 	3 	4 	2 	0 	1 	
4 	3 	0 	4 	3 	0 	0 	3 	
2 	0 	4 	3 	1 	3 	1 	0 	
1 	2 	4 	1 	4 	1 	0 	1 	
1 	2 	3 	1 	2 	4 	1 	2 	
4 	0 	3 	2 	0 	1 	0 	0 	
0 	1 	0 	4 	2 	2 	1 	3 	
0 	2 	4 	0 	1 	0 	1 	5 	
optimal policy
v	v	>	v	v	<	<	v	
v	<	v	v	v	v	v	v	
v	v	>	v	>	v	<	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	<	<	
^	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
optimal values
-1.70	-1.60	-1.83	-1.48	-1.53	-1.55	-2.33	-1.86	
-1.34	-1.57	-2.02	-1.24	-1.17	-1.58	-1.71	-1.74	
-0.98	-1.60	-1.24	-0.88	-0.93	-0.80	-0.93	-1.51	
-0.94	-0.82	-1.00	-0.64	-0.83	-0.56	-1.35	-0.72	
-0.91	-0.78	-0.75	-0.51	-0.47	-0.43	-0.56	-0.60	
-1.27	-1.40	-0.62	-0.38	-0.76	-0.06	-0.67	-0.65	
-2.03	-1.25	-1.13	-0.34	0.03	0.08	0.12	0.14	
-2.06	-1.28	-1.25	-0.89	-0.10	-0.54	0.26	0.39	
map_weights [-0.66190141 -0.32525488 -0.17754181 -0.46102613 -0.44559688  0.11607336]
MAP reward
-0.45	-0.18	-0.45	-0.46	-0.45	-0.18	-0.66	-0.33	
-0.45	-0.46	-0.66	-0.45	-0.46	-0.66	-0.66	-0.46	
-0.18	-0.66	-0.45	-0.46	-0.33	-0.46	-0.33	-0.66	
-0.33	-0.18	-0.45	-0.33	-0.45	-0.33	-0.66	-0.33	
-0.33	-0.18	-0.46	-0.33	-0.18	-0.45	-0.33	-0.18	
-0.45	-0.66	-0.46	-0.18	-0.66	-0.33	-0.66	-0.66	
-0.66	-0.33	-0.66	-0.45	-0.18	-0.18	-0.33	-0.46	
-0.66	-0.18	-0.45	-0.66	-0.33	-0.66	-0.33	0.12	
Map policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7653624119026454
mean w [-0.6184171  -0.27707954 -0.11199581 -0.41311247 -0.38959158  0.19345873]
Mean policy from posterior
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
Mean rewards
-0.39	-0.11	-0.39	-0.41	-0.39	-0.11	-0.62	-0.28	
-0.39	-0.41	-0.62	-0.39	-0.41	-0.62	-0.62	-0.41	
-0.11	-0.62	-0.39	-0.41	-0.28	-0.41	-0.28	-0.62	
-0.28	-0.11	-0.39	-0.28	-0.39	-0.28	-0.62	-0.28	
-0.28	-0.11	-0.41	-0.28	-0.11	-0.39	-0.28	-0.11	
-0.39	-0.62	-0.41	-0.11	-0.62	-0.28	-0.62	-0.62	
-0.62	-0.28	-0.62	-0.39	-0.11	-0.11	-0.28	-0.41	
-0.62	-0.11	-0.39	-0.62	-0.28	-0.62	-0.28	0.19	
mean = 0.04267605758502968, map = 0.11057654698040154
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	v	v	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	<	v	
>	v	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	^	>	>	.	
cvar = , 0.1354094829197291, 0.11057654700350905, 0.06709998558178032, 0.06709998567784281, 0.04267605759195192
==========
iteration 18
==========
weights [-0.44412316 -0.0720415  -0.15141447 -0.47629387 -0.45369036  0.58476277]
expeced value MDP LP -1.0142718814466793
demonstration
[(0, 1), (1, 1), (2, 3), (10, 1), (11, 3), (19, 3), (27, 3), (35, 1), (36, 3), (44, 3), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.03547662  0.05045327  0.31125042  0.7380748   0.57856773 -0.14079779]
w_map [-0.49241236 -0.18927693 -0.29011841 -0.43195288 -0.65178336 -0.16165018] loglik -0.6931491196974378
accepted/total = 1330/3000 = 0.44333333333333336
-------
true weights [-0.44412316 -0.0720415  -0.15141447 -0.47629387 -0.45369036  0.58476277]
features
3 	1 	1 	3 	4 	3 	4 	4 	
4 	0 	1 	1 	2 	0 	3 	4 	
4 	3 	0 	4 	0 	3 	4 	2 	
0 	0 	4 	1 	0 	0 	4 	3 	
2 	1 	3 	2 	0 	4 	3 	4 	
0 	1 	4 	4 	1 	0 	2 	3 	
2 	3 	1 	1 	1 	4 	0 	0 	
1 	2 	2 	4 	4 	1 	1 	5 	
optimal policy
>	>	v	v	v	v	v	v	
>	>	>	v	<	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	v	v	v	
v	>	v	>	v	v	v	v	
v	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-1.96	-1.50	-1.44	-1.79	-1.91	-2.35	-2.36	-2.28	
-2.25	-1.82	-1.39	-1.33	-1.47	-1.90	-1.92	-1.84	
-1.84	-1.72	-1.70	-1.27	-1.49	-1.82	-1.46	-1.40	
-1.40	-1.26	-1.27	-0.82	-1.05	-1.36	-1.02	-1.26	
-0.97	-0.82	-1.16	-0.76	-0.61	-0.92	-0.57	-0.79	
-1.19	-0.76	-0.69	-0.62	-0.17	-0.47	-0.09	-0.34	
-0.75	-0.72	-0.24	-0.17	-0.10	-0.03	0.06	0.13	
-0.60	-0.54	-0.39	-0.48	-0.03	0.43	0.51	0.58	
map_weights [-0.49241236 -0.18927693 -0.29011841 -0.43195288 -0.65178336 -0.16165018]
MAP reward
-0.43	-0.19	-0.19	-0.43	-0.65	-0.43	-0.65	-0.65	
-0.65	-0.49	-0.19	-0.19	-0.29	-0.49	-0.43	-0.65	
-0.65	-0.43	-0.49	-0.65	-0.49	-0.43	-0.65	-0.29	
-0.49	-0.49	-0.65	-0.19	-0.49	-0.49	-0.65	-0.43	
-0.29	-0.19	-0.43	-0.29	-0.49	-0.65	-0.43	-0.65	
-0.49	-0.19	-0.65	-0.65	-0.19	-0.49	-0.29	-0.43	
-0.29	-0.43	-0.19	-0.19	-0.19	-0.65	-0.49	-0.49	
-0.19	-0.29	-0.29	-0.65	-0.65	-0.19	-0.19	-0.16	
Map policy
>	>	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.7314820816389407
mean w [-0.45630507 -0.11048742 -0.18950667 -0.37867621 -0.47341861 -0.01791472]
Mean policy from posterior
>	>	v	v	v	v	v	v	
v	>	>	v	<	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.38	-0.11	-0.11	-0.38	-0.47	-0.38	-0.47	-0.47	
-0.47	-0.46	-0.11	-0.11	-0.19	-0.46	-0.38	-0.47	
-0.47	-0.38	-0.46	-0.47	-0.46	-0.38	-0.47	-0.19	
-0.46	-0.46	-0.47	-0.11	-0.46	-0.46	-0.47	-0.38	
-0.19	-0.11	-0.38	-0.19	-0.46	-0.47	-0.38	-0.47	
-0.46	-0.11	-0.47	-0.47	-0.11	-0.46	-0.19	-0.38	
-0.19	-0.38	-0.11	-0.11	-0.11	-0.47	-0.46	-0.46	
-0.11	-0.19	-0.19	-0.47	-0.47	-0.11	-0.11	-0.02	
mean = 0.009740994708667738, map = 0.04068927756142293
CVaR policy
>	>	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	>	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
v	>	>	v	<	<	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	>	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.04817385123279827, 0.04142693475454018, 0.028343256950063678, 0.02350883308062901, 0.009740994707400974
==========
iteration 19
==========
weights [-3.86537140e-01 -3.90661519e-01 -3.04991611e-01 -4.46380362e-05
 -6.17544866e-01  4.72854175e-01]
expeced value MDP LP -0.437465840610313
demonstration
[(0, 0), (0, 0), (0, 2), (0, 2), (0, 0), (0, 0), (0, 0), (0, 2), (0, 0), (0, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 0), (0, 2), (0, 2), (0, 2), (0, 2), (0, 0)]
[ 0.09826753 -0.24806119  0.70979561 -0.13775978 -0.54234097  0.33449642]
w_map [-0.20307819  0.11643523 -0.27341154  0.20403007 -0.90938258 -0.04293345] loglik -13.862943611197807
accepted/total = 2799/3000 = 0.933
-------
true weights [-3.86537140e-01 -3.90661519e-01 -3.04991611e-01 -4.46380362e-05
 -6.17544866e-01  4.72854175e-01]
features
3 	4 	4 	0 	2 	3 	4 	2 	
4 	2 	0 	0 	0 	4 	1 	0 	
2 	0 	3 	3 	4 	1 	3 	1 	
1 	1 	1 	4 	0 	0 	1 	2 	
1 	2 	1 	3 	4 	3 	2 	3 	
3 	0 	0 	4 	3 	3 	0 	4 	
0 	1 	4 	1 	3 	0 	3 	2 	
1 	0 	3 	2 	1 	4 	2 	5 	
optimal policy
<	<	v	>	>	^	<	<	
^	>	v	v	^	^	<	v	
>	>	>	<	<	v	v	v	
v	^	^	^	>	v	v	v	
v	v	^	>	v	v	>	>	
<	<	<	>	v	<	v	v	
^	v	v	>	^	>	v	v	
>	>	v	<	^	>	>	.	
optimal values
-0.00	-0.62	-1.00	-0.69	-0.31	-0.00	-0.62	-0.92	
-0.62	-0.69	-0.39	-0.39	-0.69	-0.62	-1.01	-1.08	
-0.69	-0.39	-0.00	-0.00	-0.62	-0.78	-0.69	-0.70	
-0.78	-0.78	-0.40	-0.62	-0.77	-0.39	-0.70	-0.31	
-0.40	-0.69	-0.78	-0.62	-0.62	-0.00	-0.31	-0.00	
-0.00	-0.39	-0.77	-0.62	-0.00	-0.00	-0.23	-0.46	
-0.39	-0.78	-0.62	-0.40	-0.00	-0.23	0.16	0.16	
-0.78	-0.39	-0.00	-0.31	-0.40	-0.46	0.16	0.47	
map_weights [-0.20307819  0.11643523 -0.27341154  0.20403007 -0.90938258 -0.04293345]
MAP reward
0.20	-0.91	-0.91	-0.20	-0.27	0.20	-0.91	-0.27	
-0.91	-0.27	-0.20	-0.20	-0.20	-0.91	0.12	-0.20	
-0.27	-0.20	0.20	0.20	-0.91	0.12	0.20	0.12	
0.12	0.12	0.12	-0.91	-0.20	-0.20	0.12	-0.27	
0.12	-0.27	0.12	0.20	-0.91	0.20	-0.27	0.20	
0.20	-0.20	-0.20	-0.91	0.20	0.20	-0.20	-0.91	
-0.20	0.12	-0.91	0.12	0.20	-0.20	0.20	-0.27	
0.12	-0.20	0.20	-0.27	0.12	-0.91	-0.27	-0.04	
Map policy
<	<	v	v	>	^	<	v	
^	v	v	v	<	^	v	v	
v	>	>	<	<	v	v	v	
v	>	^	^	>	v	<	v	
v	<	^	<	v	v	<	>	
<	<	^	>	>	<	<	^	
^	v	v	>	^	<	<	<	
>	>	v	<	^	<	^	.	
expeced value MDP LP 47.72078478784065
mean w [-0.21151738 -0.05050751 -0.15630746  0.49232932 -0.13792825 -0.08966253]
Mean policy from posterior
<	<	<	>	>	^	<	<	
^	^	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	v	v	<	>	
<	<	>	>	>	^	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
Mean rewards
0.49	-0.14	-0.14	-0.21	-0.16	0.49	-0.14	-0.16	
-0.14	-0.16	-0.21	-0.21	-0.21	-0.14	-0.05	-0.21	
-0.16	-0.21	0.49	0.49	-0.14	-0.05	0.49	-0.05	
-0.05	-0.05	-0.05	-0.14	-0.21	-0.21	-0.05	-0.16	
-0.05	-0.16	-0.05	0.49	-0.14	0.49	-0.16	0.49	
0.49	-0.21	-0.21	-0.14	0.49	0.49	-0.21	-0.14	
-0.21	-0.05	-0.14	-0.05	0.49	-0.21	0.49	-0.16	
-0.05	-0.21	0.49	-0.16	-0.05	-0.14	-0.16	-0.09	
mean = 0.09471858153423829, map = 0.08058203276585629
CVaR policy
<	<	<	v	>	^	<	<	
^	<	v	v	v	^	^	<	
^	>	>	<	<	^	<	v	
v	>	^	^	v	v	<	v	
v	<	>	>	>	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	^	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	v	>	^	<	<	
^	^	v	v	v	^	^	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	<	v	
v	<	^	>	v	v	<	>	
<	<	>	>	>	^	<	^	
^	>	v	>	^	<	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	^	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	^	v	v	<	>	
<	<	>	>	v	<	<	^	
^	>	v	>	^	<	<	^	
^	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	^	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	>	v	>	>	
<	<	>	>	>	<	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	v	v	v	^	<	<	
^	>	>	<	<	<	<	v	
v	>	^	^	v	v	v	v	
v	<	^	>	>	v	<	>	
<	<	>	>	>	<	<	^	
^	>	v	>	^	<	<	^	
>	>	v	<	^	<	^	.	
cvar = , 0.10066580016830029, 0.09724498781236635, 0.09471918783711697, 0.09471973046068494, 0.09472078612142265
==========
iteration 20
==========
weights [-0.56426135 -0.10031456 -0.05486098 -0.05723787 -0.54609201  0.60584135]
expeced value MDP LP -0.14838274413885394
demonstration
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (13, 1), (14, 3), (22, 3), (30, 3), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.57113554 -0.03639903 -0.48265193  0.11687756 -0.45976646 -0.46312079]
w_map [-0.69442775 -0.38657262 -0.16924984 -0.1758965  -0.55382968  0.04493675] loglik -2.7660752746072603e-06
accepted/total = 1430/3000 = 0.4766666666666667
-------
true weights [-0.56426135 -0.10031456 -0.05486098 -0.05723787 -0.54609201  0.60584135]
features
4 	2 	4 	1 	2 	2 	1 	4 	
4 	0 	4 	2 	1 	2 	3 	4 	
0 	3 	3 	1 	0 	4 	3 	0 	
4 	2 	1 	2 	3 	3 	3 	4 	
0 	4 	1 	3 	4 	2 	2 	2 	
1 	0 	1 	2 	3 	4 	4 	1 	
0 	2 	3 	3 	0 	3 	2 	3 	
3 	1 	2 	4 	3 	2 	3 	5 	
optimal policy
>	>	>	>	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	<	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	v	
>	>	>	^	<	v	v	v	
>	>	>	^	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.26	-0.72	-0.67	-0.13	-0.03	0.03	0.04	-0.51	
-1.23	-0.69	-0.62	-0.07	-0.02	0.08	0.14	-0.41	
-0.69	-0.13	-0.07	-0.02	-0.42	-0.35	0.20	-0.37	
-0.62	-0.07	-0.02	0.09	0.14	0.20	0.26	-0.17	
-1.17	-0.62	-0.07	0.03	-0.29	0.26	0.32	0.38	
-0.78	-0.69	-0.13	-0.03	-0.08	-0.13	-0.07	0.44	
-0.76	-0.19	-0.14	-0.08	-0.15	0.42	0.48	0.54	
-0.34	-0.28	-0.18	-0.13	0.42	0.48	0.54	0.61	
map_weights [-0.69442775 -0.38657262 -0.16924984 -0.1758965  -0.55382968  0.04493675]
MAP reward
-0.55	-0.17	-0.55	-0.39	-0.17	-0.17	-0.39	-0.55	
-0.55	-0.69	-0.55	-0.17	-0.39	-0.17	-0.18	-0.55	
-0.69	-0.18	-0.18	-0.39	-0.69	-0.55	-0.18	-0.69	
-0.55	-0.17	-0.39	-0.17	-0.18	-0.18	-0.18	-0.55	
-0.69	-0.55	-0.39	-0.18	-0.55	-0.17	-0.17	-0.17	
-0.39	-0.69	-0.39	-0.17	-0.18	-0.55	-0.55	-0.39	
-0.69	-0.17	-0.18	-0.18	-0.69	-0.18	-0.17	-0.18	
-0.18	-0.39	-0.17	-0.55	-0.18	-0.17	-0.18	0.04	
Map policy
>	>	>	>	>	v	v	<	
>	v	>	v	>	>	v	<	
>	v	v	v	v	v	v	<	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
v	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4277731084205285
mean w [-0.6842122  -0.29250233 -0.13775413 -0.16109798 -0.44808106 -0.04087348]
Mean policy from posterior
>	>	>	>	>	v	v	<	
>	v	>	v	>	>	v	<	
>	v	v	v	v	v	v	<	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
v	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.45	-0.14	-0.45	-0.29	-0.14	-0.14	-0.29	-0.45	
-0.45	-0.68	-0.45	-0.14	-0.29	-0.14	-0.16	-0.45	
-0.68	-0.16	-0.16	-0.29	-0.68	-0.45	-0.16	-0.68	
-0.45	-0.14	-0.29	-0.14	-0.16	-0.16	-0.16	-0.45	
-0.68	-0.45	-0.29	-0.16	-0.45	-0.14	-0.14	-0.14	
-0.29	-0.68	-0.29	-0.14	-0.16	-0.45	-0.45	-0.29	
-0.68	-0.14	-0.16	-0.16	-0.68	-0.16	-0.14	-0.16	
-0.16	-0.29	-0.14	-0.45	-0.16	-0.14	-0.16	-0.04	
mean = 0.039195928393244706, map = 0.03919592836437308
CVaR policy
>	>	>	>	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
v	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
v	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	<	
v	v	>	v	>	>	v	<	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	>	>	>	v	
>	v	>	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.07874339962477914, 0.044934537519112555, 0.04493453737943379, 0.04493453741637274, 0.0449345373790242
==========
iteration 21
==========
weights [-0.10255413 -0.17061703 -0.26391579 -0.67031573 -0.31661541  0.58408257]
expeced value MDP LP -0.7636550177943935
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 3), (26, 1), (27, 1), (28, 3), (36, 3), (44, 1), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.47860295  0.62898834  0.10186041 -0.50553934  0.17954999 -0.2777213 ]
w_map [-0.10469601 -0.56423705 -0.29749006 -0.63743452 -0.41519479  0.05886841] loglik -0.5006227745281322
accepted/total = 1475/3000 = 0.49166666666666664
-------
true weights [-0.10255413 -0.17061703 -0.26391579 -0.67031573 -0.31661541  0.58408257]
features
0 	0 	1 	2 	4 	3 	3 	4 	
3 	2 	3 	3 	4 	3 	2 	0 	
1 	2 	0 	1 	4 	0 	4 	1 	
4 	4 	0 	0 	0 	1 	4 	0 	
1 	2 	2 	1 	1 	3 	3 	1 	
3 	1 	4 	0 	4 	0 	0 	1 	
1 	2 	2 	2 	2 	4 	0 	3 	
0 	4 	0 	3 	4 	2 	1 	5 	
optimal policy
>	v	<	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	<	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
optimal values
-1.47	-1.38	-1.54	-1.67	-1.42	-2.07	-1.44	-0.83	
-1.86	-1.29	-1.45	-1.42	-1.11	-1.42	-0.77	-0.52	
-1.20	-1.04	-0.78	-0.76	-0.81	-0.76	-0.73	-0.42	
-1.30	-1.00	-0.69	-0.59	-0.49	-0.66	-0.56	-0.25	
-1.17	-1.01	-0.75	-0.49	-0.39	-0.58	-0.48	-0.15	
-1.47	-0.80	-0.64	-0.33	-0.23	0.09	0.20	0.02	
-1.22	-1.06	-0.80	-0.54	-0.28	-0.02	0.30	-0.09	
-1.29	-1.20	-0.90	-0.85	-0.18	0.14	0.41	0.58	
map_weights [-0.10469601 -0.56423705 -0.29749006 -0.63743452 -0.41519479  0.05886841]
MAP reward
-0.10	-0.10	-0.56	-0.30	-0.42	-0.64	-0.64	-0.42	
-0.64	-0.30	-0.64	-0.64	-0.42	-0.64	-0.30	-0.10	
-0.56	-0.30	-0.10	-0.56	-0.42	-0.10	-0.42	-0.56	
-0.42	-0.42	-0.10	-0.10	-0.10	-0.56	-0.42	-0.10	
-0.56	-0.30	-0.30	-0.56	-0.56	-0.64	-0.64	-0.56	
-0.64	-0.56	-0.42	-0.10	-0.42	-0.10	-0.10	-0.56	
-0.56	-0.30	-0.30	-0.30	-0.30	-0.42	-0.10	-0.64	
-0.10	-0.42	-0.10	-0.64	-0.42	-0.30	-0.56	0.06	
Map policy
>	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.2577898705611161
mean w [-0.09874956 -0.28715638 -0.25589874 -0.63192302 -0.34594431  0.28928277]
Mean policy from posterior
>	v	<	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.10	-0.10	-0.29	-0.26	-0.35	-0.63	-0.63	-0.35	
-0.63	-0.26	-0.63	-0.63	-0.35	-0.63	-0.26	-0.10	
-0.29	-0.26	-0.10	-0.29	-0.35	-0.10	-0.35	-0.29	
-0.35	-0.35	-0.10	-0.10	-0.10	-0.29	-0.35	-0.10	
-0.29	-0.26	-0.26	-0.29	-0.29	-0.63	-0.63	-0.29	
-0.63	-0.29	-0.35	-0.10	-0.35	-0.10	-0.10	-0.29	
-0.29	-0.26	-0.26	-0.26	-0.26	-0.35	-0.10	-0.63	
-0.10	-0.35	-0.10	-0.63	-0.35	-0.26	-0.29	0.29	
mean = 0.01025402391080088, map = 0.0724140031005085
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	v	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.053487732286308876, 0.05348775201272271, 0.01904557251980754, 0.01806706951193615, 0.010254023882049434
==========
iteration 22
==========
weights [-0.73515571 -0.56581614 -0.06623291 -0.25651433 -0.24663134  0.09156831]
expeced value MDP LP -1.9062027144048383
demonstration
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (12, 3), (20, 3), (28, 3), (36, 3), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.62833876 -0.11009054 -0.44737496  0.39375297  0.45792388  0.16790008]
w_map [-0.61640265 -0.52026427 -0.20541805 -0.36867739 -0.34861023 -0.22298919] loglik -0.6931493229179466
accepted/total = 1204/3000 = 0.4013333333333333
-------
true weights [-0.73515571 -0.56581614 -0.06623291 -0.25651433 -0.24663134  0.09156831]
features
0 	2 	2 	3 	4 	1 	3 	3 	
1 	1 	1 	4 	4 	2 	4 	4 	
1 	2 	3 	3 	2 	0 	4 	4 	
1 	1 	1 	2 	4 	2 	1 	2 	
3 	2 	4 	3 	1 	3 	1 	4 	
3 	0 	2 	1 	2 	1 	4 	0 	
3 	0 	0 	3 	3 	0 	4 	0 	
2 	2 	1 	2 	2 	4 	1 	5 	
optimal policy
>	>	>	>	v	v	v	v	
v	v	>	>	v	<	v	v	
>	>	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.39	-2.68	-2.64	-2.60	-2.37	-2.73	-2.57	-2.35	
-3.51	-2.97	-2.91	-2.37	-2.14	-2.19	-2.34	-2.11	
-2.97	-2.43	-2.39	-2.15	-1.91	-2.52	-2.11	-1.89	
-2.74	-2.51	-2.46	-1.91	-1.87	-1.80	-2.06	-1.66	
-2.20	-1.96	-1.92	-1.88	-1.64	-1.75	-1.51	-1.61	
-1.98	-2.40	-1.69	-1.64	-1.08	-1.51	-0.96	-1.37	
-1.74	-2.17	-1.81	-1.08	-1.02	-1.45	-0.72	-0.64	
-1.50	-1.44	-1.39	-0.83	-0.78	-0.72	-0.48	0.09	
map_weights [-0.61640265 -0.52026427 -0.20541805 -0.36867739 -0.34861023 -0.22298919]
MAP reward
-0.62	-0.21	-0.21	-0.37	-0.35	-0.52	-0.37	-0.37	
-0.52	-0.52	-0.52	-0.35	-0.35	-0.21	-0.35	-0.35	
-0.52	-0.21	-0.37	-0.37	-0.21	-0.62	-0.35	-0.35	
-0.52	-0.52	-0.52	-0.21	-0.35	-0.21	-0.52	-0.21	
-0.37	-0.21	-0.35	-0.37	-0.52	-0.37	-0.52	-0.35	
-0.37	-0.62	-0.21	-0.52	-0.21	-0.52	-0.35	-0.62	
-0.37	-0.62	-0.62	-0.37	-0.37	-0.62	-0.35	-0.62	
-0.21	-0.21	-0.52	-0.21	-0.21	-0.35	-0.52	-0.22	
Map policy
>	>	>	>	v	v	v	v	
v	v	>	>	v	>	v	v	
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8868233213439694
mean w [-0.68979395 -0.46432089 -0.12298917 -0.30154849 -0.21619595  0.02764998]
Mean policy from posterior
>	>	>	>	v	v	v	v	
v	v	>	>	v	<	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	<	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.69	-0.12	-0.12	-0.30	-0.22	-0.46	-0.30	-0.30	
-0.46	-0.46	-0.46	-0.22	-0.22	-0.12	-0.22	-0.22	
-0.46	-0.12	-0.30	-0.30	-0.12	-0.69	-0.22	-0.22	
-0.46	-0.46	-0.46	-0.12	-0.22	-0.12	-0.46	-0.12	
-0.30	-0.12	-0.22	-0.30	-0.46	-0.30	-0.46	-0.22	
-0.30	-0.69	-0.12	-0.46	-0.12	-0.46	-0.22	-0.69	
-0.30	-0.69	-0.69	-0.30	-0.30	-0.69	-0.22	-0.69	
-0.12	-0.12	-0.46	-0.12	-0.12	-0.22	-0.46	0.03	
mean = 0.026230950711698897, map = 0.013623761045787663
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	v	v	v	>	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	<	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	v	
v	>	>	>	v	>	v	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	v	v	v	v	
>	v	>	>	v	<	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	v	>	v	v	v	<	
v	>	>	>	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.013623813396510975, 0.013623766238048729, 0.027102842855799114, 0.01565199738726175, 0.026230950486588966
==========
iteration 23
==========
weights [-0.07457546 -0.5154494  -0.65889459 -0.23983411 -0.44147547  0.20539556]
expeced value MDP LP -1.8144946294694744
demonstration
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (13, 3), (21, 3), (29, 3), (37, 3), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.0382887   0.42198961  0.27626483 -0.55239122  0.28445264  0.59840381]
w_map [-0.21158761 -0.39440261 -0.66156607 -0.34120603 -0.42780101  0.25014463] loglik -1.130553002326451e-07
accepted/total = 1605/3000 = 0.535
-------
true weights [-0.07457546 -0.5154494  -0.65889459 -0.23983411 -0.44147547  0.20539556]
features
1 	2 	1 	0 	0 	0 	3 	0 	
4 	0 	2 	1 	1 	0 	0 	1 	
4 	2 	3 	1 	1 	1 	1 	2 	
4 	3 	0 	4 	0 	3 	1 	0 	
2 	3 	3 	4 	2 	0 	2 	1 	
0 	3 	3 	4 	2 	2 	4 	2 	
4 	0 	1 	4 	3 	0 	0 	1 	
0 	0 	0 	3 	1 	4 	2 	5 	
optimal policy
>	>	>	>	>	v	v	<	
>	v	v	^	>	v	<	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	v	
v	v	v	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	^	^	^	>	.	
optimal values
-3.73	-3.25	-2.61	-2.12	-2.07	-2.01	-2.23	-2.28	
-3.26	-2.85	-2.80	-2.61	-2.45	-1.96	-2.01	-2.51	
-3.00	-2.80	-2.16	-2.38	-1.96	-1.90	-2.40	-2.18	
-2.58	-2.16	-1.94	-1.89	-1.46	-1.40	-1.90	-1.53	
-2.46	-1.98	-2.00	-1.98	-1.82	-1.17	-1.47	-1.47	
-1.82	-1.76	-1.78	-1.55	-1.34	-1.11	-0.82	-0.97	
-1.96	-1.53	-1.63	-1.12	-0.69	-0.45	-0.38	-0.31	
-1.53	-1.47	-1.41	-1.35	-1.20	-0.89	-0.46	0.21	
map_weights [-0.21158761 -0.39440261 -0.66156607 -0.34120603 -0.42780101  0.25014463]
MAP reward
-0.39	-0.66	-0.39	-0.21	-0.21	-0.21	-0.34	-0.21	
-0.43	-0.21	-0.66	-0.39	-0.39	-0.21	-0.21	-0.39	
-0.43	-0.66	-0.34	-0.39	-0.39	-0.39	-0.39	-0.66	
-0.43	-0.34	-0.21	-0.43	-0.21	-0.34	-0.39	-0.21	
-0.66	-0.34	-0.34	-0.43	-0.66	-0.21	-0.66	-0.39	
-0.21	-0.34	-0.34	-0.43	-0.66	-0.66	-0.43	-0.66	
-0.43	-0.21	-0.39	-0.43	-0.34	-0.21	-0.21	-0.39	
-0.21	-0.21	-0.21	-0.34	-0.39	-0.43	-0.66	0.25	
Map policy
>	>	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	>	v	v	v	v	
>	>	v	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.2359497485777102
mean w [-0.11029507 -0.29171338 -0.45934123 -0.24872314 -0.54333385  0.46881929]
Mean policy from posterior
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	<	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
Mean rewards
-0.29	-0.46	-0.29	-0.11	-0.11	-0.11	-0.25	-0.11	
-0.54	-0.11	-0.46	-0.29	-0.29	-0.11	-0.11	-0.29	
-0.54	-0.46	-0.25	-0.29	-0.29	-0.29	-0.29	-0.46	
-0.54	-0.25	-0.11	-0.54	-0.11	-0.25	-0.29	-0.11	
-0.46	-0.25	-0.25	-0.54	-0.46	-0.11	-0.46	-0.29	
-0.11	-0.25	-0.25	-0.54	-0.46	-0.46	-0.54	-0.46	
-0.54	-0.11	-0.29	-0.54	-0.25	-0.11	-0.11	-0.29	
-0.11	-0.11	-0.11	-0.25	-0.29	-0.54	-0.46	0.47	
mean = 0.09092118856842446, map = 0.1012599651197037
CVaR policy
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	v	>	>	v	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	<	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
cvar = , 0.1219457011373668, 0.1085122791251607, 0.10851227912491335, 0.10851227874202052, 0.09092118854890896
==========
iteration 24
==========
weights [-0.3701546  -0.72728912 -0.3132146  -0.20491508 -0.35992136  0.25376989]
expeced value MDP LP -1.807155645393347
demonstration
[(0, 1), (1, 1), (2, 1), (3, 3), (11, 3), (19, 3), (27, 1), (28, 1), (29, 3), (37, 3), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.35732898  0.34458064  0.35219379 -0.74972872 -0.09734003  0.24077266]
w_map [-0.33723507 -0.75670019 -0.3051926  -0.17637542 -0.4325761   0.04800466] loglik -1.3016016879419112e-05
accepted/total = 1081/3000 = 0.36033333333333334
-------
true weights [-0.3701546  -0.72728912 -0.3132146  -0.20491508 -0.35992136  0.25376989]
features
1 	2 	2 	3 	2 	2 	4 	2 	
0 	1 	1 	2 	4 	4 	4 	4 	
0 	0 	0 	3 	1 	0 	0 	2 	
3 	4 	1 	0 	0 	4 	2 	4 	
3 	0 	2 	3 	1 	2 	1 	2 	
4 	1 	2 	4 	1 	3 	2 	1 	
2 	0 	4 	4 	0 	3 	3 	3 	
3 	4 	0 	1 	3 	0 	2 	5 	
optimal policy
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.86	-3.16	-2.88	-2.59	-2.51	-2.22	-2.53	-2.27	
-3.32	-3.51	-3.12	-2.41	-2.27	-1.92	-2.19	-1.98	
-2.98	-2.81	-2.47	-2.12	-2.29	-1.58	-1.85	-1.64	
-2.63	-2.61	-2.63	-1.93	-1.58	-1.22	-1.49	-1.34	
-2.45	-2.27	-1.92	-1.62	-1.59	-0.87	-1.19	-0.99	
-2.42	-2.44	-1.73	-1.43	-1.29	-0.56	-0.47	-0.68	
-2.08	-1.79	-1.43	-1.08	-0.73	-0.36	-0.16	0.05	
-2.24	-2.05	-1.71	-1.35	-0.63	-0.43	-0.06	0.25	
map_weights [-0.33723507 -0.75670019 -0.3051926  -0.17637542 -0.4325761   0.04800466]
MAP reward
-0.76	-0.31	-0.31	-0.18	-0.31	-0.31	-0.43	-0.31	
-0.34	-0.76	-0.76	-0.31	-0.43	-0.43	-0.43	-0.43	
-0.34	-0.34	-0.34	-0.18	-0.76	-0.34	-0.34	-0.31	
-0.18	-0.43	-0.76	-0.34	-0.34	-0.43	-0.31	-0.43	
-0.18	-0.34	-0.31	-0.18	-0.76	-0.31	-0.76	-0.31	
-0.43	-0.76	-0.31	-0.43	-0.76	-0.18	-0.31	-0.76	
-0.31	-0.34	-0.43	-0.43	-0.34	-0.18	-0.18	-0.18	
-0.18	-0.43	-0.34	-0.76	-0.18	-0.34	-0.31	0.05	
Map policy
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.644076080924116
mean w [-0.26613716 -0.75057191 -0.19635698 -0.11470729 -0.37776551 -0.12061283]
Mean policy from posterior
>	>	>	v	>	v	<	v	
v	v	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	>	v	<	v	
>	>	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.75	-0.20	-0.20	-0.11	-0.20	-0.20	-0.38	-0.20	
-0.27	-0.75	-0.75	-0.20	-0.38	-0.38	-0.38	-0.38	
-0.27	-0.27	-0.27	-0.11	-0.75	-0.27	-0.27	-0.20	
-0.11	-0.38	-0.75	-0.27	-0.27	-0.38	-0.20	-0.38	
-0.11	-0.27	-0.20	-0.11	-0.75	-0.20	-0.75	-0.20	
-0.38	-0.75	-0.20	-0.38	-0.75	-0.11	-0.20	-0.75	
-0.20	-0.27	-0.38	-0.38	-0.27	-0.11	-0.11	-0.11	
-0.11	-0.38	-0.27	-0.75	-0.11	-0.27	-0.20	-0.12	
mean = 0.002472085543379521, map = 0.00025491683880041194
CVaR policy
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
v	>	>	v	>	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	<	v	
v	v	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	>	v	<	v	
>	>	>	v	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	<	v	
v	v	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	>	v	<	v	
>	>	>	v	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	v	<	v	
v	v	>	v	>	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	>	>	v	<	v	
>	>	>	v	>	v	v	v	
v	>	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.0193713652426708, 0.01787849181177048, 0.0024720844729833047, 0.002472085300533333, 0.0024720843605081644
==========
iteration 25
==========
weights [-0.14094762 -0.52527219 -0.49991509 -0.08408736 -0.62976705  0.22501239]
expeced value MDP LP -1.1743076201594294
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 3), (34, 3), (42, 1), (43, 1), (44, 3), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 1.06412315e-01  2.46401187e-04 -7.15165999e-02  7.41084387e-01
  2.07357717e-02  6.58730365e-01]
w_map [-0.12392416 -0.50515459 -0.14410445 -0.20928761 -0.78254761  0.2291582 ] loglik -1.3862943477316334
accepted/total = 1724/3000 = 0.5746666666666667
-------
true weights [-0.14094762 -0.52527219 -0.49991509 -0.08408736 -0.62976705  0.22501239]
features
4 	3 	1 	1 	2 	4 	1 	4 	
3 	1 	0 	3 	0 	4 	3 	3 	
2 	1 	0 	4 	1 	2 	1 	2 	
4 	0 	3 	1 	2 	3 	4 	0 	
2 	4 	0 	4 	0 	4 	0 	2 	
1 	1 	0 	3 	2 	1 	1 	3 	
0 	2 	0 	1 	0 	3 	3 	3 	
0 	0 	2 	3 	0 	3 	4 	5 	
optimal policy
>	>	v	v	v	>	v	v	
>	>	v	<	<	>	>	v	
>	v	v	<	v	v	>	v	
>	>	v	<	v	v	v	v	
>	>	v	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	^	>	.	
optimal values
-2.53	-1.92	-1.86	-1.93	-2.03	-2.35	-1.74	-1.77	
-1.92	-1.86	-1.35	-1.42	-1.54	-1.84	-1.22	-1.15	
-2.21	-1.73	-1.22	-1.83	-1.81	-1.74	-1.59	-1.08	
-1.83	-1.22	-1.09	-1.60	-1.30	-1.25	-1.21	-0.58	
-2.12	-1.63	-1.01	-1.37	-0.80	-1.18	-0.58	-0.45	
-1.74	-1.40	-0.88	-0.75	-0.67	-0.56	-0.47	0.05	
-1.23	-1.32	-0.83	-0.70	-0.17	-0.03	0.05	0.14	
-1.10	-0.97	-0.83	-0.34	-0.25	-0.12	-0.41	0.23	
map_weights [-0.12392416 -0.50515459 -0.14410445 -0.20928761 -0.78254761  0.2291582 ]
MAP reward
-0.78	-0.21	-0.51	-0.51	-0.14	-0.78	-0.51	-0.78	
-0.21	-0.51	-0.12	-0.21	-0.12	-0.78	-0.21	-0.21	
-0.14	-0.51	-0.12	-0.78	-0.51	-0.14	-0.51	-0.14	
-0.78	-0.12	-0.21	-0.51	-0.14	-0.21	-0.78	-0.12	
-0.14	-0.78	-0.12	-0.78	-0.12	-0.78	-0.12	-0.14	
-0.51	-0.51	-0.12	-0.21	-0.14	-0.51	-0.51	-0.21	
-0.12	-0.14	-0.12	-0.51	-0.12	-0.21	-0.21	-0.21	
-0.12	-0.12	-0.14	-0.21	-0.12	-0.21	-0.78	0.23	
Map policy
>	>	v	>	v	>	v	v	
>	>	v	<	v	>	>	v	
>	v	v	<	v	>	>	v	
>	>	v	>	v	<	v	v	
v	>	v	>	v	>	>	v	
v	>	>	>	v	v	>	v	
v	>	^	>	>	>	>	v	
>	>	>	>	^	^	>	.	
expeced value MDP LP -1.2068164972337034
mean w [-0.15618    -0.51881719 -0.32564912 -0.16437708 -0.56839487  0.33317586]
Mean policy from posterior
>	>	v	v	v	>	v	v	
>	>	v	<	v	>	>	v	
>	v	v	<	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
Mean rewards
-0.57	-0.16	-0.52	-0.52	-0.33	-0.57	-0.52	-0.57	
-0.16	-0.52	-0.16	-0.16	-0.16	-0.57	-0.16	-0.16	
-0.33	-0.52	-0.16	-0.57	-0.52	-0.33	-0.52	-0.33	
-0.57	-0.16	-0.16	-0.52	-0.33	-0.16	-0.57	-0.16	
-0.33	-0.57	-0.16	-0.57	-0.16	-0.57	-0.16	-0.33	
-0.52	-0.52	-0.16	-0.16	-0.33	-0.52	-0.52	-0.16	
-0.16	-0.33	-0.16	-0.52	-0.16	-0.16	-0.16	-0.16	
-0.16	-0.16	-0.33	-0.16	-0.16	-0.16	-0.57	0.33	
mean = 0.023413502340088765, map = 0.053206824190389135
CVaR policy
>	>	v	v	v	>	v	v	
>	>	v	>	v	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	>	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	>	v	v	
>	>	v	>	v	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	>	v	v	
>	>	v	<	v	>	>	v	
>	v	v	<	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	>	v	v	
>	>	v	<	v	>	>	v	
>	v	v	<	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
CVaR policy
>	>	v	v	v	>	v	v	
>	>	v	<	v	>	>	v	
>	v	v	<	v	v	>	v	
>	>	v	>	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	>	>	v	v	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	^	>	.	
cvar = , 0.0846315091216201, 0.08549249479263965, 0.05833990617078588, 0.02341350227588701, 0.02341350221581151
==========
iteration 26
==========
weights [-0.5936157  -0.01014504 -0.01389096 -0.2426591  -0.22906451  0.73210008]
expeced value MDP LP -0.33357797309954346
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 1), (19, 3), (27, 3), (35, 3), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.29144775  0.7651155   0.02706192 -0.15198773 -0.4726597  -0.28708304]
w_map [-0.84056645 -0.13045276 -0.20416353 -0.33355123 -0.28996674 -0.19852015] loglik -0.0009574883862626393
accepted/total = 959/3000 = 0.31966666666666665
-------
true weights [-0.5936157  -0.01014504 -0.01389096 -0.2426591  -0.22906451  0.73210008]
features
4 	1 	1 	1 	4 	1 	0 	3 	
3 	2 	1 	3 	4 	2 	0 	1 	
0 	4 	1 	2 	3 	2 	1 	3 	
1 	3 	0 	4 	0 	4 	4 	0 	
3 	4 	1 	4 	0 	1 	4 	0 	
2 	0 	3 	4 	4 	0 	1 	0 	
2 	2 	4 	2 	3 	2 	0 	1 	
1 	1 	0 	0 	3 	3 	2 	5 	
optimal policy
>	>	v	<	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	<	v	v	>	v	v	<	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
optimal values
-0.76	-0.53	-0.53	-0.53	-0.61	-0.39	-0.98	-0.84	
-0.77	-0.53	-0.52	-0.75	-0.61	-0.38	-0.95	-0.61	
-0.93	-0.74	-0.52	-0.51	-0.61	-0.37	-0.36	-0.60	
-0.34	-0.58	-0.88	-0.51	-0.95	-0.36	-0.36	-0.95	
-0.33	-0.51	-0.29	-0.28	-0.63	-0.14	-0.13	-0.48	
-0.09	-0.66	-0.29	-0.05	-0.04	-0.16	0.10	0.11	
-0.08	-0.06	-0.05	0.18	0.20	0.44	0.11	0.71	
-0.08	-0.07	-0.64	-0.38	0.21	0.46	0.71	0.73	
map_weights [-0.84056645 -0.13045276 -0.20416353 -0.33355123 -0.28996674 -0.19852015]
MAP reward
-0.29	-0.13	-0.13	-0.13	-0.29	-0.13	-0.84	-0.33	
-0.33	-0.20	-0.13	-0.33	-0.29	-0.20	-0.84	-0.13	
-0.84	-0.29	-0.13	-0.20	-0.33	-0.20	-0.13	-0.33	
-0.13	-0.33	-0.84	-0.29	-0.84	-0.29	-0.29	-0.84	
-0.33	-0.29	-0.13	-0.29	-0.84	-0.13	-0.29	-0.84	
-0.20	-0.84	-0.33	-0.29	-0.29	-0.84	-0.13	-0.84	
-0.20	-0.20	-0.29	-0.20	-0.33	-0.20	-0.84	-0.13	
-0.13	-0.13	-0.84	-0.84	-0.33	-0.33	-0.20	-0.20	
Map policy
>	>	v	>	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
expeced value MDP LP -1.9036308330064817
mean w [-0.78098478 -0.12820825 -0.16915478 -0.2922552  -0.29243294 -0.122106  ]
Mean policy from posterior
>	>	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
v	>	v	v	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.29	-0.13	-0.13	-0.13	-0.29	-0.13	-0.78	-0.29	
-0.29	-0.17	-0.13	-0.29	-0.29	-0.17	-0.78	-0.13	
-0.78	-0.29	-0.13	-0.17	-0.29	-0.17	-0.13	-0.29	
-0.13	-0.29	-0.78	-0.29	-0.78	-0.29	-0.29	-0.78	
-0.29	-0.29	-0.13	-0.29	-0.78	-0.13	-0.29	-0.78	
-0.17	-0.78	-0.29	-0.29	-0.29	-0.78	-0.13	-0.78	
-0.17	-0.17	-0.29	-0.17	-0.29	-0.17	-0.78	-0.13	
-0.13	-0.13	-0.78	-0.78	-0.29	-0.29	-0.17	-0.12	
mean = 0.006965485431011387, map = 0.003990108578305895
CVaR policy
>	>	v	>	>	v	v	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	v	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	v	
v	>	>	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
v	>	>	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
v	>	>	v	>	>	v	<	
v	v	v	v	>	v	v	<	
v	>	>	v	v	>	v	v	
v	v	>	v	v	v	>	v	
>	>	>	>	>	v	>	v	
>	^	^	>	>	>	>	.	
cvar = , 0.019761331607382138, 0.013191413837624177, 0.008075442524144827, 0.006132718423276895, 0.006132711050468309
==========
iteration 27
==========
weights [-0.01850592 -0.18489183 -0.17904333 -0.63184099 -0.50212388  0.53109754]
expeced value MDP LP -0.5220450183958818
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 1), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.30664515 -0.30099011 -0.37836552  0.68049803 -0.042966    0.45529068]
w_map [-0.14706691 -0.41381815 -0.26784161 -0.68029995 -0.42433803  0.30416435] loglik -3.417759352608574e-05
accepted/total = 1332/3000 = 0.444
-------
true weights [-0.01850592 -0.18489183 -0.17904333 -0.63184099 -0.50212388  0.53109754]
features
0 	2 	3 	1 	3 	4 	3 	4 	
0 	3 	3 	4 	4 	4 	0 	1 	
2 	1 	3 	4 	1 	0 	1 	1 	
0 	2 	4 	3 	3 	2 	1 	2 	
0 	1 	2 	4 	2 	2 	2 	1 	
3 	2 	1 	3 	3 	1 	4 	3 	
0 	2 	2 	1 	2 	0 	1 	4 	
4 	4 	3 	3 	1 	0 	0 	5 	
optimal policy
v	<	<	v	v	v	v	v	
v	<	>	>	v	v	v	<	
v	v	>	>	>	v	<	<	
v	v	v	v	v	v	v	<	
>	v	v	>	>	v	<	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
optimal values
-0.84	-1.01	-1.64	-1.46	-1.42	-1.11	-0.94	-0.99	
-0.83	-1.46	-1.90	-1.29	-0.79	-0.61	-0.31	-0.49	
-0.82	-0.98	-1.42	-0.79	-0.29	-0.11	-0.29	-0.47	
-0.65	-0.80	-0.95	-1.22	-0.72	-0.09	-0.27	-0.45	
-0.64	-0.63	-0.45	-0.59	-0.09	0.09	-0.09	-0.27	
-0.91	-0.45	-0.27	-0.54	-0.36	0.27	-0.19	-0.61	
-0.28	-0.27	-0.09	0.09	0.28	0.46	0.32	0.02	
-0.78	-0.77	-0.72	-0.34	0.29	0.48	0.51	0.53	
map_weights [-0.14706691 -0.41381815 -0.26784161 -0.68029995 -0.42433803  0.30416435]
MAP reward
-0.15	-0.27	-0.68	-0.41	-0.68	-0.42	-0.68	-0.42	
-0.15	-0.68	-0.68	-0.42	-0.42	-0.42	-0.15	-0.41	
-0.27	-0.41	-0.68	-0.42	-0.41	-0.15	-0.41	-0.41	
-0.15	-0.27	-0.42	-0.68	-0.68	-0.27	-0.41	-0.27	
-0.15	-0.41	-0.27	-0.42	-0.27	-0.27	-0.27	-0.41	
-0.68	-0.27	-0.41	-0.68	-0.68	-0.41	-0.42	-0.68	
-0.15	-0.27	-0.27	-0.41	-0.27	-0.15	-0.41	-0.42	
-0.42	-0.42	-0.68	-0.68	-0.41	-0.15	-0.15	0.30	
Map policy
v	<	>	v	v	v	v	v	
v	<	>	>	v	v	v	<	
v	v	>	>	>	v	<	v	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
expeced value MDP LP -1.547872661015889
mean w [-0.12559328 -0.34203605 -0.1947096  -0.63020524 -0.50342503  0.07450717]
Mean policy from posterior
v	<	<	v	v	v	v	v	
v	<	>	>	v	v	v	<	
v	v	>	>	>	v	<	<	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	<	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.13	-0.19	-0.63	-0.34	-0.63	-0.50	-0.63	-0.50	
-0.13	-0.63	-0.63	-0.50	-0.50	-0.50	-0.13	-0.34	
-0.19	-0.34	-0.63	-0.50	-0.34	-0.13	-0.34	-0.34	
-0.13	-0.19	-0.50	-0.63	-0.63	-0.19	-0.34	-0.19	
-0.13	-0.34	-0.19	-0.50	-0.19	-0.19	-0.19	-0.34	
-0.63	-0.19	-0.34	-0.63	-0.63	-0.34	-0.50	-0.63	
-0.13	-0.19	-0.19	-0.34	-0.19	-0.13	-0.34	-0.50	
-0.50	-0.50	-0.63	-0.63	-0.34	-0.13	-0.13	0.07	
mean = 2.7022453163993987e-09, map = 0.04163375799597846
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	<	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	>	>	>	v	<	v	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	<	v	v	v	v	v	
v	v	>	>	v	v	v	<	
v	v	>	>	>	v	<	v	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	<	<	v	v	v	v	v	
v	v	>	>	v	v	v	<	
v	v	>	>	>	v	<	<	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	<	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	<	<	v	v	v	v	v	
v	v	>	>	v	v	v	<	
v	v	>	>	>	v	<	<	
v	v	v	v	v	v	v	v	
>	v	v	>	>	v	<	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
cvar = , 0.11575036414999407, 0.07312728561020965, 0.04083769304728346, 0.018137197447565256, 0.01813719740388109
==========
iteration 28
==========
weights [-0.5100235  -0.34968658 -0.25484199 -0.37175441 -0.63466652  0.10792566]
expeced value MDP LP -2.4444006017462887
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 3), (26, 3), (34, 1), (35, 3), (43, 1), (44, 1), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.07390333  0.09646327 -0.69596513  0.64669186  0.16083294 -0.23830249]
w_map [-0.50049046 -0.19058903 -0.25079877 -0.21172714 -0.73133607 -0.26571464] loglik -1.3862943661658491
accepted/total = 1672/3000 = 0.5573333333333333
-------
true weights [-0.5100235  -0.34968658 -0.25484199 -0.37175441 -0.63466652  0.10792566]
features
2 	2 	3 	3 	3 	1 	2 	2 	
4 	2 	0 	1 	0 	4 	0 	0 	
4 	1 	1 	4 	4 	3 	3 	4 	
4 	4 	0 	2 	4 	1 	4 	2 	
2 	4 	2 	1 	4 	4 	4 	4 	
1 	0 	0 	3 	0 	2 	0 	3 	
2 	4 	2 	4 	2 	2 	2 	3 	
4 	4 	4 	1 	2 	4 	0 	5 	
optimal policy
>	v	>	v	>	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
optimal values
-4.21	-3.99	-4.04	-3.70	-3.59	-3.26	-3.07	-2.84	
-4.37	-3.78	-3.72	-3.36	-3.42	-2.94	-2.96	-2.61	
-4.16	-3.56	-3.24	-3.04	-2.94	-2.32	-2.47	-2.12	
-3.87	-3.53	-2.92	-2.43	-2.59	-1.97	-2.12	-1.50	
-3.27	-3.04	-2.43	-2.20	-2.13	-1.64	-1.65	-1.26	
-3.05	-2.85	-2.36	-1.87	-1.51	-1.01	-1.02	-0.63	
-2.72	-2.49	-1.88	-1.64	-1.01	-0.77	-0.52	-0.26	
-3.33	-2.83	-2.21	-1.60	-1.26	-1.03	-0.40	0.11	
map_weights [-0.50049046 -0.19058903 -0.25079877 -0.21172714 -0.73133607 -0.26571464]
MAP reward
-0.25	-0.25	-0.21	-0.21	-0.21	-0.19	-0.25	-0.25	
-0.73	-0.25	-0.50	-0.19	-0.50	-0.73	-0.50	-0.50	
-0.73	-0.19	-0.19	-0.73	-0.73	-0.21	-0.21	-0.73	
-0.73	-0.73	-0.50	-0.25	-0.73	-0.19	-0.73	-0.25	
-0.25	-0.73	-0.25	-0.19	-0.73	-0.73	-0.73	-0.73	
-0.19	-0.50	-0.50	-0.21	-0.50	-0.25	-0.50	-0.21	
-0.25	-0.73	-0.25	-0.73	-0.25	-0.25	-0.25	-0.21	
-0.73	-0.73	-0.73	-0.19	-0.25	-0.73	-0.50	-0.27	
Map policy
>	v	>	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
expeced value MDP LP -1.7088019471836973
mean w [-0.42654535 -0.16625547 -0.16375565 -0.21717824 -0.71826138  0.19779479]
Mean policy from posterior
>	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
v	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
Mean rewards
-0.16	-0.16	-0.22	-0.22	-0.22	-0.17	-0.16	-0.16	
-0.72	-0.16	-0.43	-0.17	-0.43	-0.72	-0.43	-0.43	
-0.72	-0.17	-0.17	-0.72	-0.72	-0.22	-0.22	-0.72	
-0.72	-0.72	-0.43	-0.16	-0.72	-0.17	-0.72	-0.16	
-0.16	-0.72	-0.16	-0.17	-0.72	-0.72	-0.72	-0.72	
-0.17	-0.43	-0.43	-0.22	-0.43	-0.16	-0.43	-0.22	
-0.16	-0.72	-0.16	-0.72	-0.16	-0.16	-0.16	-0.22	
-0.72	-0.72	-0.72	-0.17	-0.16	-0.72	-0.43	0.20	
mean = 0.017032698476747488, map = 0.018153310914388765
CVaR policy
>	v	>	v	>	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
v	>	v	v	>	v	>	v	
v	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
v	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
CVaR policy
>	v	v	v	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	<	v	
v	>	v	v	>	v	>	v	
v	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	^	>	>	.	
cvar = , 0.005058781818280345, 0.0076433957881776315, 0.018645135263977597, 0.01703272327967076, 0.017032699049627897
==========
iteration 29
==========
weights [-0.52871357 -0.78382907 -0.29487581 -0.01777343 -0.12584647  0.05448833]
expeced value MDP LP -1.3249246001259218
demonstration
[(0, 0), (0, 2), (0, 2), (0, 0), (0, 0), (0, 2), (0, 0), (0, 0), (0, 2), (0, 2), (0, 0), (0, 0), (0, 0), (0, 0), (0, 2), (0, 2), (0, 2), (0, 0), (0, 0), (0, 0)]
[ 0.11600658  0.21295016 -0.31402286 -0.08450532  0.34183261 -0.84769904]
w_map [-0.33266325 -0.43718741  0.03002024  0.81918131 -0.13346448  0.09181686] loglik -13.862943611184164
accepted/total = 2801/3000 = 0.9336666666666666
-------
true weights [-0.52871357 -0.78382907 -0.29487581 -0.01777343 -0.12584647  0.05448833]
features
3 	4 	2 	4 	1 	3 	1 	1 	
1 	3 	2 	0 	1 	0 	0 	1 	
1 	0 	2 	0 	1 	3 	2 	2 	
2 	3 	0 	0 	3 	4 	4 	2 	
3 	1 	1 	4 	0 	1 	4 	2 	
0 	1 	4 	1 	2 	3 	3 	4 	
1 	2 	0 	2 	4 	2 	4 	0 	
2 	4 	4 	4 	1 	1 	4 	5 	
optimal policy
<	<	<	>	>	v	<	v	
^	^	<	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	>	>	v	<	
<	^	>	>	v	v	v	v	
^	>	>	>	>	>	v	<	
>	v	>	>	>	>	v	v	
>	>	>	^	^	>	>	.	
optimal values
-1.78	-1.89	-2.16	-2.00	-1.89	-1.12	-1.89	-2.58	
-2.54	-1.88	-2.16	-2.15	-1.89	-1.12	-1.27	-1.81	
-2.68	-2.15	-1.91	-1.63	-1.37	-0.59	-0.75	-1.04	
-1.91	-1.63	-1.63	-1.12	-0.59	-0.58	-0.46	-0.75	
-1.78	-2.40	-1.93	-1.16	-1.04	-1.01	-0.34	-0.63	
-2.29	-2.18	-1.41	-1.30	-0.52	-0.23	-0.21	-0.34	
-2.30	-1.53	-1.42	-0.90	-0.61	-0.49	-0.20	-0.47	
-1.53	-1.25	-1.13	-1.02	-1.39	-0.86	-0.07	0.05	
map_weights [-0.33266325 -0.43718741  0.03002024  0.81918131 -0.13346448  0.09181686]
MAP reward
0.82	-0.13	0.03	-0.13	-0.44	0.82	-0.44	-0.44	
-0.44	0.82	0.03	-0.33	-0.44	-0.33	-0.33	-0.44	
-0.44	-0.33	0.03	-0.33	-0.44	0.82	0.03	0.03	
0.03	0.82	-0.33	-0.33	0.82	-0.13	-0.13	0.03	
0.82	-0.44	-0.44	-0.13	-0.33	-0.44	-0.13	0.03	
-0.33	-0.44	-0.13	-0.44	0.03	0.82	0.82	-0.13	
-0.44	0.03	-0.33	0.03	-0.13	0.03	-0.13	-0.33	
0.03	-0.13	-0.13	-0.13	-0.44	-0.44	-0.13	0.09	
Map policy
<	<	<	>	>	^	<	<	
^	^	<	<	>	^	<	<	
v	v	^	>	>	^	<	<	
v	<	<	>	v	^	v	v	
<	<	<	>	v	v	v	v	
^	<	>	>	>	>	<	<	
^	^	>	>	>	^	^	^	
^	^	>	^	^	^	^	.	
expeced value MDP LP 44.43849593990978
mean w [-0.1797068  -0.06753091 -0.2526854   0.46299157 -0.19491542  0.0642314 ]
Mean policy from posterior
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	<	
<	<	<	<	>	v	v	v	
^	^	^	>	>	>	<	<	
^	^	^	^	>	^	^	<	
^	^	^	>	>	^	^	.	
Mean rewards
0.46	-0.19	-0.25	-0.19	-0.07	0.46	-0.07	-0.07	
-0.07	0.46	-0.25	-0.18	-0.07	-0.18	-0.18	-0.07	
-0.07	-0.18	-0.25	-0.18	-0.07	0.46	-0.25	-0.25	
-0.25	0.46	-0.18	-0.18	0.46	-0.19	-0.19	-0.25	
0.46	-0.07	-0.07	-0.19	-0.18	-0.07	-0.19	-0.25	
-0.18	-0.07	-0.19	-0.07	-0.25	0.46	0.46	-0.19	
-0.07	-0.25	-0.18	-0.25	-0.19	-0.25	-0.19	-0.18	
-0.25	-0.19	-0.19	-0.19	-0.07	-0.07	-0.19	0.06	
mean = 1.3776135481328342, map = 1.1519916857041046
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	<	>	^	<	<	
v	v	v	>	>	^	<	<	
v	<	<	>	v	^	v	v	
<	<	<	>	v	v	v	v	
^	<	>	>	>	>	<	<	
^	<	>	^	^	^	^	<	
^	^	^	^	>	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	<	^	^	^	^	
v	v	^	>	>	^	<	<	
v	<	<	>	^	v	v	v	
<	<	<	v	v	v	v	<	
^	^	>	>	>	>	<	<	
^	^	>	^	>	^	^	^	
^	^	^	^	>	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	v	
<	<	<	<	>	v	v	v	
^	^	^	>	>	>	<	<	
^	^	<	^	>	^	^	<	
^	^	^	>	>	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	<	
<	<	<	<	>	v	v	<	
^	^	^	>	>	>	<	<	
^	^	^	^	^	^	^	^	
^	^	^	>	>	^	^	.	
CVaR policy
<	<	<	>	>	^	<	<	
^	<	<	>	^	^	^	^	
^	^	<	>	>	^	<	<	
v	v	<	>	^	v	v	<	
<	<	<	<	>	v	v	<	
^	^	^	>	>	>	<	<	
^	^	^	^	^	^	^	<	
^	^	^	>	>	^	^	.	
cvar = , 1.2213490687224182, 1.2992405793055013, 1.385431032815754, 1.3776167286633523, 1.3776482687249232
==========
iteration 30
==========
weights [-0.31004306 -0.29634479 -0.21476876 -0.59271379 -0.28141103  0.58260248]
expeced value MDP LP -1.3929046319665854
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 1), (26, 1), (27, 1), (28, 3), (36, 3), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.48284408  0.20922552 -0.48710964 -0.08950067  0.66386372  0.19257484]
w_map [-0.34981138 -0.1076083  -0.06291255 -0.55550933 -0.7428922   0.04018739] loglik -0.41795713409048574
accepted/total = 1842/3000 = 0.614
-------
true weights [-0.31004306 -0.29634479 -0.21476876 -0.59271379 -0.28141103  0.58260248]
features
0 	3 	2 	3 	2 	4 	3 	2 	
3 	3 	2 	3 	0 	1 	0 	2 	
1 	3 	3 	1 	3 	1 	1 	2 	
1 	2 	2 	1 	2 	4 	0 	1 	
2 	1 	3 	4 	4 	4 	2 	0 	
4 	4 	0 	2 	1 	0 	3 	4 	
3 	4 	0 	3 	2 	3 	4 	0 	
1 	3 	1 	1 	2 	0 	0 	5 	
optimal policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	>	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	v	v	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.32	-3.25	-2.68	-2.65	-2.08	-1.88	-1.82	-1.24	
-3.04	-3.06	-2.49	-2.38	-1.91	-1.62	-1.34	-1.04	
-2.47	-2.50	-2.30	-1.81	-1.82	-1.37	-1.12	-0.83	
-2.20	-1.92	-1.73	-1.53	-1.24	-1.09	-0.84	-0.62	
-2.00	-1.82	-1.82	-1.24	-1.04	-0.81	-0.54	-0.33	
-1.81	-1.54	-1.27	-0.97	-0.76	-0.91	-0.61	-0.02	
-1.99	-1.42	-1.15	-1.06	-0.47	-0.61	-0.02	0.27	
-1.71	-1.43	-0.84	-0.55	-0.26	-0.05	0.27	0.58	
map_weights [-0.34981138 -0.1076083  -0.06291255 -0.55550933 -0.7428922   0.04018739]
MAP reward
-0.35	-0.56	-0.06	-0.56	-0.06	-0.74	-0.56	-0.06	
-0.56	-0.56	-0.06	-0.56	-0.35	-0.11	-0.35	-0.06	
-0.11	-0.56	-0.56	-0.11	-0.56	-0.11	-0.11	-0.06	
-0.11	-0.06	-0.06	-0.11	-0.06	-0.74	-0.35	-0.11	
-0.06	-0.11	-0.56	-0.74	-0.74	-0.74	-0.06	-0.35	
-0.74	-0.74	-0.35	-0.06	-0.11	-0.35	-0.56	-0.74	
-0.56	-0.74	-0.35	-0.56	-0.06	-0.56	-0.74	-0.35	
-0.11	-0.56	-0.11	-0.11	-0.06	-0.35	-0.35	0.04	
Map policy
v	>	v	>	v	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	<	v	v	
>	>	v	v	v	v	>	v	
^	>	>	>	v	<	v	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.020440288596799
mean w [-0.39715345 -0.1785385  -0.141508   -0.60825214 -0.409675   -0.13704952]
Mean policy from posterior
v	>	v	>	v	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	>	v	<	v	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.40	-0.61	-0.14	-0.61	-0.14	-0.41	-0.61	-0.14	
-0.61	-0.61	-0.14	-0.61	-0.40	-0.18	-0.40	-0.14	
-0.18	-0.61	-0.61	-0.18	-0.61	-0.18	-0.18	-0.14	
-0.18	-0.14	-0.14	-0.18	-0.14	-0.41	-0.40	-0.18	
-0.14	-0.18	-0.61	-0.41	-0.41	-0.41	-0.14	-0.40	
-0.41	-0.41	-0.40	-0.14	-0.18	-0.40	-0.61	-0.41	
-0.61	-0.41	-0.40	-0.61	-0.14	-0.61	-0.41	-0.40	
-0.18	-0.61	-0.18	-0.18	-0.14	-0.40	-0.40	-0.14	
mean = 0.014947853910139086, map = 0.0509703730903881
CVaR policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
v	v	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	v	v	
>	v	>	v	v	>	>	v	
>	>	>	>	v	>	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	>	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	v	v	
>	v	>	v	v	>	>	v	
>	>	>	>	v	<	v	v	
v	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	>	v	<	v	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	>	v	v	>	v	
v	>	v	v	>	v	>	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	>	v	<	v	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.03494170565967725, 0.010352683873659396, 0.012737766554360874, 0.014947840856703198, 0.014947840772681742
==========
iteration 31
==========
weights [-0.05358524 -0.25940048 -0.6119391  -0.55979138 -0.36199438  0.33311294]
expeced value MDP LP -1.3212044259615796
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 1), (41, 3), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.33880023  0.24823556 -0.3618086   0.15450941  0.39211521  0.71767719]
w_map [-0.2093545  -0.21169408 -0.6451427  -0.52027983 -0.43004858  0.19878211] loglik -0.6931472278767132
accepted/total = 1661/3000 = 0.5536666666666666
-------
true weights [-0.05358524 -0.25940048 -0.6119391  -0.55979138 -0.36199438  0.33311294]
features
4 	1 	0 	2 	1 	0 	0 	0 	
0 	1 	4 	2 	0 	4 	0 	0 	
0 	0 	3 	3 	0 	0 	3 	2 	
0 	2 	1 	4 	1 	2 	1 	0 	
1 	1 	2 	1 	2 	4 	3 	0 	
1 	4 	4 	1 	1 	4 	2 	4 	
2 	1 	1 	0 	4 	3 	4 	4 	
2 	0 	3 	2 	0 	1 	1 	5 	
optimal policy
v	>	>	>	>	>	>	v	
v	v	>	>	v	>	>	v	
v	<	v	>	>	>	v	v	
v	>	>	v	>	>	>	v	
v	v	>	v	v	>	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-2.43	-2.38	-2.14	-2.11	-1.51	-1.27	-1.23	-1.18	
-2.09	-2.33	-2.36	-2.02	-1.42	-1.53	-1.18	-1.14	
-2.06	-2.09	-2.29	-1.93	-1.38	-1.34	-1.30	-1.10	
-2.02	-2.34	-1.75	-1.50	-1.60	-1.35	-0.75	-0.49	
-1.99	-1.75	-1.75	-1.15	-1.46	-1.35	-1.00	-0.44	
-1.75	-1.50	-1.25	-0.90	-0.85	-1.10	-0.90	-0.39	
-1.75	-1.15	-0.90	-0.65	-0.60	-0.75	-0.29	-0.03	
-1.79	-1.19	-1.40	-0.85	-0.24	-0.19	0.07	0.33	
map_weights [-0.2093545  -0.21169408 -0.6451427  -0.52027983 -0.43004858  0.19878211]
MAP reward
-0.43	-0.21	-0.21	-0.65	-0.21	-0.21	-0.21	-0.21	
-0.21	-0.21	-0.43	-0.65	-0.21	-0.43	-0.21	-0.21	
-0.21	-0.21	-0.52	-0.52	-0.21	-0.21	-0.52	-0.65	
-0.21	-0.65	-0.21	-0.43	-0.21	-0.65	-0.21	-0.21	
-0.21	-0.21	-0.65	-0.21	-0.65	-0.43	-0.52	-0.21	
-0.21	-0.43	-0.43	-0.21	-0.21	-0.43	-0.65	-0.43	
-0.65	-0.21	-0.21	-0.21	-0.43	-0.52	-0.43	-0.43	
-0.65	-0.21	-0.52	-0.65	-0.21	-0.21	-0.21	0.20	
Map policy
v	v	v	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.5687305110902148
mean w [-0.14251228 -0.16547749 -0.64297685 -0.4409022  -0.34515143 -0.01279592]
Mean policy from posterior
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	<	v	v	>	>	v	v	
v	>	>	v	<	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.35	-0.17	-0.14	-0.64	-0.17	-0.14	-0.14	-0.14	
-0.14	-0.17	-0.35	-0.64	-0.14	-0.35	-0.14	-0.14	
-0.14	-0.14	-0.44	-0.44	-0.14	-0.14	-0.44	-0.64	
-0.14	-0.64	-0.17	-0.35	-0.17	-0.64	-0.17	-0.14	
-0.17	-0.17	-0.64	-0.17	-0.64	-0.35	-0.44	-0.14	
-0.17	-0.35	-0.35	-0.17	-0.17	-0.35	-0.64	-0.35	
-0.64	-0.17	-0.17	-0.14	-0.35	-0.44	-0.35	-0.35	
-0.64	-0.14	-0.44	-0.64	-0.14	-0.17	-0.17	-0.01	
mean = 0.0359612657253785, map = 0.07280403968267324
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	>	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	>	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
v	v	v	>	v	>	v	v	
v	<	v	v	>	>	v	v	
v	>	>	v	v	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
cvar = , 0.05291659446712105, 0.04521008855387354, 0.04521008856916442, 0.045210088613781396, 0.034490015961504694
==========
iteration 32
==========
weights [-0.4094745  -0.40134772 -0.10730514 -0.40448845 -0.6221851   0.33016821]
expeced value MDP LP -1.3553792537867473
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[ 0.34270999  0.2582485  -0.57519444  0.06877238 -0.69147377  0.04629602]
w_map [-0.57609197 -0.4971546  -0.08889267 -0.17108993 -0.61926187  0.01721645] loglik -1.2086022509970995
accepted/total = 1666/3000 = 0.5553333333333333
-------
true weights [-0.4094745  -0.40134772 -0.10730514 -0.40448845 -0.6221851   0.33016821]
features
3 	1 	0 	2 	1 	0 	2 	3 	
2 	3 	4 	0 	3 	1 	2 	1 	
1 	1 	2 	4 	0 	0 	3 	1 	
1 	4 	0 	0 	1 	2 	1 	1 	
3 	2 	3 	1 	2 	2 	0 	2 	
3 	1 	0 	3 	3 	2 	2 	4 	
3 	3 	1 	0 	3 	1 	1 	2 	
1 	1 	2 	2 	4 	2 	2 	5 	
optimal policy
v	v	>	>	v	v	v	<	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
>	^	v	>	>	>	v	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.09	-3.01	-2.64	-2.25	-2.17	-1.79	-1.58	-1.96	
-2.71	-2.64	-2.48	-2.18	-1.78	-1.39	-1.48	-1.69	
-2.63	-2.26	-1.87	-2.00	-1.39	-1.00	-1.39	-1.30	
-2.26	-2.09	-1.79	-1.39	-0.99	-0.60	-0.99	-0.90	
-1.87	-1.48	-1.39	-0.99	-0.60	-0.50	-0.70	-0.51	
-2.26	-1.87	-1.51	-1.19	-0.79	-0.39	-0.29	-0.40	
-1.89	-1.50	-1.11	-1.02	-0.69	-0.29	-0.18	0.22	
-1.50	-1.11	-0.72	-0.62	-0.51	0.11	0.22	0.33	
map_weights [-0.57609197 -0.4971546  -0.08889267 -0.17108993 -0.61926187  0.01721645]
MAP reward
-0.17	-0.50	-0.58	-0.09	-0.50	-0.58	-0.09	-0.17	
-0.09	-0.17	-0.62	-0.58	-0.17	-0.50	-0.09	-0.50	
-0.50	-0.50	-0.09	-0.62	-0.58	-0.58	-0.17	-0.50	
-0.50	-0.62	-0.58	-0.58	-0.50	-0.09	-0.50	-0.50	
-0.17	-0.09	-0.17	-0.50	-0.09	-0.09	-0.58	-0.09	
-0.17	-0.50	-0.58	-0.17	-0.17	-0.09	-0.09	-0.62	
-0.17	-0.17	-0.50	-0.58	-0.17	-0.50	-0.50	-0.09	
-0.50	-0.50	-0.09	-0.09	-0.62	-0.09	-0.09	0.02	
Map policy
v	v	>	>	v	>	v	<	
v	v	v	>	>	v	v	<	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	<	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.2795879755583162
mean w [-0.54907298 -0.2612987  -0.11128113 -0.255005   -0.58089025  0.11654288]
Mean policy from posterior
v	v	>	>	v	>	v	<	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.26	-0.26	-0.55	-0.11	-0.26	-0.55	-0.11	-0.26	
-0.11	-0.26	-0.58	-0.55	-0.26	-0.26	-0.11	-0.26	
-0.26	-0.26	-0.11	-0.58	-0.55	-0.55	-0.26	-0.26	
-0.26	-0.58	-0.55	-0.55	-0.26	-0.11	-0.26	-0.26	
-0.26	-0.11	-0.26	-0.26	-0.11	-0.11	-0.55	-0.11	
-0.26	-0.26	-0.55	-0.26	-0.26	-0.11	-0.11	-0.58	
-0.26	-0.26	-0.26	-0.55	-0.26	-0.26	-0.26	-0.11	
-0.26	-0.26	-0.11	-0.11	-0.58	-0.11	-0.11	0.12	
mean = 0.005352792239125437, map = 0.008887484306126625
CVaR policy
v	v	>	>	v	>	v	v	
v	v	v	>	>	v	>	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	<	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	>	v	<	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	v	v	
>	>	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	<	
v	v	v	>	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	<	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.04048041317107298, 0.03339342693227243, 0.004904291607143563, 0.007049599752773972, 0.0053527913433526475
==========
iteration 33
==========
weights [-0.5241945  -0.77962364 -0.22290199 -0.13972452 -0.17733602  0.12942492]
expeced value MDP LP -1.9563915891969281
demonstration
[(0, 1), (1, 1), (2, 1), (3, 3), (11, 1), (12, 1), (13, 1), (14, 3), (22, 3), (30, 3), (38, 3), (46, 3), (54, 3), (62, 1), (63, None)]
[ 0.00237649 -0.53984377 -0.29030103 -0.64761709  0.30705327  0.33256393]
w_map [-0.45124048 -0.74650654 -0.26142533 -0.20576397 -0.35772302 -0.0214996 ] loglik -4.6636916550824026e-08
accepted/total = 1632/3000 = 0.544
-------
true weights [-0.5241945  -0.77962364 -0.22290199 -0.13972452 -0.17733602  0.12942492]
features
2 	1 	4 	3 	1 	2 	3 	0 	
1 	0 	0 	0 	4 	3 	2 	1 	
0 	3 	0 	1 	0 	0 	3 	4 	
1 	4 	2 	1 	1 	3 	3 	4 	
3 	1 	2 	1 	1 	2 	3 	2 	
1 	2 	3 	4 	3 	0 	0 	1 	
1 	3 	3 	1 	1 	0 	0 	1 	
2 	0 	4 	1 	1 	4 	3 	5 	
optimal policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	^	v	v	<	
>	>	v	<	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-3.64	-3.45	-2.70	-2.55	-2.69	-1.98	-1.77	-2.28	
-3.58	-2.83	-2.93	-2.43	-1.93	-1.77	-1.65	-2.36	
-2.83	-2.33	-2.56	-3.19	-2.43	-1.95	-1.44	-1.60	
-2.97	-2.21	-2.05	-2.81	-2.20	-1.44	-1.31	-1.48	
-2.73	-2.61	-1.85	-2.28	-2.12	-1.39	-1.18	-1.39	
-2.61	-1.85	-1.64	-1.52	-1.36	-1.23	-1.05	-1.42	
-2.65	-1.89	-1.77	-2.25	-1.48	-0.71	-0.54	-0.65	
-2.59	-2.39	-1.90	-1.74	-0.97	-0.19	-0.01	0.13	
map_weights [-0.45124048 -0.74650654 -0.26142533 -0.20576397 -0.35772302 -0.0214996 ]
MAP reward
-0.26	-0.75	-0.36	-0.21	-0.75	-0.26	-0.21	-0.45	
-0.75	-0.45	-0.45	-0.45	-0.36	-0.21	-0.26	-0.75	
-0.45	-0.21	-0.45	-0.75	-0.45	-0.45	-0.21	-0.36	
-0.75	-0.36	-0.26	-0.75	-0.75	-0.21	-0.21	-0.36	
-0.21	-0.75	-0.26	-0.75	-0.75	-0.26	-0.21	-0.26	
-0.75	-0.26	-0.21	-0.36	-0.21	-0.45	-0.45	-0.75	
-0.75	-0.21	-0.21	-0.75	-0.75	-0.45	-0.45	-0.75	
-0.26	-0.45	-0.36	-0.75	-0.75	-0.36	-0.21	-0.02	
Map policy
>	>	>	v	>	>	v	<	
>	v	>	>	>	>	v	<	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	>	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8521239818410489
mean w [-0.41568391 -0.67906052 -0.26311783 -0.11744503 -0.2228166  -0.0165603 ]
Mean policy from posterior
>	>	>	v	v	>	v	<	
>	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.26	-0.68	-0.22	-0.12	-0.68	-0.26	-0.12	-0.42	
-0.68	-0.42	-0.42	-0.42	-0.22	-0.12	-0.26	-0.68	
-0.42	-0.12	-0.42	-0.68	-0.42	-0.42	-0.12	-0.22	
-0.68	-0.22	-0.26	-0.68	-0.68	-0.12	-0.12	-0.22	
-0.12	-0.68	-0.26	-0.68	-0.68	-0.26	-0.12	-0.26	
-0.68	-0.26	-0.12	-0.22	-0.12	-0.42	-0.42	-0.68	
-0.68	-0.12	-0.12	-0.68	-0.68	-0.42	-0.42	-0.68	
-0.26	-0.42	-0.22	-0.68	-0.68	-0.22	-0.12	-0.02	
mean = 0.003340601727659376, map = 0.005101047848738993
CVaR policy
>	>	>	v	v	>	v	<	
>	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	<	
>	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	<	
>	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	<	
v	v	>	>	>	>	v	v	
>	v	v	>	>	v	v	<	
>	>	v	>	>	>	v	<	
>	v	v	v	v	>	v	<	
>	>	>	>	>	v	v	v	
>	>	^	>	>	v	v	v	
>	^	>	>	>	>	>	.	
cvar = , 0.03435181628371531, 0.018414828970766406, 0.003086387866201745, 0.002865197300229383, 0.0028652229195620382
==========
iteration 34
==========
weights [-0.62439498 -0.16258313 -0.103938   -0.22088694 -0.30853585  0.65491153]
expeced value MDP LP -0.671030005140768
demonstration
[(0, 3), (8, 3), (16, 1), (17, 1), (18, 3), (26, 3), (34, 3), (42, 1), (43, 1), (44, 3), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.3521392  -0.23277716  0.31672198  0.48164304 -0.32444106  0.61988542]
w_map [-0.62332615 -0.33484536 -0.12424128 -0.32024793 -0.5509688   0.27889396] loglik -2.079441551451106
accepted/total = 1561/3000 = 0.5203333333333333
-------
true weights [-0.62439498 -0.16258313 -0.103938   -0.22088694 -0.30853585  0.65491153]
features
3 	4 	4 	0 	4 	4 	4 	1 	
4 	4 	4 	0 	1 	2 	4 	2 	
1 	3 	3 	3 	0 	0 	2 	2 	
0 	3 	2 	1 	1 	3 	0 	1 	
1 	4 	1 	4 	0 	1 	4 	1 	
2 	1 	4 	2 	2 	3 	4 	0 	
4 	2 	3 	1 	2 	2 	2 	4 	
4 	1 	3 	2 	3 	4 	3 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
v	>	v	v	>	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	^	>	^	>	>	>	.	
optimal values
-1.75	-1.69	-1.48	-2.01	-1.55	-1.40	-1.25	-0.96	
-1.54	-1.39	-1.18	-1.40	-1.26	-1.11	-1.10	-0.80	
-1.25	-1.09	-0.88	-0.78	-1.17	-1.01	-0.80	-0.70	
-1.41	-0.88	-0.67	-0.57	-0.55	-0.39	-0.92	-0.61	
-0.79	-0.84	-0.57	-0.41	-0.62	-0.17	-0.30	-0.45	
-0.64	-0.54	-0.41	-0.10	0.00	-0.01	0.01	-0.29	
-0.68	-0.38	-0.28	-0.06	0.11	0.21	0.32	0.34	
-0.84	-0.54	-0.38	-0.16	-0.11	0.11	0.43	0.65	
map_weights [-0.62332615 -0.33484536 -0.12424128 -0.32024793 -0.5509688   0.27889396]
MAP reward
-0.32	-0.55	-0.55	-0.62	-0.55	-0.55	-0.55	-0.33	
-0.55	-0.55	-0.55	-0.62	-0.33	-0.12	-0.55	-0.12	
-0.33	-0.32	-0.32	-0.32	-0.62	-0.62	-0.12	-0.12	
-0.62	-0.32	-0.12	-0.33	-0.33	-0.32	-0.62	-0.33	
-0.33	-0.55	-0.33	-0.55	-0.62	-0.33	-0.55	-0.33	
-0.12	-0.33	-0.55	-0.12	-0.12	-0.32	-0.55	-0.62	
-0.55	-0.12	-0.32	-0.33	-0.12	-0.12	-0.12	-0.55	
-0.55	-0.33	-0.32	-0.12	-0.32	-0.55	-0.32	0.28	
Map policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
v	>	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
expeced value MDP LP -1.6635924288966244
mean w [-0.62236329 -0.3288929  -0.09879916 -0.26557966 -0.50306385 -0.01219174]
Mean policy from posterior
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
Mean rewards
-0.27	-0.50	-0.50	-0.62	-0.50	-0.50	-0.50	-0.33	
-0.50	-0.50	-0.50	-0.62	-0.33	-0.10	-0.50	-0.10	
-0.33	-0.27	-0.27	-0.27	-0.62	-0.62	-0.10	-0.10	
-0.62	-0.27	-0.10	-0.33	-0.33	-0.27	-0.62	-0.33	
-0.33	-0.50	-0.33	-0.50	-0.62	-0.33	-0.50	-0.33	
-0.10	-0.33	-0.50	-0.10	-0.10	-0.27	-0.50	-0.62	
-0.50	-0.10	-0.27	-0.33	-0.10	-0.10	-0.10	-0.50	
-0.50	-0.33	-0.27	-0.10	-0.27	-0.50	-0.27	-0.01	
mean = 0.013614775782363142, map = 0.012242491266750477
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	v	v	v	v	v	>	v	
>	>	v	v	v	v	<	v	
v	v	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	.	
cvar = , 0.01346898548327835, 0.011549128215317506, 0.012200513332147556, 0.01361477485730489, 0.013614774859118328
==========
iteration 35
==========
weights [-0.06684751 -0.23871016 -0.10837398 -0.73125052 -0.41222717  0.47132302]
expeced value MDP LP -0.6096881849589867
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 3), (33, 1), (34, 3), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.49586051 -0.63404722 -0.13185044  0.14237997 -0.27978761  0.485972  ]
w_map [-0.21459208 -0.40473424 -0.38008789 -0.58337764 -0.55227874 -0.01823065] loglik -6.258141240778059e-07
accepted/total = 1690/3000 = 0.5633333333333334
-------
true weights [-0.06684751 -0.23871016 -0.10837398 -0.73125052 -0.41222717  0.47132302]
features
1 	1 	4 	0 	1 	1 	0 	3 	
0 	3 	4 	0 	4 	1 	0 	4 	
0 	1 	4 	2 	1 	3 	4 	4 	
0 	2 	3 	0 	1 	0 	3 	2 	
3 	0 	1 	4 	2 	2 	4 	3 	
2 	4 	2 	0 	2 	0 	4 	3 	
3 	0 	4 	4 	3 	2 	2 	2 	
4 	3 	2 	4 	0 	0 	2 	5 	
optimal policy
v	<	>	v	<	<	<	<	
v	<	>	v	<	v	<	<	
v	v	>	v	v	v	<	v	
>	v	>	>	>	v	<	<	
>	>	v	v	>	v	<	v	
>	>	>	>	>	v	v	v	
>	>	^	^	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-0.99	-1.22	-1.00	-0.60	-0.83	-1.06	-1.12	-1.84	
-0.76	-1.48	-0.94	-0.54	-0.94	-1.03	-1.09	-1.49	
-0.70	-0.81	-0.88	-0.47	-0.54	-0.80	-1.20	-1.30	
-0.64	-0.58	-1.10	-0.37	-0.31	-0.07	-0.80	-0.90	
-1.20	-0.48	-0.41	-0.48	-0.11	-0.00	-0.41	-1.10	
-0.69	-0.59	-0.18	-0.07	-0.00	0.11	-0.17	-0.38	
-1.37	-0.65	-0.59	-0.48	-0.52	0.18	0.25	0.36	
-1.43	-1.03	-0.30	-0.20	0.22	0.29	0.36	0.47	
map_weights [-0.21459208 -0.40473424 -0.38008789 -0.58337764 -0.55227874 -0.01823065]
MAP reward
-0.40	-0.40	-0.55	-0.21	-0.40	-0.40	-0.21	-0.58	
-0.21	-0.58	-0.55	-0.21	-0.55	-0.40	-0.21	-0.55	
-0.21	-0.40	-0.55	-0.38	-0.40	-0.58	-0.55	-0.55	
-0.21	-0.38	-0.58	-0.21	-0.40	-0.21	-0.58	-0.38	
-0.58	-0.21	-0.40	-0.55	-0.38	-0.38	-0.55	-0.58	
-0.38	-0.55	-0.38	-0.21	-0.38	-0.21	-0.55	-0.58	
-0.58	-0.21	-0.55	-0.55	-0.58	-0.38	-0.38	-0.38	
-0.55	-0.58	-0.38	-0.55	-0.21	-0.21	-0.38	-0.02	
Map policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.3689661860892595
mean w [-0.10905833 -0.26138532 -0.23124276 -0.5294443  -0.53669697  0.16788479]
Mean policy from posterior
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	<	
v	v	>	v	v	v	<	v	
>	v	>	>	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.26	-0.26	-0.54	-0.11	-0.26	-0.26	-0.11	-0.53	
-0.11	-0.53	-0.54	-0.11	-0.54	-0.26	-0.11	-0.54	
-0.11	-0.26	-0.54	-0.23	-0.26	-0.53	-0.54	-0.54	
-0.11	-0.23	-0.53	-0.11	-0.26	-0.11	-0.53	-0.23	
-0.53	-0.11	-0.26	-0.54	-0.23	-0.23	-0.54	-0.53	
-0.23	-0.54	-0.23	-0.11	-0.23	-0.11	-0.54	-0.53	
-0.53	-0.11	-0.54	-0.54	-0.53	-0.23	-0.23	-0.23	
-0.54	-0.53	-0.23	-0.54	-0.11	-0.11	-0.23	0.17	
mean = 0.03685571025683221, map = 0.05334692250177475
CVaR policy
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	v	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	<	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	>	v	<	v	
>	>	>	>	>	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	<	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
v	v	>	v	v	v	<	<	
v	v	>	v	v	v	v	v	
>	v	>	>	>	v	<	v	
>	>	v	v	>	v	<	v	
>	>	>	>	>	v	v	v	
>	v	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05131203588660105, 0.032655352896796996, 0.02210038242965462, 0.0368557102077649, 0.03685571296357548
==========
iteration 36
==========
weights [-0.01995221 -0.56123947 -0.57293475 -0.57445404 -0.15730936  0.04017778]
expeced value MDP LP -1.6125031222031943
demonstration
[(0, 3), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0), (8, 0)]
[ 0.29113904 -0.40926712 -0.61864261  0.32107318 -0.40081808  0.31823996]
w_map [ 0.32516202 -0.46093231 -0.64878033  0.2295465  -0.39316481  0.2315707 ] loglik 0.0
accepted/total = 2800/3000 = 0.9333333333333333
-------
true weights [-0.01995221 -0.56123947 -0.57293475 -0.57445404 -0.15730936  0.04017778]
features
4 	2 	2 	4 	1 	2 	0 	4 	
0 	4 	0 	4 	2 	3 	1 	2 	
2 	2 	4 	3 	3 	4 	3 	4 	
3 	0 	2 	4 	1 	0 	3 	4 	
0 	0 	4 	2 	4 	3 	4 	3 	
0 	3 	3 	1 	3 	0 	4 	2 	
3 	0 	2 	1 	3 	3 	1 	4 	
2 	0 	0 	3 	1 	0 	0 	5 	
optimal policy
v	<	v	v	<	v	^	<	
<	<	<	<	v	v	v	v	
^	v	^	v	>	v	<	v	
v	v	<	>	>	v	v	v	
v	v	<	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.13	-2.68	-2.68	-2.40	-2.94	-2.43	-2.00	-2.13	
-2.00	-2.13	-2.13	-2.27	-2.43	-1.87	-2.42	-2.10	
-2.55	-2.28	-2.27	-2.41	-1.87	-1.31	-1.87	-1.54	
-2.28	-1.72	-2.28	-1.86	-1.72	-1.17	-1.41	-1.40	
-1.72	-1.72	-1.86	-1.86	-1.30	-1.16	-0.84	-1.26	
-1.72	-1.72	-2.26	-1.71	-1.16	-0.59	-0.69	-0.69	
-1.72	-1.16	-1.70	-1.68	-1.13	-0.57	-0.54	-0.12	
-1.71	-1.15	-1.14	-1.13	-0.56	-0.00	0.02	0.04	
map_weights [ 0.32516202 -0.46093231 -0.64878033  0.2295465  -0.39316481  0.2315707 ]
MAP reward
-0.39	-0.65	-0.65	-0.39	-0.46	-0.65	0.33	-0.39	
0.33	-0.39	0.33	-0.39	-0.65	0.23	-0.46	-0.65	
-0.65	-0.65	-0.39	0.23	0.23	-0.39	0.23	-0.39	
0.23	0.33	-0.65	-0.39	-0.46	0.33	0.23	-0.39	
0.33	0.33	-0.39	-0.65	-0.39	0.23	-0.39	0.23	
0.33	0.23	0.23	-0.46	0.23	0.33	-0.39	-0.65	
0.23	0.33	-0.65	-0.46	0.23	0.23	-0.46	-0.39	
-0.65	0.33	0.33	0.23	-0.46	0.33	0.33	0.23	
Map policy
v	<	v	v	>	>	^	<	
<	<	<	<	>	>	^	^	
^	v	^	>	>	v	v	<	
v	v	<	>	>	v	<	<	
v	<	<	<	v	v	<	<	
<	v	<	>	>	v	<	<	
^	v	v	v	>	v	v	<	
>	v	<	<	>	v	v	.	
expeced value MDP LP 54.86244344729724
mean w [ 0.56743474 -0.0040122  -0.12782183 -0.11763161 -0.04424178  0.06366066]
Mean policy from posterior
v	<	v	v	>	>	^	<	
<	<	<	<	<	>	^	<	
^	v	^	<	>	^	^	<	
>	v	<	<	<	v	^	^	
v	<	<	<	<	v	v	<	
<	v	^	v	>	v	v	v	
>	v	<	v	v	v	v	<	
>	v	<	<	>	>	<	.	
Mean rewards
-0.04	-0.13	-0.13	-0.04	-0.00	-0.13	0.57	-0.04	
0.57	-0.04	0.57	-0.04	-0.13	-0.12	-0.00	-0.13	
-0.13	-0.13	-0.04	-0.12	-0.12	-0.04	-0.12	-0.04	
-0.12	0.57	-0.13	-0.04	-0.00	0.57	-0.12	-0.04	
0.57	0.57	-0.04	-0.13	-0.04	-0.12	-0.04	-0.12	
0.57	-0.12	-0.12	-0.00	-0.12	0.57	-0.04	-0.13	
-0.12	0.57	-0.13	-0.00	-0.12	-0.12	-0.00	-0.04	
-0.13	0.57	0.57	-0.12	-0.00	0.57	0.57	0.06	
mean = 0.9814808107567472, map = 1.099722385988984
CVaR policy
v	<	v	v	>	>	^	<	
<	<	<	<	<	^	^	^	
^	v	^	<	<	v	^	^	
>	v	<	<	>	v	<	<	
>	<	<	<	v	v	v	<	
<	^	^	<	>	v	<	<	
^	v	<	v	>	v	v	<	
>	^	<	<	>	>	<	.	
CVaR policy
v	<	v	v	>	>	^	<	
<	<	<	<	<	>	^	^	
^	v	^	<	<	v	^	^	
>	v	<	<	>	v	<	^	
>	<	<	<	v	v	v	<	
<	^	^	<	>	v	<	<	
^	v	<	v	>	v	v	<	
>	^	v	<	>	v	v	.	
CVaR policy
v	<	v	v	>	>	^	<	
<	<	<	<	<	>	^	^	
^	v	^	<	<	^	^	<	
>	v	<	<	<	v	^	^	
v	^	<	<	<	v	v	<	
^	v	^	<	>	v	v	v	
>	v	<	v	v	v	v	<	
>	^	v	<	>	v	v	.	
CVaR policy
v	<	v	v	>	>	^	<	
<	<	<	<	<	>	^	<	
^	v	^	<	>	^	^	<	
v	v	<	<	<	v	^	^	
>	<	<	<	<	v	v	<	
^	v	^	v	>	v	v	v	
>	v	v	v	v	v	v	<	
>	^	v	<	>	v	v	.	
CVaR policy
v	<	v	v	>	>	^	<	
<	<	<	<	<	>	^	<	
^	v	^	<	>	^	^	<	
>	v	<	<	<	v	^	^	
<	<	<	<	<	v	v	<	
<	^	^	v	>	v	v	v	
>	v	<	v	v	v	v	<	
>	v	v	<	>	v	v	.	
cvar = , 0.9764578451272528, 0.9637321722303593, 0.9629679357040539, 0.9814908324093758, 0.9814804131895778
==========
iteration 37
==========
weights [-0.13292463 -0.35914369 -0.00912155 -0.1627433  -0.90904404  0.02042576]
expeced value MDP LP -1.1796173475983402
demonstration
[(0, 3), (8, 3), (16, 1), (17, 3), (25, 3), (33, 3), (41, 1), (42, 3), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.24144917  0.52417333 -0.01261488 -0.78508025  0.20010745 -0.10193872]
w_map [-0.2583668  -0.53039611 -0.1364727  -0.42163487 -0.67229964 -0.05948948] loglik -2.993787937910497e-05
accepted/total = 1216/3000 = 0.4053333333333333
-------
true weights [-0.13292463 -0.35914369 -0.00912155 -0.1627433  -0.90904404  0.02042576]
features
3 	3 	0 	0 	1 	1 	3 	3 	
0 	4 	1 	3 	2 	4 	4 	4 	
1 	3 	4 	3 	1 	1 	0 	2 	
1 	0 	3 	1 	3 	3 	3 	1 	
0 	2 	1 	4 	3 	2 	1 	4 	
4 	0 	1 	0 	4 	4 	3 	1 	
0 	4 	2 	0 	1 	4 	3 	0 	
0 	1 	3 	0 	3 	3 	2 	5 	
optimal policy
v	>	>	v	v	<	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	>	
v	v	<	>	v	v	v	<	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.93	-1.86	-1.71	-1.59	-1.67	-2.02	-1.99	-1.96	
-1.79	-2.22	-1.82	-1.48	-1.33	-2.08	-1.85	-1.81	
-1.67	-1.32	-2.22	-1.48	-1.33	-1.18	-0.95	-0.91	
-1.52	-1.17	-1.32	-1.33	-0.98	-0.83	-0.83	-1.18	
-1.17	-1.05	-1.28	-1.60	-0.83	-0.67	-0.67	-1.38	
-1.95	-1.05	-0.93	-0.70	-1.57	-1.22	-0.31	-0.47	
-1.20	-1.48	-0.57	-0.57	-0.67	-1.06	-0.15	-0.11	
-1.08	-0.95	-0.60	-0.44	-0.31	-0.15	0.01	0.02	
map_weights [-0.2583668  -0.53039611 -0.1364727  -0.42163487 -0.67229964 -0.05948948]
MAP reward
-0.42	-0.42	-0.26	-0.26	-0.53	-0.53	-0.42	-0.42	
-0.26	-0.67	-0.53	-0.42	-0.14	-0.67	-0.67	-0.67	
-0.53	-0.42	-0.67	-0.42	-0.53	-0.53	-0.26	-0.14	
-0.53	-0.26	-0.42	-0.53	-0.42	-0.42	-0.42	-0.53	
-0.26	-0.14	-0.53	-0.67	-0.42	-0.14	-0.53	-0.67	
-0.67	-0.26	-0.53	-0.26	-0.67	-0.67	-0.42	-0.53	
-0.26	-0.67	-0.14	-0.26	-0.53	-0.67	-0.42	-0.26	
-0.26	-0.53	-0.42	-0.26	-0.42	-0.42	-0.14	-0.06	
Map policy
v	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9009433225891925
mean w [-0.18504532 -0.42886546 -0.09931865 -0.3116616  -0.63515821 -0.0883452 ]
Mean policy from posterior
v	v	>	v	v	>	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	<	
v	v	v	>	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.31	-0.31	-0.19	-0.19	-0.43	-0.43	-0.31	-0.31	
-0.19	-0.64	-0.43	-0.31	-0.10	-0.64	-0.64	-0.64	
-0.43	-0.31	-0.64	-0.31	-0.43	-0.43	-0.19	-0.10	
-0.43	-0.19	-0.31	-0.43	-0.31	-0.31	-0.31	-0.43	
-0.19	-0.10	-0.43	-0.64	-0.31	-0.10	-0.43	-0.64	
-0.64	-0.19	-0.43	-0.19	-0.64	-0.64	-0.31	-0.43	
-0.19	-0.64	-0.10	-0.19	-0.43	-0.64	-0.31	-0.19	
-0.19	-0.43	-0.31	-0.19	-0.31	-0.31	-0.10	-0.09	
mean = 0.032481023904563644, map = 0.08820346227897313
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	<	
v	v	v	>	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	<	
v	v	v	>	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.0882036462876774, 0.08820346245972721, 0.06789116886509139, 0.03248103136378866, 0.03248102623778526
==========
iteration 38
==========
weights [-0.24018941 -0.09776579 -0.34570597 -0.20586074 -0.69557571  0.5357556 ]
expeced value MDP LP -0.8340895143841156
demonstration
[(0, 3), (8, 3), (16, 1), (17, 1), (18, 3), (26, 3), (34, 3), (42, 1), (43, 1), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.69144179  0.46418029  0.01611492  0.25527832  0.39265872 -0.29468175]
w_map [-0.401877   -0.20013381 -0.57735249 -0.28289146 -0.60865247  0.12091333] loglik -2.3395059400854734e-07
accepted/total = 1706/3000 = 0.5686666666666667
-------
true weights [-0.24018941 -0.09776579 -0.34570597 -0.20586074 -0.69557571  0.5357556 ]
features
1 	2 	4 	3 	1 	1 	3 	3 	
2 	2 	4 	0 	1 	2 	1 	4 	
0 	3 	1 	4 	0 	4 	3 	0 	
0 	0 	1 	2 	2 	4 	4 	2 	
3 	1 	0 	4 	0 	4 	4 	2 	
4 	2 	0 	1 	1 	1 	4 	3 	
4 	2 	0 	0 	1 	0 	0 	0 	
0 	4 	4 	3 	1 	1 	3 	5 	
optimal policy
v	v	>	>	v	<	v	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	<	>	v	
v	v	v	>	v	<	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.67	-1.69	-1.94	-1.26	-1.07	-1.15	-1.32	-1.51	
-1.59	-1.36	-1.51	-1.21	-0.98	-1.31	-1.13	-1.53	
-1.25	-1.02	-0.83	-1.51	-0.89	-1.58	-1.04	-0.84	
-1.17	-0.97	-0.74	-0.99	-0.66	-1.34	-1.30	-0.61	
-0.93	-0.74	-0.65	-0.86	-0.31	-0.81	-0.96	-0.27	
-1.44	-0.75	-0.41	-0.17	-0.07	-0.12	-0.61	0.08	
-1.48	-0.80	-0.45	-0.22	0.02	-0.02	0.08	0.29	
-1.69	-1.47	-0.78	-0.08	0.12	0.22	0.32	0.54	
map_weights [-0.401877   -0.20013381 -0.57735249 -0.28289146 -0.60865247  0.12091333]
MAP reward
-0.20	-0.58	-0.61	-0.28	-0.20	-0.20	-0.28	-0.28	
-0.58	-0.58	-0.61	-0.40	-0.20	-0.58	-0.20	-0.61	
-0.40	-0.28	-0.20	-0.61	-0.40	-0.61	-0.28	-0.40	
-0.40	-0.40	-0.20	-0.58	-0.58	-0.61	-0.61	-0.58	
-0.28	-0.20	-0.40	-0.61	-0.40	-0.61	-0.61	-0.58	
-0.61	-0.58	-0.40	-0.20	-0.20	-0.20	-0.61	-0.28	
-0.61	-0.58	-0.40	-0.40	-0.20	-0.40	-0.40	-0.40	
-0.40	-0.61	-0.61	-0.28	-0.20	-0.20	-0.28	0.12	
Map policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5764095774091225
mean w [-0.31838211 -0.13305857 -0.57526081 -0.20175756 -0.5368836   0.07211905]
Mean policy from posterior
v	v	v	>	v	<	<	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.13	-0.58	-0.54	-0.20	-0.13	-0.13	-0.20	-0.20	
-0.58	-0.58	-0.54	-0.32	-0.13	-0.58	-0.13	-0.54	
-0.32	-0.20	-0.13	-0.54	-0.32	-0.54	-0.20	-0.32	
-0.32	-0.32	-0.13	-0.58	-0.58	-0.54	-0.54	-0.58	
-0.20	-0.13	-0.32	-0.54	-0.32	-0.54	-0.54	-0.58	
-0.54	-0.58	-0.32	-0.13	-0.13	-0.13	-0.54	-0.20	
-0.54	-0.58	-0.32	-0.32	-0.13	-0.32	-0.32	-0.32	
-0.32	-0.54	-0.54	-0.20	-0.13	-0.13	-0.20	0.07	
mean = 0.03595473799268267, map = 0.02218385397051481
CVaR policy
v	v	v	>	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	v	>	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	v	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	v	<	
v	v	v	>	v	<	v	v	
>	>	v	<	v	v	>	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	v	v	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.09950457729434503, 0.04558943364521839, 0.03836587724334073, 0.03515215552916262, 0.03515215604537136
==========
iteration 39
==========
weights [-0.04764988 -0.63173347 -0.26004631 -0.41853618 -0.52868348  0.27629598]
expeced value MDP LP -2.0470749509994812
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 3), (34, 3), (42, 1), (43, 1), (44, 3), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.49462609  0.28143632  0.32634016 -0.40415946  0.50714624 -0.38613282]
w_map [-0.19391529 -0.69179407 -0.25461746 -0.37644156 -0.52192657  0.06980106] loglik -0.6931479514185099
accepted/total = 1314/3000 = 0.438
-------
true weights [-0.04764988 -0.63173347 -0.26004631 -0.41853618 -0.52868348  0.27629598]
features
2 	0 	3 	1 	1 	0 	1 	1 	
4 	4 	2 	3 	2 	0 	0 	0 	
0 	3 	2 	1 	4 	0 	3 	2 	
2 	3 	0 	4 	4 	1 	4 	1 	
2 	2 	0 	4 	0 	3 	2 	1 	
3 	1 	2 	0 	2 	1 	2 	3 	
0 	0 	4 	4 	1 	0 	1 	1 	
2 	2 	3 	1 	4 	4 	3 	5 	
optimal policy
>	>	v	v	>	v	v	v	
v	>	v	<	>	v	v	<	
v	>	v	<	v	>	v	v	
>	>	v	<	v	v	v	v	
>	>	v	>	v	>	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
optimal values
-3.04	-2.81	-2.79	-3.40	-2.88	-2.27	-2.83	-2.85	
-3.08	-2.90	-2.40	-2.79	-2.48	-2.24	-2.22	-2.24	
-2.58	-2.56	-2.16	-2.77	-2.64	-2.22	-2.19	-2.25	
-2.55	-2.32	-1.92	-2.43	-2.13	-2.30	-1.79	-2.01	
-2.37	-2.13	-1.89	-2.13	-1.62	-1.68	-1.28	-1.40	
-2.76	-2.47	-1.86	-1.62	-1.58	-1.34	-1.03	-0.77	
-2.41	-2.39	-2.36	-1.85	-1.34	-0.71	-0.78	-0.36	
-2.65	-2.45	-2.21	-1.81	-1.19	-0.67	-0.15	0.28	
map_weights [-0.19391529 -0.69179407 -0.25461746 -0.37644156 -0.52192657  0.06980106]
MAP reward
-0.25	-0.19	-0.38	-0.69	-0.69	-0.19	-0.69	-0.69	
-0.52	-0.52	-0.25	-0.38	-0.25	-0.19	-0.19	-0.19	
-0.19	-0.38	-0.25	-0.69	-0.52	-0.19	-0.38	-0.25	
-0.25	-0.38	-0.19	-0.52	-0.52	-0.69	-0.52	-0.69	
-0.25	-0.25	-0.19	-0.52	-0.19	-0.38	-0.25	-0.69	
-0.38	-0.69	-0.25	-0.19	-0.25	-0.69	-0.25	-0.38	
-0.19	-0.19	-0.52	-0.52	-0.69	-0.19	-0.69	-0.69	
-0.25	-0.25	-0.38	-0.69	-0.52	-0.52	-0.38	0.07	
Map policy
>	>	v	v	>	v	v	v	
v	>	v	>	>	v	v	<	
v	>	v	<	>	>	v	<	
v	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.3662247510320764
mean w [-0.11712115 -0.6082196  -0.21180975 -0.30800276 -0.40110895 -0.34972795]
Mean policy from posterior
>	>	v	v	>	v	v	v	
v	>	v	>	>	v	v	<	
v	>	v	<	>	>	v	<	
v	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.21	-0.12	-0.31	-0.61	-0.61	-0.12	-0.61	-0.61	
-0.40	-0.40	-0.21	-0.31	-0.21	-0.12	-0.12	-0.12	
-0.12	-0.31	-0.21	-0.61	-0.40	-0.12	-0.31	-0.21	
-0.21	-0.31	-0.12	-0.40	-0.40	-0.61	-0.40	-0.61	
-0.21	-0.21	-0.12	-0.40	-0.12	-0.31	-0.21	-0.61	
-0.31	-0.61	-0.21	-0.12	-0.21	-0.61	-0.21	-0.31	
-0.12	-0.12	-0.40	-0.40	-0.61	-0.12	-0.61	-0.61	
-0.21	-0.21	-0.31	-0.61	-0.40	-0.40	-0.31	-0.35	
mean = 0.02361420081272847, map = 0.023614199791783363
CVaR policy
>	>	v	v	>	v	v	v	
v	>	v	>	>	>	v	v	
v	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
v	>	v	>	>	>	v	v	
v	>	v	<	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
v	>	v	>	>	v	v	<	
v	>	v	<	>	>	v	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
v	>	v	>	>	v	v	<	
v	>	v	<	>	>	v	<	
>	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	>	v	v	v	
v	>	v	>	>	v	v	<	
v	>	v	<	>	>	v	<	
v	>	v	>	v	v	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	>	v	>	v	
>	v	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.02838976744892685, 0.019645932455283788, 0.018498731801629198, 0.021235113894293, 0.023614199804192104
==========
iteration 40
==========
weights [-0.16999147 -0.30044224 -0.18595266 -0.18621804 -0.86748449  0.24300713]
expeced value MDP LP -1.2911925434252285
demonstration
[(0, 3), (8, 3), (16, 1), (17, 3), (25, 3), (33, 3), (41, 3), (49, 1), (50, 1), (51, 3), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.03493198 -0.00570941  0.56667469  0.54589302  0.60212411 -0.13066871]
w_map [-0.16784656 -0.5719502  -0.31358102 -0.50958801 -0.47950598 -0.23824677] loglik -9.206680715578841e-07
accepted/total = 1197/3000 = 0.399
-------
true weights [-0.16999147 -0.30044224 -0.18595266 -0.18621804 -0.86748449  0.24300713]
features
2 	4 	1 	0 	3 	4 	0 	0 	
2 	4 	1 	2 	3 	4 	3 	1 	
2 	0 	3 	3 	1 	2 	4 	2 	
3 	0 	2 	1 	2 	3 	3 	0 	
3 	3 	2 	1 	4 	3 	0 	2 	
2 	0 	1 	4 	1 	3 	1 	4 	
2 	2 	0 	2 	3 	2 	1 	0 	
0 	3 	3 	0 	3 	0 	1 	5 	
optimal policy
v	>	>	v	v	<	>	v	
v	v	v	v	v	v	>	v	
>	v	v	>	>	v	v	v	
>	v	v	>	>	v	v	v	
v	v	v	<	>	v	v	<	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.24	-3.04	-2.20	-1.91	-1.76	-2.61	-1.81	-1.65	
-2.08	-2.59	-2.01	-1.76	-1.59	-1.99	-1.67	-1.50	
-1.91	-1.74	-1.73	-1.59	-1.42	-1.13	-1.73	-1.21	
-1.76	-1.59	-1.56	-1.42	-1.13	-0.95	-0.87	-1.03	
-1.60	-1.43	-1.39	-1.67	-1.64	-0.78	-0.69	-0.87	
-1.43	-1.26	-1.21	-1.62	-0.89	-0.60	-0.53	-0.80	
-1.27	-1.10	-0.92	-0.76	-0.60	-0.41	-0.23	0.07	
-1.10	-0.94	-0.76	-0.58	-0.41	-0.23	-0.06	0.24	
map_weights [-0.16784656 -0.5719502  -0.31358102 -0.50958801 -0.47950598 -0.23824677]
MAP reward
-0.31	-0.48	-0.57	-0.17	-0.51	-0.48	-0.17	-0.17	
-0.31	-0.48	-0.57	-0.31	-0.51	-0.48	-0.51	-0.57	
-0.31	-0.17	-0.51	-0.51	-0.57	-0.31	-0.48	-0.31	
-0.51	-0.17	-0.31	-0.57	-0.31	-0.51	-0.51	-0.17	
-0.51	-0.51	-0.31	-0.57	-0.48	-0.51	-0.17	-0.31	
-0.31	-0.17	-0.57	-0.48	-0.57	-0.51	-0.57	-0.48	
-0.31	-0.31	-0.17	-0.31	-0.51	-0.31	-0.57	-0.17	
-0.17	-0.51	-0.51	-0.17	-0.51	-0.17	-0.57	-0.24	
Map policy
v	v	>	>	>	>	>	v	
v	v	>	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.679444157697514
mean w [-0.07973789 -0.35966634 -0.13990015 -0.26725036 -0.34175325 -0.47355951]
Mean policy from posterior
v	v	>	>	>	>	>	v	
v	v	v	>	>	v	v	v	
>	v	<	>	>	>	>	v	
>	v	v	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.14	-0.34	-0.36	-0.08	-0.27	-0.34	-0.08	-0.08	
-0.14	-0.34	-0.36	-0.14	-0.27	-0.34	-0.27	-0.36	
-0.14	-0.08	-0.27	-0.27	-0.36	-0.14	-0.34	-0.14	
-0.27	-0.08	-0.14	-0.36	-0.14	-0.27	-0.27	-0.08	
-0.27	-0.27	-0.14	-0.36	-0.34	-0.27	-0.08	-0.14	
-0.14	-0.08	-0.36	-0.34	-0.36	-0.27	-0.36	-0.34	
-0.14	-0.14	-0.08	-0.14	-0.27	-0.14	-0.36	-0.08	
-0.08	-0.27	-0.27	-0.08	-0.27	-0.08	-0.36	-0.47	
mean = 0.3030012904882038, map = 0.34026991771603887
CVaR policy
v	v	>	>	>	>	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	v	>	>	v	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
v	v	v	>	>	v	v	v	
>	v	<	>	>	>	>	v	
>	v	v	>	>	>	>	v	
v	v	v	>	>	>	>	v	
>	v	v	v	v	v	>	v	
>	>	>	v	>	v	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.2922116271442099, 0.29221162700139236, 0.29221162702378245, 0.29739700797704094, 0.3030012903594468
==========
iteration 41
==========
weights [-0.31750265 -0.52318306 -0.21493703 -0.01627862 -0.06694747  0.75797537]
expeced value MDP LP -0.4478254943158449
demonstration
[(0, 3), (8, 1), (9, 3), (17, 3), (25, 3), (33, 3), (41, 1), (42, 3), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.0995095  -0.20578131  0.41980486  0.82383584  0.08318927  0.29306969]
w_map [-0.36451548 -0.75784451 -0.39367222 -0.14006976 -0.26720716 -0.21634034] loglik -4.138266045572436e-05
accepted/total = 1061/3000 = 0.3536666666666667
-------
true weights [-0.31750265 -0.52318306 -0.21493703 -0.01627862 -0.06694747  0.75797537]
features
4 	4 	3 	1 	3 	3 	0 	3 	
3 	4 	1 	4 	1 	4 	1 	4 	
1 	0 	1 	4 	0 	0 	2 	3 	
2 	4 	3 	0 	1 	3 	1 	3 	
3 	3 	1 	0 	2 	0 	0 	3 	
4 	4 	4 	1 	3 	0 	4 	2 	
1 	1 	2 	3 	2 	2 	2 	1 	
3 	4 	3 	1 	4 	2 	0 	5 	
optimal policy
v	v	<	>	>	>	>	v	
>	v	<	v	^	^	>	v	
v	v	v	v	>	>	>	v	
v	v	<	<	v	v	>	v	
>	v	v	>	v	>	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
optimal values
-0.94	-0.93	-0.94	-0.98	-0.46	-0.45	-0.44	-0.12	
-0.88	-0.87	-1.39	-0.94	-0.98	-0.51	-0.63	-0.11	
-1.18	-0.82	-1.03	-0.89	-0.88	-0.57	-0.25	-0.04	
-0.66	-0.50	-0.51	-0.83	-0.82	-0.50	-0.55	-0.02	
-0.45	-0.44	-0.88	-0.62	-0.30	-0.49	-0.17	-0.01	
-0.49	-0.43	-0.37	-0.61	-0.09	-0.17	0.14	0.01	
-0.91	-0.82	-0.30	-0.09	-0.07	-0.00	0.21	0.23	
-0.39	-0.38	-0.31	-0.38	0.14	0.21	0.43	0.76	
map_weights [-0.36451548 -0.75784451 -0.39367222 -0.14006976 -0.26720716 -0.21634034]
MAP reward
-0.27	-0.27	-0.14	-0.76	-0.14	-0.14	-0.36	-0.14	
-0.14	-0.27	-0.76	-0.27	-0.76	-0.27	-0.76	-0.27	
-0.76	-0.36	-0.76	-0.27	-0.36	-0.36	-0.39	-0.14	
-0.39	-0.27	-0.14	-0.36	-0.76	-0.14	-0.76	-0.14	
-0.14	-0.14	-0.76	-0.36	-0.39	-0.36	-0.36	-0.14	
-0.27	-0.27	-0.27	-0.76	-0.14	-0.36	-0.27	-0.39	
-0.76	-0.76	-0.39	-0.14	-0.39	-0.39	-0.39	-0.76	
-0.14	-0.27	-0.14	-0.76	-0.27	-0.39	-0.36	-0.22	
Map policy
v	>	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	>	>	v	>	v	
v	v	>	v	>	v	v	v	
>	v	v	>	v	>	v	v	
>	>	v	>	>	>	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6981631086448161
mean w [-0.3683816  -0.71102514 -0.27090385 -0.11241726 -0.15927403  0.04341398]
Mean policy from posterior
v	v	<	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	>	>	v	
v	v	<	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.16	-0.16	-0.11	-0.71	-0.11	-0.11	-0.37	-0.11	
-0.11	-0.16	-0.71	-0.16	-0.71	-0.16	-0.71	-0.16	
-0.71	-0.37	-0.71	-0.16	-0.37	-0.37	-0.27	-0.11	
-0.27	-0.16	-0.11	-0.37	-0.71	-0.11	-0.71	-0.11	
-0.11	-0.11	-0.71	-0.37	-0.27	-0.37	-0.37	-0.11	
-0.16	-0.16	-0.16	-0.71	-0.11	-0.37	-0.16	-0.27	
-0.71	-0.71	-0.27	-0.11	-0.27	-0.27	-0.27	-0.71	
-0.11	-0.16	-0.11	-0.71	-0.16	-0.27	-0.37	0.04	
mean = 0.014066266358595259, map = 0.07251939113188349
CVaR policy
v	v	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	>	>	>	>	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	>	>	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	<	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	>	>	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	<	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	>	>	v	
v	v	<	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	v	<	>	>	>	>	v	
>	v	>	v	>	v	>	v	
v	v	v	v	>	>	>	v	
v	v	<	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	v	>	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.02680315840868841, 0.017226788218317857, 0.016510179302379635, 0.014066266540479433, 0.014066270076932452
==========
iteration 42
==========
weights [-0.48168162 -0.00285144 -0.40146165 -0.62732878 -0.19438844  0.41889732]
expeced value MDP LP -0.6802611196993763
demonstration
[(0, 3), (8, 3), (16, 0), (16, 0), (16, 0), (16, 1), (17, 0), (16, 0), (16, 1), (17, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 1), (17, 0), (16, 0)]
[-0.54892072  0.29150573  0.49498732  0.22343147  0.34326837  0.44826689]
w_map [-0.09328247  0.73544046 -0.31622257 -0.0365367  -0.57420231  0.13923281] loglik -10.397203296988664
accepted/total = 2781/3000 = 0.927
-------
true weights [-0.48168162 -0.00285144 -0.40146165 -0.62732878 -0.19438844  0.41889732]
features
4 	2 	3 	4 	2 	4 	2 	0 	
3 	2 	2 	2 	3 	3 	1 	3 	
1 	1 	3 	1 	3 	2 	1 	2 	
2 	2 	3 	1 	3 	1 	4 	0 	
3 	2 	3 	1 	1 	1 	3 	3 	
2 	1 	1 	4 	3 	0 	4 	1 	
2 	4 	4 	4 	4 	0 	4 	3 	
2 	4 	3 	4 	2 	2 	0 	5 	
optimal policy
v	v	>	v	>	>	v	<	
v	v	>	v	<	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	v	v	v	<	<	
v	v	>	>	>	<	<	v	
>	>	<	^	^	^	>	v	
>	^	^	^	<	>	v	v	
>	^	^	^	>	>	>	.	
optimal values
-1.09	-1.08	-1.49	-0.87	-1.26	-0.87	-0.68	-1.16	
-0.91	-0.68	-1.08	-0.68	-1.30	-0.91	-0.29	-0.91	
-0.29	-0.29	-0.91	-0.29	-0.91	-0.68	-0.29	-0.68	
-0.68	-0.68	-0.91	-0.29	-0.91	-0.29	-0.48	-0.95	
-1.30	-0.68	-0.91	-0.29	-0.29	-0.29	-0.91	-0.84	
-0.68	-0.29	-0.29	-0.48	-0.91	-0.76	-0.41	-0.21	
-0.87	-0.48	-0.48	-0.67	-0.85	-0.74	-0.26	-0.21	
-1.06	-0.67	-1.10	-0.85	-0.86	-0.47	-0.07	0.42	
map_weights [-0.09328247  0.73544046 -0.31622257 -0.0365367  -0.57420231  0.13923281]
MAP reward
-0.57	-0.32	-0.04	-0.57	-0.32	-0.57	-0.32	-0.09	
-0.04	-0.32	-0.32	-0.32	-0.04	-0.04	0.74	-0.04	
0.74	0.74	-0.04	0.74	-0.04	-0.32	0.74	-0.32	
-0.32	-0.32	-0.04	0.74	-0.04	0.74	-0.57	-0.09	
-0.04	-0.32	-0.04	0.74	0.74	0.74	-0.04	-0.04	
-0.32	0.74	0.74	-0.57	-0.04	-0.09	-0.57	0.74	
-0.32	-0.57	-0.57	-0.57	-0.57	-0.09	-0.57	-0.04	
-0.32	-0.57	-0.04	-0.57	-0.32	-0.32	-0.09	0.14	
Map policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	v	v	v	<	v	
^	v	>	>	>	<	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
expeced value MDP LP 55.1084948547193
mean w [-0.03936071  0.56909995 -0.21198671 -0.10794021 -0.24118505 -0.11166132]
Mean policy from posterior
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
<	<	>	v	<	v	^	<	
^	^	>	v	<	v	<	v	
^	v	>	>	<	<	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
Mean rewards
-0.24	-0.21	-0.11	-0.24	-0.21	-0.24	-0.21	-0.04	
-0.11	-0.21	-0.21	-0.21	-0.11	-0.11	0.57	-0.11	
0.57	0.57	-0.11	0.57	-0.11	-0.21	0.57	-0.21	
-0.21	-0.21	-0.11	0.57	-0.11	0.57	-0.24	-0.04	
-0.11	-0.21	-0.11	0.57	0.57	0.57	-0.11	-0.11	
-0.21	0.57	0.57	-0.24	-0.11	-0.04	-0.24	0.57	
-0.21	-0.24	-0.24	-0.24	-0.24	-0.04	-0.24	-0.11	
-0.21	-0.24	-0.11	-0.24	-0.21	-0.21	-0.04	-0.11	
mean = 0.1438151952525505, map = 0.14381714023433367
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
<	<	<	v	<	v	^	<	
^	^	>	v	<	v	<	v	
^	v	>	>	<	^	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	v	<	v	<	v	
^	v	>	^	<	<	<	v	
>	>	<	<	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	^	<	v	<	v	
^	v	>	>	<	^	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	^	<	v	<	v	
^	v	>	>	<	^	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
CVaR policy
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	<	
>	<	>	v	<	v	^	<	
^	^	>	^	<	v	<	v	
^	v	>	>	<	^	<	v	
>	>	<	^	^	^	>	>	
^	^	^	^	^	^	>	^	
^	^	^	<	^	^	^	.	
cvar = , 0.14381516457169385, 0.14381676240083385, 0.1439106516987011, 0.14381918072883837, 0.14381522392909984
==========
iteration 43
==========
weights [-0.43255586 -0.48379263 -0.00750224 -0.64402311 -0.34845398  0.20639255]
expeced value MDP LP -1.2029239591397993
demonstration
[(0, 3), (8, 0), (8, 0), (8, 3), (16, 0), (16, 0), (16, 2), (8, 0), (8, 0), (8, 3), (16, 0), (16, 2), (8, 3), (16, 2), (8, 0), (8, 3), (16, 0), (16, 3), (24, 0), (24, 0)]
[ 0.48832382  0.29932274  0.67957021  0.24080644  0.00388654 -0.39003489]
w_map [-0.36526248 -0.63808344  0.41074655 -0.4001507   0.25849434  0.25254746] loglik -17.106578869436817
accepted/total = 2759/3000 = 0.9196666666666666
-------
true weights [-0.43255586 -0.48379263 -0.00750224 -0.64402311 -0.34845398  0.20639255]
features
4 	2 	3 	3 	1 	3 	2 	0 	
2 	3 	2 	0 	2 	3 	0 	4 	
2 	0 	2 	1 	2 	4 	4 	3 	
2 	1 	0 	2 	0 	0 	3 	2 	
0 	1 	3 	3 	3 	3 	1 	4 	
2 	0 	1 	3 	1 	0 	2 	3 	
1 	4 	1 	4 	1 	3 	3 	0 	
1 	4 	0 	3 	4 	4 	4 	5 	
optimal policy
>	^	<	v	v	>	^	<	
v	^	v	>	v	<	^	^	
^	>	^	>	^	<	<	v	
^	<	^	>	^	^	>	>	
v	<	^	^	^	v	v	^	
<	<	<	v	>	>	v	v	
^	^	<	>	v	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.09	-0.75	-1.39	-1.81	-1.23	-1.39	-0.75	-1.18	
-0.75	-1.39	-0.75	-1.18	-0.75	-1.39	-1.18	-1.51	
-0.75	-1.18	-0.75	-1.23	-0.75	-1.09	-1.43	-1.39	
-0.75	-1.23	-1.18	-1.17	-1.18	-1.51	-1.39	-0.75	
-1.18	-1.65	-1.81	-1.80	-1.81	-1.84	-1.26	-1.09	
-0.75	-1.18	-1.65	-2.27	-1.68	-1.21	-0.79	-0.87	
-1.23	-1.51	-1.98	-1.65	-1.31	-1.13	-0.79	-0.23	
-1.70	-1.85	-1.89	-1.47	-0.83	-0.49	-0.14	0.21	
map_weights [-0.36526248 -0.63808344  0.41074655 -0.4001507   0.25849434  0.25254746]
MAP reward
0.26	0.41	-0.40	-0.40	-0.64	-0.40	0.41	-0.37	
0.41	-0.40	0.41	-0.37	0.41	-0.40	-0.37	0.26	
0.41	-0.37	0.41	-0.64	0.41	0.26	0.26	-0.40	
0.41	-0.64	-0.37	0.41	-0.37	-0.37	-0.40	0.41	
-0.37	-0.64	-0.40	-0.40	-0.40	-0.40	-0.64	0.26	
0.41	-0.37	-0.64	-0.40	-0.64	-0.37	0.41	-0.40	
-0.64	0.26	-0.64	0.26	-0.64	-0.40	-0.40	-0.37	
-0.64	0.26	-0.37	-0.40	0.26	0.26	0.26	0.25	
Map policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	<	
<	<	^	<	^	<	<	v	
^	<	^	<	^	^	>	>	
^	<	^	^	^	^	>	^	
<	<	<	^	^	>	>	^	
^	^	<	<	<	^	^	^	
^	^	<	<	>	>	^	.	
expeced value MDP LP 51.307656178434115
mean w [ 0.06987303 -0.13078068  0.53152166 -0.19150179 -0.10776065 -0.06096636]
Mean policy from posterior
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
^	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	<	^	>	^	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
Mean rewards
-0.11	0.53	-0.19	-0.19	-0.13	-0.19	0.53	0.07	
0.53	-0.19	0.53	0.07	0.53	-0.19	0.07	-0.11	
0.53	0.07	0.53	-0.13	0.53	-0.11	-0.11	-0.19	
0.53	-0.13	0.07	0.53	0.07	0.07	-0.19	0.53	
0.07	-0.13	-0.19	-0.19	-0.19	-0.19	-0.13	-0.11	
0.53	0.07	-0.13	-0.19	-0.13	0.07	0.53	-0.19	
-0.13	-0.11	-0.13	-0.11	-0.13	-0.19	-0.19	0.07	
-0.13	-0.11	0.07	-0.19	-0.11	-0.11	-0.11	-0.06	
mean = 0.3045241463407866, map = 0.3208505898471541
CVaR policy
v	^	<	v	v	>	^	<	
<	<	v	<	v	<	^	^	
<	<	^	<	^	<	^	v	
<	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	<	^	>	^	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
^	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	<	^	>	^	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	>	v	<	^	^	
^	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	<	^	>	^	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
<	<	v	<	v	<	^	^	
<	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	<	^	>	^	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
CVaR policy
v	^	<	v	v	>	^	<	
v	<	v	<	v	<	^	^	
v	<	^	<	^	<	^	v	
^	<	^	<	^	<	>	>	
^	<	^	^	^	^	>	^	
<	<	<	<	^	>	^	^	
^	^	<	<	^	^	^	^	
^	^	<	<	^	^	^	.	
cvar = , 0.3045314547649365, 0.3045262390775425, 0.3045264610297682, 0.30452411711553307, 0.30452665900874054
==========
iteration 44
==========
weights [-0.20169497 -0.5384595  -0.10666749 -0.08781692 -0.71497981  0.3729539 ]
expeced value MDP LP -1.6882638322738717
demonstration
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (13, 3), (21, 3), (29, 3), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.40342001 -0.2404068  -0.17172337 -0.12894732 -0.82425753  0.23225001]
w_map [-0.24295637 -0.54739296 -0.36429449 -0.19127461 -0.68663834  0.02375894] loglik -3.2664457449982365e-07
accepted/total = 1636/3000 = 0.5453333333333333
-------
true weights [-0.20169497 -0.5384595  -0.10666749 -0.08781692 -0.71497981  0.3729539 ]
features
0 	2 	3 	0 	0 	3 	0 	0 	
4 	4 	1 	0 	2 	3 	0 	1 	
1 	3 	3 	0 	4 	2 	4 	4 	
4 	3 	2 	0 	1 	4 	1 	1 	
2 	0 	4 	1 	4 	0 	3 	0 	
0 	3 	1 	2 	1 	0 	4 	1 	
0 	4 	1 	1 	0 	2 	4 	0 	
3 	0 	4 	1 	0 	4 	3 	5 	
optimal policy
>	>	>	>	>	v	<	<	
^	v	>	>	>	v	<	v	
>	v	v	^	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	>	>	>	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.48	-2.30	-2.22	-2.15	-1.97	-1.79	-1.97	-2.15	
-3.17	-2.96	-2.51	-1.99	-1.81	-1.72	-1.90	-2.33	
-2.78	-2.27	-2.22	-2.17	-2.34	-1.65	-1.89	-1.81	
-2.90	-2.20	-2.15	-2.07	-2.08	-1.55	-1.18	-1.10	
-2.22	-2.14	-2.58	-1.88	-1.55	-0.85	-0.65	-0.57	
-2.14	-1.95	-1.88	-1.36	-1.27	-0.73	-1.08	-0.37	
-2.30	-2.49	-1.79	-1.27	-0.73	-0.54	-0.44	0.17	
-2.12	-2.05	-1.87	-1.17	-0.63	-0.44	0.28	0.37	
map_weights [-0.24295637 -0.54739296 -0.36429449 -0.19127461 -0.68663834  0.02375894]
MAP reward
-0.24	-0.36	-0.19	-0.24	-0.24	-0.19	-0.24	-0.24	
-0.69	-0.69	-0.55	-0.24	-0.36	-0.19	-0.24	-0.55	
-0.55	-0.19	-0.19	-0.24	-0.69	-0.36	-0.69	-0.69	
-0.69	-0.19	-0.36	-0.24	-0.55	-0.69	-0.55	-0.55	
-0.36	-0.24	-0.69	-0.55	-0.69	-0.24	-0.19	-0.24	
-0.24	-0.19	-0.55	-0.36	-0.55	-0.24	-0.69	-0.55	
-0.24	-0.69	-0.55	-0.55	-0.24	-0.36	-0.69	-0.24	
-0.19	-0.24	-0.69	-0.55	-0.24	-0.69	-0.19	0.02	
Map policy
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1076624904966934
mean w [-0.2196936  -0.52186412 -0.32903606 -0.1353454  -0.63169604  0.11687746]
Mean policy from posterior
>	>	>	>	>	v	v	v	
^	v	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.22	-0.33	-0.14	-0.22	-0.22	-0.14	-0.22	-0.22	
-0.63	-0.63	-0.52	-0.22	-0.33	-0.14	-0.22	-0.52	
-0.52	-0.14	-0.14	-0.22	-0.63	-0.33	-0.63	-0.63	
-0.63	-0.14	-0.33	-0.22	-0.52	-0.63	-0.52	-0.52	
-0.33	-0.22	-0.63	-0.52	-0.63	-0.22	-0.14	-0.22	
-0.22	-0.14	-0.52	-0.33	-0.52	-0.22	-0.63	-0.52	
-0.22	-0.63	-0.52	-0.52	-0.22	-0.33	-0.63	-0.22	
-0.14	-0.22	-0.63	-0.52	-0.22	-0.63	-0.14	0.12	
mean = 0.07304413913353813, map = 0.07435801230872663
CVaR policy
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	v	v	v	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
^	v	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	v	v	
^	v	>	>	>	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	v	v	v	v	
v	v	>	>	>	>	>	v	
v	>	>	>	v	v	>	v	
v	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.06250333664468388, 0.06151672382191142, 0.061516723761092074, 0.06517449870265568, 0.07304413897446738
==========
iteration 45
==========
weights [-0.13132706 -0.72845255 -0.31441088 -0.54012427 -0.2334518   0.08379686]
expeced value MDP LP -2.3290516589831314
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 3), (26, 3), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.44086099 -0.02345354  0.05154128 -0.60817667  0.07237119  0.65369609]
w_map [-0.05998588 -0.73920963 -0.33831982 -0.51897831 -0.16513756  0.19723494] loglik -1.3862945523944563
accepted/total = 1551/3000 = 0.517
-------
true weights [-0.13132706 -0.72845255 -0.31441088 -0.54012427 -0.2334518   0.08379686]
features
3 	3 	1 	4 	1 	2 	2 	1 	
1 	2 	3 	4 	1 	4 	0 	4 	
3 	0 	2 	4 	3 	1 	1 	2 	
2 	2 	4 	1 	2 	1 	3 	1 	
2 	2 	3 	0 	2 	3 	1 	2 	
2 	4 	1 	3 	3 	3 	4 	2 	
1 	0 	3 	3 	3 	2 	4 	4 	
0 	2 	1 	1 	0 	1 	2 	5 	
optimal policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	v	v	>	v	^	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
optimal values
-4.49	-3.99	-4.09	-3.40	-3.33	-2.62	-2.41	-2.72	
-4.17	-3.48	-3.61	-3.19	-3.04	-2.33	-2.12	-2.01	
-3.71	-3.20	-3.10	-2.99	-2.78	-3.04	-2.50	-1.79	
-3.38	-3.10	-2.81	-2.79	-2.27	-2.39	-1.86	-1.49	
-3.17	-2.88	-2.60	-2.08	-1.97	-1.67	-1.33	-0.77	
-2.88	-2.59	-2.90	-2.20	-1.67	-1.15	-0.61	-0.46	
-3.09	-2.39	-2.28	-1.75	-1.23	-0.69	-0.38	-0.15	
-2.78	-2.68	-2.51	-1.80	-1.08	-0.96	-0.23	0.08	
map_weights [-0.05998588 -0.73920963 -0.33831982 -0.51897831 -0.16513756  0.19723494]
MAP reward
-0.52	-0.52	-0.74	-0.17	-0.74	-0.34	-0.34	-0.74	
-0.74	-0.34	-0.52	-0.17	-0.74	-0.17	-0.06	-0.17	
-0.52	-0.06	-0.34	-0.17	-0.52	-0.74	-0.74	-0.34	
-0.34	-0.34	-0.17	-0.74	-0.34	-0.74	-0.52	-0.74	
-0.34	-0.34	-0.52	-0.06	-0.34	-0.52	-0.74	-0.34	
-0.34	-0.17	-0.74	-0.52	-0.52	-0.52	-0.17	-0.34	
-0.74	-0.06	-0.52	-0.52	-0.52	-0.34	-0.17	-0.17	
-0.06	-0.34	-0.74	-0.74	-0.06	-0.74	-0.34	0.20	
Map policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	v	v	v	v	^	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.9284654797460796
mean w [-0.15629864 -0.69339031 -0.31066696 -0.45429324 -0.17096491  0.18217041]
Mean policy from posterior
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.45	-0.45	-0.69	-0.17	-0.69	-0.31	-0.31	-0.69	
-0.69	-0.31	-0.45	-0.17	-0.69	-0.17	-0.16	-0.17	
-0.45	-0.16	-0.31	-0.17	-0.45	-0.69	-0.69	-0.31	
-0.31	-0.31	-0.17	-0.69	-0.31	-0.69	-0.45	-0.69	
-0.31	-0.31	-0.45	-0.16	-0.31	-0.45	-0.69	-0.31	
-0.31	-0.17	-0.69	-0.45	-0.45	-0.45	-0.17	-0.31	
-0.69	-0.16	-0.45	-0.45	-0.45	-0.31	-0.17	-0.17	
-0.16	-0.31	-0.69	-0.69	-0.16	-0.69	-0.31	0.18	
mean = 0.0018852626830283548, map = 0.0014802101711866023
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	v	v	v	v	
>	v	>	>	v	v	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
cvar = , 0.006079920619372903, 0.0018852569500094063, 0.0018852557247712731, 0.0018852550903516452, 0.0018852674634741717
==========
iteration 46
==========
weights [-0.28003463 -0.43393688 -0.70423907 -0.3280981  -0.15792812  0.32363106]
expeced value MDP LP -1.7189552378788981
demonstration
[(0, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 3), (21, 3), (29, 1), (30, 3), (38, 3), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.50983728  0.10572797 -0.17597208 -0.14061673  0.38805804 -0.72633275]
w_map [-0.37351133 -0.60373024 -0.62718078 -0.24048037 -0.16814954 -0.1286012 ] loglik -0.6931471853309645
accepted/total = 1692/3000 = 0.564
-------
true weights [-0.28003463 -0.43393688 -0.70423907 -0.3280981  -0.15792812  0.32363106]
features
3 	1 	3 	1 	3 	2 	2 	1 	
0 	4 	4 	4 	4 	4 	4 	3 	
3 	0 	3 	0 	0 	0 	2 	0 	
3 	3 	2 	1 	0 	4 	4 	2 	
3 	2 	4 	1 	3 	0 	4 	3 	
3 	2 	3 	1 	1 	1 	0 	2 	
3 	1 	2 	0 	2 	3 	3 	3 	
0 	2 	1 	1 	1 	3 	3 	5 	
optimal policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
v	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.61	-2.45	-2.21	-2.18	-1.93	-2.17	-2.31	-2.35	
-2.30	-2.04	-1.90	-1.76	-1.62	-1.48	-1.62	-1.93	
-2.61	-2.30	-2.17	-1.86	-1.60	-1.33	-1.61	-1.87	
-2.91	-2.61	-2.44	-1.75	-1.33	-1.06	-0.91	-1.61	
-2.90	-2.60	-1.91	-1.77	-1.35	-1.04	-0.76	-1.03	
-3.16	-2.87	-2.19	-1.88	-1.46	-1.04	-0.61	-0.71	
-2.86	-2.56	-2.15	-1.46	-1.36	-0.66	-0.34	-0.01	
-2.56	-2.30	-1.61	-1.19	-0.77	-0.34	-0.01	0.32	
map_weights [-0.37351133 -0.60373024 -0.62718078 -0.24048037 -0.16814954 -0.1286012 ]
MAP reward
-0.24	-0.60	-0.24	-0.60	-0.24	-0.63	-0.63	-0.60	
-0.37	-0.17	-0.17	-0.17	-0.17	-0.17	-0.17	-0.24	
-0.24	-0.37	-0.24	-0.37	-0.37	-0.37	-0.63	-0.37	
-0.24	-0.24	-0.63	-0.60	-0.37	-0.17	-0.17	-0.63	
-0.24	-0.63	-0.17	-0.60	-0.24	-0.37	-0.17	-0.24	
-0.24	-0.63	-0.24	-0.60	-0.60	-0.60	-0.37	-0.63	
-0.24	-0.60	-0.63	-0.37	-0.63	-0.24	-0.24	-0.24	
-0.37	-0.63	-0.60	-0.60	-0.60	-0.24	-0.24	-0.13	
Map policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	<	
>	^	^	>	v	v	v	v	
^	^	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.731817241659848
mean w [-0.24091998 -0.45758767 -0.60170252 -0.24894855 -0.10702087 -0.10716672]
Mean policy from posterior
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
^	^	^	>	v	v	v	<	
^	^	>	>	>	>	v	<	
^	>	>	>	>	>	v	<	
^	>	^	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.25	-0.46	-0.25	-0.46	-0.25	-0.60	-0.60	-0.46	
-0.24	-0.11	-0.11	-0.11	-0.11	-0.11	-0.11	-0.25	
-0.25	-0.24	-0.25	-0.24	-0.24	-0.24	-0.60	-0.24	
-0.25	-0.25	-0.60	-0.46	-0.24	-0.11	-0.11	-0.60	
-0.25	-0.60	-0.11	-0.46	-0.25	-0.24	-0.11	-0.25	
-0.25	-0.60	-0.25	-0.46	-0.46	-0.46	-0.24	-0.60	
-0.25	-0.46	-0.60	-0.24	-0.60	-0.25	-0.25	-0.25	
-0.24	-0.60	-0.46	-0.46	-0.46	-0.25	-0.25	-0.11	
mean = 0.02526073303043752, map = 0.03400301744211154
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	<	
>	^	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
^	^	^	>	v	v	v	v	
^	^	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
^	>	^	v	>	>	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
>	^	^	>	v	v	v	v	
^	^	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
^	>	^	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	<	
^	^	^	>	v	v	v	<	
^	^	>	>	>	>	v	<	
>	>	>	>	>	>	v	v	
^	>	^	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05604636444594879, 0.03619857545388383, 0.014966310576774111, 0.0149663190497209, 0.014966310752633438
==========
iteration 47
==========
weights [-0.32654197 -0.65989334 -0.15157256 -0.37933918 -0.338548    0.42002847]
expeced value MDP LP -1.788102963274963
demonstration
[(0, 3), (8, 1), (9, 1), (10, 3), (18, 3), (26, 1), (27, 3), (35, 1), (36, 1), (37, 1), (38, 3), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.61360989 -0.01638767  0.61232749 -0.42697449 -0.14732789  0.21037268]
w_map [-0.23494587 -0.84682114 -0.03251174 -0.33012261 -0.33925096 -0.0506481 ] loglik -0.00034919128536259336
accepted/total = 623/3000 = 0.20766666666666667
-------
true weights [-0.32654197 -0.65989334 -0.15157256 -0.37933918 -0.338548    0.42002847]
features
3 	1 	2 	4 	1 	2 	2 	4 	
0 	2 	0 	3 	0 	2 	1 	1 	
1 	4 	2 	1 	1 	1 	2 	4 	
0 	3 	3 	0 	1 	4 	0 	2 	
2 	0 	3 	3 	3 	4 	2 	0 	
4 	2 	4 	0 	0 	4 	4 	4 	
0 	3 	4 	0 	0 	1 	0 	3 	
3 	2 	2 	4 	1 	4 	0 	5 	
optimal policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.65	-3.63	-3.00	-2.94	-2.73	-2.09	-1.96	-2.07	
-3.30	-3.00	-2.88	-2.63	-2.27	-1.96	-1.83	-1.75	
-3.36	-2.89	-2.58	-2.73	-2.47	-1.83	-1.18	-1.10	
-2.73	-2.66	-2.45	-2.09	-2.01	-1.37	-1.04	-0.77	
-2.43	-2.30	-2.15	-1.79	-1.42	-1.05	-0.72	-0.63	
-2.31	-1.99	-1.86	-1.54	-1.22	-0.91	-0.57	-0.30	
-2.19	-1.88	-1.70	-1.53	-1.21	-0.90	-0.24	0.04	
-1.88	-1.51	-1.38	-1.24	-0.91	-0.25	0.09	0.42	
map_weights [-0.23494587 -0.84682114 -0.03251174 -0.33012261 -0.33925096 -0.0506481 ]
MAP reward
-0.33	-0.85	-0.03	-0.34	-0.85	-0.03	-0.03	-0.34	
-0.23	-0.03	-0.23	-0.33	-0.23	-0.03	-0.85	-0.85	
-0.85	-0.34	-0.03	-0.85	-0.85	-0.85	-0.03	-0.34	
-0.23	-0.33	-0.33	-0.23	-0.85	-0.34	-0.23	-0.03	
-0.03	-0.23	-0.33	-0.33	-0.33	-0.34	-0.03	-0.23	
-0.34	-0.03	-0.34	-0.23	-0.23	-0.34	-0.34	-0.34	
-0.23	-0.33	-0.34	-0.23	-0.23	-0.85	-0.23	-0.33	
-0.33	-0.03	-0.03	-0.34	-0.85	-0.34	-0.23	-0.05	
Map policy
v	>	v	v	>	v	v	<	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6150972884087311
mean w [-0.21483071 -0.71033084 -0.10443444 -0.25888015 -0.32268306  0.12463055]
Mean policy from posterior
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.26	-0.71	-0.10	-0.32	-0.71	-0.10	-0.10	-0.32	
-0.21	-0.10	-0.21	-0.26	-0.21	-0.10	-0.71	-0.71	
-0.71	-0.32	-0.10	-0.71	-0.71	-0.71	-0.10	-0.32	
-0.21	-0.26	-0.26	-0.21	-0.71	-0.32	-0.21	-0.10	
-0.10	-0.21	-0.26	-0.26	-0.26	-0.32	-0.10	-0.21	
-0.32	-0.10	-0.32	-0.21	-0.21	-0.32	-0.32	-0.32	
-0.21	-0.26	-0.32	-0.21	-0.21	-0.71	-0.21	-0.26	
-0.26	-0.10	-0.10	-0.32	-0.71	-0.32	-0.21	0.12	
mean = 0.0023424946180206074, map = 0.005607513169710199
CVaR policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	>	>	v	v	
>	>	v	>	>	v	v	v	
v	>	v	v	>	>	v	v	
v	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.002342512152379239, 0.0023424946092591714, 0.0023424950039678816, 0.002342497330885651, 0.0023424945853904866
==========
iteration 48
==========
weights [-0.38330247 -0.46518363 -0.7242588  -0.08270866 -0.18571323  0.26608733]
expeced value MDP LP -1.5625245722155583
demonstration
[(0, 1), (1, 1), (2, 3), (10, 1), (11, 1), (12, 1), (13, 1), (14, 3), (22, 1), (23, 3), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.2119625   0.7821286  -0.46288215  0.07011325 -0.34918486 -0.04733852]
w_map [-0.31128692 -0.53926261 -0.71977689 -0.14550857 -0.26945037  0.02100669] loglik -7.97305288813277e-06
accepted/total = 1184/3000 = 0.39466666666666667
-------
true weights [-0.38330247 -0.46518363 -0.7242588  -0.08270866 -0.18571323  0.26608733]
features
0 	0 	0 	4 	2 	4 	1 	4 	
1 	2 	3 	1 	3 	0 	4 	2 	
0 	2 	4 	0 	3 	2 	3 	4 	
0 	2 	1 	0 	1 	2 	1 	3 	
0 	0 	3 	3 	2 	4 	3 	4 	
1 	1 	1 	2 	4 	0 	3 	4 	
3 	0 	0 	4 	4 	0 	0 	1 	
0 	1 	1 	4 	0 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	<	
>	>	>	>	>	>	v	v	
v	>	>	>	^	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.11	-2.75	-2.39	-2.13	-2.22	-1.62	-1.53	-1.70	
-3.17	-2.73	-2.03	-1.97	-1.52	-1.45	-1.07	-1.54	
-3.02	-2.82	-2.12	-1.95	-1.58	-1.61	-0.90	-0.82	
-2.67	-2.65	-2.03	-1.88	-1.89	-1.44	-1.00	-0.64	
-2.31	-1.94	-1.58	-1.51	-1.44	-0.72	-0.54	-0.57	
-2.37	-2.31	-1.94	-1.73	-1.02	-0.84	-0.46	-0.39	
-1.92	-1.86	-1.49	-1.12	-0.94	-0.76	-0.58	-0.20	
-2.21	-1.85	-1.40	-0.94	-0.76	-0.39	-0.20	0.27	
map_weights [-0.31128692 -0.53926261 -0.71977689 -0.14550857 -0.26945037  0.02100669]
MAP reward
-0.31	-0.31	-0.31	-0.27	-0.72	-0.27	-0.54	-0.27	
-0.54	-0.72	-0.15	-0.54	-0.15	-0.31	-0.27	-0.72	
-0.31	-0.72	-0.27	-0.31	-0.15	-0.72	-0.15	-0.27	
-0.31	-0.72	-0.54	-0.31	-0.54	-0.72	-0.54	-0.15	
-0.31	-0.31	-0.15	-0.15	-0.72	-0.27	-0.15	-0.27	
-0.54	-0.54	-0.54	-0.72	-0.27	-0.31	-0.15	-0.27	
-0.15	-0.31	-0.31	-0.27	-0.27	-0.31	-0.31	-0.54	
-0.31	-0.54	-0.54	-0.27	-0.31	-0.27	-0.54	0.02	
Map policy
>	>	v	v	v	v	v	v	
v	>	>	>	>	>	v	v	
v	>	>	>	^	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.6083297378924688
mean w [-0.34870588 -0.39256807 -0.72608441 -0.1228228  -0.18955935  0.16998947]
Mean policy from posterior
>	>	v	v	v	v	v	<	
>	>	>	>	>	>	v	v	
v	>	>	>	^	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.35	-0.35	-0.35	-0.19	-0.73	-0.19	-0.39	-0.19	
-0.39	-0.73	-0.12	-0.39	-0.12	-0.35	-0.19	-0.73	
-0.35	-0.73	-0.19	-0.35	-0.12	-0.73	-0.12	-0.19	
-0.35	-0.73	-0.39	-0.35	-0.39	-0.73	-0.39	-0.12	
-0.35	-0.35	-0.12	-0.12	-0.73	-0.19	-0.12	-0.19	
-0.39	-0.39	-0.39	-0.73	-0.19	-0.35	-0.12	-0.19	
-0.12	-0.35	-0.35	-0.19	-0.19	-0.35	-0.35	-0.39	
-0.35	-0.39	-0.39	-0.19	-0.35	-0.19	-0.39	0.17	
mean = -8.691868336185848e-09, map = 0.005809686502055822
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	v	>	>	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	v	>	>	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	>	>	v	v	
v	>	>	>	^	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	>	>	v	v	
v	>	>	>	^	>	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.006119139843820953, 0.005460897677162624, 0.005460897440264345, -1.4201423148563208e-08, -1.3964398970500724e-08
==========
iteration 49
==========
weights [-0.44520993 -0.27171281 -0.45007951 -0.36066169 -0.62605234  0.05805433]
expeced value MDP LP -2.4933815319654355
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 1), (41, 1), (42, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.73375053 -0.00711229 -0.25702019  0.36537834 -0.18815917 -0.47601995]
w_map [-0.51888659 -0.23771676 -0.52257357 -0.2918444  -0.53203267  0.18147286] loglik -1.3862943625760522
accepted/total = 2101/3000 = 0.7003333333333334
-------
true weights [-0.44520993 -0.27171281 -0.45007951 -0.36066169 -0.62605234  0.05805433]
features
3 	4 	0 	4 	1 	0 	4 	0 	
4 	0 	4 	3 	0 	4 	4 	4 	
4 	2 	2 	0 	1 	4 	2 	2 	
1 	3 	4 	3 	0 	2 	4 	0 	
3 	2 	2 	1 	4 	4 	1 	0 	
3 	3 	1 	3 	4 	1 	2 	1 	
1 	2 	3 	1 	1 	1 	3 	2 	
3 	1 	2 	0 	0 	0 	2 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	>	v	
v	v	>	v	>	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.87	-4.64	-4.22	-3.82	-3.64	-3.93	-3.59	-2.99	
-4.56	-4.06	-3.82	-3.22	-3.40	-3.52	-3.00	-2.57	
-3.97	-3.65	-3.31	-2.89	-2.99	-2.92	-2.40	-1.97	
-3.38	-3.23	-3.07	-2.47	-2.74	-2.32	-1.97	-1.53	
-3.14	-2.90	-2.56	-2.13	-2.50	-1.89	-1.36	-1.10	
-2.81	-2.47	-2.13	-1.88	-1.89	-1.28	-1.10	-0.66	
-2.56	-2.31	-1.88	-1.53	-1.28	-1.01	-0.75	-0.39	
-2.72	-2.39	-2.14	-1.70	-1.27	-0.83	-0.39	0.06	
map_weights [-0.51888659 -0.23771676 -0.52257357 -0.2918444  -0.53203267  0.18147286]
MAP reward
-0.29	-0.53	-0.52	-0.53	-0.24	-0.52	-0.53	-0.52	
-0.53	-0.52	-0.53	-0.29	-0.52	-0.53	-0.53	-0.53	
-0.53	-0.52	-0.52	-0.52	-0.24	-0.53	-0.52	-0.52	
-0.24	-0.29	-0.53	-0.29	-0.52	-0.52	-0.53	-0.52	
-0.29	-0.52	-0.52	-0.24	-0.53	-0.53	-0.24	-0.52	
-0.29	-0.29	-0.24	-0.29	-0.53	-0.24	-0.52	-0.24	
-0.24	-0.52	-0.29	-0.24	-0.24	-0.24	-0.29	-0.52	
-0.29	-0.24	-0.52	-0.52	-0.52	-0.52	-0.52	0.18	
Map policy
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	<	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	>	v	>	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	>	.	
expeced value MDP LP -1.9682873357866917
mean w [-0.49929619 -0.18170534 -0.51200175 -0.16147351 -0.48653056  0.01548103]
Mean policy from posterior
v	<	v	v	<	<	v	v	
v	v	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	<	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
^	>	>	>	>	>	v	v	
^	>	^	^	>	>	>	.	
Mean rewards
-0.16	-0.49	-0.50	-0.49	-0.18	-0.50	-0.49	-0.50	
-0.49	-0.50	-0.49	-0.16	-0.50	-0.49	-0.49	-0.49	
-0.49	-0.51	-0.51	-0.50	-0.18	-0.49	-0.51	-0.51	
-0.18	-0.16	-0.49	-0.16	-0.50	-0.51	-0.49	-0.50	
-0.16	-0.51	-0.51	-0.18	-0.49	-0.49	-0.18	-0.50	
-0.16	-0.16	-0.18	-0.16	-0.49	-0.18	-0.51	-0.18	
-0.18	-0.51	-0.16	-0.18	-0.18	-0.18	-0.16	-0.51	
-0.16	-0.18	-0.51	-0.50	-0.50	-0.50	-0.51	0.02	
mean = 0.0968893026850064, map = 0.021730300255629054
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
v	v	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	^	^	^	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	v	
v	>	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	>	.	
CVaR policy
v	<	v	v	<	v	v	v	
v	>	>	v	<	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	<	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
cvar = , 0.023411733988378813, 0.03637188678113912, 0.04357644328927446, 0.04917125386510035, 0.07451324041515583
==========
iteration 50
==========
weights [-0.89793034 -0.16824879 -0.15900828 -0.23235499 -0.27964434  0.0891068 ]
expeced value MDP LP -1.471682357898469
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 1), (33, 3), (41, 1), (42, 1), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.64253612 -0.28391519  0.3346709   0.04745427 -0.37414901  0.50229025]
w_map [-0.70038347 -0.09152345 -0.29990549 -0.17301717 -0.6052767  -0.12185367] loglik -1.4572316609465759
accepted/total = 1414/3000 = 0.4713333333333333
-------
true weights [-0.89793034 -0.16824879 -0.15900828 -0.23235499 -0.27964434  0.0891068 ]
features
2 	1 	2 	3 	3 	0 	0 	0 	
1 	2 	4 	4 	2 	3 	1 	2 	
2 	0 	3 	3 	2 	4 	0 	4 	
3 	1 	0 	1 	2 	4 	0 	0 	
1 	4 	4 	2 	0 	2 	4 	3 	
0 	3 	2 	2 	0 	1 	0 	0 	
1 	3 	0 	1 	1 	4 	3 	4 	
4 	3 	3 	0 	2 	2 	2 	5 	
optimal policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	<	
v	>	>	v	v	v	<	^	
v	v	>	v	>	v	v	v	
>	v	v	v	>	v	<	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-2.30	-2.16	-2.02	-1.88	-1.75	-2.46	-2.61	-2.75	
-2.16	-2.02	-1.88	-1.66	-1.53	-1.58	-1.73	-1.87	
-2.01	-2.49	-1.61	-1.39	-1.39	-1.36	-2.25	-2.14	
-1.87	-1.66	-2.06	-1.17	-1.24	-1.09	-1.98	-2.19	
-1.66	-1.50	-1.28	-1.02	-1.71	-0.82	-1.09	-1.31	
-2.12	-1.24	-1.02	-0.86	-1.44	-0.67	-1.20	-1.09	
-1.61	-1.46	-1.60	-0.71	-0.55	-0.51	-0.30	-0.19	
-1.87	-1.68	-1.50	-1.28	-0.39	-0.23	-0.07	0.09	
map_weights [-0.70038347 -0.09152345 -0.29990549 -0.17301717 -0.6052767  -0.12185367]
MAP reward
-0.30	-0.09	-0.30	-0.17	-0.17	-0.70	-0.70	-0.70	
-0.09	-0.30	-0.61	-0.61	-0.30	-0.17	-0.09	-0.30	
-0.30	-0.70	-0.17	-0.17	-0.30	-0.61	-0.70	-0.61	
-0.17	-0.09	-0.70	-0.09	-0.30	-0.61	-0.70	-0.70	
-0.09	-0.61	-0.61	-0.30	-0.70	-0.30	-0.61	-0.17	
-0.70	-0.17	-0.30	-0.30	-0.70	-0.09	-0.70	-0.70	
-0.09	-0.17	-0.70	-0.09	-0.09	-0.61	-0.17	-0.61	
-0.61	-0.17	-0.17	-0.70	-0.30	-0.30	-0.30	-0.12	
Map policy
v	>	>	v	v	<	v	v	
v	>	v	v	v	<	<	<	
v	>	>	v	<	v	v	v	
>	v	>	v	<	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.66780774273328
mean w [-0.70941864 -0.16970411 -0.23340806 -0.22462264 -0.42452181  0.23037602]
Mean policy from posterior
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	<	
v	>	>	v	<	v	<	v	
>	v	>	v	<	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.23	-0.17	-0.23	-0.22	-0.22	-0.71	-0.71	-0.71	
-0.17	-0.23	-0.42	-0.42	-0.23	-0.22	-0.17	-0.23	
-0.23	-0.71	-0.22	-0.22	-0.23	-0.42	-0.71	-0.42	
-0.22	-0.17	-0.71	-0.17	-0.23	-0.42	-0.71	-0.71	
-0.17	-0.42	-0.42	-0.23	-0.71	-0.23	-0.42	-0.22	
-0.71	-0.22	-0.23	-0.23	-0.71	-0.17	-0.71	-0.71	
-0.17	-0.22	-0.71	-0.17	-0.17	-0.42	-0.22	-0.42	
-0.42	-0.22	-0.22	-0.71	-0.23	-0.23	-0.23	0.23	
mean = 0.04113325461424533, map = 0.08951418821213775
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	v	v	<	v	
>	v	>	v	>	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	<	v	<	v	
>	v	>	v	<	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	>	v	v	v	v	v	
v	>	v	v	v	v	<	v	
v	>	>	v	<	v	<	v	
>	v	>	v	<	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	^	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.11146117607577377, 0.08264539639890489, 0.06846881123423554, 0.07241520641307386, 0.061137648812530365
==========
iteration 51
==========
weights [-0.08889064 -0.06545644 -0.44693923 -0.79932443 -0.37125299  0.10635271]
expeced value MDP LP -2.034645332989533
demonstration
[(0, 1), (1, 3), (9, 3), (17, 3), (25, 3), (33, 1), (34, 3), (42, 3), (50, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.8488304   0.14744235 -0.03320647  0.2880733   0.38959529 -0.14789972]
w_map [-0.21265204 -0.03600396 -0.47112773 -0.73321477 -0.43758127 -0.04939849] loglik -0.693147189602243
accepted/total = 1385/3000 = 0.46166666666666667
-------
true weights [-0.08889064 -0.06545644 -0.44693923 -0.79932443 -0.37125299  0.10635271]
features
3 	2 	3 	0 	2 	4 	4 	0 	
3 	4 	4 	4 	4 	4 	3 	1 	
2 	2 	3 	4 	0 	2 	2 	1 	
2 	1 	3 	3 	1 	4 	4 	3 	
3 	2 	2 	1 	0 	3 	3 	4 	
1 	2 	1 	2 	2 	4 	2 	2 	
4 	4 	0 	2 	3 	3 	0 	2 	
3 	4 	0 	0 	4 	0 	3 	5 	
optimal policy
>	v	>	v	v	>	>	v	
>	v	>	v	v	<	>	v	
v	v	>	>	v	<	>	v	
>	v	v	v	v	<	v	v	
v	>	v	>	v	v	v	v	
>	>	v	<	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.27	-3.51	-3.40	-2.62	-2.64	-2.79	-2.45	-2.10	
-3.86	-3.09	-2.91	-2.56	-2.21	-2.56	-2.81	-2.03	
-3.17	-2.75	-2.99	-2.21	-1.86	-2.29	-2.41	-1.98	
-2.75	-2.33	-2.63	-2.57	-1.79	-2.14	-2.02	-1.94	
-2.68	-2.28	-1.85	-1.79	-1.74	-2.02	-1.66	-1.15	
-1.90	-1.85	-1.42	-1.85	-1.67	-1.23	-0.87	-0.79	
-2.08	-1.73	-1.37	-1.65	-1.93	-1.22	-0.43	-0.34	
-2.43	-1.65	-1.29	-1.22	-1.14	-0.78	-0.69	0.11	
map_weights [-0.21265204 -0.03600396 -0.47112773 -0.73321477 -0.43758127 -0.04939849]
MAP reward
-0.73	-0.47	-0.73	-0.21	-0.47	-0.44	-0.44	-0.21	
-0.73	-0.44	-0.44	-0.44	-0.44	-0.44	-0.73	-0.04	
-0.47	-0.47	-0.73	-0.44	-0.21	-0.47	-0.47	-0.04	
-0.47	-0.04	-0.73	-0.73	-0.04	-0.44	-0.44	-0.73	
-0.73	-0.47	-0.47	-0.04	-0.21	-0.73	-0.73	-0.44	
-0.04	-0.47	-0.04	-0.47	-0.47	-0.44	-0.47	-0.47	
-0.44	-0.44	-0.21	-0.47	-0.73	-0.73	-0.21	-0.47	
-0.73	-0.44	-0.21	-0.21	-0.44	-0.21	-0.73	-0.05	
Map policy
>	v	>	v	v	>	>	v	
>	v	>	v	v	>	>	v	
v	v	>	>	v	<	>	v	
>	v	v	v	v	<	v	v	
v	>	v	>	v	v	v	v	
>	>	v	<	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8747334108405114
mean w [-0.13335558 -0.16718661 -0.38727928 -0.54719373 -0.39528814  0.1810743 ]
Mean policy from posterior
>	v	>	>	v	>	>	v	
v	v	>	v	v	v	>	v	
v	v	>	>	v	<	v	v	
>	v	v	v	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.55	-0.39	-0.55	-0.13	-0.39	-0.40	-0.40	-0.13	
-0.55	-0.40	-0.40	-0.40	-0.40	-0.40	-0.55	-0.17	
-0.39	-0.39	-0.55	-0.40	-0.13	-0.39	-0.39	-0.17	
-0.39	-0.17	-0.55	-0.55	-0.17	-0.40	-0.40	-0.55	
-0.55	-0.39	-0.39	-0.17	-0.13	-0.55	-0.55	-0.40	
-0.17	-0.39	-0.17	-0.39	-0.39	-0.40	-0.39	-0.39	
-0.40	-0.40	-0.13	-0.39	-0.55	-0.55	-0.13	-0.39	
-0.55	-0.40	-0.13	-0.13	-0.40	-0.13	-0.55	0.18	
mean = 0.013541399437149604, map = 0.009223123799722455
CVaR policy
>	v	>	v	v	>	>	v	
>	v	>	>	v	>	>	v	
v	v	>	>	v	>	>	v	
>	v	v	v	v	>	v	v	
v	>	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	>	>	v	
>	v	>	>	v	>	>	v	
v	v	>	>	v	<	>	v	
>	v	v	v	v	>	v	v	
v	>	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	>	>	v	
>	v	>	v	v	<	>	v	
v	v	>	>	v	<	>	v	
>	v	v	v	v	>	v	v	
v	>	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	>	v	
v	v	>	v	v	v	>	v	
v	v	>	>	v	<	v	v	
>	v	v	v	v	>	v	v	
v	>	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	>	v	
v	v	>	v	v	v	>	v	
v	v	>	>	v	<	v	v	
>	v	v	v	v	>	v	v	
v	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.02483892663593501, 0.016310190771491317, 0.007087066806998887, 0.012262145311288553, 0.013541390963824895
==========
iteration 52
==========
weights [-0.2303422  -0.27617581 -0.58030475 -0.3484603  -0.38072288  0.51724394]
expeced value MDP LP -1.4949577628705846
demonstration
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (12, 1), (13, 1), (14, 3), (22, 3), (30, 3), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.06029708  0.1042775   0.71745841  0.30956666 -0.40995829 -0.45480389]
w_map [-0.14533514 -0.28108158 -0.71154151 -0.3931826  -0.48643403 -0.04867144] loglik -1.3863404150181964
accepted/total = 1168/3000 = 0.3893333333333333
-------
true weights [-0.2303422  -0.27617581 -0.58030475 -0.3484603  -0.38072288  0.51724394]
features
3 	1 	0 	0 	1 	0 	2 	0 	
3 	1 	1 	1 	0 	1 	3 	0 	
3 	1 	2 	4 	2 	2 	1 	1 	
2 	4 	0 	0 	4 	1 	1 	4 	
1 	1 	2 	1 	4 	0 	4 	3 	
3 	3 	4 	4 	1 	4 	0 	1 	
1 	0 	4 	0 	0 	2 	4 	0 	
1 	2 	2 	2 	1 	1 	3 	5 	
optimal policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	>	v	
^	^	>	>	>	>	>	.	
optimal values
-3.20	-2.88	-2.63	-2.42	-2.21	-1.95	-2.00	-1.43	
-3.04	-2.72	-2.47	-2.21	-1.95	-1.74	-1.48	-1.21	
-2.83	-2.51	-2.46	-2.05	-2.03	-1.67	-1.14	-0.99	
-2.82	-2.26	-1.90	-1.68	-1.47	-1.10	-0.88	-0.72	
-2.28	-2.02	-2.03	-1.47	-1.20	-0.83	-0.61	-0.35	
-2.02	-1.76	-1.58	-1.21	-0.88	-0.61	-0.23	0.00	
-1.69	-1.43	-1.21	-0.84	-0.62	-0.68	-0.10	0.28	
-1.95	-2.00	-1.54	-0.97	-0.39	-0.11	0.16	0.52	
map_weights [-0.14533514 -0.28108158 -0.71154151 -0.3931826  -0.48643403 -0.04867144]
MAP reward
-0.39	-0.28	-0.15	-0.15	-0.28	-0.15	-0.71	-0.15	
-0.39	-0.28	-0.28	-0.28	-0.15	-0.28	-0.39	-0.15	
-0.39	-0.28	-0.71	-0.49	-0.71	-0.71	-0.28	-0.28	
-0.71	-0.49	-0.15	-0.15	-0.49	-0.28	-0.28	-0.49	
-0.28	-0.28	-0.71	-0.28	-0.49	-0.15	-0.49	-0.39	
-0.39	-0.39	-0.49	-0.49	-0.28	-0.49	-0.15	-0.28	
-0.28	-0.15	-0.49	-0.15	-0.15	-0.71	-0.49	-0.15	
-0.28	-0.71	-0.71	-0.71	-0.28	-0.28	-0.39	-0.05	
Map policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	v	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	>	>	>	v	>	>	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.7755668512713068
mean w [-0.1108199  -0.17040748 -0.54847247 -0.29362944 -0.52214019 -0.29610706]
Mean policy from posterior
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.29	-0.17	-0.11	-0.11	-0.17	-0.11	-0.55	-0.11	
-0.29	-0.17	-0.17	-0.17	-0.11	-0.17	-0.29	-0.11	
-0.29	-0.17	-0.55	-0.52	-0.55	-0.55	-0.17	-0.17	
-0.55	-0.52	-0.11	-0.11	-0.52	-0.17	-0.17	-0.52	
-0.17	-0.17	-0.55	-0.17	-0.52	-0.11	-0.52	-0.29	
-0.29	-0.29	-0.52	-0.52	-0.17	-0.52	-0.11	-0.17	
-0.17	-0.11	-0.52	-0.11	-0.11	-0.55	-0.52	-0.11	
-0.17	-0.55	-0.55	-0.55	-0.17	-0.17	-0.29	-0.30	
mean = 0.0028039872483995865, map = -1.4797938430177737e-09
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	v	v	v	v	v	v	v	
v	>	>	v	>	v	v	v	
>	v	>	v	v	v	v	v	
v	v	v	v	v	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
cvar = , 0.0076627871918493184, 0.0028039892979598857, 0.002803987177655065, 0.002803984782695501, 0.0028039847853009725
==========
iteration 53
==========
weights [-0.31907605 -0.02261283 -0.25637322 -0.8144772  -0.35621239  0.20418503]
expeced value MDP LP -1.159931651599198
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 3), (33, 3), (41, 1), (42, 3), (50, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.17519184  0.36703838  0.30129238 -0.29184144 -0.78216404  0.21647526]
w_map [-0.44998775 -0.22030291 -0.02975893 -0.69281883 -0.49471369  0.15281523] loglik -7.855682326862734e-09
accepted/total = 1845/3000 = 0.615
-------
true weights [-0.31907605 -0.02261283 -0.25637322 -0.8144772  -0.35621239  0.20418503]
features
0 	0 	3 	3 	3 	3 	0 	0 	
0 	0 	2 	1 	3 	3 	2 	3 	
0 	4 	4 	0 	4 	1 	1 	2 	
1 	2 	4 	4 	2 	3 	4 	1 	
4 	1 	3 	2 	0 	1 	1 	2 	
2 	1 	0 	2 	0 	3 	0 	3 	
3 	3 	2 	0 	0 	4 	2 	4 	
0 	1 	1 	1 	1 	1 	0 	5 	
optimal policy
v	v	v	v	v	>	v	<	
v	v	>	v	v	v	v	v	
v	v	v	v	>	>	v	v	
>	v	<	v	v	v	v	v	
>	v	v	v	>	>	v	<	
>	>	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.98	-2.00	-2.69	-2.45	-3.01	-2.41	-1.62	-1.92	
-1.67	-1.70	-1.90	-1.66	-2.22	-1.88	-1.31	-2.01	
-1.37	-1.39	-1.74	-1.65	-1.42	-1.08	-1.06	-1.21	
-1.06	-1.05	-1.39	-1.34	-1.28	-1.53	-1.05	-0.97	
-1.15	-0.80	-1.58	-1.00	-1.03	-0.72	-0.70	-0.95	
-1.03	-0.79	-0.77	-0.75	-0.79	-1.30	-0.69	-0.97	
-1.35	-1.03	-0.46	-0.50	-0.48	-0.49	-0.37	-0.15	
-0.54	-0.22	-0.20	-0.18	-0.16	-0.14	-0.12	0.20	
map_weights [-0.44998775 -0.22030291 -0.02975893 -0.69281883 -0.49471369  0.15281523]
MAP reward
-0.45	-0.45	-0.69	-0.69	-0.69	-0.69	-0.45	-0.45	
-0.45	-0.45	-0.03	-0.22	-0.69	-0.69	-0.03	-0.69	
-0.45	-0.49	-0.49	-0.45	-0.49	-0.22	-0.22	-0.03	
-0.22	-0.03	-0.49	-0.49	-0.03	-0.69	-0.49	-0.22	
-0.49	-0.22	-0.69	-0.03	-0.45	-0.22	-0.22	-0.03	
-0.03	-0.22	-0.45	-0.03	-0.45	-0.69	-0.45	-0.69	
-0.69	-0.69	-0.03	-0.45	-0.45	-0.49	-0.03	-0.49	
-0.45	-0.22	-0.22	-0.22	-0.22	-0.22	-0.45	0.15	
Map policy
v	v	v	v	>	>	v	<	
v	v	>	v	v	>	v	v	
v	v	>	v	>	>	>	v	
>	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	<	
>	>	v	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6961726552611145
mean w [-0.31432049 -0.14494282 -0.18885167 -0.63101066 -0.52843738 -0.03178523]
Mean policy from posterior
v	v	v	v	>	>	v	<	
v	v	>	v	v	v	v	v	
v	v	>	v	v	>	>	v	
>	v	<	v	v	v	v	v	
>	v	v	v	>	>	v	<	
>	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.31	-0.31	-0.63	-0.63	-0.63	-0.63	-0.31	-0.31	
-0.31	-0.31	-0.19	-0.14	-0.63	-0.63	-0.19	-0.63	
-0.31	-0.53	-0.53	-0.31	-0.53	-0.14	-0.14	-0.19	
-0.14	-0.19	-0.53	-0.53	-0.19	-0.63	-0.53	-0.14	
-0.53	-0.14	-0.63	-0.19	-0.31	-0.14	-0.14	-0.19	
-0.19	-0.14	-0.31	-0.19	-0.31	-0.63	-0.31	-0.63	
-0.63	-0.63	-0.19	-0.31	-0.31	-0.53	-0.19	-0.53	
-0.31	-0.14	-0.14	-0.14	-0.14	-0.14	-0.31	-0.03	
mean = 0.03550882517816745, map = 0.056737382879883835
CVaR policy
v	v	v	v	>	>	v	v	
v	v	>	v	v	>	v	v	
v	v	>	v	v	>	>	v	
>	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
>	>	v	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	>	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	>	>	v	<	
>	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	>	v	v	
>	v	<	v	v	v	v	v	
>	v	v	v	>	>	v	<	
>	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	v	v	
v	v	>	v	v	v	v	v	
v	v	>	v	v	>	>	v	
>	v	<	v	v	v	v	v	
>	v	v	v	>	>	v	<	
>	>	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.10904156462970982, 0.04684741023082095, 0.02684986580155746, 0.02227037941204535, 0.039272581308726906
==========
iteration 54
==========
weights [-0.0212071  -0.22766981 -0.10325627 -0.04689663 -0.34228555  0.90426554]
expeced value MDP LP 0.24349806328469975
demonstration
[(0, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 3), (20, 3), (28, 3), (36, 1), (37, 3), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.68940727 -0.44990227 -0.16937552 -0.0388292   0.50820156  0.18395917]
w_map [-0.17998934 -0.53149623 -0.33410585 -0.17135517 -0.70668852  0.21146539] loglik -2.236653905640651e-06
accepted/total = 1599/3000 = 0.533
-------
true weights [-0.0212071  -0.22766981 -0.10325627 -0.04689663 -0.34228555  0.90426554]
features
3 	1 	4 	2 	3 	1 	1 	1 	
2 	2 	2 	3 	3 	4 	3 	2 	
4 	2 	1 	2 	0 	1 	4 	2 	
4 	3 	1 	1 	3 	4 	2 	3 	
0 	2 	1 	0 	1 	2 	2 	3 	
2 	4 	2 	1 	4 	0 	3 	3 	
3 	3 	2 	1 	2 	2 	3 	2 	
1 	4 	0 	4 	4 	0 	3 	5 	
optimal policy
v	v	>	>	v	<	v	v	
>	>	>	>	v	>	>	v	
>	^	>	>	v	<	v	v	
v	v	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	>	>	v	v	v	
^	>	^	>	>	>	>	.	
optimal values
-0.17	-0.25	-0.26	0.09	0.19	-0.04	0.13	0.18	
-0.12	-0.02	0.09	0.19	0.24	0.01	0.36	0.41	
-0.46	-0.12	-0.05	0.18	0.29	0.06	0.17	0.52	
-0.31	-0.04	-0.12	0.11	0.31	0.25	0.52	0.63	
0.03	0.01	0.11	0.34	0.37	0.60	0.63	0.68	
0.05	-0.14	0.15	0.13	0.36	0.71	0.74	0.74	
0.16	0.21	0.26	0.36	0.60	0.71	0.79	0.79	
-0.07	-0.11	0.23	0.12	0.47	0.82	0.85	0.90	
map_weights [-0.17998934 -0.53149623 -0.33410585 -0.17135517 -0.70668852  0.21146539]
MAP reward
-0.17	-0.53	-0.71	-0.33	-0.17	-0.53	-0.53	-0.53	
-0.33	-0.33	-0.33	-0.17	-0.17	-0.71	-0.17	-0.33	
-0.71	-0.33	-0.53	-0.33	-0.18	-0.53	-0.71	-0.33	
-0.71	-0.17	-0.53	-0.53	-0.17	-0.71	-0.33	-0.17	
-0.18	-0.33	-0.53	-0.18	-0.53	-0.33	-0.33	-0.17	
-0.33	-0.71	-0.33	-0.53	-0.71	-0.18	-0.17	-0.17	
-0.17	-0.17	-0.33	-0.53	-0.33	-0.33	-0.17	-0.33	
-0.53	-0.71	-0.18	-0.71	-0.71	-0.18	-0.17	0.21	
Map policy
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.720608215420515
mean w [-0.15835298 -0.47348597 -0.32233526 -0.13325269 -0.71983118 -0.05969361]
Mean policy from posterior
v	v	>	>	v	<	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
Mean rewards
-0.13	-0.47	-0.72	-0.32	-0.13	-0.47	-0.47	-0.47	
-0.32	-0.32	-0.32	-0.13	-0.13	-0.72	-0.13	-0.32	
-0.72	-0.32	-0.47	-0.32	-0.16	-0.47	-0.72	-0.32	
-0.72	-0.13	-0.47	-0.47	-0.13	-0.72	-0.32	-0.13	
-0.16	-0.32	-0.47	-0.16	-0.47	-0.32	-0.32	-0.13	
-0.32	-0.72	-0.32	-0.47	-0.72	-0.16	-0.13	-0.13	
-0.13	-0.13	-0.32	-0.47	-0.32	-0.32	-0.13	-0.32	
-0.47	-0.72	-0.16	-0.72	-0.72	-0.16	-0.13	-0.06	
mean = 0.00791678626205261, map = 0.01188853444494492
CVaR policy
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
>	v	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
>	v	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
>	v	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	<	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	<	v	v	
>	>	>	>	v	>	>	v	
>	v	>	>	v	v	v	v	
v	v	>	>	v	v	>	v	
v	>	>	>	>	v	v	v	
v	v	v	>	>	>	v	v	
>	>	>	>	>	>	v	v	
^	^	^	>	>	>	>	.	
cvar = , 0.020092518387832048, 0.014241896775926965, 0.012652000509661199, 0.00962552533915706, 0.007916785196267317
==========
iteration 55
==========
weights [-0.47728428 -0.33526691 -0.16233473 -0.18336267 -0.06880032  0.7714194 ]
expeced value MDP LP -0.3700508609062212
demonstration
[(0, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 3), (21, 3), (29, 3), (37, 3), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.41528695  0.60831019  0.46003398 -0.03746531 -0.04784046 -0.49210957]
w_map [-0.65350101 -0.5136936  -0.23547521 -0.44439999 -0.23680645  0.00617288] loglik -1.24406405177524e-08
accepted/total = 1675/3000 = 0.5583333333333333
-------
true weights [-0.47728428 -0.33526691 -0.16233473 -0.18336267 -0.06880032  0.7714194 ]
features
4 	3 	0 	4 	0 	3 	2 	3 	
2 	4 	2 	4 	4 	2 	2 	1 	
4 	4 	3 	2 	3 	4 	1 	0 	
4 	4 	3 	4 	3 	2 	0 	4 	
4 	2 	4 	2 	0 	2 	0 	1 	
0 	4 	2 	0 	4 	3 	4 	3 	
1 	4 	4 	1 	3 	4 	0 	2 	
2 	1 	3 	1 	3 	3 	3 	5 	
optimal policy
v	v	>	v	v	v	v	<	
>	>	>	>	>	v	<	v	
v	v	>	v	>	v	<	v	
v	v	>	>	>	v	>	v	
>	v	v	v	v	v	v	v	
>	v	v	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-0.96	-0.92	-1.06	-0.59	-0.94	-0.58	-0.72	-0.89	
-0.90	-0.75	-0.69	-0.53	-0.47	-0.40	-0.56	-0.80	
-0.75	-0.69	-0.76	-0.58	-0.42	-0.24	-0.57	-0.47	
-0.69	-0.62	-0.60	-0.42	-0.36	-0.17	-0.47	0.00	
-0.62	-0.56	-0.50	-0.55	-0.40	-0.01	-0.14	0.07	
-0.88	-0.40	-0.43	-0.40	0.08	0.15	0.34	0.41	
-0.67	-0.34	-0.27	-0.20	0.13	0.32	0.12	0.60	
-0.80	-0.65	-0.32	-0.13	0.20	0.39	0.58	0.77	
map_weights [-0.65350101 -0.5136936  -0.23547521 -0.44439999 -0.23680645  0.00617288]
MAP reward
-0.24	-0.44	-0.65	-0.24	-0.65	-0.44	-0.24	-0.44	
-0.24	-0.24	-0.24	-0.24	-0.24	-0.24	-0.24	-0.51	
-0.24	-0.24	-0.44	-0.24	-0.44	-0.24	-0.51	-0.65	
-0.24	-0.24	-0.44	-0.24	-0.44	-0.24	-0.65	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.65	-0.24	-0.65	-0.51	
-0.65	-0.24	-0.24	-0.65	-0.24	-0.44	-0.24	-0.44	
-0.51	-0.24	-0.24	-0.51	-0.44	-0.24	-0.65	-0.24	
-0.24	-0.51	-0.44	-0.51	-0.44	-0.44	-0.44	0.01	
Map policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	v	
v	v	>	v	>	v	<	v	
v	v	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4932542809895692
mean w [-0.55024827 -0.45153533 -0.12518338 -0.3192689  -0.15883515  0.05591064]
Mean policy from posterior
v	v	v	v	v	v	v	<	
>	>	>	>	>	v	<	<	
^	^	>	v	>	v	<	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.16	-0.32	-0.55	-0.16	-0.55	-0.32	-0.13	-0.32	
-0.13	-0.16	-0.13	-0.16	-0.16	-0.13	-0.13	-0.45	
-0.16	-0.16	-0.32	-0.13	-0.32	-0.16	-0.45	-0.55	
-0.16	-0.16	-0.32	-0.16	-0.32	-0.13	-0.55	-0.16	
-0.16	-0.13	-0.16	-0.13	-0.55	-0.13	-0.55	-0.45	
-0.55	-0.16	-0.13	-0.55	-0.16	-0.32	-0.16	-0.32	
-0.45	-0.16	-0.16	-0.45	-0.32	-0.16	-0.55	-0.13	
-0.13	-0.45	-0.32	-0.45	-0.32	-0.32	-0.32	0.06	
mean = 0.033437140382386876, map = 0.045573800337615655
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	>	>	v	>	v	
>	>	v	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	<	v	
>	>	>	v	>	v	<	v	
v	v	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	<	
>	>	>	>	>	v	<	v	
^	^	>	v	>	v	<	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	<	
>	>	>	>	>	v	<	v	
^	^	>	v	>	v	<	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	v	<	
>	>	>	>	>	v	<	<	
^	^	>	v	>	v	<	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.09739536814754546, 0.06263459695952867, 0.032125038985317655, 0.03212477549440318, 0.03343713158797673
==========
iteration 56
==========
weights [-0.44050408 -0.11585265 -0.4575773  -0.53940492 -0.49653111  0.21367386]
expeced value MDP LP -1.8480005185521684
demonstration
[(0, 1), (1, 3), (9, 3), (17, 3), (25, 1), (26, 3), (34, 3), (42, 1), (43, 1), (44, 1), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.07689517 -0.07593245 -0.07202362  0.36389091 -0.90561542 -0.17486596]
w_map [-0.4294507  -0.19507519 -0.45096436 -0.68233661 -0.31108376  0.10859355] loglik -0.6931471885268472
accepted/total = 1687/3000 = 0.5623333333333334
-------
true weights [-0.44050408 -0.11585265 -0.4575773  -0.53940492 -0.49653111  0.21367386]
features
2 	0 	2 	4 	2 	1 	3 	0 	
3 	4 	3 	0 	0 	2 	0 	4 	
3 	1 	0 	1 	4 	3 	1 	3 	
2 	1 	2 	4 	2 	2 	1 	1 	
4 	3 	1 	3 	1 	0 	3 	2 	
0 	4 	4 	1 	0 	0 	3 	1 	
3 	4 	1 	0 	3 	0 	1 	0 	
3 	2 	4 	3 	3 	1 	1 	5 	
optimal policy
>	v	>	>	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.93	-3.51	-3.42	-2.99	-2.52	-2.08	-2.07	-2.34	
-3.61	-3.10	-3.15	-2.64	-2.41	-1.99	-1.55	-1.92	
-3.14	-2.63	-2.64	-2.22	-2.13	-1.65	-1.12	-1.43	
-2.97	-2.54	-2.45	-2.35	-1.87	-1.46	-1.01	-0.90	
-3.00	-2.53	-2.01	-1.96	-1.43	-1.33	-1.09	-0.80	
-2.81	-2.39	-1.91	-1.43	-1.33	-0.90	-0.56	-0.34	
-2.53	-2.01	-1.53	-1.43	-1.00	-0.46	-0.02	-0.23	
-2.54	-2.02	-1.58	-1.09	-0.56	-0.02	0.10	0.21	
map_weights [-0.4294507  -0.19507519 -0.45096436 -0.68233661 -0.31108376  0.10859355]
MAP reward
-0.45	-0.43	-0.45	-0.31	-0.45	-0.20	-0.68	-0.43	
-0.68	-0.31	-0.68	-0.43	-0.43	-0.45	-0.43	-0.31	
-0.68	-0.20	-0.43	-0.20	-0.31	-0.68	-0.20	-0.68	
-0.45	-0.20	-0.45	-0.31	-0.45	-0.45	-0.20	-0.20	
-0.31	-0.68	-0.20	-0.68	-0.20	-0.43	-0.68	-0.45	
-0.43	-0.31	-0.31	-0.20	-0.43	-0.43	-0.68	-0.20	
-0.68	-0.31	-0.20	-0.43	-0.68	-0.43	-0.20	-0.43	
-0.68	-0.45	-0.31	-0.68	-0.68	-0.20	-0.20	0.11	
Map policy
>	v	>	>	>	v	v	v	
>	v	>	v	>	>	v	v	
>	v	>	>	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.636700168158331
mean w [-0.25973109 -0.1254004  -0.42074756 -0.67059682 -0.30468191  0.01525858]
Mean policy from posterior
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	<	
>	v	>	v	v	>	v	v	
>	>	v	>	v	v	>	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.42	-0.26	-0.42	-0.30	-0.42	-0.13	-0.67	-0.26	
-0.67	-0.30	-0.67	-0.26	-0.26	-0.42	-0.26	-0.30	
-0.67	-0.13	-0.26	-0.13	-0.30	-0.67	-0.13	-0.67	
-0.42	-0.13	-0.42	-0.30	-0.42	-0.42	-0.13	-0.13	
-0.30	-0.67	-0.13	-0.67	-0.13	-0.26	-0.67	-0.42	
-0.26	-0.30	-0.30	-0.13	-0.26	-0.26	-0.67	-0.13	
-0.67	-0.30	-0.13	-0.26	-0.67	-0.26	-0.13	-0.26	
-0.67	-0.42	-0.30	-0.67	-0.67	-0.13	-0.13	0.02	
mean = 0.11027971745350951, map = 0.0254104738211427
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	<	
>	v	>	v	v	>	v	v	
>	>	v	>	v	>	>	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	<	
>	v	>	>	v	>	v	v	
>	>	v	>	v	v	>	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	v	v	>	>	v	<	
>	v	>	>	v	>	v	v	
>	>	v	>	v	v	>	v	
v	>	v	v	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	v	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.03586363686018945, 0.03586365062937946, 0.03929648196532409, 0.04420725321036856, 0.11027971736462039
==========
iteration 57
==========
weights [-0.2725645  -0.34215258 -0.8763279  -0.08535261 -0.12101183  0.13696965]
expeced value MDP LP -1.312996948050642
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 3), (48, 1), (49, 3), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.09444152 -0.27817045 -0.61738659  0.39974996  0.00710313  0.61047958]
w_map [-0.41778222 -0.51268926 -0.61174129 -0.22679868 -0.36303036  0.07177498] loglik -5.8980560879717814e-08
accepted/total = 1705/3000 = 0.5683333333333334
-------
true weights [-0.2725645  -0.34215258 -0.8763279  -0.08535261 -0.12101183  0.13696965]
features
3 	1 	0 	2 	3 	2 	0 	1 	
4 	2 	1 	1 	4 	0 	4 	4 	
1 	0 	3 	1 	4 	1 	0 	4 	
0 	3 	2 	3 	2 	1 	1 	3 	
3 	0 	0 	2 	0 	1 	4 	4 	
0 	2 	0 	1 	0 	1 	2 	1 	
0 	3 	1 	3 	4 	0 	4 	0 	
0 	3 	3 	3 	1 	3 	1 	5 	
optimal policy
v	<	v	>	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	>	>	^	>	>	v	
v	v	v	^	v	>	>	v	
v	<	v	v	v	>	>	v	
v	v	v	v	v	v	v	v	
>	v	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.28	-2.60	-2.29	-2.32	-1.46	-2.14	-1.28	-1.23	
-2.22	-2.89	-2.04	-1.71	-1.38	-1.28	-1.01	-0.90	
-2.12	-2.11	-1.89	-1.82	-1.49	-1.38	-1.05	-0.79	
-1.79	-1.86	-2.44	-1.89	-2.04	-1.34	-1.01	-0.67	
-1.54	-1.79	-1.58	-1.92	-1.17	-1.04	-0.71	-0.59	
-1.46	-1.81	-1.32	-1.06	-0.91	-0.86	-1.13	-0.48	
-1.20	-0.94	-1.06	-0.72	-0.64	-0.53	-0.26	-0.14	
-1.13	-0.86	-0.79	-0.71	-0.63	-0.29	-0.21	0.14	
map_weights [-0.41778222 -0.51268926 -0.61174129 -0.22679868 -0.36303036  0.07177498]
MAP reward
-0.23	-0.51	-0.42	-0.61	-0.23	-0.61	-0.42	-0.51	
-0.36	-0.61	-0.51	-0.51	-0.36	-0.42	-0.36	-0.36	
-0.51	-0.42	-0.23	-0.51	-0.36	-0.51	-0.42	-0.36	
-0.42	-0.23	-0.61	-0.23	-0.61	-0.51	-0.51	-0.23	
-0.23	-0.42	-0.42	-0.61	-0.42	-0.51	-0.36	-0.36	
-0.42	-0.61	-0.42	-0.51	-0.42	-0.51	-0.61	-0.51	
-0.42	-0.23	-0.51	-0.23	-0.36	-0.42	-0.36	-0.42	
-0.42	-0.23	-0.23	-0.23	-0.51	-0.23	-0.51	0.07	
Map policy
v	v	>	>	v	>	v	v	
v	v	v	>	>	>	>	v	
v	v	>	v	>	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	v	v	v	v	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.411150513082403
mean w [-0.29372751 -0.33795664 -0.61392228 -0.11171597 -0.35400547  0.35700173]
Mean policy from posterior
v	<	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.11	-0.34	-0.29	-0.61	-0.11	-0.61	-0.29	-0.34	
-0.35	-0.61	-0.34	-0.34	-0.35	-0.29	-0.35	-0.35	
-0.34	-0.29	-0.11	-0.34	-0.35	-0.34	-0.29	-0.35	
-0.29	-0.11	-0.61	-0.11	-0.61	-0.34	-0.34	-0.11	
-0.11	-0.29	-0.29	-0.61	-0.29	-0.34	-0.35	-0.35	
-0.29	-0.61	-0.29	-0.34	-0.29	-0.34	-0.61	-0.34	
-0.29	-0.11	-0.34	-0.11	-0.35	-0.29	-0.35	-0.29	
-0.29	-0.11	-0.11	-0.11	-0.34	-0.11	-0.34	0.36	
mean = 0.1818268709319295, map = 0.07790780173807521
CVaR policy
v	v	v	v	v	>	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	>	v	
v	v	>	v	v	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	v	>	v	v	
v	v	v	v	>	>	v	v	
v	v	>	v	>	>	>	v	
v	v	v	v	v	>	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	v	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	v	v	v	v	v	v	
v	v	v	v	>	v	v	v	
v	v	>	v	>	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	>	v	
>	v	>	v	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.14217583930693145, 0.1358257028702523, 0.13555094749455332, 0.1818268709304638, 0.1818268709371762
==========
iteration 58
==========
weights [-0.69611314 -0.19058184 -0.24465214 -0.32533667 -0.22025134  0.51468028]
expeced value MDP LP -1.3038652479637114
demonstration
[(0, 3), (8, 1), (9, 3), (17, 1), (18, 3), (26, 3), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[0.05875517 0.20671182 0.44494215 0.74436421 0.41299712 0.17663476]
w_map [-0.53569345 -0.17976098 -0.35924086 -0.52747854 -0.51973557 -0.05749653] loglik -2.75359951729115e-07
accepted/total = 1421/3000 = 0.4736666666666667
-------
true weights [-0.69611314 -0.19058184 -0.24465214 -0.32533667 -0.22025134  0.51468028]
features
4 	3 	0 	3 	1 	2 	1 	2 	
2 	1 	3 	4 	3 	0 	0 	0 	
4 	2 	1 	4 	1 	0 	1 	4 	
3 	3 	2 	2 	0 	1 	4 	3 	
4 	0 	1 	1 	4 	3 	2 	1 	
3 	3 	2 	4 	3 	4 	4 	2 	
3 	2 	4 	2 	0 	3 	0 	2 	
2 	0 	4 	0 	2 	1 	3 	5 	
optimal policy
v	v	v	v	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.54	-2.42	-2.70	-2.08	-2.08	-1.91	-1.68	-1.63	
-2.34	-2.12	-2.03	-1.78	-1.99	-2.19	-1.50	-1.40	
-2.15	-1.94	-1.72	-1.57	-1.68	-1.50	-0.82	-0.71	
-2.16	-1.85	-1.54	-1.36	-1.50	-0.82	-0.63	-0.50	
-2.02	-1.99	-1.31	-1.13	-0.95	-0.74	-0.42	-0.17	
-1.81	-1.50	-1.19	-0.96	-0.74	-0.42	-0.20	0.02	
-1.91	-1.60	-1.37	-1.18	-0.95	-0.33	-0.43	0.26	
-2.07	-1.84	-1.16	-0.95	-0.25	-0.01	0.18	0.51	
map_weights [-0.53569345 -0.17976098 -0.35924086 -0.52747854 -0.51973557 -0.05749653]
MAP reward
-0.52	-0.53	-0.54	-0.53	-0.18	-0.36	-0.18	-0.36	
-0.36	-0.18	-0.53	-0.52	-0.53	-0.54	-0.54	-0.54	
-0.52	-0.36	-0.18	-0.52	-0.18	-0.54	-0.18	-0.52	
-0.53	-0.53	-0.36	-0.36	-0.54	-0.18	-0.52	-0.53	
-0.52	-0.54	-0.18	-0.18	-0.52	-0.53	-0.36	-0.18	
-0.53	-0.53	-0.36	-0.52	-0.53	-0.52	-0.52	-0.36	
-0.53	-0.36	-0.52	-0.36	-0.54	-0.53	-0.54	-0.36	
-0.36	-0.54	-0.52	-0.54	-0.36	-0.18	-0.53	-0.06	
Map policy
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.3096347600890432
mean w [-0.52885884 -0.09234994 -0.19614102 -0.36914689 -0.34465642  0.27098715]
Mean policy from posterior
v	v	v	>	>	>	v	<	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	v	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.34	-0.37	-0.53	-0.37	-0.09	-0.20	-0.09	-0.20	
-0.20	-0.09	-0.37	-0.34	-0.37	-0.53	-0.53	-0.53	
-0.34	-0.20	-0.09	-0.34	-0.09	-0.53	-0.09	-0.34	
-0.37	-0.37	-0.20	-0.20	-0.53	-0.09	-0.34	-0.37	
-0.34	-0.53	-0.09	-0.09	-0.34	-0.37	-0.20	-0.09	
-0.37	-0.37	-0.20	-0.34	-0.37	-0.34	-0.34	-0.20	
-0.37	-0.20	-0.34	-0.20	-0.53	-0.37	-0.53	-0.20	
-0.20	-0.53	-0.34	-0.53	-0.20	-0.09	-0.37	0.27	
mean = 0.03603825935279392, map = 0.05523153437335626
CVaR policy
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	v	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	v	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	v	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	>	>	v	v	
>	v	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	v	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05321078026208381, 0.04508485616260671, 0.031724391245468775, 0.03172439059404786, 0.03172439043689934
==========
iteration 59
==========
weights [-0.02402604 -0.14085619 -0.41371611 -0.05490963 -0.51419023  0.73553695]
expeced value MDP LP -0.3784134816102686
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 3), (26, 1), (27, 1), (28, 1), (29, 3), (37, 3), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.07971731 -0.44067331  0.25122791 -0.70343261 -0.1284607   0.47435971]
w_map [-0.24358499 -0.21609052 -0.49703694 -0.45449424 -0.66356323 -0.00665915] loglik -0.6931471582703388
accepted/total = 1870/3000 = 0.6233333333333333
-------
true weights [-0.02402604 -0.14085619 -0.41371611 -0.05490963 -0.51419023  0.73553695]
features
4 	1 	2 	3 	1 	4 	4 	0 	
2 	1 	3 	1 	0 	3 	2 	3 	
1 	0 	0 	2 	4 	2 	1 	4 	
0 	0 	1 	3 	1 	3 	4 	0 	
3 	2 	3 	2 	4 	2 	4 	3 	
1 	3 	2 	4 	2 	0 	1 	2 	
1 	0 	1 	4 	2 	2 	1 	3 	
3 	1 	1 	4 	4 	0 	0 	5 	
optimal policy
>	v	v	v	v	v	>	v	
v	v	v	<	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	>	v	
^	^	^	^	v	v	v	v	
^	<	^	>	>	>	v	v	
^	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-1.24	-0.73	-0.90	-0.68	-0.71	-1.06	-0.93	-0.42	
-1.00	-0.60	-0.49	-0.63	-0.57	-0.55	-0.81	-0.40	
-0.60	-0.46	-0.44	-0.69	-0.74	-0.50	-0.48	-0.35	
-0.46	-0.44	-0.42	-0.28	-0.23	-0.09	-0.35	0.17	
-0.51	-0.85	-0.47	-0.69	-0.55	-0.04	-0.11	0.20	
-0.65	-0.69	-0.88	-0.55	-0.04	0.38	0.41	0.25	
-0.78	-0.65	-0.64	-0.68	-0.16	0.25	0.56	0.67	
-0.69	-0.64	-0.50	-0.36	0.15	0.67	0.70	0.74	
map_weights [-0.24358499 -0.21609052 -0.49703694 -0.45449424 -0.66356323 -0.00665915]
MAP reward
-0.66	-0.22	-0.50	-0.45	-0.22	-0.66	-0.66	-0.24	
-0.50	-0.22	-0.45	-0.22	-0.24	-0.45	-0.50	-0.45	
-0.22	-0.24	-0.24	-0.50	-0.66	-0.50	-0.22	-0.66	
-0.24	-0.24	-0.22	-0.45	-0.22	-0.45	-0.66	-0.24	
-0.45	-0.50	-0.45	-0.50	-0.66	-0.50	-0.66	-0.45	
-0.22	-0.45	-0.50	-0.66	-0.50	-0.24	-0.22	-0.50	
-0.22	-0.24	-0.22	-0.66	-0.50	-0.50	-0.22	-0.45	
-0.45	-0.22	-0.22	-0.66	-0.66	-0.24	-0.24	-0.01	
Map policy
>	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.850768375845119
mean w [-0.17262711 -0.16944067 -0.47635003 -0.322846   -0.57303547 -0.00563233]
Mean policy from posterior
>	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	^	^	>	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.57	-0.17	-0.48	-0.32	-0.17	-0.57	-0.57	-0.17	
-0.48	-0.17	-0.32	-0.17	-0.17	-0.32	-0.48	-0.32	
-0.17	-0.17	-0.17	-0.48	-0.57	-0.48	-0.17	-0.57	
-0.17	-0.17	-0.17	-0.32	-0.17	-0.32	-0.57	-0.17	
-0.32	-0.48	-0.32	-0.48	-0.57	-0.48	-0.57	-0.32	
-0.17	-0.32	-0.48	-0.57	-0.48	-0.17	-0.17	-0.48	
-0.17	-0.17	-0.17	-0.57	-0.48	-0.48	-0.17	-0.32	
-0.32	-0.17	-0.17	-0.57	-0.57	-0.17	-0.17	-0.01	
mean = 0.062042416296698766, map = 0.0743985964819292
CVaR policy
>	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	v	^	>	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
v	^	^	>	v	v	v	v	
v	v	>	>	>	>	v	v	
>	>	v	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.07439858797675453, 0.07439858377516345, 0.07439858796935267, 0.06609319346939607, 0.062042411528321184
==========
iteration 60
==========
weights [-0.24478746 -0.68042635 -0.21964141 -0.62989695 -0.14757166  0.10153406]
expeced value MDP LP -2.078471539219937
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 1), (27, 1), (28, 1), (29, 3), (37, 3), (45, 3), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.45975242  0.78273854 -0.0420545   0.3728031  -0.15950359 -0.09877227]
w_map [-0.26233724 -0.75283878 -0.14303842 -0.51369965 -0.27832326 -0.0510077 ] loglik -7.517471090068284e-10
accepted/total = 1830/3000 = 0.61
-------
true weights [-0.24478746 -0.68042635 -0.21964141 -0.62989695 -0.14757166  0.10153406]
features
1 	4 	3 	3 	0 	3 	3 	1 	
3 	1 	2 	0 	0 	2 	2 	1 	
1 	1 	0 	1 	1 	1 	4 	3 	
4 	4 	2 	2 	4 	4 	3 	4 	
0 	4 	1 	3 	3 	0 	0 	3 	
0 	1 	1 	0 	0 	0 	1 	1 	
4 	1 	1 	3 	1 	3 	4 	3 	
1 	0 	1 	3 	2 	1 	4 	5 	
optimal policy
>	>	v	v	v	v	v	<	
v	>	v	<	v	>	v	<	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	^	v	v	v	v	<	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.76	-3.11	-3.00	-3.21	-2.67	-2.87	-2.67	-3.32	
-3.48	-3.05	-2.39	-2.61	-2.45	-2.26	-2.06	-2.72	
-2.88	-2.75	-2.19	-2.43	-2.23	-2.09	-1.86	-2.47	
-2.22	-2.09	-1.97	-1.76	-1.56	-1.43	-1.73	-1.86	
-2.44	-2.22	-2.63	-2.14	-1.91	-1.29	-1.11	-1.73	
-2.66	-2.85	-2.19	-1.52	-1.29	-1.06	-0.87	-1.20	
-2.79	-3.10	-2.77	-2.11	-1.49	-0.82	-0.19	-0.53	
-3.10	-2.45	-2.22	-1.56	-0.94	-0.73	-0.05	0.10	
map_weights [-0.26233724 -0.75283878 -0.14303842 -0.51369965 -0.27832326 -0.0510077 ]
MAP reward
-0.75	-0.28	-0.51	-0.51	-0.26	-0.51	-0.51	-0.75	
-0.51	-0.75	-0.14	-0.26	-0.26	-0.14	-0.14	-0.75	
-0.75	-0.75	-0.26	-0.75	-0.75	-0.75	-0.28	-0.51	
-0.28	-0.28	-0.14	-0.14	-0.28	-0.28	-0.51	-0.28	
-0.26	-0.28	-0.75	-0.51	-0.51	-0.26	-0.26	-0.51	
-0.26	-0.75	-0.75	-0.26	-0.26	-0.26	-0.75	-0.75	
-0.28	-0.75	-0.75	-0.51	-0.75	-0.51	-0.28	-0.51	
-0.75	-0.26	-0.75	-0.51	-0.14	-0.75	-0.28	-0.05	
Map policy
>	>	v	v	v	v	v	<	
>	>	v	<	>	>	v	<	
v	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.658057265444508
mean w [-0.25808489 -0.69381706 -0.1756694  -0.3663291  -0.15869035  0.2656898 ]
Mean policy from posterior
>	>	v	v	v	v	v	<	
v	>	v	<	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
^	^	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.69	-0.16	-0.37	-0.37	-0.26	-0.37	-0.37	-0.69	
-0.37	-0.69	-0.18	-0.26	-0.26	-0.18	-0.18	-0.69	
-0.69	-0.69	-0.26	-0.69	-0.69	-0.69	-0.16	-0.37	
-0.16	-0.16	-0.18	-0.18	-0.16	-0.16	-0.37	-0.16	
-0.26	-0.16	-0.69	-0.37	-0.37	-0.26	-0.26	-0.37	
-0.26	-0.69	-0.69	-0.26	-0.26	-0.26	-0.69	-0.69	
-0.16	-0.69	-0.69	-0.37	-0.69	-0.37	-0.16	-0.37	
-0.69	-0.26	-0.69	-0.37	-0.18	-0.69	-0.16	0.27	
mean = 0.016335626468379605, map = 0.014164623520207531
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
v	>	v	>	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
v	>	v	<	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
v	>	v	<	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
v	>	v	<	>	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
^	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.044763766055181176, 0.027441119343449838, 0.017867435625507522, 0.017867450304061894, 0.01584905293925587
==========
iteration 61
==========
weights [-0.48174809 -0.04079798 -0.24348761 -0.71022041 -0.00888151  0.44997352]
expeced value MDP LP -0.6802533721258843
demonstration
[(0, 1), (1, 1), (2, 3), (10, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 3), (10, 2), (2, 3), (10, 2), (2, 2), (2, 2), (2, 3), (10, 2), (2, 2), (2, 2)]
[ 0.14813489  0.31166401 -0.53704032 -0.23912423  0.73051241  0.0409942 ]
w_map [-0.37944024 -0.1904149  -0.09352144 -0.5160692   0.70714768 -0.21127175] loglik -9.704058338425966
accepted/total = 2774/3000 = 0.9246666666666666
-------
true weights [-0.48174809 -0.04079798 -0.24348761 -0.71022041 -0.00888151  0.44997352]
features
0 	1 	4 	0 	1 	3 	4 	2 	
1 	2 	4 	3 	2 	1 	0 	4 	
0 	1 	3 	3 	3 	2 	2 	3 	
0 	3 	1 	2 	0 	4 	4 	3 	
2 	0 	4 	3 	3 	2 	0 	1 	
0 	4 	2 	0 	1 	3 	1 	2 	
4 	4 	4 	0 	0 	2 	4 	0 	
2 	2 	3 	1 	0 	4 	4 	5 	
optimal policy
>	>	v	>	v	v	v	<	
>	>	^	>	>	v	v	<	
>	^	v	v	>	v	v	<	
v	>	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	>	v	>	v	<	
>	>	>	v	>	v	v	v	
^	^	>	>	>	>	>	.	
optimal values
-1.39	-0.92	-0.89	-1.15	-0.68	-1.11	-0.84	-1.07	
-1.15	-1.12	-0.89	-1.35	-0.64	-0.40	-0.84	-0.84	
-1.62	-1.15	-1.56	-1.54	-1.07	-0.37	-0.36	-1.06	
-1.77	-1.56	-0.86	-0.84	-0.60	-0.12	-0.12	-0.62	
-1.30	-1.07	-0.82	-1.53	-1.05	-0.35	-0.11	0.09	
-1.07	-0.59	-0.82	-0.82	-0.35	-0.34	0.38	0.13	
-0.59	-0.59	-0.59	-0.58	-0.31	0.18	0.42	-0.04	
-0.83	-0.83	-0.81	-0.10	-0.06	0.42	0.44	0.45	
map_weights [-0.37944024 -0.1904149  -0.09352144 -0.5160692   0.70714768 -0.21127175]
MAP reward
-0.38	-0.19	0.71	-0.38	-0.19	-0.52	0.71	-0.09	
-0.19	-0.09	0.71	-0.52	-0.09	-0.19	-0.38	0.71	
-0.38	-0.19	-0.52	-0.52	-0.52	-0.09	-0.09	-0.52	
-0.38	-0.52	-0.19	-0.09	-0.38	0.71	0.71	-0.52	
-0.09	-0.38	0.71	-0.52	-0.52	-0.09	-0.38	-0.19	
-0.38	0.71	-0.09	-0.38	-0.19	-0.52	-0.19	-0.09	
0.71	0.71	0.71	-0.38	-0.38	-0.09	0.71	-0.38	
-0.09	-0.09	-0.52	-0.19	-0.38	0.71	0.71	-0.21	
Map policy
>	>	^	<	<	>	^	v	
>	>	^	<	<	v	>	>	
>	^	^	<	>	v	v	^	
v	v	v	>	>	>	<	<	
v	v	v	<	>	^	^	<	
v	v	<	<	v	v	v	<	
<	<	<	<	>	v	v	<	
^	^	^	>	>	>	<	.	
expeced value MDP LP 60.4594301508406
mean w [ 0.07182379  0.0115626  -0.00358739 -0.07759829  0.62145329 -0.00725039]
Mean policy from posterior
>	>	^	<	<	>	^	v	
>	>	^	<	<	>	>	>	
>	^	^	^	v	v	v	^	
v	v	v	>	>	>	<	<	
v	v	<	<	^	^	^	<	
>	v	<	v	v	>	v	v	
<	<	<	<	v	>	v	<	
^	^	^	>	>	>	<	.	
Mean rewards
0.07	0.01	0.62	0.07	0.01	-0.08	0.62	-0.00	
0.01	-0.00	0.62	-0.08	-0.00	0.01	0.07	0.62	
0.07	0.01	-0.08	-0.08	-0.08	-0.00	-0.00	-0.08	
0.07	-0.08	0.01	-0.00	0.07	0.62	0.62	-0.08	
-0.00	0.07	0.62	-0.08	-0.08	-0.00	0.07	0.01	
0.07	0.62	-0.00	0.07	0.01	-0.08	0.01	-0.00	
0.62	0.62	0.62	0.07	0.07	-0.00	0.62	0.07	
-0.00	-0.00	-0.08	0.01	0.07	0.62	0.62	-0.01	
mean = 0.6436016984254163, map = 0.6068576704893209
CVaR policy
>	>	v	<	<	>	^	v	
>	>	^	<	<	>	^	>	
v	^	^	<	v	v	v	^	
v	v	v	>	>	>	<	<	
v	v	<	<	^	^	^	<	
>	v	<	v	v	v	v	v	
>	<	<	<	v	>	v	<	
^	^	^	^	>	v	v	.	
CVaR policy
>	>	v	<	<	>	^	v	
>	>	^	<	<	>	^	>	
^	^	^	<	v	v	v	^	
v	v	v	>	>	>	<	<	
v	v	<	<	^	^	^	<	
>	v	v	v	v	v	v	v	
>	^	<	<	v	>	v	<	
^	^	^	^	>	>	<	.	
CVaR policy
>	>	^	<	<	>	^	<	
>	>	^	<	<	>	^	>	
>	^	^	<	v	v	v	^	
v	v	v	>	>	>	<	<	
v	v	<	<	^	^	^	<	
>	v	<	v	v	v	v	v	
<	<	<	<	v	>	v	<	
^	^	^	>	>	v	v	.	
CVaR policy
>	>	^	<	<	>	^	<	
>	>	^	<	<	>	^	>	
^	^	^	<	v	v	v	^	
v	v	v	>	>	>	<	<	
v	v	<	<	^	^	^	<	
>	v	<	v	v	v	v	v	
>	<	<	<	v	>	v	<	
^	^	^	>	>	>	<	.	
CVaR policy
>	>	^	<	<	>	^	v	
>	>	^	<	<	>	^	>	
^	^	^	<	v	v	v	^	
v	v	v	>	>	>	<	<	
v	v	<	<	^	^	^	<	
>	v	v	v	v	>	v	v	
>	>	<	<	v	>	v	<	
^	^	^	>	>	>	<	.	
cvar = , 0.6607271033597846, 0.6467375944124603, 0.6467370505605228, 0.6467370520223148, 0.6436017124869555
==========
iteration 62
==========
weights [-0.1408033  -0.10463389 -0.60361841 -0.16941572 -0.70980778  0.26896509]
expeced value MDP LP -1.2165179906560697
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 1), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[0.46635532 0.17025168 0.15839214 0.03863998 0.85182397 0.0366317 ]
w_map [-0.30676141 -0.34066528 -0.57485183 -0.27943241 -0.58019407  0.21138199] loglik -0.6931471806445906
accepted/total = 1687/3000 = 0.5623333333333334
-------
true weights [-0.1408033  -0.10463389 -0.60361841 -0.16941572 -0.70980778  0.26896509]
features
0 	4 	4 	0 	3 	3 	0 	2 	
2 	1 	4 	3 	0 	4 	4 	1 	
1 	1 	4 	4 	4 	3 	1 	4 	
3 	2 	2 	1 	2 	4 	2 	2 	
1 	2 	1 	1 	1 	4 	2 	1 	
0 	0 	3 	3 	4 	2 	3 	1 	
0 	4 	2 	1 	0 	0 	4 	1 	
3 	0 	2 	4 	0 	3 	3 	5 	
optimal policy
v	v	>	v	v	<	>	v	
v	v	<	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	<	v	v	v	>	v	v	
v	>	>	v	<	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	>	v	>	v	
^	<	>	>	>	>	>	.	
optimal values
-2.10	-2.26	-2.49	-1.79	-1.95	-2.10	-2.16	-2.04	
-1.98	-1.57	-2.26	-1.67	-1.79	-2.19	-2.04	-1.45	
-1.39	-1.48	-2.11	-1.52	-2.11	-1.50	-1.34	-1.36	
-1.30	-1.89	-1.41	-0.81	-1.41	-1.95	-1.25	-0.65	
-1.14	-1.41	-0.81	-0.72	-0.81	-1.36	-0.65	-0.05	
-1.05	-0.91	-0.78	-0.62	-1.06	-0.72	-0.11	0.06	
-1.18	-1.62	-1.05	-0.45	-0.35	-0.21	-0.55	0.16	
-1.33	-1.46	-1.52	-0.92	-0.21	-0.07	0.10	0.27	
map_weights [-0.30676141 -0.34066528 -0.57485183 -0.27943241 -0.58019407  0.21138199]
MAP reward
-0.31	-0.58	-0.58	-0.31	-0.28	-0.28	-0.31	-0.57	
-0.57	-0.34	-0.58	-0.28	-0.31	-0.58	-0.58	-0.34	
-0.34	-0.34	-0.58	-0.58	-0.58	-0.28	-0.34	-0.58	
-0.28	-0.57	-0.57	-0.34	-0.57	-0.58	-0.57	-0.57	
-0.34	-0.57	-0.34	-0.34	-0.34	-0.58	-0.57	-0.34	
-0.31	-0.31	-0.28	-0.28	-0.58	-0.57	-0.28	-0.34	
-0.31	-0.58	-0.57	-0.34	-0.31	-0.31	-0.58	-0.34	
-0.28	-0.31	-0.57	-0.58	-0.31	-0.28	-0.28	0.21	
Map policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.356127579428982
mean w [-0.13333013 -0.22063152 -0.40540466 -0.13613485 -0.52682797  0.07875809]
Mean policy from posterior
v	v	>	v	v	<	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	^	>	>	v	v	v	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.13	-0.53	-0.53	-0.13	-0.14	-0.14	-0.13	-0.41	
-0.41	-0.22	-0.53	-0.14	-0.13	-0.53	-0.53	-0.22	
-0.22	-0.22	-0.53	-0.53	-0.53	-0.14	-0.22	-0.53	
-0.14	-0.41	-0.41	-0.22	-0.41	-0.53	-0.41	-0.41	
-0.22	-0.41	-0.22	-0.22	-0.22	-0.53	-0.41	-0.22	
-0.13	-0.13	-0.14	-0.14	-0.53	-0.41	-0.14	-0.22	
-0.13	-0.53	-0.41	-0.22	-0.13	-0.13	-0.53	-0.22	
-0.14	-0.13	-0.41	-0.53	-0.13	-0.14	-0.14	0.08	
mean = 0.04388679491933534, map = 0.1256779582113925
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	v	v	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	>	v	v	
v	v	>	v	<	v	v	v	
v	<	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
cvar = , 0.1251517681081482, 0.1135783024139927, 0.05746376865115477, 0.05029889529030873, 0.05027324446908432
==========
iteration 63
==========
weights [-0.53480199 -0.71108202 -0.26845986 -0.03036318 -0.15956608  0.33150451]
expeced value MDP LP -1.3665223606979628
demonstration
[(0, 1), (1, 1), (2, 1), (3, 3), (11, 3), (19, 3), (27, 3), (35, 3), (43, 1), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.39781786 -0.31661101 -0.31383946 -0.09242407 -0.55954016  0.56690018]
w_map [-0.50879578 -0.65149107 -0.45190171 -0.23748676 -0.22217757 -0.0819039 ] loglik -1.0315787335457571e-08
accepted/total = 1807/3000 = 0.6023333333333334
-------
true weights [-0.53480199 -0.71108202 -0.26845986 -0.03036318 -0.15956608  0.33150451]
features
1 	0 	4 	3 	2 	3 	2 	4 	
1 	1 	2 	4 	3 	0 	3 	0 	
2 	3 	0 	3 	0 	1 	3 	4 	
1 	2 	2 	3 	2 	1 	2 	4 	
4 	2 	2 	4 	1 	0 	3 	1 	
0 	0 	0 	2 	3 	4 	4 	3 	
1 	3 	2 	2 	1 	2 	1 	1 	
2 	3 	3 	0 	1 	0 	0 	5 	
optimal policy
>	>	>	v	>	>	v	<	
v	>	>	v	<	>	v	<	
>	v	>	v	<	>	v	<	
>	>	>	v	<	>	v	<	
>	>	>	v	v	>	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	^	^	v	v	
>	>	^	^	>	>	>	.	
optimal values
-2.71	-2.02	-1.50	-1.35	-1.43	-1.17	-1.16	-1.30	
-2.64	-2.29	-1.59	-1.34	-1.35	-1.42	-0.90	-1.42	
-1.95	-1.70	-1.71	-1.19	-1.71	-1.58	-0.87	-1.02	
-2.38	-1.68	-1.43	-1.17	-1.43	-1.55	-0.85	-1.00	
-1.81	-1.66	-1.41	-1.15	-1.45	-1.12	-0.59	-1.12	
-2.32	-2.05	-1.53	-1.00	-0.74	-0.72	-0.56	-0.41	
-2.23	-1.53	-1.52	-1.26	-1.45	-0.98	-0.92	-0.38	
-1.80	-1.55	-1.53	-1.78	-1.44	-0.74	-0.21	0.33	
map_weights [-0.50879578 -0.65149107 -0.45190171 -0.23748676 -0.22217757 -0.0819039 ]
MAP reward
-0.65	-0.51	-0.22	-0.24	-0.45	-0.24	-0.45	-0.22	
-0.65	-0.65	-0.45	-0.22	-0.24	-0.51	-0.24	-0.51	
-0.45	-0.24	-0.51	-0.24	-0.51	-0.65	-0.24	-0.22	
-0.65	-0.45	-0.45	-0.24	-0.45	-0.65	-0.45	-0.22	
-0.22	-0.45	-0.45	-0.22	-0.65	-0.51	-0.24	-0.65	
-0.51	-0.51	-0.51	-0.45	-0.24	-0.22	-0.22	-0.24	
-0.65	-0.24	-0.45	-0.45	-0.65	-0.45	-0.65	-0.65	
-0.45	-0.24	-0.24	-0.51	-0.65	-0.51	-0.51	-0.08	
Map policy
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	^	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7388957991925302
mean w [-0.50863967 -0.56664722 -0.40147413 -0.16520312 -0.15217649  0.28773038]
Mean policy from posterior
>	>	>	v	>	>	v	v	
v	>	>	v	<	>	v	v	
>	>	>	v	<	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	^	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.57	-0.51	-0.15	-0.17	-0.40	-0.17	-0.40	-0.15	
-0.57	-0.57	-0.40	-0.15	-0.17	-0.51	-0.17	-0.51	
-0.40	-0.17	-0.51	-0.17	-0.51	-0.57	-0.17	-0.15	
-0.57	-0.40	-0.40	-0.17	-0.40	-0.57	-0.40	-0.15	
-0.15	-0.40	-0.40	-0.15	-0.57	-0.51	-0.17	-0.57	
-0.51	-0.51	-0.51	-0.40	-0.17	-0.15	-0.15	-0.17	
-0.57	-0.17	-0.40	-0.40	-0.57	-0.40	-0.57	-0.57	
-0.40	-0.17	-0.17	-0.51	-0.57	-0.51	-0.51	0.29	
mean = 0.06705545361332144, map = 0.10564540463395655
CVaR policy
>	>	>	v	>	>	v	v	
>	>	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
v	>	>	v	>	>	v	v	
>	>	>	v	v	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	^	^	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
v	>	>	v	<	>	v	v	
>	>	>	v	<	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	^	^	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
v	>	>	v	<	>	v	v	
>	>	>	v	<	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	v	>	^	^	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
v	>	>	v	<	>	v	v	
>	>	>	v	<	>	v	v	
v	>	>	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	^	^	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.12735249116234737, 0.09062383709225319, 0.08144500146927625, 0.08144500069164451, 0.06705545361838916
==========
iteration 64
==========
weights [-0.48701902 -0.16726154 -0.51772544 -0.39916227 -0.45686498  0.31422967]
expeced value MDP LP -2.404170670378075
demonstration
[(0, 3), (8, 3), (16, 1), (17, 3), (25, 3), (33, 3), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.60556872  0.05558246 -0.52917474  0.05093109 -0.25016768  0.53384771]
w_map [-0.6245196  -0.21882211 -0.59852728 -0.226717   -0.39020022 -0.01415754] loglik -0.6931473599664741
accepted/total = 1729/3000 = 0.5763333333333334
-------
true weights [-0.48701902 -0.16726154 -0.51772544 -0.39916227 -0.45686498  0.31422967]
features
1 	3 	2 	1 	2 	0 	0 	3 	
1 	2 	2 	4 	2 	2 	2 	3 	
3 	1 	0 	4 	4 	0 	0 	0 	
4 	1 	4 	3 	1 	2 	2 	2 	
1 	1 	0 	0 	0 	4 	3 	2 	
1 	3 	4 	1 	0 	4 	4 	2 	
2 	3 	2 	4 	1 	2 	4 	0 	
3 	0 	2 	2 	2 	3 	4 	5 	
optimal policy
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.68	-3.89	-4.12	-3.64	-3.71	-3.79	-3.36	-2.93	
-3.54	-3.53	-3.98	-3.51	-3.23	-3.33	-2.91	-2.56	
-3.41	-3.04	-3.50	-3.08	-2.74	-2.85	-2.41	-2.18	
-3.33	-2.90	-3.08	-2.65	-2.30	-2.38	-1.94	-1.71	
-2.90	-2.76	-2.71	-2.28	-2.16	-1.88	-1.44	-1.20	
-2.76	-2.62	-2.25	-1.81	-1.69	-1.50	-1.05	-0.69	
-3.03	-2.54	-2.16	-1.66	-1.21	-1.06	-0.60	-0.18	
-2.91	-2.53	-2.07	-1.56	-1.06	-0.54	-0.15	0.31	
map_weights [-0.6245196  -0.21882211 -0.59852728 -0.226717   -0.39020022 -0.01415754]
MAP reward
-0.22	-0.23	-0.60	-0.22	-0.60	-0.62	-0.62	-0.23	
-0.22	-0.60	-0.60	-0.39	-0.60	-0.60	-0.60	-0.23	
-0.23	-0.22	-0.62	-0.39	-0.39	-0.62	-0.62	-0.62	
-0.39	-0.22	-0.39	-0.23	-0.22	-0.60	-0.60	-0.60	
-0.22	-0.22	-0.62	-0.62	-0.62	-0.39	-0.23	-0.60	
-0.22	-0.23	-0.39	-0.22	-0.62	-0.39	-0.39	-0.60	
-0.60	-0.23	-0.60	-0.39	-0.22	-0.60	-0.39	-0.62	
-0.23	-0.62	-0.60	-0.60	-0.60	-0.23	-0.39	-0.01	
Map policy
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.1362239566082146
mean w [-0.61363569 -0.14542962 -0.52725804 -0.18352937 -0.3323282   0.05257835]
Mean policy from posterior
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.15	-0.18	-0.53	-0.15	-0.53	-0.61	-0.61	-0.18	
-0.15	-0.53	-0.53	-0.33	-0.53	-0.53	-0.53	-0.18	
-0.18	-0.15	-0.61	-0.33	-0.33	-0.61	-0.61	-0.61	
-0.33	-0.15	-0.33	-0.18	-0.15	-0.53	-0.53	-0.53	
-0.15	-0.15	-0.61	-0.61	-0.61	-0.33	-0.18	-0.53	
-0.15	-0.18	-0.33	-0.15	-0.61	-0.33	-0.33	-0.53	
-0.53	-0.18	-0.53	-0.33	-0.15	-0.53	-0.33	-0.61	
-0.18	-0.61	-0.53	-0.53	-0.53	-0.18	-0.33	0.05	
mean = 0.04553248278208777, map = 0.0499410522414796
CVaR policy
v	v	>	v	v	v	>	v	
v	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	<	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	<	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.004377358500204487, 0.016937291000450383, 0.0455324963215773, 0.04553248285543665, 0.045532482901502025
==========
iteration 65
==========
weights [-0.50408971 -0.18216383 -0.26848375 -0.1616642  -0.43175195  0.65427924]
expeced value MDP LP -1.0195725831184053
demonstration
[(0, 3), (8, 1), (9, 3), (17, 3), (25, 1), (26, 1), (27, 1), (28, 1), (29, 3), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.16733801  0.21040627  0.6301979  -0.70116119  0.15985623  0.11574441]
w_map [-0.59555787 -0.28544659 -0.3929467  -0.14190451 -0.59126756  0.19922285] loglik -2.591333280577146e-05
accepted/total = 1277/3000 = 0.4256666666666667
-------
true weights [-0.50408971 -0.18216383 -0.26848375 -0.1616642  -0.43175195  0.65427924]
features
0 	4 	3 	2 	3 	3 	4 	4 	
2 	3 	4 	4 	3 	0 	0 	4 	
4 	3 	0 	3 	4 	4 	4 	0 	
4 	2 	3 	1 	2 	3 	2 	0 	
3 	2 	1 	2 	4 	1 	3 	3 	
0 	1 	3 	1 	4 	4 	2 	2 	
2 	4 	2 	0 	0 	3 	1 	2 	
4 	2 	2 	1 	3 	3 	4 	5 	
optimal policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.43	-2.11	-1.93	-1.79	-1.54	-1.62	-1.82	-1.90	
-1.95	-1.69	-1.98	-1.56	-1.39	-1.48	-1.41	-1.48	
-1.96	-1.55	-1.64	-1.14	-1.24	-0.98	-0.91	-1.06	
-1.82	-1.40	-1.14	-0.99	-0.82	-0.55	-0.48	-0.56	
-1.55	-1.40	-1.15	-1.08	-0.82	-0.40	-0.22	-0.06	
-1.64	-1.15	-0.97	-0.97	-0.83	-0.40	-0.08	0.11	
-1.50	-1.24	-0.82	-0.79	-0.47	0.03	0.19	0.38	
-1.24	-0.82	-0.56	-0.29	-0.11	0.05	0.22	0.65	
map_weights [-0.59555787 -0.28544659 -0.3929467  -0.14190451 -0.59126756  0.19922285]
MAP reward
-0.60	-0.59	-0.14	-0.39	-0.14	-0.14	-0.59	-0.59	
-0.39	-0.14	-0.59	-0.59	-0.14	-0.60	-0.60	-0.59	
-0.59	-0.14	-0.60	-0.14	-0.59	-0.59	-0.59	-0.60	
-0.59	-0.39	-0.14	-0.29	-0.39	-0.14	-0.39	-0.60	
-0.14	-0.39	-0.29	-0.39	-0.59	-0.29	-0.14	-0.14	
-0.60	-0.29	-0.14	-0.29	-0.59	-0.59	-0.39	-0.39	
-0.39	-0.59	-0.39	-0.60	-0.60	-0.14	-0.29	-0.39	
-0.59	-0.39	-0.39	-0.29	-0.14	-0.14	-0.59	0.20	
Map policy
v	v	>	>	v	v	v	v	
>	v	>	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6222032646955586
mean w [-0.515291   -0.19904774 -0.28048438 -0.13365066 -0.51451446  0.0452529 ]
Mean policy from posterior
v	v	>	>	v	<	v	v	
>	v	<	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	>	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.52	-0.51	-0.13	-0.28	-0.13	-0.13	-0.51	-0.51	
-0.28	-0.13	-0.51	-0.51	-0.13	-0.52	-0.52	-0.51	
-0.51	-0.13	-0.52	-0.13	-0.51	-0.51	-0.51	-0.52	
-0.51	-0.28	-0.13	-0.20	-0.28	-0.13	-0.28	-0.52	
-0.13	-0.28	-0.20	-0.28	-0.51	-0.20	-0.13	-0.13	
-0.52	-0.20	-0.13	-0.20	-0.51	-0.51	-0.28	-0.28	
-0.28	-0.51	-0.28	-0.52	-0.52	-0.13	-0.20	-0.28	
-0.51	-0.28	-0.28	-0.20	-0.13	-0.13	-0.51	0.05	
mean = 0.0029950471808439794, map = 3.770183054641052e-09
CVaR policy
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	v	v	
>	>	v	v	v	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	v	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	v	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	v	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	>	>	>	>	v	
>	>	v	v	v	v	v	v	
>	v	v	v	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.007288184974403311, 0.0022379495109001724, 0.0022379745859306688, 0.0022379540144927823, 0.002237953458650299
==========
iteration 66
==========
weights [-0.30989402 -0.1210639  -0.0412376  -0.20132667 -0.15194379  0.90773859]
expeced value MDP LP 0.00484872204110065
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 1), (26, 1), (27, 3), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 3), (62, 1), (63, None)]
[ 0.06003498 -0.52868884  0.33663041  0.5038062   0.50151385  0.31341194]
w_map [-0.65715009 -0.2396845  -0.262562   -0.44705306 -0.47896834  0.11179978] loglik -0.6931472237549148
accepted/total = 1699/3000 = 0.5663333333333334
-------
true weights [-0.30989402 -0.1210639  -0.0412376  -0.20132667 -0.15194379  0.90773859]
features
0 	4 	4 	1 	4 	0 	4 	2 	
4 	2 	0 	3 	2 	0 	1 	0 	
1 	4 	1 	1 	1 	3 	3 	0 	
2 	2 	1 	1 	0 	4 	3 	2 	
0 	4 	4 	4 	2 	1 	4 	3 	
1 	4 	4 	2 	3 	1 	2 	4 	
1 	4 	0 	1 	4 	3 	1 	1 	
3 	0 	2 	0 	1 	3 	1 	5 	
optimal policy
v	v	>	v	v	>	v	<	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-0.75	-0.47	-0.56	-0.41	-0.32	-0.55	-0.24	-0.28	
-0.44	-0.32	-0.52	-0.30	-0.17	-0.32	-0.09	-0.26	
-0.29	-0.29	-0.22	-0.10	-0.13	-0.01	0.03	0.05	
-0.18	-0.14	-0.10	0.03	-0.01	0.19	0.24	0.37	
-0.47	-0.16	-0.00	0.15	0.30	0.35	0.44	0.41	
-0.20	-0.08	0.07	0.22	0.27	0.47	0.60	0.62	
-0.32	-0.23	-0.15	0.16	0.29	0.44	0.65	0.78	
-0.42	-0.23	0.08	0.13	0.44	0.57	0.78	0.91	
map_weights [-0.65715009 -0.2396845  -0.262562   -0.44705306 -0.47896834  0.11179978]
MAP reward
-0.66	-0.48	-0.48	-0.24	-0.48	-0.66	-0.48	-0.26	
-0.48	-0.26	-0.66	-0.45	-0.26	-0.66	-0.24	-0.66	
-0.24	-0.48	-0.24	-0.24	-0.24	-0.45	-0.45	-0.66	
-0.26	-0.26	-0.24	-0.24	-0.66	-0.48	-0.45	-0.26	
-0.66	-0.48	-0.48	-0.48	-0.26	-0.24	-0.48	-0.45	
-0.24	-0.48	-0.48	-0.26	-0.45	-0.24	-0.26	-0.48	
-0.24	-0.48	-0.66	-0.24	-0.48	-0.45	-0.24	-0.24	
-0.45	-0.66	-0.26	-0.66	-0.24	-0.45	-0.24	0.11	
Map policy
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.5003048427408467
mean w [-0.62024967 -0.16358279 -0.13024822 -0.46785666 -0.36559569  0.07823059]
Mean policy from posterior
v	v	>	v	v	<	v	<	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	^	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.62	-0.37	-0.37	-0.16	-0.37	-0.62	-0.37	-0.13	
-0.37	-0.13	-0.62	-0.47	-0.13	-0.62	-0.16	-0.62	
-0.16	-0.37	-0.16	-0.16	-0.16	-0.47	-0.47	-0.62	
-0.13	-0.13	-0.16	-0.16	-0.62	-0.37	-0.47	-0.13	
-0.62	-0.37	-0.37	-0.37	-0.13	-0.16	-0.37	-0.47	
-0.16	-0.37	-0.37	-0.13	-0.47	-0.16	-0.13	-0.37	
-0.16	-0.37	-0.62	-0.16	-0.37	-0.47	-0.16	-0.16	
-0.47	-0.62	-0.13	-0.62	-0.16	-0.47	-0.16	0.08	
mean = 0.0015546490949936986, map = 0.008499296367037001
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	<	v	<	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	<	v	<	
v	v	v	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.009558298570523815, 0.004881701011152595, 0.002899624803784695, 0.0025940244748118846, 0.002594023792861666
==========
iteration 67
==========
weights [-0.11013276 -0.67319968 -0.20101732 -0.12934998 -0.68243955  0.1086731 ]
expeced value MDP LP -1.7041313103468287
demonstration
[(0, 1), (1, 3), (9, 3), (17, 3), (25, 1), (26, 3), (34, 1), (35, 1), (36, 1), (37, 3), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.09768096 -0.1975525   0.3828673   0.14473259  0.7648078   0.44605557]
w_map [-0.12833563 -0.54157991 -0.43169392 -0.16302387 -0.68160286  0.11270442] loglik -3.115441808532182e-05
accepted/total = 762/3000 = 0.254
-------
true weights [-0.11013276 -0.67319968 -0.20101732 -0.12934998 -0.68243955  0.1086731 ]
features
3 	1 	0 	4 	4 	0 	4 	1 	
4 	3 	2 	1 	1 	3 	3 	2 	
1 	2 	4 	1 	0 	3 	3 	3 	
2 	0 	3 	4 	1 	1 	1 	3 	
0 	4 	1 	2 	2 	3 	3 	4 	
1 	2 	1 	4 	4 	0 	1 	2 	
4 	0 	0 	0 	1 	3 	4 	0 	
0 	1 	4 	2 	3 	1 	2 	5 	
optimal policy
>	v	v	>	>	v	v	v	
>	v	<	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
optimal values
-3.29	-3.20	-2.81	-2.90	-2.24	-1.57	-2.03	-1.97	
-3.21	-2.55	-2.72	-2.77	-2.11	-1.47	-1.36	-1.31	
-3.09	-2.44	-2.78	-2.11	-1.46	-1.36	-1.24	-1.12	
-2.44	-2.27	-2.18	-2.08	-1.88	-1.69	-1.66	-1.00	
-2.31	-2.23	-2.07	-1.41	-1.22	-1.03	-1.00	-0.88	
-2.22	-1.56	-1.94	-1.85	-1.58	-0.91	-0.87	-0.20	
-2.04	-1.37	-1.28	-1.18	-1.47	-0.81	-0.68	-0.00	
-2.12	-2.03	-1.75	-1.08	-0.89	-0.77	-0.09	0.11	
map_weights [-0.12833563 -0.54157991 -0.43169392 -0.16302387 -0.68160286  0.11270442]
MAP reward
-0.16	-0.54	-0.13	-0.68	-0.68	-0.13	-0.68	-0.54	
-0.68	-0.16	-0.43	-0.54	-0.54	-0.16	-0.16	-0.43	
-0.54	-0.43	-0.68	-0.54	-0.13	-0.16	-0.16	-0.16	
-0.43	-0.13	-0.16	-0.68	-0.54	-0.54	-0.54	-0.16	
-0.13	-0.68	-0.54	-0.43	-0.43	-0.16	-0.16	-0.68	
-0.54	-0.43	-0.54	-0.68	-0.68	-0.13	-0.54	-0.43	
-0.68	-0.13	-0.13	-0.13	-0.54	-0.16	-0.68	-0.13	
-0.13	-0.54	-0.68	-0.43	-0.16	-0.54	-0.43	0.11	
Map policy
>	v	v	>	>	v	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	.	
expeced value MDP LP -1.0866108589245251
mean w [-0.10969363 -0.43743997 -0.27352983 -0.11932211 -0.51185748  0.52560678]
Mean policy from posterior
>	v	v	>	>	v	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
Mean rewards
-0.12	-0.44	-0.11	-0.51	-0.51	-0.11	-0.51	-0.44	
-0.51	-0.12	-0.27	-0.44	-0.44	-0.12	-0.12	-0.27	
-0.44	-0.27	-0.51	-0.44	-0.11	-0.12	-0.12	-0.12	
-0.27	-0.11	-0.12	-0.51	-0.44	-0.44	-0.44	-0.12	
-0.11	-0.51	-0.44	-0.27	-0.27	-0.12	-0.12	-0.51	
-0.44	-0.27	-0.44	-0.51	-0.51	-0.11	-0.44	-0.27	
-0.51	-0.11	-0.11	-0.11	-0.44	-0.12	-0.51	-0.11	
-0.11	-0.44	-0.51	-0.27	-0.12	-0.44	-0.27	0.53	
mean = 0.006690265623888303, map = 0.07772870605113247
CVaR policy
>	v	>	>	>	v	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	v	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	v	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	v	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	v	>	>	>	v	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	>	>	>	v	
>	>	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
>	v	v	v	>	v	>	v	
>	>	>	v	>	>	>	v	
>	^	>	>	>	>	>	.	
cvar = , 0.07692202397507342, 0.006030599254885249, 0.00603058946519619, 0.006030591061481516, 0.006030589421432753
==========
iteration 68
==========
weights [-0.4487679  -0.35687817 -0.53436972 -0.42553897 -0.44563112  0.07761331]
expeced value MDP LP -2.8139914937535053
demonstration
[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 3), (14, 3), (22, 3), (30, 3), (38, 3), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.16446239  0.58860583  0.09861887  0.33138051 -0.70777474 -0.07753377]
w_map [-0.40743196 -0.21060482 -0.59155939 -0.39456917 -0.52199057  0.1074398 ] loglik -9.869005168638978e-08
accepted/total = 1420/3000 = 0.47333333333333333
-------
true weights [-0.4487679  -0.35687817 -0.53436972 -0.42553897 -0.44563112  0.07761331]
features
3 	4 	1 	1 	3 	3 	1 	4 	
4 	0 	4 	0 	4 	2 	1 	2 	
2 	3 	3 	2 	1 	4 	3 	3 	
3 	1 	4 	4 	0 	4 	4 	3 	
4 	2 	1 	0 	3 	2 	1 	2 	
3 	3 	4 	1 	4 	0 	2 	3 	
2 	0 	4 	3 	2 	0 	1 	0 	
2 	0 	3 	3 	2 	4 	2 	5 	
optimal policy
>	>	>	>	>	>	v	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	v	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-5.29	-4.91	-4.51	-4.20	-3.88	-3.49	-3.09	-3.07	
-5.05	-4.65	-4.33	-4.01	-3.59	-3.27	-2.76	-2.65	
-4.74	-4.24	-3.93	-3.68	-3.18	-2.85	-2.43	-2.14	
-4.24	-3.86	-3.54	-3.21	-2.86	-2.45	-2.03	-1.73	
-3.99	-3.62	-3.12	-2.79	-2.44	-2.11	-1.60	-1.32	
-3.58	-3.19	-2.79	-2.37	-2.03	-1.60	-1.25	-0.79	
-3.45	-2.95	-2.52	-2.10	-1.69	-1.17	-0.73	-0.37	
-3.18	-2.67	-2.24	-1.84	-1.42	-0.90	-0.46	0.08	
map_weights [-0.40743196 -0.21060482 -0.59155939 -0.39456917 -0.52199057  0.1074398 ]
MAP reward
-0.39	-0.52	-0.21	-0.21	-0.39	-0.39	-0.21	-0.52	
-0.52	-0.41	-0.52	-0.41	-0.52	-0.59	-0.21	-0.59	
-0.59	-0.39	-0.39	-0.59	-0.21	-0.52	-0.39	-0.39	
-0.39	-0.21	-0.52	-0.52	-0.41	-0.52	-0.52	-0.39	
-0.52	-0.59	-0.21	-0.41	-0.39	-0.59	-0.21	-0.59	
-0.39	-0.39	-0.52	-0.21	-0.52	-0.41	-0.59	-0.39	
-0.59	-0.41	-0.52	-0.39	-0.59	-0.41	-0.21	-0.41	
-0.59	-0.41	-0.39	-0.39	-0.59	-0.52	-0.59	0.11	
Map policy
>	>	>	>	>	>	v	<	
>	v	v	>	v	>	v	v	
v	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.2004800593641196
mean w [-0.42490864 -0.12817769 -0.58989729 -0.298593   -0.3871758   0.034523  ]
Mean policy from posterior
>	>	>	>	>	>	v	<	
>	v	^	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.30	-0.39	-0.13	-0.13	-0.30	-0.30	-0.13	-0.39	
-0.39	-0.42	-0.39	-0.42	-0.39	-0.59	-0.13	-0.59	
-0.59	-0.30	-0.30	-0.59	-0.13	-0.39	-0.30	-0.30	
-0.30	-0.13	-0.39	-0.39	-0.42	-0.39	-0.39	-0.30	
-0.39	-0.59	-0.13	-0.42	-0.30	-0.59	-0.13	-0.59	
-0.30	-0.30	-0.39	-0.13	-0.39	-0.42	-0.59	-0.30	
-0.59	-0.42	-0.39	-0.30	-0.59	-0.42	-0.13	-0.42	
-0.59	-0.42	-0.30	-0.30	-0.59	-0.39	-0.59	0.03	
mean = 0.016070901205806898, map = 0.008616886846067207
CVaR policy
>	>	>	>	>	>	v	v	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
v	>	v	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	<	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	<	
>	v	v	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	<	
>	v	^	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	>	>	>	v	<	
>	v	^	>	v	>	v	v	
v	v	v	>	>	>	v	v	
>	>	v	v	>	>	v	v	
v	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.0010137854385581235, 0.0070027295330832295, 0.007002729487799897, 0.016070901390475623, 0.01607090119786969
==========
iteration 69
==========
weights [-0.37050983 -0.25833116 -0.23439414 -0.08713256 -0.79646328  0.31480316]
expeced value MDP LP -1.8806386965914959
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 1), (19, 1), (20, 3), (28, 1), (29, 3), (37, 1), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.67713008  0.27295679  0.61055319  0.17309622  0.21082341  0.14073181]
w_map [-0.30148246 -0.49676769 -0.40627811 -0.19009077 -0.67774739  0.04233517] loglik -0.693282943041396
accepted/total = 1087/3000 = 0.36233333333333334
-------
true weights [-0.37050983 -0.25833116 -0.23439414 -0.08713256 -0.79646328  0.31480316]
features
3 	2 	2 	2 	1 	4 	0 	4 	
4 	0 	4 	4 	0 	4 	4 	3 	
2 	2 	1 	3 	3 	4 	3 	1 	
4 	2 	3 	2 	0 	1 	2 	0 	
3 	2 	1 	0 	1 	3 	1 	1 	
4 	1 	4 	2 	2 	4 	3 	2 	
3 	2 	2 	0 	1 	2 	1 	4 	
0 	0 	4 	1 	0 	0 	4 	5 	
optimal policy
>	v	>	>	v	<	v	v	
v	v	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	v	v	
^	^	>	>	>	>	>	.	
optimal values
-2.93	-2.87	-2.80	-2.60	-2.38	-3.16	-2.48	-2.42	
-3.30	-2.66	-2.88	-2.64	-2.15	-2.90	-2.13	-1.64	
-2.53	-2.32	-2.10	-1.86	-1.80	-2.13	-1.34	-1.57	
-3.00	-2.22	-2.01	-1.94	-1.73	-1.37	-1.27	-1.33	
-2.25	-2.18	-1.97	-1.73	-1.37	-1.12	-1.04	-0.97	
-2.85	-2.25	-2.44	-1.66	-1.44	-1.58	-0.79	-0.71	
-2.07	-2.01	-1.79	-1.57	-1.21	-0.97	-0.74	-0.48	
-2.42	-2.36	-2.24	-1.46	-1.21	-0.85	-0.48	0.31	
map_weights [-0.30148246 -0.49676769 -0.40627811 -0.19009077 -0.67774739  0.04233517]
MAP reward
-0.19	-0.41	-0.41	-0.41	-0.50	-0.68	-0.30	-0.68	
-0.68	-0.30	-0.68	-0.68	-0.30	-0.68	-0.68	-0.19	
-0.41	-0.41	-0.50	-0.19	-0.19	-0.68	-0.19	-0.50	
-0.68	-0.41	-0.19	-0.41	-0.30	-0.50	-0.41	-0.30	
-0.19	-0.41	-0.50	-0.30	-0.50	-0.19	-0.50	-0.50	
-0.68	-0.50	-0.68	-0.41	-0.41	-0.68	-0.19	-0.41	
-0.19	-0.41	-0.41	-0.30	-0.50	-0.41	-0.50	-0.68	
-0.30	-0.30	-0.68	-0.50	-0.30	-0.30	-0.68	0.04	
Map policy
>	v	>	>	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.359837048586155
mean w [-0.31698461 -0.36908813 -0.31278463 -0.12499441 -0.68419614 -0.00745601]
Mean policy from posterior
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.12	-0.31	-0.31	-0.31	-0.37	-0.68	-0.32	-0.68	
-0.68	-0.32	-0.68	-0.68	-0.32	-0.68	-0.68	-0.12	
-0.31	-0.31	-0.37	-0.12	-0.12	-0.68	-0.12	-0.37	
-0.68	-0.31	-0.12	-0.31	-0.32	-0.37	-0.31	-0.32	
-0.12	-0.31	-0.37	-0.32	-0.37	-0.12	-0.37	-0.37	
-0.68	-0.37	-0.68	-0.31	-0.31	-0.68	-0.12	-0.31	
-0.12	-0.31	-0.31	-0.32	-0.37	-0.31	-0.37	-0.68	
-0.32	-0.32	-0.68	-0.37	-0.32	-0.32	-0.68	-0.01	
mean = 0.03145141300270393, map = 0.05828226393220537
CVaR policy
>	v	>	>	v	>	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	v	v	>	>	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.058282262097737014, 0.031451450868204445, 0.03145141462757839, 0.031451412762457664, 0.03145141280383301
==========
iteration 70
==========
weights [-0.22513359 -0.28905385 -0.03580942 -0.69498026 -0.53881297  0.30193286]
expeced value MDP LP -1.2175531702620517
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 3), (34, 1), (35, 1), (36, 1), (37, 3), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.04693738 -0.00411594 -0.13185457  0.42452474 -0.81460497  0.36958329]
w_map [-0.33444517 -0.34251946 -0.16572127 -0.50963117 -0.69112434  0.07737266] loglik -5.9623932884278474e-06
accepted/total = 849/3000 = 0.283
-------
true weights [-0.22513359 -0.28905385 -0.03580942 -0.69498026 -0.53881297  0.30193286]
features
2 	2 	1 	2 	1 	2 	1 	4 	
1 	4 	1 	4 	1 	4 	3 	2 	
0 	1 	1 	0 	1 	0 	3 	2 	
0 	0 	2 	0 	1 	2 	2 	1 	
0 	4 	2 	2 	0 	1 	3 	1 	
2 	2 	0 	1 	3 	0 	3 	1 	
0 	3 	0 	0 	4 	0 	2 	1 	
4 	0 	1 	0 	4 	4 	4 	5 	
optimal policy
>	>	v	>	>	v	>	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	<	v	
v	>	>	>	>	v	>	v	
>	>	^	^	>	v	v	v	
^	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
optimal values
-1.91	-1.90	-1.88	-1.84	-1.83	-1.55	-1.71	-1.43	
-1.96	-2.07	-1.61	-1.95	-1.56	-1.53	-1.59	-0.90	
-1.69	-1.54	-1.33	-1.43	-1.28	-1.00	-1.50	-0.88	
-1.48	-1.27	-1.05	-1.22	-1.07	-0.79	-0.81	-0.85	
-1.50	-1.56	-1.03	-1.00	-0.98	-0.76	-1.25	-0.57	
-1.29	-1.27	-1.24	-1.28	-1.16	-0.47	-0.72	-0.28	
-1.50	-1.90	-1.22	-1.00	-0.79	-0.25	-0.03	0.01	
-2.02	-1.71	-1.50	-1.22	-1.31	-0.78	-0.24	0.30	
map_weights [-0.33444517 -0.34251946 -0.16572127 -0.50963117 -0.69112434  0.07737266]
MAP reward
-0.17	-0.17	-0.34	-0.17	-0.34	-0.17	-0.34	-0.69	
-0.34	-0.69	-0.34	-0.69	-0.34	-0.69	-0.51	-0.17	
-0.33	-0.34	-0.34	-0.33	-0.34	-0.33	-0.51	-0.17	
-0.33	-0.33	-0.17	-0.33	-0.34	-0.17	-0.17	-0.34	
-0.33	-0.69	-0.17	-0.17	-0.33	-0.34	-0.51	-0.34	
-0.17	-0.17	-0.33	-0.34	-0.51	-0.33	-0.51	-0.34	
-0.33	-0.51	-0.33	-0.33	-0.69	-0.33	-0.17	-0.34	
-0.69	-0.33	-0.34	-0.33	-0.69	-0.69	-0.69	0.08	
Map policy
>	>	v	>	v	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	.	
expeced value MDP LP -1.2259983270073196
mean w [-0.23945854 -0.25184102 -0.1009048  -0.47582196 -0.57365925  0.37005378]
Mean policy from posterior
>	>	v	>	v	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	>	v	
>	>	^	>	>	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	.	
Mean rewards
-0.10	-0.10	-0.25	-0.10	-0.25	-0.10	-0.25	-0.57	
-0.25	-0.57	-0.25	-0.57	-0.25	-0.57	-0.48	-0.10	
-0.24	-0.25	-0.25	-0.24	-0.25	-0.24	-0.48	-0.10	
-0.24	-0.24	-0.10	-0.24	-0.25	-0.10	-0.10	-0.25	
-0.24	-0.57	-0.10	-0.10	-0.24	-0.25	-0.48	-0.25	
-0.10	-0.10	-0.24	-0.25	-0.48	-0.24	-0.48	-0.25	
-0.24	-0.48	-0.24	-0.24	-0.57	-0.24	-0.10	-0.25	
-0.57	-0.24	-0.25	-0.24	-0.57	-0.57	-0.57	0.37	
mean = 0.01533631151089132, map = 0.04918096147903617
CVaR policy
>	>	v	>	v	v	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	>	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	v	
>	>	^	^	>	>	>	.	
CVaR policy
>	>	v	>	v	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	>	v	
>	>	^	>	>	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	.	
CVaR policy
>	>	v	>	v	>	v	v	
v	v	v	v	v	v	>	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	v	>	v	
v	>	>	>	>	v	>	v	
>	>	^	>	>	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	^	>	>	>	.	
cvar = , 0.05230415327603022, 0.06852130034176418, 0.04677998104018766, 0.015336329986959996, 0.01533631739592467
==========
iteration 71
==========
weights [-0.61278202 -0.25491264 -0.35505055 -0.54428494 -0.317345    0.19105732]
expeced value MDP LP -2.3003258457556166
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 3), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.66707742  0.13621466 -0.22262127 -0.65759905 -0.01237102 -0.23303113]
w_map [-0.63002087 -0.20124087 -0.4097188  -0.56503584 -0.22232995 -0.16127683] loglik -5.11042898665437e-07
accepted/total = 1558/3000 = 0.5193333333333333
-------
true weights [-0.61278202 -0.25491264 -0.35505055 -0.54428494 -0.317345    0.19105732]
features
3 	1 	2 	1 	1 	1 	3 	3 	
2 	4 	2 	1 	0 	2 	3 	4 	
3 	1 	1 	4 	1 	2 	4 	1 	
1 	2 	4 	2 	1 	4 	3 	1 	
2 	3 	0 	3 	3 	4 	0 	1 	
3 	3 	3 	0 	3 	2 	0 	0 	
0 	4 	3 	0 	4 	2 	3 	4 	
0 	0 	3 	4 	4 	4 	3 	5 	
optimal policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	v	>	v	
^	^	>	>	>	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.11	-3.60	-3.42	-3.10	-2.91	-2.68	-2.83	-2.30	
-3.70	-3.38	-3.20	-2.87	-2.94	-2.45	-2.30	-1.78	
-3.61	-3.10	-2.87	-2.64	-2.35	-2.11	-1.78	-1.47	
-3.38	-3.15	-2.83	-2.53	-2.20	-1.97	-1.76	-1.23	
-3.70	-3.67	-3.30	-2.72	-2.19	-1.67	-1.59	-0.99	
-3.67	-3.16	-2.87	-2.41	-1.82	-1.36	-1.28	-0.74	
-3.22	-2.64	-2.34	-1.89	-1.29	-1.02	-0.67	-0.13	
-3.00	-2.41	-1.82	-1.29	-0.98	-0.67	-0.36	0.19	
map_weights [-0.63002087 -0.20124087 -0.4097188  -0.56503584 -0.22232995 -0.16127683]
MAP reward
-0.57	-0.20	-0.41	-0.20	-0.20	-0.20	-0.57	-0.57	
-0.41	-0.22	-0.41	-0.20	-0.63	-0.41	-0.57	-0.22	
-0.57	-0.20	-0.20	-0.22	-0.20	-0.41	-0.22	-0.20	
-0.20	-0.41	-0.22	-0.41	-0.20	-0.22	-0.57	-0.20	
-0.41	-0.57	-0.63	-0.57	-0.57	-0.22	-0.63	-0.20	
-0.57	-0.57	-0.57	-0.63	-0.57	-0.41	-0.63	-0.63	
-0.63	-0.22	-0.57	-0.63	-0.22	-0.41	-0.57	-0.22	
-0.63	-0.63	-0.57	-0.22	-0.22	-0.22	-0.57	-0.16	
Map policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	v	
^	v	^	>	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.6148260911079921
mean w [-0.47689652 -0.10881639 -0.33934719 -0.64183923 -0.18865373  0.19826883]
Mean policy from posterior
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	^	^	>	^	v	>	v	
^	^	^	^	>	>	>	v	
^	v	>	v	v	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.64	-0.11	-0.34	-0.11	-0.11	-0.11	-0.64	-0.64	
-0.34	-0.19	-0.34	-0.11	-0.48	-0.34	-0.64	-0.19	
-0.64	-0.11	-0.11	-0.19	-0.11	-0.34	-0.19	-0.11	
-0.11	-0.34	-0.19	-0.34	-0.11	-0.19	-0.64	-0.11	
-0.34	-0.64	-0.48	-0.64	-0.64	-0.19	-0.48	-0.11	
-0.64	-0.64	-0.64	-0.48	-0.64	-0.34	-0.48	-0.48	
-0.48	-0.19	-0.64	-0.48	-0.19	-0.34	-0.64	-0.19	
-0.48	-0.48	-0.64	-0.19	-0.19	-0.19	-0.64	0.20	
mean = 0.08789587541144073, map = 0.026326646640027818
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	^	>	>	>	>	>	v	
^	^	^	>	>	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	v	>	v	
^	^	^	>	>	>	>	v	
v	v	>	v	v	v	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	v	>	v	
^	^	^	>	>	>	>	v	
^	v	>	v	v	v	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	^	^	>	^	v	>	v	
^	^	^	^	>	>	>	v	
^	v	>	v	v	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	>	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	^	^	>	^	v	>	v	
^	^	^	^	>	>	>	v	
^	v	>	v	v	>	>	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.046737346826276394, 0.059993746918259205, 0.07033262271602769, 0.08789587370556884, 0.0878958737965645
==========
iteration 72
==========
weights [-0.17961962 -0.64037079 -0.66922549 -0.27033157 -0.18633629  0.04470917]
expeced value MDP LP -1.6678488754885077
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 1), (27, 3), (35, 1), (36, 1), (37, 3), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[ 0.63039474  0.16820285 -0.51570817 -0.15185829  0.15949299  0.50976112]
w_map [-0.171257   -0.70175419 -0.30544946 -0.53271272 -0.3173726  -0.02011253] loglik -1.1085694318069272e-05
accepted/total = 1408/3000 = 0.4693333333333333
-------
true weights [-0.17961962 -0.64037079 -0.66922549 -0.27033157 -0.18633629  0.04470917]
features
0 	0 	0 	4 	2 	1 	0 	4 	
2 	2 	4 	4 	0 	3 	0 	4 	
2 	2 	4 	3 	1 	4 	2 	2 	
2 	3 	4 	3 	3 	3 	2 	3 	
3 	4 	1 	4 	0 	0 	3 	4 	
0 	4 	3 	2 	1 	0 	3 	1 	
1 	3 	3 	1 	3 	1 	4 	0 	
3 	4 	2 	1 	4 	3 	4 	5 	
optimal policy
>	>	v	v	v	v	v	<	
^	>	v	>	>	v	<	<	
>	>	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	v	^	>	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.52	-2.36	-2.20	-2.12	-2.44	-2.25	-1.95	-2.12	
-3.16	-2.69	-2.04	-1.96	-1.79	-1.63	-1.79	-1.96	
-3.17	-2.53	-1.88	-1.79	-1.99	-1.37	-2.02	-1.87	
-2.61	-1.96	-1.71	-1.54	-1.36	-1.19	-1.51	-1.21	
-2.32	-2.07	-1.91	-1.28	-1.10	-0.93	-0.85	-0.95	
-2.32	-2.16	-2.00	-1.94	-1.39	-0.76	-0.59	-0.77	
-2.62	-2.00	-1.74	-1.49	-0.86	-0.96	-0.32	-0.14	
-2.30	-2.05	-1.88	-1.23	-0.59	-0.41	-0.14	0.04	
map_weights [-0.171257   -0.70175419 -0.30544946 -0.53271272 -0.3173726  -0.02011253]
MAP reward
-0.17	-0.17	-0.17	-0.32	-0.31	-0.70	-0.17	-0.32	
-0.31	-0.31	-0.32	-0.32	-0.17	-0.53	-0.17	-0.32	
-0.31	-0.31	-0.32	-0.53	-0.70	-0.32	-0.31	-0.31	
-0.31	-0.53	-0.32	-0.53	-0.53	-0.53	-0.31	-0.53	
-0.53	-0.32	-0.70	-0.32	-0.17	-0.17	-0.53	-0.32	
-0.17	-0.32	-0.53	-0.31	-0.70	-0.17	-0.53	-0.70	
-0.70	-0.53	-0.53	-0.70	-0.53	-0.70	-0.32	-0.17	
-0.53	-0.32	-0.31	-0.70	-0.32	-0.53	-0.32	-0.02	
Map policy
>	>	v	>	v	>	v	v	
>	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	v	v	
v	v	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4394683836425446
mean w [-0.10617156 -0.6137926  -0.44642293 -0.34859024 -0.1819817   0.1515331 ]
Mean policy from posterior
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	^	>	>	v	v	
^	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.11	-0.11	-0.11	-0.18	-0.45	-0.61	-0.11	-0.18	
-0.45	-0.45	-0.18	-0.18	-0.11	-0.35	-0.11	-0.18	
-0.45	-0.45	-0.18	-0.35	-0.61	-0.18	-0.45	-0.45	
-0.45	-0.35	-0.18	-0.35	-0.35	-0.35	-0.45	-0.35	
-0.35	-0.18	-0.61	-0.18	-0.11	-0.11	-0.35	-0.18	
-0.11	-0.18	-0.35	-0.45	-0.61	-0.11	-0.35	-0.61	
-0.61	-0.35	-0.35	-0.61	-0.35	-0.61	-0.18	-0.11	
-0.35	-0.18	-0.45	-0.61	-0.18	-0.35	-0.18	0.15	
mean = 0.018839189367763298, map = 0.08414497933407139
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	v	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	v	v	
v	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	<	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	v	v	
>	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	v	v	
^	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
^	>	v	>	>	v	<	v	
>	>	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	v	v	
^	v	>	>	v	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.05559201101455424, 0.042459351981725124, 0.023332366557609063, 0.024010681748892093, 0.024010681696408742
==========
iteration 73
==========
weights [-0.0065879  -0.31779271 -0.10417501 -0.6774825  -0.03263814  0.65426614]
expeced value MDP LP 0.05408183179890491
demonstration
[(0, 3), (8, 3), (16, 1), (17, 1), (18, 3), (26, 3), (34, 3), (42, 1), (43, 3), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.0996805  -0.25667569  0.62157272  0.40371795  0.60841134 -0.06838271]
w_map [-0.25064294 -0.47258702 -0.50252772 -0.59741842 -0.25554605 -0.1977194 ] loglik -1.511878622295626e-09
accepted/total = 1931/3000 = 0.6436666666666667
-------
true weights [-0.0065879  -0.31779271 -0.10417501 -0.6774825  -0.03263814  0.65426614]
features
3 	3 	0 	4 	1 	2 	2 	0 	
4 	2 	3 	0 	0 	4 	3 	2 	
4 	4 	0 	3 	4 	4 	2 	3 	
2 	2 	4 	2 	1 	0 	3 	2 	
4 	4 	4 	1 	1 	3 	0 	0 	
2 	2 	4 	4 	3 	1 	2 	4 	
1 	1 	1 	0 	0 	3 	3 	0 	
0 	4 	4 	3 	4 	4 	2 	5 	
optimal policy
v	v	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	<	v	v	v	v	
v	v	v	<	<	v	v	v	
>	>	v	v	<	>	>	v	
>	>	>	v	v	>	>	v	
^	^	>	>	v	v	>	v	
>	>	^	>	>	>	>	.	
optimal values
-0.50	-0.53	-0.20	-0.20	-0.48	-0.28	-0.38	-0.31	
0.18	0.15	-0.39	-0.17	-0.16	-0.17	-0.85	-0.31	
0.22	0.25	0.29	-0.39	-0.16	-0.14	-0.21	-0.20	
0.16	0.19	0.30	0.19	-0.13	-0.11	-0.11	0.48	
0.26	0.30	0.34	0.09	-0.23	-0.11	0.58	0.59	
0.16	0.26	0.37	0.41	-0.23	0.17	0.49	0.60	
-0.16	-0.06	0.12	0.45	0.46	-0.18	-0.04	0.64	
0.05	0.06	0.09	-0.21	0.47	0.51	0.54	0.65	
map_weights [-0.25064294 -0.47258702 -0.50252772 -0.59741842 -0.25554605 -0.1977194 ]
MAP reward
-0.60	-0.60	-0.25	-0.26	-0.47	-0.50	-0.50	-0.25	
-0.26	-0.50	-0.60	-0.25	-0.25	-0.26	-0.60	-0.50	
-0.26	-0.26	-0.25	-0.60	-0.26	-0.26	-0.50	-0.60	
-0.50	-0.50	-0.26	-0.50	-0.47	-0.25	-0.60	-0.50	
-0.26	-0.26	-0.26	-0.47	-0.47	-0.60	-0.25	-0.25	
-0.50	-0.50	-0.26	-0.26	-0.60	-0.47	-0.50	-0.26	
-0.47	-0.47	-0.47	-0.25	-0.25	-0.60	-0.60	-0.25	
-0.25	-0.26	-0.26	-0.60	-0.26	-0.26	-0.50	-0.20	
Map policy
v	>	>	v	v	v	>	v	
v	v	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
>	>	>	v	v	>	>	v	
v	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.3939597209826602
mean w [-0.18866065 -0.45095091 -0.43678822 -0.54890336 -0.1220548   0.12211797]
Mean policy from posterior
v	v	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	>	>	v	v	v	
v	v	v	<	>	>	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.55	-0.55	-0.19	-0.12	-0.45	-0.44	-0.44	-0.19	
-0.12	-0.44	-0.55	-0.19	-0.19	-0.12	-0.55	-0.44	
-0.12	-0.12	-0.19	-0.55	-0.12	-0.12	-0.44	-0.55	
-0.44	-0.44	-0.12	-0.44	-0.45	-0.19	-0.55	-0.44	
-0.12	-0.12	-0.12	-0.45	-0.45	-0.55	-0.19	-0.19	
-0.44	-0.44	-0.12	-0.12	-0.55	-0.45	-0.44	-0.12	
-0.45	-0.45	-0.45	-0.19	-0.19	-0.55	-0.55	-0.19	
-0.19	-0.12	-0.12	-0.55	-0.12	-0.12	-0.44	0.12	
mean = 0.04686229303457104, map = 0.050695173468457894
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
>	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	v	>	v	v	v	
>	>	v	v	>	>	>	v	
>	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
v	v	v	<	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	>	v	
v	v	v	>	v	v	<	v	
>	>	v	>	>	v	v	v	
v	v	v	<	>	v	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	v	v	<	v	
v	v	v	>	v	v	<	v	
>	>	v	>	>	v	v	v	
v	v	v	<	>	>	v	v	
>	>	v	v	>	>	>	v	
^	>	>	v	v	>	>	v	
v	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.051181229185307514, 0.051181229141872175, 0.04789061072766135, 0.04735133230039884, 0.04686229306038742
==========
iteration 74
==========
weights [-0.30557272 -0.31797187 -0.23414576 -0.67327985 -0.14898029  0.52458944]
expeced value MDP LP -1.2535152997443926
demonstration
[(0, 3), (8, 1), (9, 1), (10, 3), (18, 3), (26, 1), (27, 1), (28, 1), (29, 1), (30, 3), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[ 0.26177654 -0.19495327  0.07527325  0.42471669  0.82627213 -0.15713149]
w_map [-0.39923482 -0.59466019 -0.26893347 -0.55591059 -0.15083155  0.2878869 ] loglik -2.079469145856203
accepted/total = 1414/3000 = 0.4713333333333333
-------
true weights [-0.30557272 -0.31797187 -0.23414576 -0.67327985 -0.14898029  0.52458944]
features
4 	0 	1 	4 	1 	4 	1 	3 	
2 	2 	2 	4 	3 	0 	1 	0 	
0 	2 	4 	0 	3 	1 	3 	4 	
1 	1 	0 	2 	2 	4 	4 	3 	
1 	0 	2 	2 	0 	2 	0 	0 	
4 	0 	2 	2 	2 	1 	1 	0 	
2 	0 	2 	2 	3 	3 	3 	4 	
0 	1 	1 	2 	0 	1 	2 	5 	
optimal policy
v	>	>	v	>	v	<	v	
>	>	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	v	>	>	>	v	
v	>	>	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.48	-2.42	-2.14	-1.84	-1.87	-1.57	-1.87	-2.01	
-2.35	-2.14	-1.93	-1.71	-2.10	-1.44	-1.66	-1.35	
-2.21	-1.93	-1.71	-1.58	-1.72	-1.14	-1.36	-1.06	
-2.18	-1.88	-1.58	-1.28	-1.06	-0.83	-0.69	-0.92	
-1.98	-1.76	-1.47	-1.25	-1.07	-0.78	-0.55	-0.25	
-1.68	-1.54	-1.25	-1.03	-0.80	-0.57	-0.26	0.06	
-1.54	-1.32	-1.03	-0.80	-1.01	-0.71	-0.31	0.37	
-1.49	-1.19	-0.88	-0.57	-0.34	-0.04	0.29	0.52	
map_weights [-0.39923482 -0.59466019 -0.26893347 -0.55591059 -0.15083155  0.2878869 ]
MAP reward
-0.15	-0.40	-0.59	-0.15	-0.59	-0.15	-0.59	-0.56	
-0.27	-0.27	-0.27	-0.15	-0.56	-0.40	-0.59	-0.40	
-0.40	-0.27	-0.15	-0.40	-0.56	-0.59	-0.56	-0.15	
-0.59	-0.59	-0.40	-0.27	-0.27	-0.15	-0.15	-0.56	
-0.59	-0.40	-0.27	-0.27	-0.40	-0.27	-0.40	-0.40	
-0.15	-0.40	-0.27	-0.27	-0.27	-0.59	-0.59	-0.40	
-0.27	-0.40	-0.27	-0.27	-0.56	-0.56	-0.56	-0.15	
-0.40	-0.59	-0.59	-0.27	-0.40	-0.59	-0.27	0.29	
Map policy
v	v	>	v	>	v	>	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	>	v	
v	>	>	v	>	>	>	v	
>	>	>	v	v	>	>	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.8334862519284423
mean w [-0.3081426  -0.64606604 -0.20256746 -0.48157781 -0.10577258 -0.0743596 ]
Mean policy from posterior
v	v	>	v	<	v	>	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	>	>	v	^	>	>	v	
>	>	>	v	>	>	>	v	
^	>	>	>	>	>	>	.	
Mean rewards
-0.11	-0.31	-0.65	-0.11	-0.65	-0.11	-0.65	-0.48	
-0.20	-0.20	-0.20	-0.11	-0.48	-0.31	-0.65	-0.31	
-0.31	-0.20	-0.11	-0.31	-0.48	-0.65	-0.48	-0.11	
-0.65	-0.65	-0.31	-0.20	-0.20	-0.11	-0.11	-0.48	
-0.65	-0.31	-0.20	-0.20	-0.31	-0.20	-0.31	-0.31	
-0.11	-0.31	-0.20	-0.20	-0.20	-0.65	-0.65	-0.31	
-0.20	-0.31	-0.20	-0.20	-0.48	-0.48	-0.48	-0.11	
-0.31	-0.65	-0.65	-0.20	-0.31	-0.65	-0.20	-0.07	
mean = 0.05201852823806652, map = 0.023019773805304178
CVaR policy
v	v	>	v	>	v	>	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
v	>	>	>	>	>	>	v	
v	>	>	v	v	>	>	v	
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	>	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	>	>	v	v	>	>	v	
>	>	>	v	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	<	v	>	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	>	>	v	v	>	>	v	
>	>	>	v	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	<	v	>	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	>	>	v	v	>	>	v	
>	>	>	v	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	>	v	<	v	>	v	
>	>	v	v	v	v	>	v	
>	>	v	v	v	v	v	v	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
v	>	>	v	^	>	>	v	
>	>	>	v	>	>	>	v	
^	>	>	>	>	>	>	.	
cvar = , 0.04396469756155996, 0.051437715365406866, 0.055596230506101296, 0.060768572166389756, 0.05201852712966226
==========
iteration 75
==========
weights [-0.08194328 -0.39520843 -0.45167315 -0.29919102 -0.07673882  0.73326861]
expeced value MDP LP -0.7526650300552158
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 1), (26, 3), (34, 3), (42, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 3), (62, 1), (63, None)]
[-0.40949697  0.10555686 -0.49603505 -0.29831461 -0.56110941  0.41386451]
w_map [-0.33391415 -0.59891275 -0.49251765 -0.47695119 -0.1869202  -0.15751071] loglik -4.746418930778873e-06
accepted/total = 1316/3000 = 0.43866666666666665
-------
true weights [-0.08194328 -0.39520843 -0.45167315 -0.29919102 -0.07673882  0.73326861]
features
4 	3 	2 	0 	1 	2 	3 	3 	
2 	3 	4 	2 	2 	4 	2 	4 	
2 	1 	1 	4 	4 	1 	4 	0 	
4 	0 	4 	0 	3 	1 	2 	0 	
1 	0 	3 	2 	2 	4 	1 	2 	
0 	2 	4 	2 	1 	0 	2 	1 	
0 	2 	0 	4 	3 	2 	0 	2 	
2 	1 	3 	1 	1 	3 	0 	5 	
optimal policy
v	v	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	>	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
optimal values
-1.87	-1.82	-1.68	-1.40	-1.65	-1.36	-1.37	-1.09	
-1.82	-1.53	-1.25	-1.33	-1.26	-0.91	-1.24	-0.80	
-1.38	-1.25	-1.18	-0.89	-0.82	-0.85	-0.80	-0.73	
-0.94	-0.87	-0.79	-0.83	-0.75	-0.46	-0.75	-0.65	
-1.19	-0.80	-0.72	-0.96	-0.51	-0.06	-0.30	-0.57	
-0.95	-0.88	-0.43	-0.73	-0.38	0.02	0.10	-0.12	
-0.88	-0.80	-0.36	-0.28	-0.20	0.10	0.56	0.27	
-1.32	-1.04	-0.65	-0.45	-0.06	0.34	0.64	0.73	
map_weights [-0.33391415 -0.59891275 -0.49251765 -0.47695119 -0.1869202  -0.15751071]
MAP reward
-0.19	-0.48	-0.49	-0.33	-0.60	-0.49	-0.48	-0.48	
-0.49	-0.48	-0.19	-0.49	-0.49	-0.19	-0.49	-0.19	
-0.49	-0.60	-0.60	-0.19	-0.19	-0.60	-0.19	-0.33	
-0.19	-0.33	-0.19	-0.33	-0.48	-0.60	-0.49	-0.33	
-0.60	-0.33	-0.48	-0.49	-0.49	-0.19	-0.60	-0.49	
-0.33	-0.49	-0.19	-0.49	-0.60	-0.33	-0.49	-0.60	
-0.33	-0.49	-0.33	-0.19	-0.48	-0.49	-0.33	-0.49	
-0.49	-0.60	-0.48	-0.60	-0.60	-0.48	-0.33	-0.16	
Map policy
v	v	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.9391645841995446
mean w [-0.23623957 -0.62363149 -0.45197032 -0.40176246 -0.11906582  0.16031363]
Mean policy from posterior
v	v	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
Mean rewards
-0.12	-0.40	-0.45	-0.24	-0.62	-0.45	-0.40	-0.40	
-0.45	-0.40	-0.12	-0.45	-0.45	-0.12	-0.45	-0.12	
-0.45	-0.62	-0.62	-0.12	-0.12	-0.62	-0.12	-0.24	
-0.12	-0.24	-0.12	-0.24	-0.40	-0.62	-0.45	-0.24	
-0.62	-0.24	-0.40	-0.45	-0.45	-0.12	-0.62	-0.45	
-0.24	-0.45	-0.12	-0.45	-0.62	-0.24	-0.45	-0.62	
-0.24	-0.45	-0.24	-0.12	-0.40	-0.45	-0.24	-0.45	
-0.45	-0.62	-0.40	-0.62	-0.62	-0.40	-0.24	0.16	
mean = 0.019401389157551074, map = 0.02245327289019816
CVaR policy
v	v	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	v	>	>	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	.	
cvar = , 0.026467076235251574, 0.02245327358631921, 0.019401377221927696, 0.0194013770788668, 0.019401377089945493
==========
iteration 76
==========
weights [-0.79956774 -0.37369359 -0.32428528 -0.04562172 -0.14464077  0.30476431]
expeced value MDP LP -0.8024153454466554
demonstration
[(0, 3), (8, 1), (9, 1), (10, 1), (11, 3), (19, 3), (27, 3), (35, 3), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.28895949 -0.73113293  0.52757557  0.25811706 -0.17602256 -0.07747714]
w_map [-0.50125506 -0.56051711 -0.43227726 -0.15503171 -0.45901476  0.11388996] loglik -0.6931543920973127
accepted/total = 1242/3000 = 0.414
-------
true weights [-0.79956774 -0.37369359 -0.32428528 -0.04562172 -0.14464077  0.30476431]
features
4 	4 	3 	3 	2 	4 	4 	2 	
3 	3 	3 	3 	1 	4 	3 	1 	
4 	4 	3 	3 	3 	1 	1 	4 	
0 	4 	4 	1 	2 	3 	4 	0 	
1 	0 	2 	3 	2 	1 	3 	4 	
2 	4 	2 	3 	4 	2 	2 	0 	
1 	3 	3 	3 	4 	2 	4 	1 	
1 	1 	0 	0 	1 	3 	4 	5 	
optimal policy
v	>	>	v	<	v	v	<	
>	>	v	v	v	>	v	<	
^	>	>	v	v	v	v	<	
>	>	v	v	>	>	v	v	
v	v	>	v	<	>	v	<	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	^	>	>	>	>	.	
optimal values
-1.17	-1.13	-1.00	-0.96	-1.28	-1.17	-1.04	-1.35	
-1.03	-1.00	-0.96	-0.93	-1.26	-1.04	-0.90	-1.27	
-1.17	-1.06	-0.93	-0.89	-0.89	-0.91	-0.87	-1.00	
-1.86	-1.07	-0.94	-0.85	-0.86	-0.54	-0.50	-1.29	
-1.31	-1.42	-0.80	-0.48	-0.80	-0.73	-0.36	-0.50	
-0.94	-0.62	-0.76	-0.44	-0.50	-0.54	-0.31	-0.87	
-0.85	-0.48	-0.44	-0.40	-0.36	-0.22	0.01	-0.07	
-1.22	-0.85	-1.24	-1.06	-0.26	0.11	0.16	0.30	
map_weights [-0.50125506 -0.56051711 -0.43227726 -0.15503171 -0.45901476  0.11388996]
MAP reward
-0.46	-0.46	-0.16	-0.16	-0.43	-0.46	-0.46	-0.43	
-0.16	-0.16	-0.16	-0.16	-0.56	-0.46	-0.16	-0.56	
-0.46	-0.46	-0.16	-0.16	-0.16	-0.56	-0.56	-0.46	
-0.50	-0.46	-0.46	-0.56	-0.43	-0.16	-0.46	-0.50	
-0.56	-0.50	-0.43	-0.16	-0.43	-0.56	-0.16	-0.46	
-0.43	-0.46	-0.43	-0.16	-0.46	-0.43	-0.43	-0.50	
-0.56	-0.16	-0.16	-0.16	-0.46	-0.43	-0.46	-0.56	
-0.56	-0.56	-0.50	-0.50	-0.56	-0.16	-0.46	0.11	
Map policy
v	>	>	v	<	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	v	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -2.086757164714859
mean w [-0.41281993 -0.58245394 -0.43397928 -0.10728336 -0.38590798 -0.09846744]
Mean policy from posterior
v	>	>	v	<	v	v	<	
>	>	v	v	v	>	v	v	
^	>	>	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	<	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.39	-0.39	-0.11	-0.11	-0.43	-0.39	-0.39	-0.43	
-0.11	-0.11	-0.11	-0.11	-0.58	-0.39	-0.11	-0.58	
-0.39	-0.39	-0.11	-0.11	-0.11	-0.58	-0.58	-0.39	
-0.41	-0.39	-0.39	-0.58	-0.43	-0.11	-0.39	-0.41	
-0.58	-0.41	-0.43	-0.11	-0.43	-0.58	-0.11	-0.39	
-0.43	-0.39	-0.43	-0.11	-0.39	-0.43	-0.43	-0.41	
-0.58	-0.11	-0.11	-0.11	-0.39	-0.43	-0.39	-0.58	
-0.58	-0.58	-0.41	-0.41	-0.58	-0.11	-0.39	-0.10	
mean = 0.07765171111147562, map = 0.08614837895383631
CVaR policy
v	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	v	v	v	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	>	v	v	<	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	v	v	v	
>	>	v	v	>	>	v	v	
v	>	>	v	v	>	v	v	
>	v	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	>	v	v	
>	>	v	v	v	>	v	v	
^	>	>	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	<	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	>	v	v	
>	>	v	v	v	>	v	v	
^	>	>	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	<	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
v	>	>	v	<	v	v	<	
>	>	v	v	v	>	v	v	
^	>	>	v	v	v	v	v	
>	v	v	v	>	>	v	v	
>	v	>	v	<	>	v	v	
>	v	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	^	>	>	>	>	>	.	
cvar = , 0.20948310775331014, 0.08376245974605157, 0.09681755936980507, 0.09681755937414116, 0.07967465275580488
==========
iteration 77
==========
weights [-0.23061708 -0.30649039 -0.59327348 -0.4776827  -0.23756431  0.46506819]
expeced value MDP LP -1.6328825901316075
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 3), (33, 1), (34, 1), (35, 1), (36, 1), (37, 3), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[ 0.10319999 -0.23693982  0.11048983 -0.53060178  0.52766988  0.60085567]
w_map [-0.18842615 -0.35004903 -0.51205259 -0.59563597 -0.39459567 -0.26320232] loglik -1.3862948869883525
accepted/total = 1641/3000 = 0.547
-------
true weights [-0.23061708 -0.30649039 -0.59327348 -0.4776827  -0.23756431  0.46506819]
features
4 	2 	2 	4 	4 	3 	2 	2 	
3 	4 	0 	2 	0 	2 	2 	0 	
0 	4 	4 	4 	0 	2 	0 	3 	
0 	0 	2 	3 	3 	4 	3 	2 	
1 	0 	0 	1 	1 	0 	3 	3 	
3 	2 	4 	2 	0 	4 	0 	4 	
4 	2 	1 	1 	2 	4 	4 	1 	
0 	3 	3 	2 	3 	3 	3 	5 	
optimal policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
v	v	>	>	v	v	v	v	
>	v	v	v	>	v	v	v	
>	>	>	>	v	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	^	>	>	>	>	.	
optimal values
-3.09	-3.01	-2.91	-2.34	-2.12	-2.58	-2.63	-2.41	
-2.89	-2.45	-2.34	-2.48	-1.90	-2.16	-2.06	-1.83	
-2.43	-2.23	-2.13	-1.91	-1.69	-1.59	-1.48	-1.62	
-2.22	-2.01	-2.16	-1.83	-1.47	-1.00	-1.26	-1.15	
-2.09	-1.80	-1.59	-1.37	-1.07	-0.77	-0.79	-0.56	
-2.55	-2.16	-1.58	-1.36	-0.77	-0.55	-0.31	-0.09	
-2.30	-2.08	-1.50	-1.21	-0.91	-0.32	-0.09	0.15	
-2.51	-2.42	-1.97	-1.55	-0.97	-0.49	-0.02	0.47	
map_weights [-0.18842615 -0.35004903 -0.51205259 -0.59563597 -0.39459567 -0.26320232]
MAP reward
-0.39	-0.51	-0.51	-0.39	-0.39	-0.60	-0.51	-0.51	
-0.60	-0.39	-0.19	-0.51	-0.19	-0.51	-0.51	-0.19	
-0.19	-0.39	-0.39	-0.39	-0.19	-0.51	-0.19	-0.60	
-0.19	-0.19	-0.51	-0.60	-0.60	-0.39	-0.60	-0.51	
-0.35	-0.19	-0.19	-0.35	-0.35	-0.19	-0.60	-0.60	
-0.60	-0.51	-0.39	-0.51	-0.19	-0.39	-0.19	-0.39	
-0.39	-0.51	-0.35	-0.35	-0.51	-0.39	-0.39	-0.35	
-0.19	-0.60	-0.60	-0.51	-0.60	-0.60	-0.60	-0.26	
Map policy
v	v	v	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
^	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -2.0141437947118375
mean w [-0.11048721 -0.24330707 -0.5170329  -0.55016263 -0.32897929 -0.12767124]
Mean policy from posterior
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
v	v	<	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
Mean rewards
-0.33	-0.52	-0.52	-0.33	-0.33	-0.55	-0.52	-0.52	
-0.55	-0.33	-0.11	-0.52	-0.11	-0.52	-0.52	-0.11	
-0.11	-0.33	-0.33	-0.33	-0.11	-0.52	-0.11	-0.55	
-0.11	-0.11	-0.52	-0.55	-0.55	-0.33	-0.55	-0.52	
-0.24	-0.11	-0.11	-0.24	-0.24	-0.11	-0.55	-0.55	
-0.55	-0.52	-0.33	-0.52	-0.11	-0.33	-0.11	-0.33	
-0.33	-0.52	-0.24	-0.24	-0.52	-0.33	-0.33	-0.24	
-0.11	-0.55	-0.55	-0.52	-0.55	-0.55	-0.55	-0.13	
mean = 0.03201213995525287, map = 0.03298072797041929
CVaR policy
v	v	v	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
CVaR policy
v	v	v	>	v	<	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	v	v	v	v	
>	v	v	v	v	v	v	v	
>	>	>	>	v	v	v	v	
^	^	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	^	>	>	>	>	.	
cvar = , 0.03569894380952343, 0.02222581888174524, 0.024844661270378765, 0.025175786482802387, 0.02517578638380047
==========
iteration 78
==========
weights [-0.56973416 -0.21635767 -0.19048689 -0.55703297 -0.51310729  0.13690242]
expeced value MDP LP -2.1029111882317597
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 3), (40, 3), (48, 3), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.08263506  0.59633234  0.36273382 -0.29511988  0.35313129  0.54238908]
w_map [-0.5715706  -0.32117271 -0.26209429 -0.50338239 -0.47495553  0.14995046] loglik -2.083218930692965e-09
accepted/total = 1801/3000 = 0.6003333333333334
-------
true weights [-0.56973416 -0.21635767 -0.19048689 -0.55703297 -0.51310729  0.13690242]
features
0 	0 	0 	4 	1 	2 	4 	2 	
0 	4 	0 	2 	3 	3 	4 	0 	
1 	4 	3 	0 	4 	3 	3 	1 	
1 	4 	3 	0 	4 	0 	1 	4 	
3 	4 	0 	3 	0 	2 	4 	1 	
4 	0 	2 	3 	0 	1 	4 	1 	
1 	4 	0 	1 	3 	3 	0 	0 	
2 	0 	2 	2 	2 	1 	1 	5 	
optimal policy
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-4.27	-4.61	-4.14	-3.61	-3.13	-2.94	-2.78	-2.29	
-3.73	-4.08	-3.75	-3.22	-3.31	-2.87	-2.60	-2.12	
-3.20	-3.60	-3.21	-3.06	-2.78	-2.33	-2.10	-1.56	
-3.01	-3.12	-2.68	-2.52	-2.29	-1.80	-1.56	-1.36	
-2.82	-2.64	-2.14	-1.97	-1.80	-1.24	-1.36	-0.86	
-2.29	-2.14	-1.59	-1.43	-1.60	-1.06	-1.15	-0.65	
-1.79	-1.91	-1.41	-0.88	-1.04	-0.85	-0.65	-0.43	
-1.59	-1.41	-0.85	-0.67	-0.48	-0.30	-0.08	0.14	
map_weights [-0.5715706  -0.32117271 -0.26209429 -0.50338239 -0.47495553  0.14995046]
MAP reward
-0.57	-0.57	-0.57	-0.47	-0.32	-0.26	-0.47	-0.26	
-0.57	-0.47	-0.57	-0.26	-0.50	-0.50	-0.47	-0.57	
-0.32	-0.47	-0.50	-0.57	-0.47	-0.50	-0.50	-0.32	
-0.32	-0.47	-0.50	-0.57	-0.47	-0.57	-0.32	-0.47	
-0.50	-0.47	-0.57	-0.50	-0.57	-0.26	-0.47	-0.32	
-0.47	-0.57	-0.26	-0.50	-0.57	-0.32	-0.47	-0.32	
-0.32	-0.47	-0.57	-0.32	-0.50	-0.50	-0.57	-0.57	
-0.26	-0.57	-0.26	-0.26	-0.26	-0.32	-0.32	0.15	
Map policy
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.751048913325409
mean w [-0.57788819 -0.1826419  -0.13177443 -0.38864297 -0.46510714  0.12128286]
Mean policy from posterior
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.58	-0.58	-0.58	-0.47	-0.18	-0.13	-0.47	-0.13	
-0.58	-0.47	-0.58	-0.13	-0.39	-0.39	-0.47	-0.58	
-0.18	-0.47	-0.39	-0.58	-0.47	-0.39	-0.39	-0.18	
-0.18	-0.47	-0.39	-0.58	-0.47	-0.58	-0.18	-0.47	
-0.39	-0.47	-0.58	-0.39	-0.58	-0.13	-0.47	-0.18	
-0.47	-0.58	-0.13	-0.39	-0.58	-0.18	-0.47	-0.18	
-0.18	-0.47	-0.58	-0.18	-0.39	-0.39	-0.58	-0.58	
-0.13	-0.58	-0.13	-0.13	-0.13	-0.18	-0.18	0.12	
mean = 0.014425337377429237, map = 0.00213584707844916
CVaR policy
v	v	v	>	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
v	>	v	v	>	v	v	v	
v	>	v	v	v	v	v	v	
v	v	v	v	>	v	>	v	
v	>	>	v	v	v	>	v	
v	>	v	v	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.01540458529594968, 0.014425348424562667, 0.01442533866644391, 0.014425337403868088, 0.014425337380567171
==========
iteration 79
==========
weights [-0.54965462 -0.14332739 -0.68734027 -0.28927885 -0.31835763  0.14094887]
expeced value MDP LP -1.958254054509639
demonstration
[(0, 3), (8, 3), (16, 1), (17, 1), (18, 3), (26, 1), (27, 1), (28, 1), (29, 1), (30, 3), (38, 3), (46, 3), (54, 3), (62, 1), (63, None)]
[-0.36784204 -0.61078305 -0.20716999 -0.64919822  0.1203453  -0.11302915]
w_map [-0.65841528 -0.19994932 -0.55962641 -0.13941003 -0.36917662 -0.24000276] loglik -1.5052137314341962e-10
accepted/total = 1711/3000 = 0.5703333333333334
-------
true weights [-0.54965462 -0.14332739 -0.68734027 -0.28927885 -0.31835763  0.14094887]
features
4 	0 	2 	3 	3 	4 	0 	4 	
0 	0 	4 	2 	0 	0 	2 	0 	
4 	1 	3 	2 	2 	4 	1 	4 	
0 	4 	1 	0 	4 	3 	3 	4 	
3 	4 	0 	4 	0 	2 	1 	2 	
1 	2 	0 	2 	0 	0 	1 	1 	
1 	2 	0 	1 	3 	0 	3 	0 	
3 	4 	1 	3 	1 	2 	4 	5 	
optimal policy
v	v	v	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	>	>	>	v	<	
v	>	>	>	>	>	v	<	
v	<	>	>	>	>	v	v	
v	<	v	v	>	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
optimal values
-3.74	-3.69	-3.48	-2.83	-2.56	-2.30	-2.36	-2.30	
-3.46	-3.17	-2.82	-3.19	-2.53	-2.00	-1.83	-2.00	
-2.94	-2.65	-2.53	-2.80	-2.14	-1.46	-1.16	-1.46	
-3.02	-2.56	-2.26	-2.14	-1.61	-1.30	-1.02	-1.33	
-2.50	-2.79	-2.78	-2.26	-1.96	-1.42	-0.74	-1.23	
-2.23	-2.90	-2.47	-2.08	-1.69	-1.15	-0.60	-0.55	
-2.11	-2.38	-1.94	-1.41	-1.28	-1.01	-0.47	-0.41	
-1.99	-1.71	-1.41	-1.28	-1.00	-0.86	-0.18	0.14	
map_weights [-0.65841528 -0.19994932 -0.55962641 -0.13941003 -0.36917662 -0.24000276]
MAP reward
-0.37	-0.66	-0.56	-0.14	-0.14	-0.37	-0.66	-0.37	
-0.66	-0.66	-0.37	-0.56	-0.66	-0.66	-0.56	-0.66	
-0.37	-0.20	-0.14	-0.56	-0.56	-0.37	-0.20	-0.37	
-0.66	-0.37	-0.20	-0.66	-0.37	-0.14	-0.14	-0.37	
-0.14	-0.37	-0.66	-0.37	-0.66	-0.56	-0.20	-0.56	
-0.20	-0.56	-0.66	-0.56	-0.66	-0.66	-0.20	-0.20	
-0.20	-0.56	-0.66	-0.20	-0.14	-0.66	-0.14	-0.66	
-0.14	-0.37	-0.20	-0.14	-0.20	-0.56	-0.37	-0.24	
Map policy
v	v	>	>	>	v	v	v	
v	v	v	v	v	v	v	v	
>	>	v	>	>	v	v	<	
v	>	>	>	>	>	v	<	
v	<	>	v	>	>	v	v	
v	<	>	v	v	>	v	v	
v	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.4532604483247447
mean w [-0.49395634 -0.1629084  -0.59584958 -0.12210276 -0.27334241  0.20324314]
Mean policy from posterior
v	v	>	>	>	v	v	v	
v	v	v	^	>	v	v	v	
>	>	v	v	>	v	v	<	
v	>	>	>	>	>	v	<	
v	<	^	>	^	>	v	v	
v	<	v	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.27	-0.49	-0.60	-0.12	-0.12	-0.27	-0.49	-0.27	
-0.49	-0.49	-0.27	-0.60	-0.49	-0.49	-0.60	-0.49	
-0.27	-0.16	-0.12	-0.60	-0.60	-0.27	-0.16	-0.27	
-0.49	-0.27	-0.16	-0.49	-0.27	-0.12	-0.12	-0.27	
-0.12	-0.27	-0.49	-0.27	-0.49	-0.60	-0.16	-0.60	
-0.16	-0.60	-0.49	-0.60	-0.49	-0.49	-0.16	-0.16	
-0.16	-0.60	-0.49	-0.16	-0.12	-0.49	-0.12	-0.49	
-0.12	-0.27	-0.16	-0.12	-0.16	-0.60	-0.27	0.20	
mean = 0.0335451837758598, map = 0.03645883805620431
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	<	
v	>	>	>	>	>	v	<	
v	<	>	v	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	<	
v	>	>	>	>	>	v	<	
v	<	^	v	>	>	v	v	
v	<	v	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	<	
v	>	>	>	>	>	v	<	
v	<	^	v	^	>	v	v	
v	<	v	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	>	>	v	v	v	
>	>	v	v	v	v	v	<	
v	>	>	>	>	>	v	<	
v	<	^	>	^	>	v	v	
v	<	v	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	v	^	>	v	v	v	
>	>	v	v	v	v	v	<	
v	>	>	>	>	>	v	<	
v	<	^	>	^	>	v	v	
v	<	v	v	v	>	v	v	
v	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.029673073080069612, 0.025344341279421423, 0.02822620878844373, 0.028969028486122372, 0.0335451767810262
==========
iteration 80
==========
weights [-0.11326459 -0.49346822 -0.27584755 -0.3739412  -0.68835187  0.23218109]
expeced value MDP LP -1.878082627336454
demonstration
[(0, 1), (1, 3), (9, 3), (17, 3), (25, 3), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[ 0.07699676 -0.71397793 -0.09669871 -0.17923307 -0.56270974  0.35523179]
w_map [-0.21622081 -0.60800347 -0.21361    -0.39796004 -0.61218693 -0.06932582] loglik -0.6931472883875216
accepted/total = 1518/3000 = 0.506
-------
true weights [-0.11326459 -0.49346822 -0.27584755 -0.3739412  -0.68835187  0.23218109]
features
4 	0 	2 	1 	2 	3 	2 	0 	
1 	1 	4 	0 	0 	0 	2 	4 	
1 	3 	1 	0 	4 	4 	0 	1 	
2 	0 	4 	3 	4 	1 	4 	3 	
0 	3 	0 	1 	3 	0 	1 	0 	
3 	0 	4 	0 	1 	2 	2 	3 	
0 	2 	0 	2 	2 	1 	3 	4 	
0 	0 	2 	4 	1 	0 	3 	5 	
optimal policy
>	v	>	v	v	v	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-3.75	-3.09	-3.11	-2.87	-2.57	-2.57	-2.39	-2.47	
-3.38	-3.01	-3.06	-2.40	-2.31	-2.22	-2.13	-2.45	
-2.91	-2.54	-2.78	-2.31	-2.82	-2.27	-1.87	-1.78	
-2.44	-2.19	-2.62	-2.22	-2.15	-1.60	-1.95	-1.30	
-2.19	-2.10	-1.96	-1.86	-1.48	-1.12	-1.27	-0.93	
-2.10	-1.74	-2.06	-1.38	-1.50	-1.02	-0.79	-0.83	
-1.74	-1.64	-1.38	-1.28	-1.02	-0.75	-0.52	-0.46	
-1.84	-1.74	-1.64	-1.43	-0.75	-0.26	-0.14	0.23	
map_weights [-0.21622081 -0.60800347 -0.21361    -0.39796004 -0.61218693 -0.06932582]
MAP reward
-0.61	-0.22	-0.21	-0.61	-0.21	-0.40	-0.21	-0.22	
-0.61	-0.61	-0.61	-0.22	-0.22	-0.22	-0.21	-0.61	
-0.61	-0.40	-0.61	-0.22	-0.61	-0.61	-0.22	-0.61	
-0.21	-0.22	-0.61	-0.40	-0.61	-0.61	-0.61	-0.40	
-0.22	-0.40	-0.22	-0.61	-0.40	-0.22	-0.61	-0.22	
-0.40	-0.22	-0.61	-0.22	-0.61	-0.21	-0.21	-0.40	
-0.22	-0.21	-0.22	-0.21	-0.21	-0.61	-0.40	-0.61	
-0.22	-0.22	-0.21	-0.61	-0.61	-0.22	-0.40	-0.07	
Map policy
>	v	>	v	v	>	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
expeced value MDP LP -1.7459776076349989
mean w [-0.13668374 -0.44815638 -0.17760532 -0.26986644 -0.69536053  0.01230632]
Mean policy from posterior
>	v	>	v	v	v	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.70	-0.14	-0.18	-0.45	-0.18	-0.27	-0.18	-0.14	
-0.45	-0.45	-0.70	-0.14	-0.14	-0.14	-0.18	-0.70	
-0.45	-0.27	-0.45	-0.14	-0.70	-0.70	-0.14	-0.45	
-0.18	-0.14	-0.70	-0.27	-0.70	-0.45	-0.70	-0.27	
-0.14	-0.27	-0.14	-0.45	-0.27	-0.14	-0.45	-0.14	
-0.27	-0.14	-0.70	-0.14	-0.45	-0.18	-0.18	-0.27	
-0.14	-0.18	-0.14	-0.18	-0.18	-0.45	-0.27	-0.70	
-0.14	-0.14	-0.18	-0.70	-0.45	-0.14	-0.27	0.01	
mean = 0.025871838285792537, map = 0.028386793163722057
CVaR policy
>	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	v	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	v	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
CVaR policy
>	v	>	v	v	v	v	<	
v	v	>	v	>	>	v	v	
v	v	>	v	v	v	>	v	
v	v	v	v	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	v	>	>	v	v	
>	>	>	>	>	v	v	v	
>	>	^	>	>	>	>	.	
cvar = , 0.008017188046200108, 0.028982681191496162, 0.025871838051096052, 0.025871850098572402, 0.025871846318100467
==========
iteration 81
==========
weights [-0.18275572 -0.13082037 -0.49748576 -0.01509867 -0.3286618   0.77055028]
expeced value MDP LP -0.13444942017680966
demonstration
[(0, 1), (1, 3), (9, 3), (17, 3), (25, 1), (26, 3), (34, 1), (35, 1), (36, 3), (44, 1), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.58917633 -0.12779421 -0.30653237 -0.52166371 -0.20471388 -0.47805544]
w_map [-0.18602176 -0.35033875 -0.72635083 -0.18835529 -0.52846963 -0.01775574] loglik -2.0576535675331797e-07
accepted/total = 1615/3000 = 0.5383333333333333
-------
true weights [-0.18275572 -0.13082037 -0.49748576 -0.01509867 -0.3286618   0.77055028]
features
4 	4 	2 	3 	1 	1 	1 	2 	
2 	3 	1 	2 	4 	2 	0 	4 	
3 	4 	4 	3 	4 	2 	0 	0 	
4 	3 	1 	4 	4 	3 	0 	3 	
2 	2 	3 	1 	3 	4 	3 	0 	
0 	4 	3 	4 	3 	3 	0 	3 	
1 	4 	3 	4 	0 	1 	4 	0 	
0 	2 	2 	2 	3 	0 	4 	5 	
optimal policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
^	>	>	>	v	v	>	v	
>	>	^	>	>	>	>	v	
>	>	^	>	^	^	>	v	
^	^	^	>	>	>	>	.	
optimal values
-0.97	-0.65	-0.92	-0.59	-0.58	-0.46	-0.33	-0.66	
-0.82	-0.32	-0.43	-0.66	-0.67	-0.70	-0.20	-0.16	
-0.32	-0.31	-0.30	-0.16	-0.35	-0.35	-0.02	0.17	
-0.31	0.02	0.03	-0.15	-0.02	0.15	0.17	0.35	
-0.81	-0.34	0.16	0.18	0.31	0.02	0.35	0.37	
-0.36	-0.18	0.15	0.00	0.33	0.35	0.37	0.56	
-0.33	-0.20	0.13	-0.18	0.15	0.22	0.25	0.58	
-0.51	-0.69	-0.37	-0.27	0.23	0.25	0.43	0.77	
map_weights [-0.18602176 -0.35033875 -0.72635083 -0.18835529 -0.52846963 -0.01775574]
MAP reward
-0.53	-0.53	-0.73	-0.19	-0.35	-0.35	-0.35	-0.73	
-0.73	-0.19	-0.35	-0.73	-0.53	-0.73	-0.19	-0.53	
-0.19	-0.53	-0.53	-0.19	-0.53	-0.73	-0.19	-0.19	
-0.53	-0.19	-0.35	-0.53	-0.53	-0.19	-0.19	-0.19	
-0.73	-0.73	-0.19	-0.35	-0.19	-0.53	-0.19	-0.19	
-0.19	-0.53	-0.19	-0.53	-0.19	-0.19	-0.19	-0.19	
-0.35	-0.53	-0.19	-0.53	-0.19	-0.35	-0.53	-0.19	
-0.19	-0.73	-0.73	-0.73	-0.19	-0.19	-0.53	-0.02	
Map policy
>	v	>	>	>	>	v	v	
v	v	v	v	>	>	v	v	
v	v	v	v	v	>	v	v	
>	>	v	v	>	>	v	v	
v	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.6445924991526784
mean w [-0.22244164 -0.25318375 -0.62715451 -0.11689247 -0.42175408 -0.18782615]
Mean policy from posterior
>	v	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	^	>	>	>	>	>	.	
Mean rewards
-0.42	-0.42	-0.63	-0.12	-0.25	-0.25	-0.25	-0.63	
-0.63	-0.12	-0.25	-0.63	-0.42	-0.63	-0.22	-0.42	
-0.12	-0.42	-0.42	-0.12	-0.42	-0.63	-0.22	-0.22	
-0.42	-0.12	-0.25	-0.42	-0.42	-0.12	-0.22	-0.12	
-0.63	-0.63	-0.12	-0.25	-0.12	-0.42	-0.12	-0.22	
-0.22	-0.42	-0.12	-0.42	-0.12	-0.12	-0.22	-0.12	
-0.25	-0.42	-0.12	-0.42	-0.22	-0.25	-0.42	-0.22	
-0.22	-0.63	-0.63	-0.63	-0.12	-0.22	-0.42	-0.19	
mean = 0.033702356300142816, map = 0.07034879716601256
CVaR policy
>	v	v	v	v	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	v	v	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	v	v	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	v	v	>	v	
^	^	>	>	>	>	>	.	
cvar = , 0.0662422975777609, 0.05393220712733665, 0.05393221234300388, 0.03110555955894842, 0.03110554698865986
==========
iteration 82
==========
weights [-0.0970151  -0.30073936 -0.15961981 -0.07668966 -0.86338445  0.3512142 ]
expeced value MDP LP -1.1579534958900672
demonstration
[(0, 3), (8, 1), (9, 1), (10, 3), (18, 3), (26, 3), (34, 1), (35, 3), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.45810799  0.1283837   0.34414942 -0.67957235 -0.16085752 -0.4092947 ]
w_map [-0.23454013 -0.47259915 -0.23805118 -0.23695905 -0.74747504 -0.22383941] loglik -1.3862943702872101
accepted/total = 1699/3000 = 0.5663333333333334
-------
true weights [-0.0970151  -0.30073936 -0.15961981 -0.07668966 -0.86338445  0.3512142 ]
features
0 	1 	3 	2 	0 	3 	4 	0 	
2 	2 	2 	3 	3 	0 	4 	2 	
1 	4 	0 	0 	4 	0 	4 	2 	
0 	3 	1 	4 	1 	4 	4 	3 	
1 	4 	2 	1 	0 	3 	1 	1 	
3 	3 	4 	0 	2 	1 	3 	0 	
2 	1 	2 	2 	2 	1 	4 	4 	
0 	4 	4 	2 	4 	0 	0 	5 	
optimal policy
v	>	v	v	v	v	>	v	
>	>	v	v	<	v	>	v	
v	v	v	<	v	v	>	v	
v	>	v	>	v	v	v	v	
v	v	>	v	v	v	v	v	
>	v	>	>	v	v	<	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
optimal values
-1.89	-1.87	-1.59	-1.67	-1.67	-1.61	-2.20	-1.35	
-1.81	-1.67	-1.52	-1.52	-1.59	-1.55	-2.12	-1.27	
-1.71	-2.21	-1.38	-1.46	-1.71	-1.46	-1.97	-1.12	
-1.42	-1.36	-1.30	-1.71	-0.85	-1.38	-1.67	-0.97	
-1.34	-1.84	-1.00	-0.85	-0.56	-0.52	-0.82	-0.90	
-1.05	-0.98	-1.42	-0.56	-0.47	-0.45	-0.52	-0.61	
-1.07	-0.92	-0.62	-0.47	-0.31	-0.15	-0.62	-0.52	
-1.15	-1.77	-1.48	-0.62	-0.71	0.15	0.25	0.35	
map_weights [-0.23454013 -0.47259915 -0.23805118 -0.23695905 -0.74747504 -0.22383941]
MAP reward
-0.23	-0.47	-0.24	-0.24	-0.23	-0.24	-0.75	-0.23	
-0.24	-0.24	-0.24	-0.24	-0.24	-0.23	-0.75	-0.24	
-0.47	-0.75	-0.23	-0.23	-0.75	-0.23	-0.75	-0.24	
-0.23	-0.24	-0.47	-0.75	-0.47	-0.75	-0.75	-0.24	
-0.47	-0.75	-0.24	-0.47	-0.23	-0.24	-0.47	-0.47	
-0.24	-0.24	-0.75	-0.23	-0.24	-0.47	-0.24	-0.23	
-0.24	-0.47	-0.24	-0.24	-0.24	-0.47	-0.75	-0.75	
-0.23	-0.75	-0.75	-0.24	-0.75	-0.23	-0.23	-0.22	
Map policy
v	>	v	>	>	v	>	v	
>	>	v	>	>	v	>	v	
v	>	v	v	v	v	>	v	
>	>	v	>	v	v	>	v	
v	>	>	v	v	v	v	v	
>	v	>	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.336891928634624
mean w [-0.1699252  -0.27997578 -0.12346101 -0.45611283 -0.62062597  0.28054156]
Mean policy from posterior
v	v	v	v	v	v	>	v	
>	>	v	<	v	v	>	v	
^	>	v	<	v	v	>	v	
v	>	v	v	v	<	v	v	
v	>	>	v	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
Mean rewards
-0.17	-0.28	-0.46	-0.12	-0.17	-0.46	-0.62	-0.17	
-0.12	-0.12	-0.12	-0.46	-0.46	-0.17	-0.62	-0.12	
-0.28	-0.62	-0.17	-0.17	-0.62	-0.17	-0.62	-0.12	
-0.17	-0.46	-0.28	-0.62	-0.28	-0.62	-0.62	-0.46	
-0.28	-0.62	-0.12	-0.28	-0.17	-0.46	-0.28	-0.28	
-0.46	-0.46	-0.62	-0.17	-0.12	-0.28	-0.46	-0.17	
-0.12	-0.28	-0.12	-0.12	-0.12	-0.28	-0.62	-0.62	
-0.17	-0.62	-0.62	-0.12	-0.62	-0.17	-0.17	0.28	
mean = 0.054585017011836934, map = 0.026598489035523976
CVaR policy
v	v	v	>	v	v	>	v	
>	>	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	>	v	v	v	v	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
>	>	v	v	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	>	v	<	v	v	>	v	
v	>	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
v	>	>	v	v	v	>	v	
v	v	v	>	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	>	v	<	v	v	>	v	
^	>	v	<	v	v	>	v	
v	>	v	>	v	<	v	v	
v	>	>	v	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
CVaR policy
v	v	v	v	v	v	>	v	
>	>	v	<	v	v	>	v	
^	>	v	<	v	<	>	v	
v	>	v	v	v	<	v	v	
v	>	>	v	v	v	>	v	
v	v	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
^	^	>	^	>	>	>	.	
cvar = , 0.04568506221184654, 0.05048301285871526, 0.0347462576341262, 0.05458501501343882, 0.05458501545937389
==========
iteration 83
==========
weights [-0.56886229 -0.39226077 -0.38195774 -0.41244511 -0.31038352  0.33194363]
expeced value MDP LP -2.414623707571926
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 3), (30, 3), (38, 3), (46, 3), (54, 1), (55, 3), (63, None)]
[ 0.75318796 -0.2460004  -0.38434836 -0.44101687  0.00424176 -0.17307272]
w_map [-0.84033357 -0.35846173 -0.1727611  -0.22678685 -0.15571333  0.24457997] loglik -0.6931471850701156
accepted/total = 1463/3000 = 0.4876666666666667
-------
true weights [-0.56886229 -0.39226077 -0.38195774 -0.41244511 -0.31038352  0.33194363]
features
1 	3 	1 	1 	0 	3 	3 	2 	
0 	4 	0 	0 	2 	4 	0 	3 	
0 	1 	2 	2 	2 	3 	4 	0 	
0 	4 	4 	4 	0 	1 	4 	4 	
3 	0 	3 	1 	3 	0 	1 	2 	
3 	2 	3 	3 	0 	2 	1 	0 	
1 	4 	0 	0 	3 	0 	3 	3 	
1 	0 	1 	3 	4 	0 	3 	5 	
optimal policy
>	v	>	>	v	v	v	v	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
>	v	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-4.67	-4.33	-4.15	-3.79	-3.44	-2.93	-2.80	-2.64	
-4.48	-3.95	-3.86	-3.44	-2.90	-2.54	-2.41	-2.28	
-4.21	-3.68	-3.32	-2.97	-2.61	-2.25	-1.86	-1.88	
-3.87	-3.34	-3.06	-2.78	-2.49	-1.94	-1.56	-1.33	
-3.69	-3.47	-2.96	-2.58	-2.21	-1.81	-1.27	-1.03	
-3.32	-2.93	-2.60	-2.21	-1.81	-1.26	-0.88	-0.65	
-2.94	-2.58	-2.29	-1.91	-1.36	-1.06	-0.50	-0.08	
-2.66	-2.29	-1.74	-1.36	-0.96	-0.65	-0.08	0.33	
map_weights [-0.84033357 -0.35846173 -0.1727611  -0.22678685 -0.15571333  0.24457997]
MAP reward
-0.36	-0.23	-0.36	-0.36	-0.84	-0.23	-0.23	-0.17	
-0.84	-0.16	-0.84	-0.84	-0.17	-0.16	-0.84	-0.23	
-0.84	-0.36	-0.17	-0.17	-0.17	-0.23	-0.16	-0.84	
-0.84	-0.16	-0.16	-0.16	-0.84	-0.36	-0.16	-0.16	
-0.23	-0.84	-0.23	-0.36	-0.23	-0.84	-0.36	-0.17	
-0.23	-0.17	-0.23	-0.23	-0.84	-0.17	-0.36	-0.84	
-0.36	-0.16	-0.84	-0.84	-0.23	-0.84	-0.23	-0.23	
-0.36	-0.84	-0.36	-0.23	-0.16	-0.84	-0.23	0.24	
Map policy
>	v	<	>	v	v	<	<	
>	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	^	>	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	^	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0515401868309113
mean w [-0.72076842 -0.28584138 -0.17594036 -0.24055186 -0.186142   -0.12206454]
Mean policy from posterior
>	v	<	v	v	v	<	<	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	^	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.29	-0.24	-0.29	-0.29	-0.72	-0.24	-0.24	-0.18	
-0.72	-0.19	-0.72	-0.72	-0.18	-0.19	-0.72	-0.24	
-0.72	-0.29	-0.18	-0.18	-0.18	-0.24	-0.19	-0.72	
-0.72	-0.19	-0.19	-0.19	-0.72	-0.29	-0.19	-0.19	
-0.24	-0.72	-0.24	-0.29	-0.24	-0.72	-0.29	-0.18	
-0.24	-0.18	-0.24	-0.24	-0.72	-0.18	-0.29	-0.72	
-0.29	-0.19	-0.72	-0.72	-0.24	-0.72	-0.24	-0.24	
-0.29	-0.72	-0.29	-0.24	-0.19	-0.72	-0.24	-0.12	
mean = 0.09104093239115718, map = 0.08336805860858254
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	v	v	
v	v	>	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	v	v	v	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	v	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	^	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	v	v	<	v	
>	v	v	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	^	^	>	>	v	v	
v	v	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	^	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.00659104407525124, 0.03900252902193024, 0.039314756152178276, 0.07504676731107685, 0.07504676711594183
==========
iteration 84
==========
weights [-0.05882344 -0.44639466 -0.61216112 -0.57223911 -0.20929647  0.22642385]
expeced value MDP LP -1.361321481278447
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 1), (19, 1), (20, 3), (28, 3), (36, 1), (37, 3), (45, 1), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.09076659 -0.14164782  0.07360991  0.94777996 -0.08385701 -0.24690088]
w_map [-0.15422698 -0.47113673 -0.67183472 -0.46463418 -0.29318311  0.03226571] loglik -5.497771907059246e-06
accepted/total = 1513/3000 = 0.5043333333333333
-------
true weights [-0.05882344 -0.44639466 -0.61216112 -0.57223911 -0.20929647  0.22642385]
features
2 	0 	4 	0 	3 	2 	3 	4 	
3 	1 	0 	2 	0 	4 	2 	3 	
0 	2 	0 	4 	0 	2 	4 	3 	
1 	2 	4 	2 	1 	1 	4 	3 	
2 	4 	3 	1 	4 	4 	1 	4 	
2 	2 	2 	0 	3 	4 	4 	0 	
0 	0 	1 	1 	0 	2 	4 	0 	
2 	1 	1 	4 	2 	2 	1 	5 	
optimal policy
>	>	v	<	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
optimal values
-2.33	-1.73	-1.69	-1.73	-1.82	-2.05	-2.12	-1.99	
-2.48	-1.93	-1.50	-1.86	-1.26	-1.45	-1.56	-1.80	
-2.09	-2.05	-1.45	-1.41	-1.21	-1.56	-0.96	-1.24	
-2.51	-2.24	-1.65	-1.76	-1.16	-0.96	-0.75	-0.68	
-2.51	-1.92	-1.72	-1.16	-0.72	-0.52	-0.55	-0.11	
-2.26	-2.13	-1.54	-0.93	-0.88	-0.31	-0.11	0.10	
-1.67	-1.63	-1.58	-1.15	-0.71	-0.66	-0.05	0.17	
-2.26	-2.06	-1.78	-1.35	-1.31	-0.83	-0.22	0.23	
map_weights [-0.15422698 -0.47113673 -0.67183472 -0.46463418 -0.29318311  0.03226571]
MAP reward
-0.67	-0.15	-0.29	-0.15	-0.46	-0.67	-0.46	-0.29	
-0.46	-0.47	-0.15	-0.67	-0.15	-0.29	-0.67	-0.46	
-0.15	-0.67	-0.15	-0.29	-0.15	-0.67	-0.29	-0.46	
-0.47	-0.67	-0.29	-0.67	-0.47	-0.47	-0.29	-0.46	
-0.67	-0.29	-0.46	-0.47	-0.29	-0.29	-0.47	-0.29	
-0.67	-0.67	-0.67	-0.15	-0.46	-0.29	-0.29	-0.15	
-0.15	-0.15	-0.47	-0.47	-0.15	-0.67	-0.29	-0.15	
-0.67	-0.47	-0.47	-0.29	-0.67	-0.67	-0.47	0.03	
Map policy
>	>	v	>	v	v	>	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	v	v	v	v	>	v	
>	>	>	>	>	v	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
expeced value MDP LP -1.6272856386544785
mean w [-0.10477921 -0.38664788 -0.4827804  -0.51174777 -0.1870806  -0.13266683]
Mean policy from posterior
>	>	v	<	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	^	>	>	.	
Mean rewards
-0.48	-0.10	-0.19	-0.10	-0.51	-0.48	-0.51	-0.19	
-0.51	-0.39	-0.10	-0.48	-0.10	-0.19	-0.48	-0.51	
-0.10	-0.48	-0.10	-0.19	-0.10	-0.48	-0.19	-0.51	
-0.39	-0.48	-0.19	-0.48	-0.39	-0.39	-0.19	-0.51	
-0.48	-0.19	-0.51	-0.39	-0.19	-0.19	-0.39	-0.19	
-0.48	-0.48	-0.48	-0.10	-0.51	-0.19	-0.19	-0.10	
-0.10	-0.10	-0.39	-0.39	-0.10	-0.48	-0.19	-0.10	
-0.48	-0.39	-0.39	-0.19	-0.48	-0.48	-0.39	-0.13	
mean = -1.2223273948563929e-08, map = 0.026606885477108433
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	>	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	v	>	v	v	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	v	>	v	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	v	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
CVaR policy
>	>	v	<	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
CVaR policy
>	>	v	<	v	v	v	v	
>	>	v	>	v	<	v	v	
>	>	>	>	v	>	v	v	
^	>	^	>	v	v	v	v	
>	>	>	>	>	v	>	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
cvar = , 0.05689305338341821, 0.0435557330744023, 0.0058363259517839605, 0.001896556939432914, 0.0018965324946713658
==========
iteration 85
==========
weights [-0.26584721 -0.30756227 -0.47704667 -0.36451169 -0.47507618  0.49858905]
expeced value MDP LP -1.85289582480823
demonstration
[(0, 3), (8, 3), (16, 3), (24, 3), (32, 1), (33, 3), (41, 1), (42, 1), (43, 3), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[-0.67259705 -0.149985    0.54516711 -0.20142106 -0.08210428 -0.42496937]
w_map [-0.24966321 -0.27661872 -0.51691841 -0.53152154 -0.51991714 -0.20277264] loglik -8.478906465825276e-10
accepted/total = 1913/3000 = 0.6376666666666667
-------
true weights [-0.26584721 -0.30756227 -0.47704667 -0.36451169 -0.47507618  0.49858905]
features
1 	4 	2 	4 	4 	3 	4 	2 	
3 	1 	4 	3 	0 	3 	3 	4 	
1 	2 	4 	3 	1 	2 	4 	0 	
1 	3 	1 	3 	3 	4 	1 	4 	
0 	1 	3 	2 	3 	2 	1 	1 	
4 	1 	4 	1 	2 	2 	4 	4 	
1 	2 	2 	1 	0 	1 	3 	0 	
1 	3 	3 	3 	3 	3 	2 	5 	
optimal policy
v	v	>	v	v	v	v	v	
v	v	>	>	v	>	v	v	
v	v	v	v	v	v	v	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
optimal values
-3.75	-3.85	-3.64	-3.20	-2.86	-2.65	-2.42	-2.20	
-3.48	-3.41	-3.20	-2.75	-2.41	-2.31	-1.97	-1.74	
-3.15	-3.13	-2.85	-2.46	-2.17	-2.08	-1.62	-1.28	
-2.87	-2.68	-2.40	-2.12	-1.88	-1.62	-1.16	-1.02	
-2.59	-2.34	-2.11	-1.77	-1.53	-1.33	-0.86	-0.55	
-2.51	-2.06	-1.77	-1.30	-1.18	-0.92	-0.61	-0.25	
-2.22	-1.94	-1.47	-1.01	-0.71	-0.45	-0.14	0.23	
-2.06	-1.77	-1.42	-1.07	-0.71	-0.35	0.02	0.50	
map_weights [-0.24966321 -0.27661872 -0.51691841 -0.53152154 -0.51991714 -0.20277264]
MAP reward
-0.28	-0.52	-0.52	-0.52	-0.52	-0.53	-0.52	-0.52	
-0.53	-0.28	-0.52	-0.53	-0.25	-0.53	-0.53	-0.52	
-0.28	-0.52	-0.52	-0.53	-0.28	-0.52	-0.52	-0.25	
-0.28	-0.53	-0.28	-0.53	-0.53	-0.52	-0.28	-0.52	
-0.25	-0.28	-0.53	-0.52	-0.53	-0.52	-0.28	-0.28	
-0.52	-0.28	-0.52	-0.28	-0.52	-0.52	-0.52	-0.52	
-0.28	-0.52	-0.52	-0.28	-0.25	-0.28	-0.53	-0.25	
-0.28	-0.53	-0.53	-0.53	-0.53	-0.53	-0.52	-0.20	
Map policy
v	v	>	>	v	v	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	v	>	v	v	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
^	^	^	^	>	>	>	.	
expeced value MDP LP -2.0493985167110367
mean w [-0.18998308 -0.16259746 -0.57161976 -0.46365918 -0.42005598 -0.1482949 ]
Mean policy from posterior
v	v	>	>	v	>	v	v	
v	<	>	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
^	>	>	>	>	>	>	v	
^	>	>	^	^	>	>	.	
Mean rewards
-0.16	-0.42	-0.57	-0.42	-0.42	-0.46	-0.42	-0.57	
-0.46	-0.16	-0.42	-0.46	-0.19	-0.46	-0.46	-0.42	
-0.16	-0.57	-0.42	-0.46	-0.16	-0.57	-0.42	-0.19	
-0.16	-0.46	-0.16	-0.46	-0.46	-0.42	-0.16	-0.42	
-0.19	-0.16	-0.46	-0.57	-0.46	-0.57	-0.16	-0.16	
-0.42	-0.16	-0.42	-0.16	-0.57	-0.57	-0.42	-0.42	
-0.16	-0.57	-0.57	-0.16	-0.19	-0.16	-0.46	-0.19	
-0.16	-0.46	-0.46	-0.46	-0.46	-0.46	-0.57	-0.15	
mean = 0.07666385616280302, map = 0.061217456916784485
CVaR policy
v	v	>	>	v	>	v	v	
v	v	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
CVaR policy
v	v	>	>	v	>	v	v	
v	<	>	>	v	>	v	v	
v	v	v	>	v	v	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	^	>	>	>	.	
CVaR policy
v	<	>	>	v	>	v	v	
v	<	>	>	v	>	v	v	
v	v	v	>	v	>	v	v	
v	v	v	v	>	>	v	v	
>	v	v	v	>	>	>	v	
>	>	>	v	v	v	>	v	
>	>	>	>	>	>	>	v	
^	>	>	^	^	>	>	.	
cvar = , 0.03749800825728378, 0.042816368808145544, 0.04281636958285984, 0.053429370362714224, 0.058974602388160324
==========
iteration 86
==========
weights [-0.75097374 -0.0187764  -0.15173761 -0.13237438 -0.48458289  0.40039735]
expeced value MDP LP -1.0553840498000273
demonstration
[(0, 3), (8, 1), (9, 1), (10, 1), (11, 1), (12, 3), (20, 3), (28, 1), (29, 3), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.57637069 -0.02905017 -0.68324686 -0.39332681 -0.03628111  0.21001037]
w_map [-0.42979829 -0.0527658  -0.56600825 -0.18226892 -0.67470827  0.06058587] loglik -1.018438716443356
accepted/total = 1443/3000 = 0.481
-------
true weights [-0.75097374 -0.0187764  -0.15173761 -0.13237438 -0.48458289  0.40039735]
features
3 	4 	3 	0 	0 	3 	3 	3 	
4 	3 	3 	2 	1 	3 	3 	0 	
2 	2 	2 	3 	1 	2 	1 	4 	
1 	2 	3 	0 	3 	2 	4 	4 	
1 	4 	4 	1 	4 	3 	1 	1 	
4 	2 	4 	2 	4 	0 	2 	0 	
0 	3 	1 	3 	1 	0 	0 	2 	
1 	2 	0 	2 	4 	4 	1 	5 	
optimal policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	<	v	<	
>	>	>	>	v	v	<	<	
>	>	^	v	>	v	v	v	
^	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	^	>	>	>	>	>	.	
optimal values
-1.92	-1.81	-1.34	-1.83	-1.69	-1.20	-1.21	-1.33	
-1.81	-1.34	-1.21	-1.09	-0.95	-1.07	-1.08	-1.82	
-1.48	-1.35	-1.21	-1.06	-0.94	-0.95	-0.96	-1.44	
-1.47	-1.46	-1.33	-1.63	-0.93	-0.81	-1.02	-1.00	
-1.47	-1.49	-1.36	-0.89	-1.14	-0.66	-0.54	-0.52	
-1.49	-1.01	-1.22	-0.88	-1.09	-1.27	-0.53	-0.51	
-1.61	-0.87	-0.74	-0.73	-0.61	-0.86	-0.38	0.24	
-1.02	-1.01	-1.48	-0.74	-0.59	-0.11	0.38	0.40	
map_weights [-0.42979829 -0.0527658  -0.56600825 -0.18226892 -0.67470827  0.06058587]
MAP reward
-0.18	-0.67	-0.18	-0.43	-0.43	-0.18	-0.18	-0.18	
-0.67	-0.18	-0.18	-0.57	-0.05	-0.18	-0.18	-0.43	
-0.57	-0.57	-0.57	-0.18	-0.05	-0.57	-0.05	-0.67	
-0.05	-0.57	-0.18	-0.43	-0.18	-0.57	-0.67	-0.67	
-0.05	-0.67	-0.67	-0.05	-0.67	-0.18	-0.05	-0.05	
-0.67	-0.57	-0.67	-0.57	-0.67	-0.43	-0.57	-0.43	
-0.43	-0.18	-0.05	-0.18	-0.05	-0.43	-0.43	-0.57	
-0.05	-0.57	-0.43	-0.57	-0.67	-0.67	-0.05	0.06	
Map policy
>	>	v	>	v	v	v	<	
>	>	>	>	v	>	v	<	
v	>	>	>	v	>	v	v	
v	>	>	v	>	v	v	v	
v	v	>	v	>	>	>	v	
v	v	v	v	v	v	v	v	
>	>	>	>	>	>	v	v	
^	^	^	^	>	>	>	.	
expeced value MDP LP -2.0125788175789907
mean w [-0.49649573 -0.11018006 -0.35548407 -0.19805664 -0.61554827 -0.10642368]
Mean policy from posterior
>	>	v	v	v	v	v	<	
>	>	>	>	v	v	v	<	
>	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	>	>	>	>	.	
Mean rewards
-0.20	-0.62	-0.20	-0.50	-0.50	-0.20	-0.20	-0.20	
-0.62	-0.20	-0.20	-0.36	-0.11	-0.20	-0.20	-0.50	
-0.36	-0.36	-0.36	-0.20	-0.11	-0.36	-0.11	-0.62	
-0.11	-0.36	-0.20	-0.50	-0.20	-0.36	-0.62	-0.62	
-0.11	-0.62	-0.62	-0.11	-0.62	-0.20	-0.11	-0.11	
-0.62	-0.36	-0.62	-0.36	-0.62	-0.50	-0.36	-0.50	
-0.50	-0.20	-0.11	-0.20	-0.11	-0.50	-0.50	-0.36	
-0.11	-0.36	-0.50	-0.36	-0.62	-0.62	-0.11	-0.11	
mean = 0.16807133021256782, map = 0.28313108913941454
CVaR policy
>	>	v	v	v	v	v	v	
>	>	>	>	v	>	v	v	
v	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	>	v	v	
>	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	v	v	<	
>	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
v	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	v	v	<	
>	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
>	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	<	
>	>	>	>	v	v	v	<	
>	>	>	>	v	v	v	v	
v	>	>	>	>	v	v	v	
>	v	>	v	>	>	>	v	
>	v	v	v	v	>	v	v	
>	>	>	>	>	>	v	v	
>	^	^	>	>	>	>	.	
cvar = , 0.19885658930966477, 0.17112946734934353, 0.1697273180244836, 0.1680713315974145, 0.16807133111673656
==========
iteration 87
==========
weights [-0.02163477 -0.78721642 -0.11620583 -0.11232309 -0.39244965  0.44686155]
expeced value MDP LP -0.6133073751083885
demonstration
[(0, 1), (1, 1), (2, 3), (10, 3), (18, 3), (26, 3), (34, 1), (35, 1), (36, 3), (44, 3), (52, 3), (60, 1), (61, 1), (62, 1), (63, None)]
[-0.55131911  0.45620416 -0.13946753  0.18590298 -0.2011233   0.62726654]
w_map [-0.20263609 -0.59773087 -0.41494537 -0.19241384 -0.5975958   0.18797058] loglik -0.6931474130111326
accepted/total = 1516/3000 = 0.5053333333333333
-------
true weights [-0.02163477 -0.78721642 -0.11620583 -0.11232309 -0.39244965  0.44686155]
features
0 	2 	2 	4 	1 	4 	0 	1 	
4 	2 	0 	2 	4 	1 	4 	2 	
3 	4 	3 	0 	4 	2 	1 	0 	
4 	4 	0 	1 	1 	3 	0 	4 	
1 	2 	2 	2 	3 	2 	3 	4 	
1 	3 	3 	4 	0 	4 	2 	2 	
3 	0 	4 	4 	0 	4 	2 	1 	
2 	3 	1 	0 	0 	4 	2 	5 	
optimal policy
>	>	v	v	v	>	v	v	
>	>	v	v	v	v	>	v	
>	>	v	<	>	v	v	v	
>	>	v	v	>	>	v	<	
>	>	>	>	v	>	v	<	
>	>	^	>	v	>	v	<	
>	^	>	v	v	>	v	v	
>	^	>	>	>	>	>	.	
optimal values
-0.84	-0.83	-0.72	-1.10	-1.82	-1.35	-0.97	-1.35	
-1.10	-0.72	-0.61	-0.72	-1.05	-1.06	-0.95	-0.57	
-1.08	-0.98	-0.59	-0.61	-0.66	-0.27	-0.83	-0.46	
-1.26	-0.87	-0.49	-1.14	-0.94	-0.16	-0.05	-0.44	
-1.36	-0.58	-0.47	-0.36	-0.24	-0.14	-0.02	-0.42	
-1.46	-0.68	-0.58	-0.52	-0.13	-0.30	0.09	-0.03	
-0.80	-0.70	-0.89	-0.50	-0.11	-0.19	0.21	-0.34	
-0.91	-0.80	-0.90	-0.11	-0.09	-0.07	0.33	0.45	
map_weights [-0.20263609 -0.59773087 -0.41494537 -0.19241384 -0.5975958   0.18797058]
MAP reward
-0.20	-0.41	-0.41	-0.60	-0.60	-0.60	-0.20	-0.60	
-0.60	-0.41	-0.20	-0.41	-0.60	-0.60	-0.60	-0.41	
-0.19	-0.60	-0.19	-0.20	-0.60	-0.41	-0.60	-0.20	
-0.60	-0.60	-0.20	-0.60	-0.60	-0.19	-0.20	-0.60	
-0.60	-0.41	-0.41	-0.41	-0.19	-0.41	-0.19	-0.60	
-0.60	-0.19	-0.19	-0.60	-0.20	-0.60	-0.41	-0.41	
-0.19	-0.20	-0.60	-0.60	-0.20	-0.60	-0.41	-0.60	
-0.41	-0.19	-0.60	-0.20	-0.20	-0.60	-0.41	0.19	
Map policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	v	>	>	v	>	v	v	
v	v	>	>	v	>	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.827377178524665
mean w [-0.11815447 -0.55087739 -0.31303489 -0.23653664 -0.50083467  0.10280459]
Mean policy from posterior
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.12	-0.31	-0.31	-0.50	-0.55	-0.50	-0.12	-0.55	
-0.50	-0.31	-0.12	-0.31	-0.50	-0.55	-0.50	-0.31	
-0.24	-0.50	-0.24	-0.12	-0.50	-0.31	-0.55	-0.12	
-0.50	-0.50	-0.12	-0.55	-0.55	-0.24	-0.12	-0.50	
-0.55	-0.31	-0.31	-0.31	-0.24	-0.31	-0.24	-0.50	
-0.55	-0.24	-0.24	-0.50	-0.12	-0.50	-0.31	-0.31	
-0.24	-0.12	-0.50	-0.50	-0.12	-0.50	-0.31	-0.55	
-0.31	-0.24	-0.55	-0.12	-0.12	-0.50	-0.31	0.10	
mean = 0.06778207829778038, map = 0.14813679789558465
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	v	v	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	>	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	v	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	v	
>	>	>	>	v	>	v	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.16905991622611116, 0.1165022445899968, 0.11650224340369009, 0.06778207387236246, 0.06778207449174622
==========
iteration 88
==========
weights [-0.06630414 -0.07244412 -0.08869352 -0.20137503 -0.97022566  0.02448124]
expeced value MDP LP -0.928148503246208
demonstration
[(0, 3), (8, 3), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 3), (30, 3), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.05182758 -0.32608284  0.4724304   0.04482594  0.80935289 -0.10359504]
w_map [-0.0875778  -0.25208473 -0.06411651 -0.19352196 -0.93564904  0.10854778] loglik -4.007279291017894
accepted/total = 1093/3000 = 0.36433333333333334
-------
true weights [-0.06630414 -0.07244412 -0.08869352 -0.20137503 -0.97022566  0.02448124]
features
0 	4 	4 	0 	4 	0 	1 	0 	
4 	1 	0 	4 	0 	2 	4 	3 	
0 	1 	2 	1 	2 	0 	1 	3 	
2 	1 	1 	2 	4 	4 	3 	1 	
0 	0 	4 	4 	1 	3 	1 	1 	
4 	0 	4 	3 	3 	4 	1 	0 	
1 	1 	0 	4 	0 	0 	0 	0 	
3 	1 	1 	4 	0 	4 	4 	5 	
optimal policy
v	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	>	^	v	v	v	v	
>	^	^	>	v	>	v	v	
^	^	>	>	v	v	v	v	
>	^	<	>	>	>	>	v	
^	^	^	>	^	^	>	.	
optimal values
-1.94	-1.89	-1.83	-1.73	-1.68	-0.72	-0.77	-0.71	
-1.89	-0.93	-0.87	-1.68	-0.72	-0.66	-1.48	-0.65	
-0.93	-0.87	-0.81	-0.73	-0.66	-0.58	-0.52	-0.45	
-1.01	-0.94	-0.87	-0.81	-1.47	-1.41	-0.45	-0.25	
-1.05	-0.99	-1.83	-1.47	-0.50	-0.45	-0.25	-0.18	
-2.01	-1.05	-1.60	-0.63	-0.44	-1.14	-0.18	-0.11	
-1.17	-1.11	-1.17	-1.21	-0.24	-0.17	-0.11	-0.04	
-1.36	-1.17	-1.23	-1.27	-0.30	-1.14	-0.95	0.02	
map_weights [-0.0875778  -0.25208473 -0.06411651 -0.19352196 -0.93564904  0.10854778]
MAP reward
-0.09	-0.94	-0.94	-0.09	-0.94	-0.09	-0.25	-0.09	
-0.94	-0.25	-0.09	-0.94	-0.09	-0.06	-0.94	-0.19	
-0.09	-0.25	-0.06	-0.25	-0.06	-0.09	-0.25	-0.19	
-0.06	-0.25	-0.25	-0.06	-0.94	-0.94	-0.19	-0.25	
-0.09	-0.09	-0.94	-0.94	-0.25	-0.19	-0.25	-0.25	
-0.94	-0.09	-0.94	-0.19	-0.19	-0.94	-0.25	-0.09	
-0.25	-0.25	-0.09	-0.94	-0.09	-0.09	-0.09	-0.09	
-0.19	-0.25	-0.25	-0.94	-0.09	-0.94	-0.94	0.11	
Map policy
v	v	v	>	>	v	>	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	>	v	v	
v	v	>	^	v	v	v	v	
>	v	v	v	v	>	v	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
expeced value MDP LP -1.8250043876083606
mean w [-0.18569022 -0.22044595 -0.15804283 -0.31717463 -0.75095654 -0.1232379 ]
Mean policy from posterior
v	v	v	>	>	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
^	>	>	^	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
Mean rewards
-0.19	-0.75	-0.75	-0.19	-0.75	-0.19	-0.22	-0.19	
-0.75	-0.22	-0.19	-0.75	-0.19	-0.16	-0.75	-0.32	
-0.19	-0.22	-0.16	-0.22	-0.16	-0.19	-0.22	-0.32	
-0.16	-0.22	-0.22	-0.16	-0.75	-0.75	-0.32	-0.22	
-0.19	-0.19	-0.75	-0.75	-0.22	-0.32	-0.22	-0.22	
-0.75	-0.19	-0.75	-0.32	-0.32	-0.75	-0.22	-0.19	
-0.22	-0.22	-0.19	-0.75	-0.19	-0.19	-0.19	-0.19	
-0.32	-0.22	-0.22	-0.75	-0.19	-0.75	-0.75	-0.12	
mean = 0.04851784076525223, map = 0.07150089797589332
CVaR policy
v	v	v	>	>	v	>	v	
v	>	v	>	>	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	v	v	v	v	v	
>	v	v	v	v	>	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	^	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	>	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	^	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
CVaR policy
v	v	v	>	v	v	>	v	
v	>	v	>	>	v	v	v	
>	>	>	>	>	>	v	v	
v	v	>	^	v	v	v	v	
>	v	>	>	v	>	v	v	
>	v	>	>	v	v	>	v	
>	>	>	>	>	>	>	v	
>	>	^	>	^	^	>	.	
cvar = , 0.2635911988512709, 0.10587465798123286, 0.07749210013672281, 0.0655376425899965, 0.0655376422056051
==========
iteration 89
==========
weights [-0.58568471 -0.0204329  -0.03012558 -0.5176621  -0.36880953  0.50165112]
expeced value MDP LP -1.1214171522536454
demonstration
[(0, 1), (1, 3), (9, 1), (10, 3), (18, 1), (19, 3), (27, 3), (35, 3), (43, 3), (51, 1), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.29490212 -0.70777919  0.46792116  0.11436355  0.01776531  0.42395344]
w_map [-0.63688386 -0.20220227 -0.32507429 -0.48470738 -0.45502459 -0.0763628 ] loglik -0.6931545673519679
accepted/total = 1440/3000 = 0.48
-------
true weights [-0.58568471 -0.0204329  -0.03012558 -0.5176621  -0.36880953  0.50165112]
features
4 	1 	4 	0 	1 	1 	3 	1 	
4 	1 	2 	3 	3 	4 	0 	2 	
1 	4 	2 	1 	3 	1 	1 	2 	
4 	0 	1 	4 	2 	0 	0 	3 	
2 	3 	3 	1 	2 	4 	0 	3 	
2 	4 	4 	1 	4 	2 	4 	3 	
1 	0 	0 	1 	4 	1 	0 	4 	
0 	1 	4 	0 	4 	4 	4 	5 	
optimal policy
>	v	v	<	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	<	<	<	
v	>	>	v	v	v	^	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	v	<	v	
^	>	>	>	>	v	>	v	
^	>	>	>	>	>	>	.	
optimal values
-1.46	-1.10	-1.44	-2.01	-1.59	-1.58	-1.78	-1.27	
-1.45	-1.09	-1.08	-1.55	-1.72	-1.58	-1.80	-1.26	
-1.42	-1.42	-1.06	-1.04	-1.21	-1.22	-1.23	-1.25	
-1.76	-1.62	-1.04	-1.03	-0.70	-1.23	-1.80	-1.41	
-1.41	-1.69	-1.18	-0.67	-0.68	-0.65	-1.23	-0.90	
-1.39	-1.38	-1.02	-0.65	-0.65	-0.29	-0.65	-0.39	
-1.40	-1.79	-1.22	-0.64	-0.63	-0.26	-0.46	0.13	
-1.97	-1.55	-1.55	-1.19	-0.61	-0.24	0.13	0.50	
map_weights [-0.63688386 -0.20220227 -0.32507429 -0.48470738 -0.45502459 -0.0763628 ]
MAP reward
-0.46	-0.20	-0.46	-0.64	-0.20	-0.20	-0.48	-0.20	
-0.46	-0.20	-0.33	-0.48	-0.48	-0.46	-0.64	-0.33	
-0.20	-0.46	-0.33	-0.20	-0.48	-0.20	-0.20	-0.33	
-0.46	-0.64	-0.20	-0.46	-0.33	-0.64	-0.64	-0.48	
-0.33	-0.48	-0.48	-0.20	-0.33	-0.46	-0.64	-0.48	
-0.33	-0.46	-0.46	-0.20	-0.46	-0.33	-0.46	-0.48	
-0.20	-0.64	-0.64	-0.20	-0.46	-0.20	-0.64	-0.46	
-0.64	-0.20	-0.46	-0.64	-0.46	-0.46	-0.46	-0.08	
Map policy
>	v	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	>	>	v	
v	>	>	v	v	v	>	v	
>	>	>	v	v	v	v	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -2.0560214101243868
mean w [-0.55191191 -0.11886933 -0.22117737 -0.55823386 -0.319521   -0.23675702]
Mean policy from posterior
>	v	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	v	<	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.32	-0.12	-0.32	-0.55	-0.12	-0.12	-0.56	-0.12	
-0.32	-0.12	-0.22	-0.56	-0.56	-0.32	-0.55	-0.22	
-0.12	-0.32	-0.22	-0.12	-0.56	-0.12	-0.12	-0.22	
-0.32	-0.55	-0.12	-0.32	-0.22	-0.55	-0.55	-0.56	
-0.22	-0.56	-0.56	-0.12	-0.22	-0.32	-0.55	-0.56	
-0.22	-0.32	-0.32	-0.12	-0.32	-0.22	-0.32	-0.56	
-0.12	-0.55	-0.55	-0.12	-0.32	-0.12	-0.55	-0.32	
-0.55	-0.12	-0.32	-0.55	-0.32	-0.32	-0.32	-0.24	
mean = 0.03661407029867503, map = 0.05678607119124601
CVaR policy
>	v	v	v	>	v	>	v	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	>	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	>	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	v	<	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	v	<	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	v	<	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
^	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	v	>	v	<	v	
>	>	v	v	>	v	v	v	
>	>	v	v	v	v	<	v	
v	>	>	v	v	v	v	v	
v	v	>	v	v	v	v	v	
>	>	>	v	>	v	v	v	
^	>	>	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.04050979783758457, 0.04067929145615867, 0.03790499297708183, 0.03661406953415991, 0.036614069509036895
==========
iteration 90
==========
weights [-0.27930672 -0.15031688 -0.42450686 -0.63538588 -0.23820077  0.50865676]
expeced value MDP LP -1.4310444342424207
demonstration
[(0, 1), (1, 3), (9, 3), (17, 1), (18, 3), (26, 1), (27, 3), (35, 1), (36, 3), (44, 3), (52, 1), (53, 3), (61, 1), (62, 1), (63, None)]
[-0.10381262 -0.11220948  0.17631033 -0.20523411 -0.26221428 -0.91360236]
w_map [-0.40998373 -0.14158203 -0.52199631 -0.64983238 -0.27093997  0.20903858] loglik -9.138168759648124e-05
accepted/total = 1165/3000 = 0.3883333333333333
-------
true weights [-0.27930672 -0.15031688 -0.42450686 -0.63538588 -0.23820077  0.50865676]
features
1 	1 	3 	3 	4 	2 	1 	1 	
3 	4 	3 	0 	3 	1 	0 	4 	
4 	1 	0 	0 	2 	3 	1 	1 	
1 	2 	1 	0 	2 	1 	4 	2 	
2 	3 	2 	1 	1 	4 	0 	3 	
4 	1 	1 	3 	0 	0 	2 	3 	
4 	4 	3 	1 	1 	2 	0 	3 	
2 	0 	2 	0 	3 	4 	0 	5 	
optimal policy
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	>	>	>	v	v	v	<	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
optimal values
-2.42	-2.29	-2.90	-2.55	-2.16	-1.94	-1.53	-1.63	
-2.77	-2.16	-2.43	-1.94	-2.15	-1.53	-1.39	-1.49	
-2.16	-1.94	-1.81	-1.67	-1.83	-1.71	-1.13	-1.27	
-2.09	-1.96	-1.55	-1.41	-1.42	-1.09	-0.99	-1.40	
-2.26	-2.17	-1.55	-1.14	-1.00	-0.95	-0.76	-1.38	
-1.85	-1.63	-1.50	-1.36	-0.86	-0.72	-0.48	-0.77	
-1.81	-1.58	-1.36	-0.73	-0.59	-0.44	-0.06	-0.13	
-2.01	-1.61	-1.34	-0.92	-0.65	-0.02	0.22	0.51	
map_weights [-0.40998373 -0.14158203 -0.52199631 -0.64983238 -0.27093997  0.20903858]
MAP reward
-0.14	-0.14	-0.65	-0.65	-0.27	-0.52	-0.14	-0.14	
-0.65	-0.27	-0.65	-0.41	-0.65	-0.14	-0.41	-0.27	
-0.27	-0.14	-0.41	-0.41	-0.52	-0.65	-0.14	-0.14	
-0.14	-0.52	-0.14	-0.41	-0.52	-0.14	-0.27	-0.52	
-0.52	-0.65	-0.52	-0.14	-0.14	-0.27	-0.41	-0.65	
-0.27	-0.14	-0.14	-0.65	-0.41	-0.41	-0.52	-0.65	
-0.27	-0.27	-0.65	-0.14	-0.14	-0.52	-0.41	-0.65	
-0.52	-0.41	-0.52	-0.41	-0.65	-0.27	-0.41	0.21	
Map policy
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
^	>	>	>	>	>	>	.	
expeced value MDP LP -1.908921420022462
mean w [-0.27600021 -0.09704685 -0.44737705 -0.65806169 -0.18290967 -0.16357786]
Mean policy from posterior
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	>	>	>	v	v	v	<	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	>	.	
Mean rewards
-0.10	-0.10	-0.66	-0.66	-0.18	-0.45	-0.10	-0.10	
-0.66	-0.18	-0.66	-0.28	-0.66	-0.10	-0.28	-0.18	
-0.18	-0.10	-0.28	-0.28	-0.45	-0.66	-0.10	-0.10	
-0.10	-0.45	-0.10	-0.28	-0.45	-0.10	-0.18	-0.45	
-0.45	-0.66	-0.45	-0.10	-0.10	-0.18	-0.28	-0.66	
-0.18	-0.10	-0.10	-0.66	-0.28	-0.28	-0.45	-0.66	
-0.18	-0.18	-0.66	-0.10	-0.10	-0.45	-0.28	-0.66	
-0.45	-0.28	-0.45	-0.28	-0.66	-0.18	-0.28	-0.16	
mean = 0.00484487111083709, map = 0.010736216885932626
CVaR policy
>	v	v	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	>	v	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	v	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	>	>	>	v	v	v	v	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	>	.	
CVaR policy
>	v	<	v	>	>	v	v	
v	v	v	v	>	>	v	v	
>	>	v	v	v	v	v	<	
>	>	>	v	v	v	v	<	
v	>	>	>	v	v	v	<	
>	>	>	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	^	>	>	>	.	
cvar = , 0.040882757272734604, 0.001333969748153363, 0.0013339704671557762, 0.005002177197339197, 0.004844860712679866
==========
iteration 91
==========
weights [-0.5155032  -0.0692184  -0.77005212 -0.15576571 -0.27468915  0.19174962]
expeced value MDP LP -1.6912166083940687
demonstration
[(0, 1), (1, 3), (9, 1), (10, 1), (11, 1), (12, 1), (13, 3), (21, 3), (29, 3), (37, 3), (45, 3), (53, 3), (61, 1), (62, 1), (63, None)]
[ 0.64350646 -0.3913706  -0.22048031 -0.03128371  0.30020263  0.54131011]
w_map [-0.43998826 -0.22442638 -0.68730404 -0.45597195 -0.27511311 -0.00765729] loglik -1.2530847470770823e-09
accepted/total = 1943/3000 = 0.6476666666666666
-------
true weights [-0.5155032  -0.0692184  -0.77005212 -0.15576571 -0.27468915  0.19174962]
features
4 	3 	3 	4 	4 	2 	1 	1 	
2 	1 	1 	4 	0 	1 	0 	1 	
4 	3 	2 	2 	2 	1 	0 	1 	
0 	1 	2 	1 	0 	3 	2 	0 	
3 	2 	3 	0 	0 	0 	3 	2 	
2 	4 	3 	0 	3 	4 	2 	3 	
4 	3 	1 	0 	3 	1 	0 	2 	
4 	1 	3 	2 	1 	1 	0 	5 	
optimal policy
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	<	v	
>	v	v	v	>	v	<	<	
>	>	v	v	v	v	v	v	
>	>	v	v	v	v	<	v	
>	v	v	>	v	v	<	v	
>	>	>	>	v	v	v	v	
>	>	^	>	>	>	>	.	
optimal values
-2.72	-2.47	-2.42	-2.50	-2.24	-2.24	-2.04	-2.08	
-3.08	-2.34	-2.29	-2.24	-1.99	-1.49	-1.99	-2.03	
-2.65	-2.40	-2.97	-2.57	-2.19	-1.43	-1.94	-1.99	
-2.76	-2.26	-2.22	-1.82	-1.77	-1.38	-2.13	-1.99	
-2.35	-2.22	-1.46	-1.77	-1.27	-1.24	-1.38	-1.49	
-2.34	-1.58	-1.32	-1.27	-0.76	-0.73	-1.49	-0.73	
-1.58	-1.32	-1.18	-1.12	-0.61	-0.46	-0.84	-0.58	
-1.64	-1.38	-1.32	-1.22	-0.46	-0.39	-0.33	0.19	
map_weights [-0.43998826 -0.22442638 -0.68730404 -0.45597195 -0.27511311 -0.00765729]
MAP reward
-0.28	-0.46	-0.46	-0.28	-0.28	-0.69	-0.22	-0.22	
-0.69	-0.22	-0.22	-0.28	-0.44	-0.22	-0.44	-0.22	
-0.28	-0.46	-0.69	-0.69	-0.69	-0.22	-0.44	-0.22	
-0.44	-0.22	-0.69	-0.22	-0.44	-0.46	-0.69	-0.44	
-0.46	-0.69	-0.46	-0.44	-0.44	-0.44	-0.46	-0.69	
-0.69	-0.28	-0.46	-0.44	-0.46	-0.28	-0.69	-0.46	
-0.28	-0.46	-0.22	-0.44	-0.46	-0.22	-0.44	-0.69	
-0.28	-0.22	-0.46	-0.69	-0.22	-0.22	-0.44	-0.01	
Map policy
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	<	v	
v	^	^	v	>	v	<	v	
>	>	>	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.8028752489800532
mean w [-0.38107389 -0.12115017 -0.67802038 -0.35947471 -0.25275609  0.1584891 ]
Mean policy from posterior
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	<	v	
>	^	^	v	>	v	<	<	
>	v	>	>	>	v	v	v	
v	v	v	v	v	v	<	v	
v	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.25	-0.36	-0.36	-0.25	-0.25	-0.68	-0.12	-0.12	
-0.68	-0.12	-0.12	-0.25	-0.38	-0.12	-0.38	-0.12	
-0.25	-0.36	-0.68	-0.68	-0.68	-0.12	-0.38	-0.12	
-0.38	-0.12	-0.68	-0.12	-0.38	-0.36	-0.68	-0.38	
-0.36	-0.68	-0.36	-0.38	-0.38	-0.38	-0.36	-0.68	
-0.68	-0.25	-0.36	-0.38	-0.36	-0.25	-0.68	-0.36	
-0.25	-0.36	-0.12	-0.38	-0.36	-0.12	-0.38	-0.68	
-0.25	-0.12	-0.36	-0.68	-0.12	-0.12	-0.38	0.16	
mean = 0.03425600649296312, map = 0.12170275933133823
CVaR policy
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	>	v	
>	v	>	v	>	v	>	v	
>	v	>	>	>	v	v	v	
v	v	v	>	v	v	v	v	
v	v	v	>	>	v	v	v	
v	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	>	v	
>	>	>	>	>	v	<	v	
>	v	^	v	>	v	<	v	
>	v	>	>	>	v	v	v	
v	v	v	>	v	v	v	v	
v	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	<	v	
>	^	^	v	>	v	<	v	
>	v	>	>	>	v	v	v	
v	v	v	>	v	v	<	v	
v	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	<	v	
>	^	^	v	>	v	<	v	
>	v	>	>	>	v	v	v	
v	v	v	>	v	v	<	v	
v	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	v	v	>	v	v	v	v	
>	>	>	>	>	v	<	v	
>	^	^	v	>	v	<	v	
>	v	>	>	>	v	<	v	
v	v	v	v	v	v	<	v	
v	>	v	>	>	v	v	v	
>	>	>	>	v	v	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.09323913574560283, 0.05181473541537862, 0.036930591594996454, 0.03693058580298203, 0.03693059125478215
==========
iteration 92
==========
weights [-0.3587393  -0.00138778 -0.51618354 -0.20340655 -0.72640482  0.18926319]
expeced value MDP LP -0.6299052228996818
demonstration
[(0, 3), (8, 3), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0), (16, 0)]
[0.52763548 0.05954934 0.02173722 0.6612942  0.4619878  0.25853321]
w_map [ 0.31158268  0.61734885 -0.43991283  0.45563696  0.27702795 -0.20958007] loglik 0.0
accepted/total = 2751/3000 = 0.917
-------
true weights [-0.3587393  -0.00138778 -0.51618354 -0.20340655 -0.72640482  0.18926319]
features
2 	4 	4 	0 	1 	2 	3 	0 	
4 	2 	2 	3 	0 	3 	1 	2 	
1 	2 	2 	4 	4 	3 	1 	2 	
2 	1 	2 	4 	4 	3 	1 	3 	
1 	4 	2 	1 	4 	3 	1 	4 	
4 	1 	4 	1 	3 	2 	1 	3 	
1 	3 	3 	2 	4 	0 	2 	2 	
3 	3 	0 	4 	4 	0 	0 	5 	
optimal policy
v	v	>	>	^	<	v	<	
v	v	>	>	^	>	v	<	
<	<	<	^	>	>	v	<	
v	<	<	v	>	>	v	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	v	^	v	
^	<	<	^	>	>	>	.	
optimal values
-1.37	-1.88	-1.22	-0.50	-0.14	-0.65	-0.34	-0.70	
-0.86	-1.16	-1.20	-0.69	-0.50	-0.34	-0.14	-0.65	
-0.14	-0.65	-1.16	-1.41	-1.06	-0.34	-0.14	-0.65	
-0.65	-0.65	-1.16	-0.86	-1.06	-0.34	-0.14	-0.34	
-0.14	-0.86	-0.65	-0.14	-0.86	-0.34	-0.14	-0.86	
-0.86	-0.34	-0.86	-0.14	-0.34	-0.65	-0.14	-0.34	
-0.14	-0.34	-0.54	-0.65	-1.06	-0.88	-0.65	-0.33	
-0.34	-0.54	-0.89	-1.37	-1.25	-0.53	-0.17	0.19	
map_weights [ 0.31158268  0.61734885 -0.43991283  0.45563696  0.27702795 -0.20958007]
MAP reward
-0.44	0.28	0.28	0.31	0.62	-0.44	0.46	0.31	
0.28	-0.44	-0.44	0.46	0.31	0.46	0.62	-0.44	
0.62	-0.44	-0.44	0.28	0.28	0.46	0.62	-0.44	
-0.44	0.62	-0.44	0.28	0.28	0.46	0.62	0.46	
0.62	0.28	-0.44	0.62	0.28	0.46	0.62	0.28	
0.28	0.62	0.28	0.62	0.46	-0.44	0.62	0.46	
0.62	0.46	0.46	-0.44	0.28	0.31	-0.44	-0.44	
0.46	0.46	0.31	0.28	0.28	0.31	0.31	-0.21	
Map policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	>	v	>	>	v	<	
^	v	<	v	>	>	v	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	<	^	^	
^	<	<	<	^	^	^	.	
expeced value MDP LP 49.7920393447467
mean w [ 0.00699354  0.51374047 -0.32394336 -0.0095057  -0.13752335  0.14209546]
Mean policy from posterior
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	v	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	^	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
Mean rewards
-0.32	-0.14	-0.14	0.01	0.51	-0.32	-0.01	0.01	
-0.14	-0.32	-0.32	-0.01	0.01	-0.01	0.51	-0.32	
0.51	-0.32	-0.32	-0.14	-0.14	-0.01	0.51	-0.32	
-0.32	0.51	-0.32	-0.14	-0.14	-0.01	0.51	-0.01	
0.51	-0.14	-0.32	0.51	-0.14	-0.01	0.51	-0.14	
-0.14	0.51	-0.14	0.51	-0.01	-0.32	0.51	-0.01	
0.51	-0.01	-0.01	-0.32	-0.14	0.01	-0.32	-0.32	
-0.01	-0.01	0.01	-0.14	-0.14	0.01	0.01	0.14	
mean = 0.05994681877326791, map = 0.08824757681119544
CVaR policy
v	>	>	>	^	<	v	<	
v	<	^	^	^	>	v	<	
<	<	<	v	^	>	v	<	
^	v	<	v	v	>	v	<	
<	<	>	v	<	>	^	<	
v	<	>	^	<	>	^	<	
<	<	^	^	^	^	^	^	
^	<	^	^	^	>	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
^	v	<	v	>	>	^	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	>	^	^	
^	<	<	^	^	>	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
v	v	<	v	>	>	v	<	
<	<	>	v	<	>	^	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	>	^	^	
^	<	<	^	^	>	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	v	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	>	^	^	
^	<	<	^	^	^	^	.	
CVaR policy
v	>	>	>	^	<	v	<	
v	<	>	>	^	>	v	<	
<	<	<	v	^	>	^	<	
v	v	<	v	>	>	^	<	
<	<	>	v	<	>	^	<	
^	v	>	^	<	>	^	<	
<	<	<	^	^	^	^	^	
^	<	<	^	^	^	^	.	
cvar = , 0.10031657848909958, 0.05997192232626891, 0.059946877588826664, 0.059946830645905225, 0.059954664790480394
==========
iteration 93
==========
weights [-0.42813831 -0.01364042 -0.26249877 -0.12055687 -0.38381049  0.76535056]
expeced value MDP LP -0.6234165451809174
demonstration
[(0, 3), (8, 3), (16, 3), (24, 1), (25, 3), (33, 3), (41, 3), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.39613001 -0.17166773 -0.16120975  0.36180624  0.53510305 -0.60859148]
w_map [-0.56044513 -0.19982801 -0.37626942 -0.29421884 -0.59038601  0.26319382] loglik -1.3862946691377402
accepted/total = 1593/3000 = 0.531
-------
true weights [-0.42813831 -0.01364042 -0.26249877 -0.12055687 -0.38381049  0.76535056]
features
1 	2 	1 	0 	0 	1 	3 	4 	
1 	2 	4 	0 	4 	1 	1 	3 	
1 	2 	4 	4 	4 	0 	4 	3 	
2 	1 	2 	4 	2 	4 	2 	2 	
0 	2 	0 	2 	0 	0 	4 	3 	
0 	3 	1 	0 	3 	4 	0 	3 	
3 	1 	3 	0 	2 	2 	1 	2 	
1 	4 	0 	1 	4 	4 	4 	5 	
optimal policy
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	>	^	^	>	v	
>	v	<	v	v	>	>	v	
>	v	v	>	v	>	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
optimal values
-1.24	-1.39	-1.14	-1.14	-0.72	-0.29	-0.39	-0.64	
-1.24	-1.48	-1.46	-1.08	-0.66	-0.28	-0.27	-0.26	
-1.23	-1.23	-1.60	-1.41	-1.04	-0.71	-0.52	-0.14	
-1.23	-0.98	-1.23	-1.23	-0.86	-0.66	-0.28	-0.02	
-1.39	-0.98	-1.03	-0.86	-0.60	-0.57	-0.14	0.25	
-1.14	-0.72	-0.61	-0.60	-0.18	-0.18	0.04	0.37	
-0.72	-0.61	-0.60	-0.48	-0.06	0.21	0.48	0.50	
-0.73	-0.98	-0.83	-0.41	-0.40	-0.01	0.37	0.77	
map_weights [-0.56044513 -0.19982801 -0.37626942 -0.29421884 -0.59038601  0.26319382]
MAP reward
-0.20	-0.38	-0.20	-0.56	-0.56	-0.20	-0.29	-0.59	
-0.20	-0.38	-0.59	-0.56	-0.59	-0.20	-0.20	-0.29	
-0.20	-0.38	-0.59	-0.59	-0.59	-0.56	-0.59	-0.29	
-0.38	-0.20	-0.38	-0.59	-0.38	-0.59	-0.38	-0.38	
-0.56	-0.38	-0.56	-0.38	-0.56	-0.56	-0.59	-0.29	
-0.56	-0.29	-0.20	-0.56	-0.29	-0.59	-0.56	-0.29	
-0.29	-0.20	-0.29	-0.56	-0.38	-0.38	-0.20	-0.38	
-0.20	-0.59	-0.56	-0.20	-0.59	-0.59	-0.59	0.26	
Map policy
v	>	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	>	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
expeced value MDP LP -1.7985946376457653
mean w [-0.56103371 -0.12028122 -0.24531325 -0.2522272  -0.55836428  0.04690192]
Mean policy from posterior
v	v	>	>	>	v	v	v	
v	v	<	>	>	>	>	v	
v	v	v	v	v	^	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	^	>	>	.	
Mean rewards
-0.12	-0.25	-0.12	-0.56	-0.56	-0.12	-0.25	-0.56	
-0.12	-0.25	-0.56	-0.56	-0.56	-0.12	-0.12	-0.25	
-0.12	-0.25	-0.56	-0.56	-0.56	-0.56	-0.56	-0.25	
-0.25	-0.12	-0.25	-0.56	-0.25	-0.56	-0.25	-0.25	
-0.56	-0.25	-0.56	-0.25	-0.56	-0.56	-0.56	-0.25	
-0.56	-0.25	-0.12	-0.56	-0.25	-0.56	-0.56	-0.25	
-0.25	-0.12	-0.25	-0.56	-0.25	-0.25	-0.12	-0.25	
-0.12	-0.56	-0.56	-0.12	-0.56	-0.56	-0.56	0.05	
mean = 0.02365843187180461, map = 0.011811390680329548
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	v	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	>	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	v	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
>	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	^	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	^	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	^	>	>	.	
CVaR policy
v	v	>	>	>	v	v	v	
v	v	>	>	>	>	>	v	
v	v	v	v	v	^	v	v	
>	v	v	v	v	>	>	v	
>	v	v	v	v	v	>	v	
v	>	v	>	v	v	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	^	>	>	.	
cvar = , 0.035592467471003975, 0.021478118190951467, 0.023388841838567598, 0.01748308634758078, 0.017483087033214773
==========
iteration 94
==========
weights [-0.2696454  -0.64597038 -0.38127187 -0.21671282 -0.12610708  0.5493432 ]
expeced value MDP LP -0.990944579120997
demonstration
[(0, 1), (1, 1), (2, 3), (10, 1), (11, 1), (12, 3), (20, 1), (21, 3), (29, 3), (37, 1), (38, 3), (46, 1), (47, 3), (55, 3), (63, None)]
[-0.62098013 -0.23895657  0.66030228 -0.17442592  0.29165076  0.07615619]
w_map [-0.31259931 -0.63191436 -0.4689562  -0.5261109  -0.07792943  0.0134283 ] loglik -1.1923448682611664
accepted/total = 1752/3000 = 0.584
-------
true weights [-0.2696454  -0.64597038 -0.38127187 -0.21671282 -0.12610708  0.5493432 ]
features
2 	2 	4 	0 	1 	1 	0 	3 	
2 	1 	0 	4 	3 	3 	1 	4 	
0 	0 	3 	1 	4 	4 	0 	4 	
4 	0 	1 	3 	3 	3 	1 	2 	
4 	0 	3 	3 	4 	4 	4 	2 	
2 	0 	4 	2 	0 	1 	0 	4 	
2 	2 	3 	1 	0 	3 	1 	3 	
3 	4 	4 	2 	0 	3 	1 	5 	
optimal policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	^	>	>	v	<	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	^	>	>	v	>	v	
>	>	>	>	>	>	>	.	
optimal values
-2.20	-1.84	-1.47	-1.36	-1.62	-1.50	-1.27	-1.01	
-1.98	-1.99	-1.36	-1.10	-0.99	-0.87	-1.44	-0.80	
-1.61	-1.64	-1.56	-1.42	-0.78	-0.66	-0.92	-0.69	
-1.36	-1.39	-1.51	-0.87	-0.66	-0.54	-0.84	-0.56	
-1.24	-1.13	-0.87	-0.66	-0.45	-0.32	-0.20	-0.19	
-1.61	-1.25	-0.99	-1.03	-0.71	-0.72	-0.07	0.20	
-1.76	-1.56	-1.19	-1.43	-0.80	-0.53	-0.32	0.33	
-1.40	-1.19	-1.08	-0.96	-0.58	-0.32	-0.10	0.55	
map_weights [-0.31259931 -0.63191436 -0.4689562  -0.5261109  -0.07792943  0.0134283 ]
MAP reward
-0.47	-0.47	-0.08	-0.31	-0.63	-0.63	-0.31	-0.53	
-0.47	-0.63	-0.31	-0.08	-0.53	-0.53	-0.63	-0.08	
-0.31	-0.31	-0.53	-0.63	-0.08	-0.08	-0.31	-0.08	
-0.08	-0.31	-0.63	-0.53	-0.53	-0.53	-0.63	-0.47	
-0.08	-0.31	-0.53	-0.53	-0.08	-0.08	-0.08	-0.47	
-0.47	-0.31	-0.08	-0.47	-0.31	-0.63	-0.31	-0.08	
-0.47	-0.47	-0.53	-0.63	-0.31	-0.53	-0.63	-0.53	
-0.53	-0.08	-0.08	-0.47	-0.31	-0.53	-0.63	0.01	
Map policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	>	>	>	v	>	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	>	>	^	>	>	v	
>	v	^	>	^	v	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7811664878456597
mean w [-0.28802307 -0.54785738 -0.5281135  -0.28081935 -0.11740432 -0.1463889 ]
Mean policy from posterior
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	<	v	
v	v	>	>	>	v	<	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	^	>	>	v	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.53	-0.53	-0.12	-0.29	-0.55	-0.55	-0.29	-0.28	
-0.53	-0.55	-0.29	-0.12	-0.28	-0.28	-0.55	-0.12	
-0.29	-0.29	-0.28	-0.55	-0.12	-0.12	-0.29	-0.12	
-0.12	-0.29	-0.55	-0.28	-0.28	-0.28	-0.55	-0.53	
-0.12	-0.29	-0.28	-0.28	-0.12	-0.12	-0.12	-0.53	
-0.53	-0.29	-0.12	-0.53	-0.29	-0.55	-0.29	-0.12	
-0.53	-0.53	-0.28	-0.55	-0.29	-0.28	-0.55	-0.28	
-0.28	-0.12	-0.12	-0.53	-0.29	-0.28	-0.55	-0.15	
mean = 0.0018171492066874073, map = 0.023491191647064813
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
>	>	v	>	>	>	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	>	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	>	>	>	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	v	v	
v	v	>	>	>	v	v	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	<	v	
v	v	>	>	>	v	<	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	v	>	>	v	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	>	v	
v	>	>	>	v	v	<	v	
v	v	>	>	>	v	<	v	
v	v	>	>	v	v	v	v	
>	>	>	>	>	>	v	v	
^	>	^	^	^	>	>	v	
v	v	^	>	>	v	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.033350038062076814, 0.014326919884186884, 0.009745868825337123, 0.00321239370635229, 0.0018171446212355002
==========
iteration 95
==========
weights [-0.49978479 -0.13993913 -0.09484483 -0.68582209 -0.12961334  0.48423663]
expeced value MDP LP -1.2930192472989095
demonstration
[(0, 1), (1, 1), (2, 1), (3, 3), (11, 1), (12, 3), (20, 3), (28, 3), (36, 1), (37, 3), (45, 1), (46, 3), (54, 1), (55, 3), (63, None)]
[-0.10811559 -0.50755585  0.50947054 -0.02270831 -0.48619175  0.48398322]
w_map [-0.50454526 -0.23483116 -0.25517662 -0.73731768 -0.24008539  0.1545799 ] loglik -8.563974418507314e-09
accepted/total = 1607/3000 = 0.5356666666666666
-------
true weights [-0.49978479 -0.13993913 -0.09484483 -0.68582209 -0.12961334  0.48423663]
features
1 	4 	1 	1 	0 	1 	0 	4 	
4 	4 	0 	1 	4 	0 	4 	4 	
1 	0 	1 	0 	4 	3 	2 	3 	
3 	0 	4 	2 	4 	0 	4 	3 	
0 	0 	1 	1 	2 	0 	3 	3 	
4 	3 	4 	1 	3 	0 	1 	0 	
2 	0 	0 	3 	1 	0 	4 	1 	
4 	2 	1 	0 	3 	0 	0 	5 	
optimal policy
>	>	>	v	v	v	v	v	
>	^	v	>	v	>	v	<	
>	>	v	v	v	>	v	<	
>	>	>	>	v	>	v	<	
>	>	>	>	>	v	v	v	
v	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	^	>	>	.	
optimal values
-1.98	-1.86	-1.75	-1.62	-1.86	-1.57	-1.44	-1.19	
-2.08	-1.97	-1.95	-1.50	-1.37	-1.44	-0.95	-1.07	
-2.07	-1.95	-1.46	-1.71	-1.25	-1.51	-0.83	-1.51	
-2.49	-1.82	-1.34	-1.22	-1.14	-1.24	-0.75	-1.42	
-2.24	-1.76	-1.28	-1.15	-1.02	-0.93	-0.62	-0.85	
-2.10	-2.03	-1.36	-1.24	-1.11	-0.44	0.06	-0.16	
-1.99	-2.09	-1.60	-1.11	-0.43	-0.30	0.21	0.34	
-1.92	-1.80	-1.73	-1.60	-1.11	-0.52	-0.02	0.48	
map_weights [-0.50454526 -0.23483116 -0.25517662 -0.73731768 -0.24008539  0.1545799 ]
MAP reward
-0.23	-0.24	-0.23	-0.23	-0.50	-0.23	-0.50	-0.24	
-0.24	-0.24	-0.50	-0.23	-0.24	-0.50	-0.24	-0.24	
-0.23	-0.50	-0.23	-0.50	-0.24	-0.74	-0.26	-0.74	
-0.74	-0.50	-0.24	-0.26	-0.24	-0.50	-0.24	-0.74	
-0.50	-0.50	-0.23	-0.23	-0.26	-0.50	-0.74	-0.74	
-0.24	-0.74	-0.24	-0.23	-0.74	-0.50	-0.23	-0.50	
-0.26	-0.50	-0.50	-0.74	-0.23	-0.50	-0.24	-0.23	
-0.24	-0.26	-0.23	-0.50	-0.74	-0.50	-0.50	0.15	
Map policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	>	v	>	v	<	
>	>	v	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.9586691162028353
mean w [-0.4385437  -0.17194973 -0.2094438  -0.73123802 -0.17221184 -0.05174859]
Mean policy from posterior
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	>	v	>	v	<	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.17	-0.17	-0.17	-0.17	-0.44	-0.17	-0.44	-0.17	
-0.17	-0.17	-0.44	-0.17	-0.17	-0.44	-0.17	-0.17	
-0.17	-0.44	-0.17	-0.44	-0.17	-0.73	-0.21	-0.73	
-0.73	-0.44	-0.17	-0.21	-0.17	-0.44	-0.17	-0.73	
-0.44	-0.44	-0.17	-0.17	-0.21	-0.44	-0.73	-0.73	
-0.17	-0.73	-0.17	-0.17	-0.73	-0.44	-0.17	-0.44	
-0.21	-0.44	-0.44	-0.73	-0.17	-0.44	-0.17	-0.17	
-0.17	-0.21	-0.17	-0.44	-0.73	-0.44	-0.44	-0.05	
mean = 0.025558039180089587, map = 0.02132896569765741
CVaR policy
>	>	>	v	v	>	v	v	
>	>	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
>	>	>	v	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	v	v	v	
>	>	>	>	v	>	v	<	
>	>	v	>	v	>	v	<	
>	>	v	>	v	>	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
>	>	>	>	v	>	v	<	
>	>	v	>	v	>	v	<	
>	>	v	>	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	>	v	v	
v	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	>	v	>	v	<	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	v	>	v	v	
v	>	v	>	v	>	v	<	
>	>	v	>	v	>	v	<	
>	>	v	v	v	v	v	v	
>	>	>	>	>	v	v	v	
>	>	>	>	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
cvar = , 0.06321635812029958, 0.020352893278958062, 0.023703876582024996, 0.025558037279970858, 0.02555803587250871
==========
iteration 96
==========
weights [-0.12573145 -0.02573221 -0.16504818 -0.023658   -0.77151134  0.60041578]
expeced value MDP LP -0.11446347967932347
demonstration
[(0, 1), (1, 1), (2, 1), (3, 3), (11, 3), (19, 3), (27, 1), (28, 1), (29, 1), (30, 1), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.23417579  0.38047057 -0.56641901 -0.47784174 -0.07257816  0.49595668]
w_map [-0.4032988  -0.27443927 -0.35795292 -0.20645274 -0.75046219  0.16759071] loglik -4.816645997607338e-07
accepted/total = 1553/3000 = 0.5176666666666667
-------
true weights [-0.12573145 -0.02573221 -0.16504818 -0.023658   -0.77151134  0.60041578]
features
3 	3 	3 	3 	3 	4 	1 	0 	
4 	4 	2 	0 	4 	2 	3 	1 	
4 	2 	0 	3 	4 	1 	2 	0 	
0 	2 	2 	3 	0 	1 	2 	1 	
2 	4 	0 	4 	4 	0 	4 	1 	
4 	2 	3 	0 	2 	0 	4 	1 	
3 	1 	2 	0 	2 	1 	2 	2 	
2 	1 	1 	3 	1 	4 	1 	5 	
optimal policy
>	>	>	v	<	>	v	v	
^	^	v	v	>	>	>	v	
>	>	>	v	>	v	>	v	
>	>	>	>	>	>	>	v	
^	v	v	^	>	v	>	v	
v	v	v	v	>	v	>	v	
>	v	v	>	>	>	v	v	
>	>	>	>	^	>	>	.	
optimal values
-0.24	-0.22	-0.20	-0.18	-0.20	-0.64	0.13	0.06	
-1.01	-0.99	-0.32	-0.16	-0.78	-0.01	0.16	0.18	
-1.09	-0.32	-0.16	-0.03	-0.66	0.12	0.04	0.21	
-0.46	-0.34	-0.17	-0.01	0.02	0.14	0.17	0.34	
-0.62	-0.87	-0.20	-0.78	-0.66	0.11	-0.41	0.37	
-0.73	-0.10	-0.07	-0.05	0.07	0.24	-0.38	0.40	
0.04	0.07	-0.05	0.07	0.20	0.37	0.40	0.43	
-0.07	0.09	0.12	0.15	0.17	-0.21	0.57	0.60	
map_weights [-0.4032988  -0.27443927 -0.35795292 -0.20645274 -0.75046219  0.16759071]
MAP reward
-0.21	-0.21	-0.21	-0.21	-0.21	-0.75	-0.27	-0.40	
-0.75	-0.75	-0.36	-0.40	-0.75	-0.36	-0.21	-0.27	
-0.75	-0.36	-0.40	-0.21	-0.75	-0.27	-0.36	-0.40	
-0.40	-0.36	-0.36	-0.21	-0.40	-0.27	-0.36	-0.27	
-0.36	-0.75	-0.40	-0.75	-0.75	-0.40	-0.75	-0.27	
-0.75	-0.36	-0.21	-0.40	-0.36	-0.40	-0.75	-0.27	
-0.21	-0.27	-0.36	-0.40	-0.36	-0.27	-0.36	-0.36	
-0.36	-0.27	-0.27	-0.21	-0.27	-0.75	-0.27	0.17	
Map policy
>	>	>	v	>	>	v	v	
^	>	v	v	>	>	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	>	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
expeced value MDP LP -1.7697094373408682
mean w [-0.35531754 -0.1826041  -0.31214652 -0.15449268 -0.75792717 -0.00185542]
Mean policy from posterior
>	>	>	v	<	>	v	v	
^	>	v	v	>	v	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
Mean rewards
-0.15	-0.15	-0.15	-0.15	-0.15	-0.76	-0.18	-0.36	
-0.76	-0.76	-0.31	-0.36	-0.76	-0.31	-0.15	-0.18	
-0.76	-0.31	-0.36	-0.15	-0.76	-0.18	-0.31	-0.36	
-0.36	-0.31	-0.31	-0.15	-0.36	-0.18	-0.31	-0.18	
-0.31	-0.76	-0.36	-0.76	-0.76	-0.36	-0.76	-0.18	
-0.76	-0.31	-0.15	-0.36	-0.31	-0.36	-0.76	-0.18	
-0.15	-0.18	-0.31	-0.36	-0.31	-0.18	-0.31	-0.31	
-0.31	-0.18	-0.18	-0.15	-0.18	-0.76	-0.18	-0.00	
mean = 0.07398328373182417, map = 0.06738304876017084
CVaR policy
>	>	>	v	>	>	v	v	
^	>	v	v	>	>	v	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	v	v	v	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
^	>	v	v	>	>	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
^	>	v	v	>	>	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	v	v	
^	>	>	v	>	>	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	<	>	v	v	
^	>	v	v	>	v	>	v	
>	>	>	v	>	v	v	v	
>	>	>	>	>	>	>	v	
v	v	v	v	v	v	>	v	
v	>	v	>	v	v	>	v	
>	v	>	>	>	>	v	v	
>	>	>	>	>	>	>	.	
cvar = , 0.126354962078151, 0.07991952745518999, 0.0799195271903354, 0.0799195327761543, 0.07398328373724813
==========
iteration 97
==========
weights [-0.1466662  -0.34992279 -0.23201547 -0.79428403 -0.39089502  0.13610969]
expeced value MDP LP -1.9597827850887861
demonstration
[(0, 1), (1, 3), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 3), (22, 1), (23, 3), (31, 3), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.31192764  0.51225537 -0.69985502 -0.29340666  0.07163388  0.24347413]
w_map [-0.21661232 -0.53265069 -0.22259924 -0.69184847 -0.37568499  0.00429101] loglik -2.0794420591178486
accepted/total = 1536/3000 = 0.512
-------
true weights [-0.1466662  -0.34992279 -0.23201547 -0.79428403 -0.39089502  0.13610969]
features
2 	0 	2 	4 	1 	2 	4 	0 	
1 	2 	4 	0 	2 	2 	1 	0 	
3 	3 	0 	2 	3 	3 	0 	4 	
2 	2 	2 	3 	3 	4 	4 	0 	
0 	2 	3 	4 	0 	4 	3 	3 	
3 	2 	2 	3 	2 	4 	1 	2 	
1 	4 	1 	0 	4 	1 	0 	2 	
3 	3 	3 	4 	4 	3 	1 	5 	
optimal policy
>	>	>	v	>	>	>	v	
v	>	>	>	>	>	v	v	
v	v	v	v	v	v	v	v	
v	v	<	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	^	^	^	>	>	.	
optimal values
-3.52	-3.32	-3.20	-3.00	-2.80	-2.47	-2.26	-1.89	
-3.52	-3.20	-3.00	-2.64	-2.52	-2.31	-2.10	-1.76	
-3.20	-3.08	-2.64	-2.69	-2.89	-2.51	-1.76	-1.63	
-2.43	-2.31	-2.52	-2.49	-2.11	-1.73	-1.63	-1.25	
-2.22	-2.10	-2.45	-1.71	-1.33	-1.36	-1.38	-1.12	
-2.66	-1.88	-1.67	-1.90	-1.20	-0.98	-0.59	-0.33	
-2.16	-1.83	-1.45	-1.11	-0.98	-0.59	-0.24	-0.10	
-2.93	-2.60	-2.23	-1.49	-1.36	-1.01	-0.22	0.14	
map_weights [-0.21661232 -0.53265069 -0.22259924 -0.69184847 -0.37568499  0.00429101]
MAP reward
-0.22	-0.22	-0.22	-0.38	-0.53	-0.22	-0.38	-0.22	
-0.53	-0.22	-0.38	-0.22	-0.22	-0.22	-0.53	-0.22	
-0.69	-0.69	-0.22	-0.22	-0.69	-0.69	-0.22	-0.38	
-0.22	-0.22	-0.22	-0.69	-0.69	-0.38	-0.38	-0.22	
-0.22	-0.22	-0.69	-0.38	-0.22	-0.38	-0.69	-0.69	
-0.69	-0.22	-0.22	-0.69	-0.22	-0.38	-0.53	-0.22	
-0.53	-0.38	-0.53	-0.22	-0.38	-0.53	-0.22	-0.22	
-0.69	-0.69	-0.69	-0.38	-0.38	-0.69	-0.53	0.00	
Map policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
expeced value MDP LP -2.25975203340356
mean w [-0.14847364 -0.45163691 -0.2818939  -0.62587024 -0.42479344 -0.01295576]
Mean policy from posterior
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
Mean rewards
-0.28	-0.15	-0.28	-0.42	-0.45	-0.28	-0.42	-0.15	
-0.45	-0.28	-0.42	-0.15	-0.28	-0.28	-0.45	-0.15	
-0.63	-0.63	-0.15	-0.28	-0.63	-0.63	-0.15	-0.42	
-0.28	-0.28	-0.28	-0.63	-0.63	-0.42	-0.42	-0.15	
-0.15	-0.28	-0.63	-0.42	-0.15	-0.42	-0.63	-0.63	
-0.63	-0.28	-0.28	-0.63	-0.28	-0.42	-0.45	-0.28	
-0.45	-0.42	-0.45	-0.15	-0.42	-0.45	-0.15	-0.28	
-0.63	-0.63	-0.63	-0.42	-0.42	-0.63	-0.45	-0.01	
mean = 0.007749434997751381, map = 0.007749435537726779
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	v	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	>	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	>	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
CVaR policy
>	>	>	v	>	>	>	v	
>	>	>	>	>	>	v	v	
v	v	v	v	v	>	v	v	
v	v	v	v	v	v	>	v	
>	v	>	>	v	v	v	v	
>	>	v	v	>	>	v	v	
>	>	>	>	>	>	>	v	
^	^	>	^	>	>	>	.	
cvar = , 0.04320063772648153, 0.027286849017731818, 0.016238491502037267, 0.007749435166982455, 0.0077494350024285286
==========
iteration 98
==========
weights [-0.17623077 -0.53731002 -0.07272226 -0.1766744  -0.2995494   0.74431744]
expeced value MDP LP -0.31006315109263427
demonstration
[(0, 3), (8, 1), (9, 3), (17, 3), (25, 1), (26, 3), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 3), (47, 3), (55, 3), (63, None)]
[-0.12056969 -0.23865346 -0.18422624 -0.31267056  0.89196976 -0.03457223]
w_map [-0.24968329 -0.56003235 -0.14725176 -0.35154823 -0.52057148  0.45580491] loglik -1.386388043183012
accepted/total = 1041/3000 = 0.347
-------
true weights [-0.17623077 -0.53731002 -0.07272226 -0.1766744  -0.2995494   0.74431744]
features
2 	3 	4 	3 	0 	1 	2 	0 	
0 	2 	0 	0 	2 	1 	2 	2 	
1 	0 	2 	1 	0 	1 	4 	3 	
3 	2 	2 	4 	4 	1 	0 	0 	
0 	1 	3 	3 	3 	3 	2 	2 	
2 	3 	3 	4 	4 	3 	0 	3 	
3 	2 	4 	2 	0 	3 	3 	2 	
0 	0 	4 	0 	4 	4 	4 	5 	
optimal policy
v	v	v	>	v	>	v	v	
>	v	v	>	v	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
optimal values
-0.99	-0.93	-0.98	-0.91	-0.74	-0.71	-0.17	-0.20	
-0.93	-0.76	-0.69	-0.74	-0.57	-0.64	-0.10	-0.03	
-1.22	-0.69	-0.52	-1.04	-0.50	-0.69	-0.15	0.04	
-0.69	-0.52	-0.45	-0.51	-0.33	-0.39	0.15	0.22	
-0.74	-0.92	-0.38	-0.21	-0.03	0.15	0.33	0.40	
-0.57	-0.50	-0.43	-0.25	-0.18	0.12	0.30	0.48	
-0.50	-0.32	-0.25	0.05	0.12	0.30	0.48	0.66	
-0.67	-0.50	-0.43	-0.13	-0.17	0.13	0.44	0.74	
map_weights [-0.24968329 -0.56003235 -0.14725176 -0.35154823 -0.52057148  0.45580491]
MAP reward
-0.15	-0.35	-0.52	-0.35	-0.25	-0.56	-0.15	-0.25	
-0.25	-0.15	-0.25	-0.25	-0.15	-0.56	-0.15	-0.15	
-0.56	-0.25	-0.15	-0.56	-0.25	-0.56	-0.52	-0.35	
-0.35	-0.15	-0.15	-0.52	-0.52	-0.56	-0.25	-0.25	
-0.25	-0.56	-0.35	-0.35	-0.35	-0.35	-0.15	-0.15	
-0.15	-0.35	-0.35	-0.52	-0.52	-0.35	-0.25	-0.35	
-0.35	-0.15	-0.52	-0.15	-0.25	-0.35	-0.35	-0.15	
-0.25	-0.25	-0.52	-0.25	-0.52	-0.52	-0.52	0.46	
Map policy
v	v	v	>	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
expeced value MDP LP -1.358818464641275
mean w [-0.22059159 -0.58887859 -0.11185375 -0.27772344 -0.5072329   0.15544921]
Mean policy from posterior
v	v	v	>	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	^	>	>	.	
Mean rewards
-0.11	-0.28	-0.51	-0.28	-0.22	-0.59	-0.11	-0.22	
-0.22	-0.11	-0.22	-0.22	-0.11	-0.59	-0.11	-0.11	
-0.59	-0.22	-0.11	-0.59	-0.22	-0.59	-0.51	-0.28	
-0.28	-0.11	-0.11	-0.51	-0.51	-0.59	-0.22	-0.22	
-0.22	-0.59	-0.28	-0.28	-0.28	-0.28	-0.11	-0.11	
-0.11	-0.28	-0.28	-0.51	-0.51	-0.28	-0.22	-0.28	
-0.28	-0.11	-0.51	-0.11	-0.22	-0.28	-0.28	-0.11	
-0.22	-0.22	-0.51	-0.22	-0.51	-0.51	-0.51	0.16	
mean = 0.008417954721254617, map = 0.008212649055985233
CVaR policy
v	v	v	v	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
>	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	>	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	>	v	v	>	v	v	
>	>	>	>	>	>	>	v	
>	^	>	^	>	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	^	>	>	.	
CVaR policy
v	v	v	v	v	>	v	v	
>	v	v	>	>	>	>	v	
>	v	v	>	v	>	v	v	
>	>	v	v	v	>	v	v	
v	>	>	>	>	>	>	v	
v	v	>	v	v	>	>	v	
>	>	>	>	>	>	>	v	
>	^	>	^	^	>	>	.	
cvar = , 0.02446173297510107, 0.008212634173227928, 0.008212634205830516, 0.00841795391643868, 0.0084179541420068
==========
iteration 99
==========
weights [-0.83363091 -0.26301518 -0.31569054 -0.05727325 -0.36344545  0.02914067]
expeced value MDP LP -1.533172126056173
demonstration
[(0, 3), (8, 3), (16, 1), (17, 1), (18, 1), (19, 3), (27, 3), (35, 1), (36, 1), (37, 3), (45, 3), (53, 1), (54, 1), (55, 3), (63, None)]
[ 0.39571275  0.41708329  0.20211364 -0.68494783  0.39580468  0.05280293]
w_map [-0.73624092 -0.17893125 -0.34324053 -0.19439846 -0.36945168 -0.36583268] loglik -1.7823065832089924e-06
accepted/total = 1471/3000 = 0.49033333333333334
-------
true weights [-0.83363091 -0.26301518 -0.31569054 -0.05727325 -0.36344545  0.02914067]
features
2 	2 	2 	0 	1 	1 	3 	4 	
3 	2 	0 	1 	0 	1 	0 	4 	
1 	3 	2 	1 	1 	0 	3 	4 	
1 	2 	4 	2 	0 	1 	4 	3 	
4 	0 	0 	4 	3 	1 	4 	3 	
3 	0 	4 	0 	4 	3 	4 	2 	
2 	1 	2 	3 	2 	1 	1 	3 	
0 	3 	3 	0 	1 	2 	4 	5 	
optimal policy
v	v	<	>	>	>	>	v	
v	v	>	v	>	>	v	v	
>	>	>	v	>	>	v	v	
v	>	>	v	v	>	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
optimal values
-2.71	-2.73	-3.02	-2.86	-2.05	-1.81	-1.56	-1.52	
-2.42	-2.44	-2.87	-2.06	-2.74	-1.93	-1.68	-1.17	
-2.39	-2.15	-2.11	-1.81	-1.93	-1.68	-0.86	-0.81	
-2.39	-2.21	-1.91	-1.57	-1.73	-1.06	-0.81	-0.45	
-2.15	-2.90	-2.08	-1.26	-0.91	-0.86	-0.76	-0.40	
-1.81	-2.29	-1.57	-1.73	-0.96	-0.60	-0.65	-0.34	
-1.77	-1.47	-1.22	-0.91	-0.86	-0.55	-0.29	-0.03	
-2.13	-1.31	-1.26	-1.73	-0.90	-0.65	-0.33	0.03	
map_weights [-0.73624092 -0.17893125 -0.34324053 -0.19439846 -0.36945168 -0.36583268]
MAP reward
-0.34	-0.34	-0.34	-0.74	-0.18	-0.18	-0.19	-0.37	
-0.19	-0.34	-0.74	-0.18	-0.74	-0.18	-0.74	-0.37	
-0.18	-0.19	-0.34	-0.18	-0.18	-0.74	-0.19	-0.37	
-0.18	-0.34	-0.37	-0.34	-0.74	-0.18	-0.37	-0.19	
-0.37	-0.74	-0.74	-0.37	-0.19	-0.18	-0.37	-0.19	
-0.19	-0.74	-0.37	-0.74	-0.37	-0.19	-0.37	-0.34	
-0.34	-0.18	-0.34	-0.19	-0.34	-0.18	-0.18	-0.19	
-0.74	-0.19	-0.19	-0.74	-0.18	-0.34	-0.37	-0.37	
Map policy
v	v	>	v	>	v	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	>	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	^	^	>	>	>	>	.	
expeced value MDP LP -1.3897041888394206
mean w [-0.712096   -0.14815122 -0.31751827 -0.12631004 -0.32057711  0.03275269]
Mean policy from posterior
v	v	v	v	>	v	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
Mean rewards
-0.32	-0.32	-0.32	-0.71	-0.15	-0.15	-0.13	-0.32	
-0.13	-0.32	-0.71	-0.15	-0.71	-0.15	-0.71	-0.32	
-0.15	-0.13	-0.32	-0.15	-0.15	-0.71	-0.13	-0.32	
-0.15	-0.32	-0.32	-0.32	-0.71	-0.15	-0.32	-0.13	
-0.32	-0.71	-0.71	-0.32	-0.13	-0.15	-0.32	-0.13	
-0.13	-0.71	-0.32	-0.71	-0.32	-0.13	-0.32	-0.32	
-0.32	-0.15	-0.32	-0.13	-0.32	-0.15	-0.15	-0.13	
-0.71	-0.13	-0.13	-0.71	-0.15	-0.32	-0.32	0.03	
mean = 0.03152712505076227, map = 0.04731299271466316
CVaR policy
v	v	v	v	>	>	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	>	>	>	>	>	.	
CVaR policy
v	v	v	v	>	>	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	v	>	v	>	v	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
CVaR policy
v	v	v	v	>	v	>	v	
v	v	>	v	v	v	v	v	
>	>	>	v	v	v	v	v	
v	>	>	v	v	v	>	v	
v	>	>	>	>	v	>	v	
v	v	v	v	>	v	v	v	
>	>	>	>	>	>	>	v	
>	>	^	>	>	>	>	.	
cvar = , 0.03589382712467648, 0.012398360583530055, 0.03152712620789622, 0.031527126735543254, 0.03152712548481107
