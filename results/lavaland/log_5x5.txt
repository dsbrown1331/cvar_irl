##############
Trial  0
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	<	v	
>	>	v	<	v	
>	>	>	<	<	
^	>	^	^	^	
^	>	^	<	<	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
-5.00	-5.00	1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	1 	0 	1 	0 	
1 	1 	2 	0 	0 	
0 	1 	1 	1 	1 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (3, 0), (2, 3), (1, 1), (13, 0), (4, 0)]
w_map [ 0.19397316 -0.51267241  0.71349125 -0.43643059] loglik -2.0794395882277286
accepted/total = 1721/2000 = 0.8605
MAP Policy on Train MDP
map_weights [ 0.19397316 -0.51267241  0.71349125 -0.43643059]
map reward
-0.51	0.19	0.19	0.19	0.19	
0.19	-0.51	0.19	-0.51	0.19	
-0.51	-0.51	0.71	0.19	0.19	
0.19	-0.51	-0.51	-0.51	-0.51	
0.19	-0.51	0.19	0.19	0.19	
Map policy
>	>	v	<	v	
>	>	v	v	v	
>	>	^	<	<	
>	^	^	^	^	
^	>	^	<	<	
MEAN policy on Train MDP
mean_weights [ 0.01558736 -0.49643156  0.49269392  0.11224221]
mean reward
-0.50	0.02	0.02	0.02	0.02	
0.02	-0.50	0.02	-0.50	0.02	
-0.50	-0.50	0.49	0.02	0.02	
0.02	-0.50	-0.50	-0.50	-0.50	
0.02	-0.50	0.02	0.02	0.02	
mean policy
>	>	v	<	v	
>	>	v	v	v	
>	>	^	<	<	
>	^	^	^	^	
^	>	^	<	<	
Optimal Policy
>	>	v	<	v	
>	>	v	<	v	
>	>	>	<	<	
^	>	^	^	^	
^	>	^	<	<	
MAP policy loss 1.2794712584782847e-06
Mean policy loss 2.886744614009895e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
v	v	v	>	v	
>	>	<	<	v	
>	^	^	<	<	
>	^	<	<	^	
reward
-5.00	-100.00	-5.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	1.00	-100.00	-5.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
1 	3 	1 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	2 	3 	1 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.19397316 -0.51267241  0.71349125 -0.43643059]
map reward
-0.51	-0.44	-0.51	-0.51	0.19	
0.19	-0.51	-0.44	0.19	0.19	
0.19	0.19	0.71	-0.44	-0.51	
-0.51	0.19	-0.51	0.19	0.19	
0.19	0.19	0.19	-0.51	0.19	
Map policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
>	^	<	<	^	
MEAN policy on test env
mean_weights [ 0.01558736 -0.49643156  0.49269392  0.11224221]
mean reward
-0.50	0.11	-0.50	-0.50	0.02	
0.02	-0.50	0.11	0.02	0.02	
0.02	0.02	0.49	0.11	-0.50	
-0.50	0.02	-0.50	0.02	0.02	
0.02	0.02	0.02	-0.50	0.02	
mean policy
v	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
>	^	^	^	^	
features
1 	3 	1 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	2 	3 	1 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	v	
^	^	^	<	<	
>	^	<	<	^	
-------- IRD Solution -------
ird reward
-13.37	-13.25	-13.37	-13.37	-12.58	
-12.58	-13.37	-13.25	-12.58	-12.58	
-12.58	-12.58	-12.46	-13.25	-13.37	
-13.37	-12.58	-13.37	-12.58	-12.58	
-12.58	-12.58	-12.58	-13.37	-12.58	
ird policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
>	^	<	<	^	
MAP policy loss 900.5351523605286
mean policy loss 894.9214034876795
robust policy loss 767.166550436291
regret policy loss 16.001653102907312
ird policy loss 29.85179810415903
MAP lava occupancy 9.224853647637016
Mean lava occupancy 9.224853647637016
Robust lava occupancy 7.89640052852054
Regret lava occupancy 0.3024949999848016
IRD lava occupancy 0.4488899999987591
##############
Trial  1
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	<	<	
>	v	v	<	<	
>	>	<	<	^	
>	>	^	<	^	
^	^	^	<	^	
reward
-1.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-5.00	
features
0 	1 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	1 	0 	
0 	0 	1 	1 	1 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 0), (8, 0), (17, 2), (11, 1), (10, 1), (4, 3), (0, 3), (12, 3), (5, 3)]
w_map [ 0.21018264 -0.69375981  0.68140364  0.10104288] loglik -4.68212678411237
accepted/total = 1637/2000 = 0.8185
MAP Policy on Train MDP
map_weights [ 0.21018264 -0.69375981  0.68140364  0.10104288]
map reward
0.21	-0.69	0.21	0.21	-0.69	
0.21	0.21	0.21	0.21	0.21	
0.21	0.21	0.68	-0.69	0.21	
0.21	0.21	0.21	-0.69	0.21	
0.21	0.21	-0.69	-0.69	-0.69	
Map policy
v	v	v	<	<	
>	>	v	<	<	
>	>	<	<	^	
>	>	^	<	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.10858043 -0.34870071  0.53288078 -0.06521326]
mean reward
0.11	-0.35	0.11	0.11	-0.35	
0.11	0.11	0.11	0.11	0.11	
0.11	0.11	0.53	-0.35	0.11	
0.11	0.11	0.11	-0.35	0.11	
0.11	0.11	-0.35	-0.35	-0.35	
mean policy
v	v	v	<	<	
>	v	v	<	<	
>	>	^	<	^	
>	^	^	<	^	
^	^	^	<	^	
Optimal Policy
v	v	v	<	<	
>	v	v	<	<	
>	>	<	<	^	
>	>	^	<	^	
^	^	^	<	^	
MAP policy loss 1.2150517396981864e-06
Mean policy loss 3.3192463499975844e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	v	v	
>	^	v	v	v	
^	>	v	<	<	
>	>	^	^	^	
^	^	^	^	<	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-100.00	-100.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-100.00	-100.00	-1.00	-100.00	
features
1 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	2 	0 	0 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	3 	
MAP on testing env
map_weights [ 0.21018264 -0.69375981  0.68140364  0.10104288]
map reward
-0.69	0.21	0.21	0.21	0.21	
0.21	0.21	0.10	0.21	0.21	
0.10	0.10	0.68	0.21	0.21	
-0.69	0.21	0.21	-0.69	-0.69	
0.21	0.10	0.10	0.21	0.10	
Map policy
>	v	v	v	v	
>	>	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.10858043 -0.34870071  0.53288078 -0.06521326]
mean reward
-0.35	0.11	0.11	0.11	0.11	
0.11	0.11	-0.07	0.11	0.11	
-0.07	-0.07	0.53	0.11	0.11	
-0.35	0.11	0.11	-0.35	-0.35	
0.11	-0.07	-0.07	0.11	-0.07	
mean policy
>	v	v	v	v	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	<	<	
features
1 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	2 	0 	0 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
v	v	v	<	<	
>	>	>	<	<	
^	^	^	^	^	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	>	v	v	
>	^	v	v	v	
^	>	v	<	<	
>	>	^	<	^	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-13.59	-12.89	-12.89	-12.89	-12.89	
-12.89	-12.89	-13.47	-12.89	-12.89	
-13.47	-13.47	-12.86	-12.89	-12.89	
-13.59	-12.89	-12.89	-13.59	-13.59	
-12.89	-13.47	-13.47	-12.89	-13.47	
ird policy
>	>	>	v	v	
>	^	v	v	v	
^	>	v	<	<	
>	>	^	^	^	
<	^	^	v	<	
MAP policy loss 622.9049625163196
mean policy loss 31.808533811366818
robust policy loss 225.04973737337667
regret policy loss 6.449237699435173e-08
ird policy loss 1.5917130337407095
MAP lava occupancy 6.537993708208148
Mean lava occupancy 6.537993708208148
Robust lava occupancy 2.524526350506359
Regret lava occupancy 0.24000000054073295
IRD lava occupancy 0.24000000165922872
##############
Trial  2
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	<	
>	>	v	<	v	
^	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-5.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	1 	1 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
demonstration
[(7, 3), (12, 2), (3, 3), (6, 1), (8, 0), (17, 2), (5, 1), (0, 3), (12, 3), (4, 0)]
w_map [ 0.09562044 -0.55492262  0.7530749   0.34028784] loglik -2.772587103353999
accepted/total = 1717/2000 = 0.8585
MAP Policy on Train MDP
map_weights [ 0.09562044 -0.55492262  0.7530749   0.34028784]
map reward
0.10	0.10	-0.55	0.10	0.10	
0.10	0.10	0.10	-0.55	-0.55	
0.10	-0.55	0.75	-0.55	0.10	
0.10	0.10	0.10	0.10	-0.55	
0.10	0.10	0.10	-0.55	0.10	
Map policy
>	v	v	<	<	
>	>	v	<	v	
^	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.07889849 -0.54141692  0.46168457  0.07052287]
mean reward
-0.08	-0.08	-0.54	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.54	-0.54	
-0.08	-0.54	0.46	-0.54	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.54	
-0.08	-0.08	-0.08	-0.54	-0.08	
mean policy
>	v	v	<	<	
>	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
Optimal Policy
>	v	v	<	<	
>	>	v	<	v	
^	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
MAP policy loss 1.0694578883333958e-06
Mean policy loss 0.07980021875512337
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	v	
>	v	v	v	v	
>	>	<	<	<	
^	>	^	^	<	
>	>	^	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-5.00	
-100.00	-1.00	-1.00	-100.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-100.00	
features
0 	1 	0 	1 	1 	
3 	0 	0 	3 	1 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
1 	0 	0 	0 	3 	
MAP on testing env
map_weights [ 0.09562044 -0.55492262  0.7530749   0.34028784]
map reward
0.10	-0.55	0.10	-0.55	-0.55	
0.34	0.10	0.10	0.34	-0.55	
0.10	0.10	0.75	0.10	0.10	
0.10	-0.55	0.10	0.10	0.10	
-0.55	0.10	0.10	0.10	0.34	
Map policy
v	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
^	>	^	^	<	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-0.07889849 -0.54141692  0.46168457  0.07052287]
mean reward
-0.08	-0.54	-0.08	-0.54	-0.54	
0.07	-0.08	-0.08	0.07	-0.54	
-0.08	-0.08	0.46	-0.08	-0.08	
-0.08	-0.54	-0.08	-0.08	-0.08	
-0.54	-0.08	-0.08	-0.08	0.07	
mean policy
v	>	v	v	v	
>	>	v	v	<	
>	>	v	<	<	
^	>	^	^	<	
>	>	^	^	^	
features
0 	1 	0 	1 	1 	
3 	0 	0 	3 	1 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
1 	0 	0 	0 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
<	>	v	v	<	
>	>	<	<	<	
^	>	^	^	v	
^	>	^	>	>	
------ Regret Solution ---------
expert u_sa [11.13715254  0.45125     8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	>	v	<	v	
>	>	<	<	<	
^	^	^	^	<	
^	>	^	^	<	
-------- IRD Solution -------
ird reward
-12.33	-12.89	-12.33	-12.89	-12.89	
-12.81	-12.33	-12.33	-12.81	-12.89	
-12.33	-12.33	-12.12	-12.33	-12.33	
-12.33	-12.89	-12.33	-12.33	-12.33	
-12.89	-12.33	-12.33	-12.33	-12.81	
ird policy
v	v	v	<	v	
>	>	v	v	v	
>	>	<	<	<	
^	>	^	^	<	
>	>	^	^	^	
MAP policy loss 7.776791985922468
mean policy loss 14.707900143382027
robust policy loss 454.20336874399976
regret policy loss 2.051199575470175e-08
ird policy loss 3.610000000545768
MAP lava occupancy 0.19463204150230382
Mean lava occupancy 0.19463204150230382
Robust lava occupancy 4.6683669609839065
Regret lava occupancy 0.12000000017735976
IRD lava occupancy 0.1580000000060448
##############
Trial  3
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	<	<	
>	>	v	<	v	
>	>	>	<	<	
>	^	^	^	^	
>	^	<	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	1 	0 	1 	0 	
1 	0 	2 	0 	0 	
1 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [ 0.17045304 -0.85037393  0.48490772 -0.11258082] loglik -4.6903966497915235
accepted/total = 1617/2000 = 0.8085
MAP Policy on Train MDP
map_weights [ 0.17045304 -0.85037393  0.48490772 -0.11258082]
map reward
0.17	-0.85	0.17	0.17	0.17	
0.17	-0.85	0.17	-0.85	0.17	
-0.85	0.17	0.48	0.17	0.17	
-0.85	0.17	-0.85	0.17	-0.85	
0.17	0.17	0.17	0.17	0.17	
Map policy
v	>	v	<	<	
>	v	v	<	v	
>	>	>	<	<	
>	^	^	^	^	
>	^	<	^	<	
MEAN policy on Train MDP
mean_weights [ 0.10370631 -0.34309901  0.60159022  0.0026599 ]
mean reward
0.10	-0.34	0.10	0.10	0.10	
0.10	-0.34	0.10	-0.34	0.10	
-0.34	0.10	0.60	0.10	0.10	
-0.34	0.10	-0.34	0.10	-0.34	
0.10	0.10	0.10	0.10	0.10	
mean policy
v	>	v	<	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
>	^	^	^	<	
Optimal Policy
v	>	v	<	<	
>	>	v	<	v	
>	>	>	<	<	
>	^	^	^	^	
>	^	<	^	<	
MAP policy loss 3.533342146557622e-08
Mean policy loss 0.07980000593549252
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	v	v	v	
v	v	v	<	<	
>	>	v	<	^	
>	^	^	<	^	
^	^	^	^	>	
reward
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	1 	3 	
0 	0 	1 	3 	0 	
MAP on testing env
map_weights [ 0.17045304 -0.85037393  0.48490772 -0.11258082]
map reward
0.17	0.17	-0.11	-0.11	0.17	
0.17	-0.85	0.17	0.17	0.17	
0.17	0.17	0.48	-0.85	0.17	
0.17	0.17	0.17	-0.85	-0.11	
0.17	0.17	-0.85	-0.11	0.17	
Map policy
v	>	v	v	v	
v	v	v	<	<	
>	>	^	<	^	
>	^	^	<	^	
^	^	^	>	^	
MEAN policy on test env
mean_weights [ 0.10370631 -0.34309901  0.60159022  0.0026599 ]
mean reward
0.10	0.10	0.00	0.00	0.10	
0.10	-0.34	0.10	0.10	0.10	
0.10	0.10	0.60	-0.34	0.10	
0.10	0.10	0.10	-0.34	0.00	
0.10	0.10	-0.34	0.00	0.10	
mean policy
v	>	v	v	v	
v	v	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
^	^	^	<	^	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	1 	3 	
0 	0 	1 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	<	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.13715254  0.45125     8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	<	v	v	v	
v	>	v	<	<	
>	>	v	<	^	
>	>	^	<	^	
^	^	^	<	^	
-------- IRD Solution -------
ird reward
-12.93	-12.93	-13.59	-13.59	-12.93	
-12.93	-13.56	-12.93	-12.93	-12.93	
-12.93	-12.93	-12.84	-13.56	-12.93	
-12.93	-12.93	-12.93	-13.56	-13.59	
-12.93	-12.93	-13.56	-13.59	-12.93	
ird policy
v	<	v	v	v	
v	v	v	<	<	
>	>	v	<	^	
>	>	^	<	^	
^	^	^	<	^	
MAP policy loss 29.02829746101776
mean policy loss 7.079887121094988
robust policy loss 21.751691870300416
regret policy loss 3.158847601839013
ird policy loss 3.1588475999914367
MAP lava occupancy 0.32941933149116037
Mean lava occupancy 0.32941933149116037
Robust lava occupancy 0.3842000451495619
Regret lava occupancy 0.19800000000152823
IRD lava occupancy 0.1979999999435074
##############
Trial  4
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	>	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	^	^	
>	^	^	<	<	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	1 	1 	0 	
0 	0 	2 	0 	0 	
1 	0 	0 	1 	0 	
1 	0 	0 	0 	0 	
demonstration
[(12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [-0.36788509 -0.74322009  0.47367016 -0.29651482] loglik -3.9889840465647746
accepted/total = 1676/2000 = 0.838
MAP Policy on Train MDP
map_weights [-0.36788509 -0.74322009  0.47367016 -0.29651482]
map reward
-0.37	-0.74	-0.37	-0.74	-0.37	
-0.37	-0.37	-0.74	-0.74	-0.37	
-0.37	-0.37	0.47	-0.37	-0.37	
-0.74	-0.37	-0.37	-0.74	-0.37	
-0.74	-0.37	-0.37	-0.37	-0.37	
Map policy
v	v	v	<	v	
v	v	v	v	v	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.02080094 -0.52829421  0.37856921 -0.28175693]
mean reward
-0.02	-0.53	-0.02	-0.53	-0.02	
-0.02	-0.02	-0.53	-0.53	-0.02	
-0.02	-0.02	0.38	-0.02	-0.02	
-0.53	-0.02	-0.02	-0.53	-0.02	
-0.53	-0.02	-0.02	-0.02	-0.02	
mean policy
v	v	v	>	v	
v	v	v	v	v	
>	>	v	<	<	
>	^	^	<	^	
>	^	^	<	<	
Optimal Policy
v	v	v	>	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	^	^	
>	^	^	<	<	
MAP policy loss 0.07580999887428241
Mean policy loss -8.589845501560944e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	v	v	
>	v	v	v	<	
>	>	<	<	<	
^	^	^	^	^	
^	>	>	^	^	
reward
-100.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
3 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.36788509 -0.74322009  0.47367016 -0.29651482]
map reward
-0.30	-0.37	-0.37	-0.30	-0.37	
-0.37	-0.37	-0.74	-0.37	-0.74	
-0.74	-0.37	0.47	-0.37	-0.37	
-0.37	-0.74	-0.74	-0.37	-0.37	
-0.37	-0.30	-0.37	-0.37	-0.37	
Map policy
v	v	v	v	<	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	^	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-0.02080094 -0.52829421  0.37856921 -0.28175693]
mean reward
-0.28	-0.02	-0.02	-0.28	-0.02	
-0.02	-0.02	-0.53	-0.02	-0.53	
-0.53	-0.02	0.38	-0.02	-0.02	
-0.02	-0.53	-0.53	-0.02	-0.02	
-0.02	-0.28	-0.02	-0.02	-0.02	
mean policy
v	v	<	v	<	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	^	^	
^	>	>	^	^	
features
3 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
<	v	>	v	<	
>	v	v	v	v	
>	>	<	<	<	
v	^	^	^	<	
>	v	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	<	v	<	
>	v	v	v	v	
>	>	<	<	<	
v	^	^	^	^	
>	>	>	^	^	
-------- IRD Solution -------
ird reward
-13.60	-12.87	-12.87	-13.60	-12.87	
-12.87	-12.87	-13.63	-12.87	-13.63	
-13.63	-12.87	-12.72	-12.87	-12.87	
-12.87	-13.63	-13.63	-12.87	-12.87	
-12.87	-13.60	-12.87	-12.87	-12.87	
ird policy
v	v	<	v	<	
>	v	v	v	v	
>	>	<	<	<	
^	^	^	^	^	
^	>	>	^	^	
MAP policy loss 16.37777595154751
mean policy loss 3.6100000531168877
robust policy loss 311.29362185237403
regret policy loss 8.701258394980838
ird policy loss 3.6100000014123212
MAP lava occupancy 0.15800000000013648
Mean lava occupancy 0.15800000000013648
Robust lava occupancy 3.240748570592423
Regret lava occupancy 0.21031336381136176
IRD lava occupancy 0.158000000007936
##############
Trial  5
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	^	^	
>	^	>	^	^	
reward
-5.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
1 	1 	0 	0 	1 	
0 	0 	0 	0 	1 	
1 	0 	2 	0 	0 	
1 	0 	1 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (8, 3), (12, 1), (3, 3), (12, 0), (6, 1), (11, 1), (5, 1), (0, 3), (13, 0), (4, 0)]
w_map [ 0.09352723 -0.67374062  0.69668558 -0.22793737] loglik -5.375275598365988
accepted/total = 1698/2000 = 0.849
MAP Policy on Train MDP
map_weights [ 0.09352723 -0.67374062  0.69668558 -0.22793737]
map reward
-0.67	-0.67	0.09	0.09	-0.67	
0.09	0.09	0.09	0.09	-0.67	
-0.67	0.09	0.70	0.09	0.09	
-0.67	0.09	-0.67	0.09	0.09	
0.09	-0.67	0.09	0.09	0.09	
Map policy
v	>	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
>	^	^	^	^	
>	^	>	^	^	
MEAN policy on Train MDP
mean_weights [ 0.07220147 -0.36968598  0.57009664 -0.05582389]
mean reward
-0.37	-0.37	0.07	0.07	-0.37	
0.07	0.07	0.07	0.07	-0.37	
-0.37	0.07	0.57	0.07	0.07	
-0.37	0.07	-0.37	0.07	0.07	
0.07	-0.37	0.07	0.07	0.07	
mean policy
v	>	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
>	^	^	^	^	
>	^	^	^	^	
Optimal Policy
v	v	v	v	<	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	^	^	
>	^	>	^	^	
MAP policy loss 1.0030995405210796e-06
Mean policy loss 0.07980001353672247
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	<	
>	v	v	<	<	
>	>	^	<	^	
^	^	^	v	<	
^	^	<	<	<	
reward
-1.00	-5.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	3 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	1 	0 	0 	
1 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.09352723 -0.67374062  0.69668558 -0.22793737]
map reward
0.09	-0.67	0.09	0.09	-0.23	
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.70	-0.67	-0.67	
0.09	0.09	-0.67	0.09	0.09	
-0.67	0.09	0.09	0.09	0.09	
Map policy
v	v	v	<	<	
>	v	v	<	<	
>	>	^	<	^	
^	^	^	^	<	
^	^	<	<	<	
MEAN policy on test env
mean_weights [ 0.07220147 -0.36968598  0.57009664 -0.05582389]
mean reward
0.07	-0.37	0.07	0.07	-0.06	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.57	-0.37	-0.37	
0.07	0.07	-0.37	0.07	0.07	
-0.37	0.07	0.07	0.07	0.07	
mean policy
v	v	v	<	<	
>	v	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	<	<	
features
0 	1 	0 	0 	3 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	1 	0 	0 	
1 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	>	
>	>	v	<	^	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [10.58840254  1.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	<	<	
>	>	^	<	^	
>	^	^	<	<	
^	^	<	<	<	
-------- IRD Solution -------
ird reward
-12.38	-13.11	-12.38	-12.38	-12.91	
-12.38	-12.38	-12.38	-12.38	-12.38	
-12.38	-12.38	-12.32	-13.11	-13.11	
-12.38	-12.38	-13.11	-12.38	-12.38	
-13.11	-12.38	-12.38	-12.38	-12.38	
ird policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	v	<	
^	^	<	<	<	
MAP policy loss 12.131693536282649
mean policy loss 0.33597654315238556
robust policy loss 141.91699409197088
regret policy loss 0.14411435160459957
ird policy loss 5.987739977453277e-09
MAP lava occupancy 0.04000001933983599
Mean lava occupancy 0.04000001933983599
Robust lava occupancy 1.457215746354752
Regret lava occupancy 0.04000000006816313
IRD lava occupancy 0.04000000002447271
##############
Trial  6
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	<	<	<	
v	v	v	^	<	
>	>	<	<	<	
>	>	^	<	<	
^	^	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-5.00	-5.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	0 	1 	0 	
1 	1 	1 	1 	0 	
demonstration
[(1, 3), (12, 0), (3, 0), (6, 3), (17, 2), (2, 0), (11, 1), (5, 1), (0, 3), (12, 3), (4, 0)]
w_map [ 0.13080969 -0.73936017  0.6593776   0.03816476] loglik -2.772588728517121
accepted/total = 1352/2000 = 0.676
MAP Policy on Train MDP
map_weights [ 0.13080969 -0.73936017  0.6593776   0.03816476]
map reward
0.13	0.13	0.13	0.13	0.13	
0.13	0.13	-0.74	0.13	0.13	
0.13	0.13	0.66	-0.74	-0.74	
0.13	0.13	0.13	-0.74	0.13	
-0.74	-0.74	-0.74	-0.74	0.13	
Map policy
v	v	<	<	<	
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	<	^	
MEAN policy on Train MDP
mean_weights [ 0.26244499 -0.37022595  0.52645735 -0.227725  ]
mean reward
0.26	0.26	0.26	0.26	0.26	
0.26	0.26	-0.37	0.26	0.26	
0.26	0.26	0.53	-0.37	-0.37	
0.26	0.26	0.26	-0.37	0.26	
-0.37	-0.37	-0.37	-0.37	0.26	
mean policy
v	v	<	<	<	
v	v	v	^	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	<	^	
Optimal Policy
v	v	<	<	<	
v	v	v	^	<	
>	>	<	<	<	
>	>	^	<	<	
^	^	^	<	^	
MAP policy loss 0.028547031216579574
Mean policy loss 8.147888135370818e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	v	<	
v	v	v	v	v	
>	>	<	<	<	
^	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-5.00	-100.00	-1.00	-100.00	
-5.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	0 	3 	
1 	1 	1 	0 	1 	
0 	0 	2 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.13080969 -0.73936017  0.6593776   0.03816476]
map reward
0.13	-0.74	0.04	0.13	0.04	
-0.74	-0.74	-0.74	0.13	-0.74	
0.13	0.13	0.66	0.13	0.13	
-0.74	-0.74	0.13	0.13	0.13	
0.13	0.13	0.13	0.13	0.13	
Map policy
v	>	>	v	<	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.26244499 -0.37022595  0.52645735 -0.227725  ]
mean reward
0.26	-0.37	-0.23	0.26	-0.23	
-0.37	-0.37	-0.37	0.26	-0.37	
0.26	0.26	0.53	0.26	0.26	
-0.37	-0.37	0.26	0.26	0.26	
0.26	0.26	0.26	0.26	0.26	
mean policy
v	v	>	v	<	
v	v	v	v	v	
>	>	>	<	<	
^	>	^	^	<	
>	>	^	^	^	
features
0 	1 	3 	0 	3 	
1 	1 	1 	0 	1 	
0 	0 	2 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	>	v	<	
v	v	v	v	v	
>	>	<	<	<	
^	>	^	^	<	
>	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.99565566  0.          7.88593375  0.        ]
Policy for lambda=0.0 and alpha=0.95
<	<	>	v	<	
v	v	v	v	v	
>	>	<	<	<	
^	>	^	<	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-13.17	-13.93	-13.85	-13.17	-13.85	
-13.93	-13.93	-13.93	-13.17	-13.93	
-13.17	-13.17	-13.11	-13.17	-13.17	
-13.93	-13.93	-13.17	-13.17	-13.17	
-13.17	-13.17	-13.17	-13.17	-13.17	
ird policy
<	<	>	v	<	
v	v	v	v	v	
>	>	<	<	<	
^	^	^	^	<	
>	>	^	^	^	
MAP policy loss 9.122142625654336
mean policy loss 1.3468059946106226e-07
robust policy loss 4.6089455777928445e-08
regret policy loss 1.5661802479757818
ird policy loss 1.0678022692555533
MAP lava occupancy 0.15410000048300845
Mean lava occupancy 0.15410000048300845
Robust lava occupancy 0.08000000036435345
Regret lava occupancy 0.08605622860029681
IRD lava occupancy 0.0800000170031942
##############
Trial  7
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	<	>	v	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-5.00	-5.00	
features
0 	0 	0 	1 	0 	
1 	0 	1 	1 	0 	
0 	0 	2 	0 	0 	
1 	1 	0 	0 	0 	
1 	0 	1 	1 	1 	
demonstration
[(0, 1), (1, 3), (12, 1), (12, 0), (9, 3), (6, 3), (12, 3), (17, 2), (11, 1), (14, 0), (4, 3), (13, 0)]
w_map [ 0.07502086 -0.27666125  0.95671258  0.05031357] loglik -3.295836866004038
accepted/total = 1670/2000 = 0.835
MAP Policy on Train MDP
map_weights [ 0.07502086 -0.27666125  0.95671258  0.05031357]
map reward
0.08	0.08	0.08	-0.28	0.08	
-0.28	0.08	-0.28	-0.28	0.08	
0.08	0.08	0.96	0.08	0.08	
-0.28	-0.28	0.08	0.08	0.08	
-0.28	0.08	-0.28	-0.28	-0.28	
Map policy
>	v	v	<	v	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.08638186 -0.49813308  0.3546836  -0.33360249]
mean reward
-0.09	-0.09	-0.09	-0.50	-0.09	
-0.50	-0.09	-0.50	-0.50	-0.09	
-0.09	-0.09	0.35	-0.09	-0.09	
-0.50	-0.50	-0.09	-0.09	-0.09	
-0.50	-0.09	-0.50	-0.50	-0.50	
mean policy
>	v	v	<	v	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
Optimal Policy
>	v	<	>	v	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
MAP policy loss 0.15561104301113907
Mean policy loss 0.15560998643568266
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	<	
>	v	v	v	v	
>	>	^	<	<	
>	^	^	<	^	
<	^	^	<	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-100.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	3 	
MAP on testing env
map_weights [ 0.07502086 -0.27666125  0.95671258  0.05031357]
map reward
0.08	-0.28	0.08	0.08	0.08	
0.08	0.08	0.08	-0.28	0.08	
0.08	0.08	0.96	0.08	0.08	
0.05	0.08	0.08	-0.28	0.08	
0.08	0.05	0.08	0.05	0.05	
Map policy
v	v	v	<	<	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	^	^	
^	^	^	<	^	
MEAN policy on test env
mean_weights [-0.08638186 -0.49813308  0.3546836  -0.33360249]
mean reward
-0.09	-0.50	-0.09	-0.09	-0.09	
-0.09	-0.09	-0.09	-0.50	-0.09	
-0.09	-0.09	0.35	-0.09	-0.09	
-0.33	-0.09	-0.09	-0.50	-0.09	
-0.09	-0.33	-0.09	-0.33	-0.33	
mean policy
v	v	v	<	<	
>	>	v	<	v	
>	>	<	<	<	
>	^	^	^	^	
^	^	^	<	^	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	v	
>	v	v	<	v	
>	>	v	<	<	
<	^	^	^	v	
>	v	^	v	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	^	^	
<	^	^	<	^	
-------- IRD Solution -------
ird reward
-12.68	-13.51	-12.68	-12.68	-12.68	
-12.68	-12.68	-12.68	-13.51	-12.68	
-12.68	-12.68	-12.58	-12.68	-12.68	
-13.46	-12.68	-12.68	-13.51	-12.68	
-12.68	-13.46	-12.68	-13.46	-13.46	
ird policy
v	v	v	<	v	
>	>	v	<	v	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	<	^	
MAP policy loss 3.5923922687354812
mean policy loss 3.0936873759279147
robust policy loss 396.53103434469995
regret policy loss 2.284856226025811e-07
ird policy loss 3.0936884895036063
MAP lava occupancy 0.1980000615974134
Mean lava occupancy 0.1980000615974134
Robust lava occupancy 4.135653071606512
Regret lava occupancy 0.1600000021411573
IRD lava occupancy 0.19800000722736336
##############
Trial  8
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
>	^	^	<	^	
reward
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	1 	1 	0 	0 	
1 	1 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	1 	
demonstration
[(12, 2), (7, 3), (8, 3), (12, 1), (3, 3), (12, 0), (4, 0), (12, 3), (17, 2), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [ 0.08795654 -0.7527711   0.63886902 -0.13208218] loglik -6.23832460723645
accepted/total = 1643/2000 = 0.8215
MAP Policy on Train MDP
map_weights [ 0.08795654 -0.7527711   0.63886902 -0.13208218]
map reward
0.09	-0.75	-0.75	0.09	0.09	
-0.75	-0.75	0.09	0.09	-0.75	
0.09	0.09	0.64	0.09	0.09	
0.09	0.09	0.09	-0.75	0.09	
0.09	0.09	0.09	0.09	-0.75	
Map policy
v	v	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
>	^	^	<	^	
^	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.07979645 -0.45318384  0.43677142 -0.01733147]
mean reward
-0.08	-0.45	-0.45	-0.08	-0.08	
-0.45	-0.45	-0.08	-0.08	-0.45	
-0.08	-0.08	0.44	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.45	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.45	
mean policy
v	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	<	^	
^	^	^	<	<	
Optimal Policy
v	v	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
>	^	^	<	^	
MAP policy loss 6.916650517989342e-07
Mean policy loss 6.23794583314019e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
<	<	^	v	>	
^	<	>	>	^	
^	^	>	v	v	
^	v	>	>	v	
>	>	>	>	>	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	1.00	-5.00	-100.00	
-5.00	-5.00	-100.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	1 	3 	
1 	1 	3 	0 	0 	
1 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.08795654 -0.7527711   0.63886902 -0.13208218]
map reward
0.09	-0.75	0.09	-0.75	0.09	
0.09	0.09	-0.13	0.09	0.09	
0.09	-0.13	0.64	-0.75	-0.13	
-0.75	-0.75	-0.13	0.09	0.09	
-0.75	0.09	0.09	0.09	0.09	
Map policy
v	v	v	<	v	
>	v	v	<	<	
>	>	<	<	v	
^	>	^	<	<	
>	>	^	<	<	
MEAN policy on test env
mean_weights [-0.07979645 -0.45318384  0.43677142 -0.01733147]
mean reward
-0.08	-0.45	-0.08	-0.45	-0.08	
-0.08	-0.08	-0.02	-0.08	-0.08	
-0.08	-0.02	0.44	-0.45	-0.02	
-0.45	-0.45	-0.02	-0.08	-0.08	
-0.45	-0.08	-0.08	-0.08	-0.08	
mean policy
v	v	v	<	v	
>	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
>	>	^	<	<	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	1 	3 	
1 	1 	3 	0 	0 	
1 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	<	^	>	v	
>	<	v	>	<	
^	>	>	<	<	
^	v	^	>	<	
>	>	>	^	^	
-------- IRD Solution -------
ird reward
-12.88	-13.50	-12.88	-13.50	-12.88	
-12.88	-12.88	-13.49	-12.88	-12.88	
-12.88	-13.49	-12.77	-13.50	-13.49	
-13.50	-13.50	-13.49	-12.88	-12.88	
-13.50	-12.88	-12.88	-12.88	-12.88	
ird policy
<	<	^	>	>	
^	<	>	>	^	
^	<	^	v	v	
^	v	>	>	v	
>	>	>	>	>	
MAP policy loss 691.7688521988268
mean policy loss 904.5709024298385
robust policy loss 527.1780341555262
regret policy loss 200.89823991659614
ird policy loss 3.6100002536358415
MAP lava occupancy 7.231205416951927
Mean lava occupancy 7.231205416951927
Robust lava occupancy 5.52569928378497
Regret lava occupancy 2.1736150935391354
IRD lava occupancy 0.1980000033235223
##############
Trial  9
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	<	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	<	<	<	<	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	1 	0 	0 	1 	
0 	0 	2 	0 	1 	
0 	1 	1 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (3, 3), (12, 0), (8, 0), (4, 0), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [-0.04666611 -0.33819853  0.92527163 -0.16527686] loglik -3.9889840465608586
accepted/total = 1629/2000 = 0.8145
MAP Policy on Train MDP
map_weights [-0.04666611 -0.33819853  0.92527163 -0.16527686]
map reward
-0.05	-0.05	-0.34	-0.05	-0.05	
-0.05	-0.34	-0.05	-0.05	-0.34	
-0.05	-0.05	0.93	-0.05	-0.34	
-0.05	-0.34	-0.34	-0.34	-0.05	
-0.05	-0.05	-0.05	-0.05	-0.05	
Map policy
v	v	v	v	<	
v	>	v	v	<	
>	>	^	<	<	
^	^	^	^	<	
^	>	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.07920013 -0.48442702  0.37192879  0.11024119]
mean reward
-0.08	-0.08	-0.48	-0.08	-0.08	
-0.08	-0.48	-0.08	-0.08	-0.48	
-0.08	-0.08	0.37	-0.08	-0.48	
-0.08	-0.48	-0.48	-0.48	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
mean policy
v	v	v	v	<	
v	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	>	^	<	<	
Optimal Policy
v	<	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	<	<	<	<	
MAP policy loss 0.20097917818245833
Mean policy loss 0.20097917397631848
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	<	>	v	
v	v	v	v	v	
>	>	>	<	<	
>	^	^	^	<	
^	^	>	^	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-100.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.04666611 -0.33819853  0.92527163 -0.16527686]
map reward
-0.05	-0.05	-0.05	-0.17	-0.05	
-0.05	-0.17	-0.34	-0.34	-0.05	
-0.05	-0.05	0.93	-0.05	-0.05	
-0.05	-0.05	-0.17	-0.05	-0.17	
-0.05	-0.17	-0.05	-0.05	-0.05	
Map policy
v	v	v	<	v	
v	v	v	v	v	
>	>	>	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on test env
mean_weights [-0.07920013 -0.48442702  0.37192879  0.11024119]
mean reward
-0.08	-0.08	-0.08	0.11	-0.08	
-0.08	0.11	-0.48	-0.48	-0.08	
-0.08	-0.08	0.37	-0.08	-0.08	
-0.08	-0.08	0.11	-0.08	0.11	
-0.08	0.11	-0.08	-0.08	-0.08	
mean policy
v	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	<	<	>	v	
v	v	v	v	v	
>	>	>	<	<	
>	^	^	^	^	
^	^	>	^	<	
-------- IRD Solution -------
ird reward
-12.98	-12.98	-12.98	-13.52	-12.98	
-12.98	-13.52	-13.60	-13.60	-12.98	
-12.98	-12.98	-12.79	-12.98	-12.98	
-12.98	-12.98	-13.52	-12.98	-13.52	
-12.98	-13.52	-12.98	-12.98	-12.98	
ird policy
v	<	<	>	v	
v	v	v	v	v	
>	>	>	<	<	
>	^	^	^	^	
^	^	>	^	<	
MAP policy loss 32.64107424592179
mean policy loss 905.092165388151
robust policy loss 350.1127547334969
regret policy loss 2.0808896873669713e-07
ird policy loss 1.0819226426567186e-07
MAP lava occupancy 0.4050330827958026
Mean lava occupancy 0.4050330827958026
Robust lava occupancy 3.737000961924921
Regret lava occupancy 0.20000000190432607
IRD lava occupancy 0.20000000100664225
##############
Trial  10
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	<	v	<	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	^	>	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
1 	0 	1 	0 	1 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (8, 3), (1, 3), (12, 1), (3, 3), (12, 0), (6, 3), (11, 1), (13, 0), (4, 0)]
w_map [-0.20399805 -0.65235371  0.71778604  0.1326749 ] loglik -1.3862943611197807
accepted/total = 1711/2000 = 0.8555
MAP Policy on Train MDP
map_weights [-0.20399805 -0.65235371  0.71778604  0.1326749 ]
map reward
-0.65	-0.20	-0.20	-0.20	-0.20	
-0.65	-0.20	-0.65	-0.20	-0.65	
-0.20	-0.20	0.72	-0.20	-0.65	
-0.20	-0.20	-0.65	-0.20	-0.20	
-0.20	-0.65	-0.20	-0.20	-0.20	
Map policy
>	v	v	v	<	
v	v	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.10702675 -0.40461238  0.56897651 -0.04696395]
mean reward
-0.40	0.11	0.11	0.11	0.11	
-0.40	0.11	-0.40	0.11	-0.40	
0.11	0.11	0.57	0.11	-0.40	
0.11	0.11	-0.40	0.11	0.11	
0.11	-0.40	0.11	0.11	0.11	
mean policy
>	v	>	v	<	
v	v	v	v	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	>	^	^	
Optimal Policy
>	v	<	v	<	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	^	>	^	^	
MAP policy loss 0.1596001380721442
Mean policy loss -9.031556270217012e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	>	>	
>	^	<	^	^	
^	^	^	v	^	
^	v	>	>	^	
>	^	>	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-100.00	-1.00	-100.00	-1.00	-1.00	
-5.00	-5.00	-100.00	-5.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	3 	0 	
3 	0 	3 	0 	0 	
1 	1 	3 	1 	0 	
MAP on testing env
map_weights [-0.20399805 -0.65235371  0.71778604  0.1326749 ]
map reward
-0.65	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	0.13	-0.20	-0.20	
-0.20	0.13	0.72	0.13	-0.20	
0.13	-0.20	0.13	-0.20	-0.20	
-0.65	-0.65	0.13	-0.65	-0.20	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	>	^	<	^	
MEAN policy on test env
mean_weights [ 0.10702675 -0.40461238  0.56897651 -0.04696395]
mean reward
-0.40	0.11	0.11	0.11	0.11	
0.11	0.11	-0.05	0.11	0.11	
0.11	-0.05	0.57	-0.05	0.11	
-0.05	0.11	-0.05	0.11	0.11	
-0.40	-0.40	-0.05	-0.40	0.11	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
features
1 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	3 	0 	
3 	0 	3 	0 	0 	
1 	1 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	>	v	v	
>	^	v	>	<	
^	>	>	<	^	
^	>	^	>	<	
>	^	^	>	^	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	>	>	>	
>	^	v	>	^	
^	>	^	<	^	
^	>	^	>	^	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-13.61	-12.96	-12.96	-12.96	-12.96	
-12.96	-12.96	-13.69	-12.96	-12.96	
-12.96	-13.69	-12.92	-13.69	-12.96	
-13.69	-12.96	-13.69	-12.96	-12.96	
-13.61	-13.61	-13.69	-13.61	-12.96	
ird policy
>	>	>	>	>	
>	^	>	>	^	
^	^	>	^	^	
^	^	>	>	^	
^	^	>	>	^	
MAP policy loss 903.6558189605533
mean policy loss 903.5819293971717
robust policy loss 633.7838598553878
regret policy loss 384.39278942701503
ird policy loss 6.498926223599004
MAP lava occupancy 9.635743602470606
Mean lava occupancy 9.635743602470606
Robust lava occupancy 6.856362865236222
Regret lava occupancy 4.287968730208283
IRD lava occupancy 0.3901000054362495
##############
Trial  11
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	<	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	v	<	
^	^	<	<	<	
reward
-1.00	-1.00	-5.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	1 	0 	
0 	0 	1 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (9, 0), (1, 3), (8, 3), (12, 0), (6, 3), (11, 1), (4, 3), (13, 0)]
w_map [ 0.37082391 -0.11800516  0.87811662 -0.27834441] loglik -1.3862935663503322
accepted/total = 1666/2000 = 0.833
MAP Policy on Train MDP
map_weights [ 0.37082391 -0.11800516  0.87811662 -0.27834441]
map reward
0.37	0.37	-0.12	-0.12	0.37	
0.37	0.37	-0.12	0.37	0.37	
0.37	0.37	0.88	-0.12	-0.12	
0.37	0.37	-0.12	0.37	0.37	
0.37	0.37	0.37	0.37	0.37	
Map policy
v	v	<	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
^	^	<	<	<	
MEAN policy on Train MDP
mean_weights [ 0.02704127 -0.37965776  0.51461758  0.18208364]
mean reward
0.03	0.03	-0.38	-0.38	0.03	
0.03	0.03	-0.38	0.03	0.03	
0.03	0.03	0.51	-0.38	-0.38	
0.03	0.03	-0.38	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
mean policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	<	<	
Optimal Policy
v	v	<	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	v	<	
^	^	<	<	<	
MAP policy loss 0.028548353310279925
Mean policy loss 0.3359766362642359
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	>	
>	>	v	<	^	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-100.00	-1.00	-1.00	-5.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	2 	1 	3 	
0 	0 	0 	1 	0 	
1 	3 	0 	0 	1 	
MAP on testing env
map_weights [ 0.37082391 -0.11800516  0.87811662 -0.27834441]
map reward
0.37	0.37	0.37	-0.28	0.37	
0.37	0.37	0.37	-0.28	0.37	
-0.28	0.37	0.88	-0.12	-0.28	
0.37	0.37	0.37	-0.12	0.37	
-0.12	-0.28	0.37	0.37	-0.12	
Map policy
>	v	v	<	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.02704127 -0.37965776  0.51461758  0.18208364]
mean reward
0.03	0.03	0.03	0.18	0.03	
0.03	0.03	0.03	0.18	0.03	
0.18	0.03	0.51	-0.38	0.18	
0.03	0.03	0.03	-0.38	0.03	
-0.38	0.18	0.03	0.03	-0.38	
mean policy
v	v	v	v	<	
v	>	v	<	<	
>	>	^	<	^	
^	^	^	<	^	
>	^	^	<	<	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	2 	1 	3 	
0 	0 	0 	1 	0 	
1 	3 	0 	0 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
v	v	v	^	<	
<	>	v	<	>	
^	>	^	<	^	
>	v	^	<	^	
------ Regret Solution ---------
expert u_sa [11.15971504  0.4286875   8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
^	^	^	<	<	
-------- IRD Solution -------
ird reward
-12.64	-12.64	-12.64	-13.16	-12.64	
-12.64	-12.64	-12.64	-13.16	-12.64	
-13.16	-12.64	-12.47	-13.34	-13.16	
-12.64	-12.64	-12.64	-13.34	-12.64	
-13.34	-13.16	-12.64	-12.64	-13.34	
ird policy
>	v	v	<	v	
>	>	v	<	<	
>	>	<	<	^	
>	^	^	<	<	
^	^	^	<	<	
MAP policy loss 29.31716959210624
mean policy loss 38.91589511148093
robust policy loss 509.46539612114884
regret policy loss 5.9640999998174
ird policy loss 9.458200006421972
MAP lava occupancy 0.5030579021848604
Mean lava occupancy 0.5030579021848604
Robust lava occupancy 5.317093061984677
Regret lava occupancy 0.27409999999981316
IRD lava occupancy 0.31020000005877274
##############
Trial  12
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
^	>	^	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	1 	0 	1 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.05463504 -0.25034781  0.96417756 -0.06857567] loglik -7.62461848511839
accepted/total = 1668/2000 = 0.834
MAP Policy on Train MDP
map_weights [-0.05463504 -0.25034781  0.96417756 -0.06857567]
map reward
-0.25	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.05	-0.05	-0.25	-0.05	
-0.05	-0.05	0.96	-0.05	-0.05	
-0.05	-0.25	-0.05	-0.05	-0.05	
-0.05	-0.25	-0.05	-0.25	-0.05	
Map policy
v	v	v	<	<	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
^	>	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.13621289 -0.29526442  0.604009    0.06819985]
mean reward
-0.30	0.14	0.14	0.14	0.14	
0.14	0.14	0.14	-0.30	0.14	
0.14	0.14	0.60	0.14	0.14	
0.14	-0.30	0.14	0.14	0.14	
0.14	-0.30	0.14	-0.30	0.14	
mean policy
v	v	v	<	<	
>	v	v	<	v	
>	>	>	<	<	
^	^	^	<	<	
^	>	^	^	^	
Optimal Policy
>	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
^	>	^	^	^	
MAP policy loss 9.015041904448032e-07
Mean policy loss 3.796261850708138e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
^	>	v	v	^	
>	>	>	<	<	
v	^	^	^	v	
>	>	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
-100.00	-1.00	1.00	-1.00	-100.00	
-1.00	-100.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	2 	0 	3 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.05463504 -0.25034781  0.96417756 -0.06857567]
map reward
-0.05	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.25	-0.05	-0.07	-0.05	
-0.07	-0.05	0.96	-0.05	-0.07	
-0.05	-0.07	-0.25	-0.07	-0.05	
-0.05	-0.05	-0.05	-0.25	-0.05	
Map policy
>	>	v	<	<	
v	v	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.13621289 -0.29526442  0.604009    0.06819985]
mean reward
0.14	0.14	0.14	0.14	0.14	
0.14	-0.30	0.14	0.07	0.14	
0.07	0.14	0.60	0.14	0.07	
0.14	0.07	-0.30	0.07	0.14	
0.14	0.14	0.14	-0.30	0.14	
mean policy
>	>	v	<	<	
v	>	v	<	<	
>	>	>	<	<	
^	^	^	^	^	
^	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	2 	0 	3 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	v	
v	>	v	v	<	
<	>	^	<	>	
^	^	^	^	^	
^	^	<	^	^	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
^	v	v	<	^	
>	>	>	<	<	
v	^	^	^	v	
>	>	^	<	<	
-------- IRD Solution -------
ird reward
-12.93	-12.93	-12.93	-12.93	-12.93	
-12.93	-13.53	-12.93	-13.58	-12.93	
-13.58	-12.93	-12.86	-12.93	-13.58	
-12.93	-13.58	-13.53	-13.58	-12.93	
-12.93	-12.93	-12.93	-13.53	-12.93	
ird policy
>	>	v	<	<	
^	>	v	<	^	
>	>	<	<	<	
v	^	^	^	v	
>	>	^	<	^	
MAP policy loss 32.60149972104476
mean policy loss 28.32523834094067
robust policy loss 239.54176896553236
regret policy loss 5.27665446004022e-07
ird policy loss 0.7393088527035886
MAP lava occupancy 0.44749188562160563
Mean lava occupancy 0.44749188562160563
Robust lava occupancy 2.6151310883959025
Regret lava occupancy 0.20000000605319335
IRD lava occupancy 0.20000000005681373
##############
Trial  13
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	v	
>	>	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
1 	0 	1 	0 	1 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (12, 0), (9, 3), (6, 3), (12, 3), (17, 2), (11, 1), (14, 0), (4, 3), (13, 0)]
w_map [ 0.20480437 -0.73106276  0.65082987  0.00478471] loglik -8.317765995425134
accepted/total = 1711/2000 = 0.8555
MAP Policy on Train MDP
map_weights [ 0.20480437 -0.73106276  0.65082987  0.00478471]
map reward
0.20	0.20	0.20	0.20	-0.73	
0.20	0.20	0.20	-0.73	0.20	
0.20	0.20	0.65	0.20	0.20	
0.20	-0.73	0.20	0.20	0.20	
-0.73	0.20	-0.73	0.20	-0.73	
Map policy
>	v	v	<	<	
>	v	v	<	v	
>	>	v	<	<	
^	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.09709152 -0.55055353  0.40243105 -0.123163  ]
mean reward
-0.10	-0.10	-0.10	-0.10	-0.55	
-0.10	-0.10	-0.10	-0.55	-0.10	
-0.10	-0.10	0.40	-0.10	-0.10	
-0.10	-0.55	-0.10	-0.10	-0.10	
-0.55	-0.10	-0.55	-0.10	-0.55	
mean policy
v	v	v	<	<	
>	v	v	<	v	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	^	
Optimal Policy
>	v	v	<	v	
>	>	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	^	^	
MAP policy loss 6.129697664690459e-07
Mean policy loss 4.594003799943125e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	<	<	
>	v	v	v	^	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	2 	0 	1 	
3 	0 	0 	0 	0 	
0 	0 	3 	1 	0 	
MAP on testing env
map_weights [ 0.20480437 -0.73106276  0.65082987  0.00478471]
map reward
0.20	0.20	0.20	0.20	0.20	
0.20	0.20	-0.73	-0.73	0.20	
-0.73	0.20	0.65	0.20	-0.73	
0.00	0.20	0.20	0.20	0.20	
0.20	0.20	0.00	-0.73	0.20	
Map policy
v	v	<	<	<	
>	v	v	v	^	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	^	^	
MEAN policy on test env
mean_weights [-0.09709152 -0.55055353  0.40243105 -0.123163  ]
mean reward
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.55	-0.55	-0.10	
-0.55	-0.10	0.40	-0.10	-0.55	
-0.12	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.12	-0.55	-0.10	
mean policy
v	v	v	<	<	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	2 	0 	1 	
3 	0 	0 	0 	0 	
0 	0 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	v	v	v	<	
v	>	>	<	<	
<	^	^	^	<	
^	>	v	<	^	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	<	<	<	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-12.77	-12.77	-12.77	-12.77	-12.77	
-12.77	-12.77	-13.44	-13.44	-12.77	
-13.44	-12.77	-12.62	-12.77	-13.44	
-13.47	-12.77	-12.77	-12.77	-12.77	
-12.77	-12.77	-13.47	-13.44	-12.77	
ird policy
v	v	<	<	<	
>	v	v	v	^	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 15.072571452290152
mean policy loss 0.24913737724251733
robust policy loss 280.80931380594484
regret policy loss 0.013101023535198156
ird policy loss 2.0393774131238773e-08
MAP lava occupancy 0.13345292362242198
Mean lava occupancy 0.13345292362242198
Robust lava occupancy 2.888154564126029
Regret lava occupancy 0.08000000339614377
IRD lava occupancy 0.08000000018183202
##############
Trial  14
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
reward
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-5.00	1.00	-5.00	-5.00	
-5.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-5.00	
features
0 	1 	1 	0 	0 	
1 	0 	0 	1 	0 	
1 	1 	2 	1 	1 	
1 	0 	0 	0 	1 	
0 	0 	0 	1 	1 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (3, 0), (6, 1), (17, 2), (2, 3), (12, 3), (4, 0)]
w_map [ 0.39143695 -0.16629582  0.8570457  -0.29085303] loglik -3.465734225013648
accepted/total = 1693/2000 = 0.8465
MAP Policy on Train MDP
map_weights [ 0.39143695 -0.16629582  0.8570457  -0.29085303]
map reward
0.39	-0.17	-0.17	0.39	0.39	
-0.17	0.39	0.39	-0.17	0.39	
-0.17	-0.17	0.86	-0.17	-0.17	
-0.17	0.39	0.39	0.39	-0.17	
0.39	0.39	0.39	-0.17	-0.17	
Map policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
MEAN policy on Train MDP
mean_weights [ 0.0506878  -0.38577265  0.55912651 -0.15358194]
mean reward
0.05	-0.39	-0.39	0.05	0.05	
-0.39	0.05	0.05	-0.39	0.05	
-0.39	-0.39	0.56	-0.39	-0.39	
-0.39	0.05	0.05	0.05	-0.39	
0.05	0.05	0.05	-0.39	-0.39	
mean policy
>	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
Optimal Policy
>	v	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
MAP policy loss 1.3201725477668402e-06
Mean policy loss 1.1893798107509568e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	<	v	<	
v	v	v	v	v	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-5.00	-1.00	-100.00	-1.00	-5.00	
-1.00	-5.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
1 	0 	3 	0 	1 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.39143695 -0.16629582  0.8570457  -0.29085303]
map reward
0.39	0.39	-0.29	0.39	0.39	
-0.17	0.39	-0.29	0.39	-0.17	
0.39	-0.17	0.86	-0.17	0.39	
0.39	0.39	0.39	0.39	0.39	
-0.29	0.39	0.39	0.39	0.39	
Map policy
>	v	v	v	<	
v	v	v	v	v	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
MEAN policy on test env
mean_weights [ 0.0506878  -0.38577265  0.55912651 -0.15358194]
mean reward
0.05	0.05	-0.15	0.05	0.05	
-0.39	0.05	-0.15	0.05	-0.39	
0.05	-0.39	0.56	-0.39	0.05	
0.05	0.05	0.05	0.05	0.05	
-0.15	0.05	0.05	0.05	0.05	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
features
0 	0 	3 	0 	0 	
1 	0 	3 	0 	1 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	>	v	<	<	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
------ Regret Solution ---------
expert u_sa [10.66215254  0.92625     8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	<	v	<	
v	v	v	v	v	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
-------- IRD Solution -------
ird reward
-12.35	-12.35	-13.12	-12.35	-12.35	
-12.93	-12.35	-13.12	-12.35	-12.93	
-12.35	-12.93	-12.15	-12.93	-12.35	
-12.35	-12.35	-12.35	-12.35	-12.35	
-13.12	-12.35	-12.35	-12.35	-12.35	
ird policy
>	v	<	v	<	
v	v	v	v	v	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
MAP policy loss 463.50609287136064
mean policy loss 31.317889985104834
robust policy loss 31.158289984592944
regret policy loss 3.0754125357468354e-09
ird policy loss -1.579612524116314e-08
MAP lava occupancy 4.811242737072372
Mean lava occupancy 4.811242737072372
Robust lava occupancy 0.44699000000644395
Regret lava occupancy 0.120000000165924
IRD lava occupancy 0.12000000000307755
##############
Trial  15
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	<	^	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
1 	0 	1 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 1), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.00179698 -0.16810727  0.91040697 -0.3780157 ] loglik -6.76156937088092
accepted/total = 1633/2000 = 0.8165
MAP Policy on Train MDP
map_weights [-0.00179698 -0.16810727  0.91040697 -0.3780157 ]
map reward
-0.00	-0.00	-0.00	-0.17	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	0.91	-0.00	-0.00	
-0.00	-0.00	-0.17	-0.00	-0.00	
-0.17	-0.00	-0.17	-0.00	-0.00	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.13964307 -0.49765763  0.3577269   0.30139297]
mean reward
-0.14	-0.14	-0.14	-0.50	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.14	0.36	-0.14	-0.14	
-0.14	-0.14	-0.50	-0.14	-0.14	
-0.50	-0.14	-0.50	-0.14	-0.14	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
v	v	v	v	v	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	<	^	^	
MAP policy loss 0.07980072399130755
Mean policy loss 0.07979999992865909
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	>	v	<	<	
^	>	>	<	<	
v	>	^	^	<	
>	>	^	^	<	
reward
-100.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-5.00	1.00	-1.00	-5.00	
-5.00	-100.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
3 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	2 	0 	1 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [-0.00179698 -0.16810727  0.91040697 -0.3780157 ]
map reward
-0.38	-0.00	-0.17	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.38	-0.17	0.91	-0.00	-0.17	
-0.17	-0.38	-0.00	-0.00	-0.17	
-0.00	-0.17	-0.00	-0.00	-0.00	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	>	^	<	<	
>	>	^	^	<	
MEAN policy on test env
mean_weights [-0.13964307 -0.49765763  0.3577269   0.30139297]
mean reward
0.30	-0.14	-0.50	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
0.30	-0.50	0.36	-0.14	-0.50	
-0.50	0.30	-0.14	-0.14	-0.50	
-0.14	-0.50	-0.14	-0.14	-0.14	
mean policy
<	<	<	<	<	
v	<	v	<	<	
<	<	<	<	<	
^	<	^	<	<	
^	^	^	^	<	
features
3 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	2 	0 	1 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
<	<	v	v	<	
v	<	v	<	<	
<	>	>	<	<	
^	>	^	<	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	>	v	v	<	
^	>	>	<	<	
>	>	^	<	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-13.28	-12.64	-13.12	-12.64	-12.64	
-12.64	-12.64	-12.64	-12.64	-12.64	
-13.28	-13.12	-12.44	-12.64	-13.12	
-13.12	-13.28	-12.64	-12.64	-13.12	
-12.64	-13.12	-12.64	-12.64	-12.64	
ird policy
>	v	v	v	v	
>	>	v	v	<	
^	>	>	<	<	
v	>	^	<	<	
>	>	^	^	<	
MAP policy loss 25.927192628709136
mean policy loss 1735.9788900019198
robust policy loss 546.7212072977626
regret policy loss 3.549010016734087
ird policy loss 4.207292425024295e-06
MAP lava occupancy 0.2541497180398608
Mean lava occupancy 0.2541497180398608
Robust lava occupancy 5.5935537279389695
Regret lava occupancy 0.1580000001488962
IRD lava occupancy 0.120000042126821
##############
Trial  16
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	<	<	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-5.00	-5.00	-5.00	-5.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
1 	1 	1 	1 	1 	
1 	0 	2 	0 	0 	
0 	0 	0 	0 	1 	
1 	1 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 1), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [ 0.26112957 -0.79213745  0.55162878  0.00594108] loglik -3.3111361607118397
accepted/total = 1631/2000 = 0.8155
MAP Policy on Train MDP
map_weights [ 0.26112957 -0.79213745  0.55162878  0.00594108]
map reward
0.26	0.26	0.26	0.26	-0.79	
-0.79	-0.79	-0.79	-0.79	-0.79	
-0.79	0.26	0.55	0.26	0.26	
0.26	0.26	0.26	0.26	-0.79	
-0.79	-0.79	0.26	0.26	0.26	
Map policy
>	>	v	<	<	
v	v	v	v	v	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.05811345 -0.51560201  0.50043157 -0.10439595]
mean reward
0.06	0.06	0.06	0.06	-0.52	
-0.52	-0.52	-0.52	-0.52	-0.52	
-0.52	0.06	0.50	0.06	0.06	
0.06	0.06	0.06	0.06	-0.52	
-0.52	-0.52	0.06	0.06	0.06	
mean policy
>	>	v	<	<	
v	v	v	v	v	
>	>	>	<	<	
>	>	^	<	<	
^	>	^	^	<	
Optimal Policy
>	>	v	<	<	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	<	
^	^	^	^	<	
MAP policy loss -2.4004466390309398e-08
Mean policy loss -1.3561950441137727e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	^	<	
>	>	^	^	^	
reward
-1.00	-100.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-5.00	-1.00	-1.00	
features
0 	3 	0 	0 	1 	
0 	1 	1 	0 	0 	
1 	0 	2 	0 	0 	
0 	3 	0 	0 	0 	
3 	0 	1 	0 	0 	
MAP on testing env
map_weights [ 0.26112957 -0.79213745  0.55162878  0.00594108]
map reward
0.26	0.01	0.26	0.26	-0.79	
0.26	-0.79	-0.79	0.26	0.26	
-0.79	0.26	0.55	0.26	0.26	
0.26	0.01	0.26	0.26	0.26	
0.01	0.26	-0.79	0.26	0.26	
Map policy
>	>	>	v	v	
^	v	v	v	v	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.05811345 -0.51560201  0.50043157 -0.10439595]
mean reward
0.06	-0.10	0.06	0.06	-0.52	
0.06	-0.52	-0.52	0.06	0.06	
-0.52	0.06	0.50	0.06	0.06	
0.06	-0.10	0.06	0.06	0.06	
-0.10	0.06	-0.52	0.06	0.06	
mean policy
>	>	>	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
features
0 	3 	0 	0 	1 	
0 	1 	1 	0 	0 	
1 	0 	2 	0 	0 	
0 	3 	0 	0 	0 	
3 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	v	v	v	
^	v	v	v	v	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [10.23102754  1.357375    8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	>	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-12.23	-12.81	-12.23	-12.23	-12.64	
-12.23	-12.64	-12.64	-12.23	-12.23	
-12.64	-12.23	-12.12	-12.23	-12.23	
-12.23	-12.81	-12.23	-12.23	-12.23	
-12.81	-12.23	-12.64	-12.23	-12.23	
ird policy
v	>	>	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
>	>	^	^	^	
MAP policy loss 13.245431212341098
mean policy loss 14.332260387612859
robust policy loss 244.22890290171912
regret policy loss 0.027372119247989588
ird policy loss -3.09469173065402e-08
MAP lava occupancy 0.25202961150451836
Mean lava occupancy 0.25202961150451836
Robust lava occupancy 2.572635392247221
Regret lava occupancy 0.1200000000514392
IRD lava occupancy 0.12000000000194815
##############
Trial  17
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	<	v	
>	>	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	>	^	<	
reward
-1.00	-5.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	1 	
0 	0 	0 	1 	0 	
1 	0 	2 	0 	1 	
0 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (6, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [ 0.24565863 -0.80266451  0.52619797 -0.13600448] loglik -4.689325221390959
accepted/total = 1657/2000 = 0.8285
MAP Policy on Train MDP
map_weights [ 0.24565863 -0.80266451  0.52619797 -0.13600448]
map reward
0.25	-0.80	0.25	-0.80	-0.80	
0.25	0.25	0.25	-0.80	0.25	
-0.80	0.25	0.53	0.25	-0.80	
0.25	0.25	-0.80	0.25	-0.80	
0.25	0.25	0.25	0.25	0.25	
Map policy
v	v	v	<	v	
>	>	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	>	^	<	
MEAN policy on Train MDP
mean_weights [-0.02228206 -0.48560755  0.48040235  0.16604325]
mean reward
-0.02	-0.49	-0.02	-0.49	-0.49	
-0.02	-0.02	-0.02	-0.49	-0.02	
-0.49	-0.02	0.48	-0.02	-0.49	
-0.02	-0.02	-0.49	-0.02	-0.49	
-0.02	-0.02	-0.02	-0.02	-0.02	
mean policy
v	v	v	<	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
v	v	v	<	v	
>	>	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	>	^	<	
MAP policy loss -9.269720128643466e-09
Mean policy loss 0.079800080569178
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	v	
>	v	v	<	<	
>	>	^	<	^	
^	^	^	^	^	
^	<	<	^	<	
reward
-1.00	-100.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-5.00	-100.00	
-1.00	-1.00	-100.00	-5.00	-1.00	
features
0 	3 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	1 	3 	1 	3 	
0 	0 	3 	1 	0 	
MAP on testing env
map_weights [ 0.24565863 -0.80266451  0.52619797 -0.13600448]
map reward
0.25	-0.14	0.25	-0.80	0.25	
-0.80	0.25	0.25	0.25	0.25	
0.25	0.25	0.53	-0.80	0.25	
0.25	-0.80	-0.14	-0.80	-0.14	
0.25	0.25	-0.14	-0.80	0.25	
Map policy
>	v	v	<	v	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	<	^	
^	<	^	<	^	
MEAN policy on test env
mean_weights [-0.02228206 -0.48560755  0.48040235  0.16604325]
mean reward
-0.02	0.17	-0.02	-0.49	-0.02	
-0.49	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	0.48	-0.49	-0.02	
-0.02	-0.49	0.17	-0.49	0.17	
-0.02	-0.02	0.17	-0.49	-0.02	
mean policy
>	v	v	<	v	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
>	>	^	<	^	
features
0 	3 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	1 	3 	1 	3 	
0 	0 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	<	^	
>	>	^	<	^	
------ Regret Solution ---------
expert u_sa [10.63715254  0.95125     8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	v	
>	v	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	<	^	^	^	
-------- IRD Solution -------
ird reward
-12.55	-13.20	-12.55	-13.05	-12.55	
-13.05	-12.55	-12.55	-12.55	-12.55	
-12.55	-12.55	-12.42	-13.05	-12.55	
-12.55	-13.05	-13.20	-13.05	-13.20	
-12.55	-12.55	-13.20	-13.05	-12.55	
ird policy
v	v	v	<	v	
>	v	v	<	<	
>	>	^	<	^	
^	^	^	^	^	
^	<	<	>	^	
MAP policy loss 12.464617933148698
mean policy loss 900.6362848649766
robust policy loss 827.0044547887071
regret policy loss 7.161308216171067
ird policy loss 6.801572956003941
MAP lava occupancy 0.16000000029025954
Mean lava occupancy 0.16000000029025954
Robust lava occupancy 8.523147427305522
Regret lava occupancy 0.23600000000057306
IRD lava occupancy 0.23410000000583472
##############
Trial  18
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
>	v	v	v	v	
>	>	>	<	<	
^	^	^	^	^	
^	<	>	^	^	
reward
-1.00	-5.00	-5.00	-1.00	-5.00	
-5.00	-1.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-5.00	
features
0 	1 	1 	0 	1 	
1 	0 	1 	1 	0 	
0 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	1 	0 	0 	1 	
demonstration
[(0, 1), (1, 3), (12, 1), (12, 0), (9, 3), (6, 3), (11, 1), (14, 0), (4, 3), (13, 0)]
w_map [-0.29363636 -0.70019101  0.38370247 -0.52562596] loglik -2.079441541700067
accepted/total = 1708/2000 = 0.854
MAP Policy on Train MDP
map_weights [-0.29363636 -0.70019101  0.38370247 -0.52562596]
map reward
-0.29	-0.70	-0.70	-0.29	-0.70	
-0.70	-0.29	-0.70	-0.70	-0.29	
-0.29	-0.29	0.38	-0.29	-0.29	
-0.29	-0.70	-0.70	-0.29	-0.29	
-0.29	-0.70	-0.29	-0.29	-0.70	
Map policy
v	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
^	^	^	^	<	
^	>	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.09908818 -0.5698135   0.3912434  -0.01497397]
mean reward
-0.10	-0.57	-0.57	-0.10	-0.57	
-0.57	-0.10	-0.57	-0.57	-0.10	
-0.10	-0.10	0.39	-0.10	-0.10	
-0.10	-0.57	-0.57	-0.10	-0.10	
-0.10	-0.57	-0.10	-0.10	-0.57	
mean policy
v	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
^	^	^	^	^	
^	>	>	^	^	
Optimal Policy
v	v	v	v	v	
>	v	v	v	v	
>	>	>	<	<	
^	^	^	^	^	
^	<	>	^	^	
MAP policy loss 0.15560997924663908
Mean policy loss -2.072091302013357e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	v	v	<	
v	v	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.29363636 -0.70019101  0.38370247 -0.52562596]
map reward
-0.29	-0.29	-0.53	-0.29	-0.29	
-0.29	-0.70	-0.29	-0.29	-0.29	
-0.29	-0.29	0.38	-0.29	-0.70	
-0.29	-0.29	-0.29	-0.29	-0.29	
-0.29	-0.29	-0.70	-0.29	-0.29	
Map policy
v	>	v	v	<	
v	v	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.09908818 -0.5698135   0.3912434  -0.01497397]
mean reward
-0.10	-0.10	-0.01	-0.10	-0.10	
-0.10	-0.57	-0.10	-0.10	-0.10	
-0.10	-0.10	0.39	-0.10	-0.57	
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.57	-0.10	-0.10	
mean policy
>	>	v	<	<	
v	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	^	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	^	<	<	
^	>	v	<	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [10.61340254  0.975       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
v	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-12.34	-12.34	-12.99	-12.34	-12.34	
-12.34	-12.77	-12.34	-12.34	-12.34	
-12.34	-12.34	-12.12	-12.34	-12.77	
-12.34	-12.34	-12.34	-12.34	-12.34	
-12.34	-12.34	-12.77	-12.34	-12.34	
ird policy
v	<	v	v	v	
v	v	v	v	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
MAP policy loss 0.8303377753863492
mean policy loss 14.603209996753039
robust policy loss 636.458954716548
regret policy loss 12.210590967584062
ird policy loss -7.379893571246576e-11
MAP lava occupancy 0.04000000000006096
Mean lava occupancy 0.04000000000006096
Robust lava occupancy 6.406996627505746
Regret lava occupancy 0.1640321303677711
IRD lava occupancy 0.040000000000798175
##############
Trial  19
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-5.00	-5.00	1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-5.00	-1.00	
features
0 	1 	0 	1 	0 	
1 	0 	1 	0 	0 	
1 	1 	2 	0 	0 	
1 	0 	1 	0 	0 	
1 	0 	1 	1 	0 	
demonstration
[(12, 1), (9, 3), (6, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.08750973 -0.40374725  0.90569763 -0.09508944] loglik -2.0794388599886133
accepted/total = 1611/2000 = 0.8055
MAP Policy on Train MDP
map_weights [-0.08750973 -0.40374725  0.90569763 -0.09508944]
map reward
-0.09	-0.40	-0.09	-0.40	-0.09	
-0.40	-0.09	-0.40	-0.09	-0.09	
-0.40	-0.40	0.91	-0.09	-0.09	
-0.40	-0.09	-0.40	-0.09	-0.09	
-0.40	-0.09	-0.40	-0.40	-0.09	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.27802171 -0.66469882  0.23242675 -0.10442019]
mean reward
-0.28	-0.66	-0.28	-0.66	-0.28	
-0.66	-0.28	-0.66	-0.28	-0.28	
-0.66	-0.66	0.23	-0.28	-0.28	
-0.66	-0.28	-0.66	-0.28	-0.28	
-0.66	-0.28	-0.66	-0.66	-0.28	
mean policy
>	v	v	v	v	
>	v	v	v	v	
>	>	>	<	<	
>	^	^	^	^	
>	^	^	^	^	
Optimal Policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
MAP policy loss 1.1968303278708792e-06
Mean policy loss 7.638469343151932e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	^	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.08750973 -0.40374725  0.90569763 -0.09508944]
map reward
-0.09	-0.09	-0.09	-0.09	-0.40	
-0.09	-0.09	-0.09	-0.09	-0.09	
-0.09	-0.40	0.91	-0.40	-0.09	
-0.09	-0.09	-0.09	-0.09	-0.09	
-0.09	-0.09	-0.10	-0.09	-0.09	
Map policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [-0.27802171 -0.66469882  0.23242675 -0.10442019]
mean reward
-0.28	-0.28	-0.28	-0.28	-0.66	
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.66	0.23	-0.66	-0.28	
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	-0.10	-0.28	-0.28	
mean policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	>	v	<	<	
------ Regret Solution ---------
expert u_sa [10.68471504  0.9036875   8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
-------- IRD Solution -------
ird reward
-11.30	-11.30	-11.30	-11.30	-11.75	
-11.30	-11.30	-11.30	-11.30	-11.30	
-11.30	-11.75	-10.90	-11.75	-11.30	
-11.30	-11.30	-11.30	-11.30	-11.30	
-11.30	-11.30	-12.05	-11.30	-11.30	
ird policy
>	>	v	<	<	
>	>	v	<	<	
v	>	^	<	v	
>	>	^	<	<	
>	^	^	^	<	
MAP policy loss 36.35623388840871
mean policy loss 14.831400091095388
robust policy loss 338.32152186146016
regret policy loss 6.347379059283184
ird policy loss 1.1849016870457696e-07
MAP lava occupancy 0.04000001576980475
Mean lava occupancy 0.04000001576980475
Robust lava occupancy 3.423930866111261
Regret lava occupancy 0.10250281917994424
IRD lava occupancy 0.040000001211382696
##############
Trial  20
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	<	>	v	<	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	1 	0 	0 	1 	
0 	1 	1 	0 	1 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	1 	
0 	0 	0 	0 	1 	
demonstration
[(8, 3), (12, 1), (3, 3), (12, 0), (4, 0), (12, 3), (17, 2), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [ 0.08993235 -0.35763123  0.91727412 -0.15040032] loglik -3.295836866004038
accepted/total = 1718/2000 = 0.859
MAP Policy on Train MDP
map_weights [ 0.08993235 -0.35763123  0.91727412 -0.15040032]
map reward
0.09	-0.36	0.09	0.09	-0.36	
0.09	-0.36	-0.36	0.09	-0.36	
0.09	0.09	0.92	0.09	0.09	
-0.36	0.09	0.09	0.09	-0.36	
0.09	0.09	0.09	0.09	-0.36	
Map policy
v	>	v	v	<	
v	v	v	v	v	
>	>	v	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.10228125 -0.53694466  0.42635157 -0.03440795]
mean reward
-0.10	-0.54	-0.10	-0.10	-0.54	
-0.10	-0.54	-0.54	-0.10	-0.54	
-0.10	-0.10	0.43	-0.10	-0.10	
-0.54	-0.10	-0.10	-0.10	-0.54	
-0.10	-0.10	-0.10	-0.10	-0.54	
mean policy
v	>	v	v	<	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
v	<	>	v	<	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	<	
MAP policy loss 0.15561002804661622
Mean policy loss 0.1556100375384524
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	v	
>	>	>	<	<	
>	^	^	^	^	
^	^	<	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
1 	0 	2 	0 	0 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [ 0.08993235 -0.35763123  0.91727412 -0.15040032]
map reward
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.09	-0.15	0.09	
-0.36	0.09	0.92	0.09	0.09	
0.09	0.09	-0.15	-0.36	-0.36	
0.09	0.09	-0.36	0.09	0.09	
Map policy
>	v	v	<	v	
>	v	v	v	v	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	^	<	
MEAN policy on test env
mean_weights [-0.10228125 -0.53694466  0.42635157 -0.03440795]
mean reward
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.03	-0.10	
-0.54	-0.10	0.43	-0.10	-0.10	
-0.10	-0.10	-0.03	-0.54	-0.54	
-0.10	-0.10	-0.54	-0.10	-0.10	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	^	
>	^	^	^	<	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
1 	0 	2 	0 	0 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	<	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	<	^	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	>	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
^	^	<	^	<	
-------- IRD Solution -------
ird reward
-12.29	-12.29	-12.29	-12.29	-12.29	
-12.29	-12.29	-12.29	-12.92	-12.29	
-13.12	-12.29	-12.16	-12.29	-12.29	
-12.29	-12.29	-12.92	-13.12	-13.12	
-12.29	-12.29	-13.12	-12.29	-12.29	
ird policy
>	v	v	<	v	
>	v	v	<	v	
>	>	<	<	<	
>	^	^	^	^	
^	^	<	^	<	
MAP policy loss 315.13936954810487
mean policy loss 893.9724025693463
robust policy loss 302.4741154570527
regret policy loss 2.2171367852763524e-07
ird policy loss 1.4353137731126786e-07
MAP lava occupancy 3.2583302858003287
Mean lava occupancy 3.2583302858003287
Robust lava occupancy 3.136023388404903
Regret lava occupancy 0.0800000016493724
IRD lava occupancy 0.08000000107403246
##############
Trial  21
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	>	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	1 	
0 	1 	0 	0 	0 	
demonstration
[(7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (6, 1), (8, 0), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.0738269  -0.25078201  0.35719373 -0.89669985] loglik -6.068424854770143
accepted/total = 1650/2000 = 0.825
MAP Policy on Train MDP
map_weights [-0.0738269  -0.25078201  0.35719373 -0.89669985]
map reward
-0.07	-0.25	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.07	0.36	-0.07	-0.25	
-0.07	-0.07	-0.25	-0.07	-0.25	
-0.07	-0.25	-0.07	-0.07	-0.07	
Map policy
v	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.06853664 -0.3772889   0.51492373  0.24355055]
mean reward
0.07	-0.38	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.51	0.07	-0.38	
0.07	0.07	-0.38	0.07	-0.38	
0.07	-0.38	0.07	0.07	0.07	
mean policy
v	v	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	>	^	<	
Optimal Policy
v	v	v	v	<	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	>	^	<	
MAP policy loss 0.07980068591485456
Mean policy loss 4.409571942309842e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	<	^	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.0738269  -0.25078201  0.35719373 -0.89669985]
map reward
-0.07	-0.07	-0.90	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.25	-0.90	
-0.07	-0.07	0.36	-0.07	-0.25	
-0.07	-0.07	-0.25	-0.07	-0.07	
-0.07	-0.07	-0.25	-0.07	-0.07	
Map policy
v	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.06853664 -0.3772889   0.51492373  0.24355055]
mean reward
0.07	0.07	0.24	0.07	0.07	
0.07	0.07	0.07	-0.38	0.24	
0.07	0.07	0.51	0.07	-0.38	
0.07	0.07	-0.38	0.07	0.07	
0.07	0.07	-0.38	0.07	0.07	
mean policy
>	>	v	<	<	
>	>	v	<	^	
>	>	<	<	<	
^	^	^	^	<	
^	^	<	^	^	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	^	<	v	
>	>	v	<	>	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	>	v	v	^	
>	>	>	<	<	
^	^	^	^	<	
^	^	<	^	^	
-------- IRD Solution -------
ird reward
-13.15	-13.15	-13.90	-13.15	-13.15	
-13.15	-13.15	-13.15	-13.85	-13.90	
-13.15	-13.15	-13.07	-13.15	-13.85	
-13.15	-13.15	-13.85	-13.15	-13.15	
-13.15	-13.15	-13.85	-13.15	-13.15	
ird policy
v	v	v	v	<	
>	>	v	<	v	
>	>	^	<	<	
^	^	^	^	<	
^	^	<	^	^	
MAP policy loss 5.863691193023755
mean policy loss 17.68719518091471
robust policy loss 304.1772830467526
regret policy loss 10.351294879609387
ird policy loss 5.702583849506726e-07
MAP lava occupancy 0.13949173736115367
Mean lava occupancy 0.13949173736115367
Robust lava occupancy 3.1284249904910815
Regret lava occupancy 0.18839499873287188
IRD lava occupancy 0.08000000145079501
##############
Trial  22
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	<	^	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-5.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	1 	
0 	1 	1 	1 	0 	
0 	1 	1 	0 	0 	
demonstration
[(7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (6, 1), (11, 1), (2, 3), (5, 1), (0, 3), (13, 0), (4, 0)]
w_map [-0.43047389 -0.79059398  0.43536974  0.01032371] loglik -5.375278407698538
accepted/total = 1710/2000 = 0.855
MAP Policy on Train MDP
map_weights [-0.43047389 -0.79059398  0.43536974  0.01032371]
map reward
-0.43	-0.79	-0.43	-0.43	-0.43	
-0.43	-0.43	-0.43	-0.43	-0.79	
-0.43	-0.43	0.44	-0.43	-0.79	
-0.43	-0.79	-0.79	-0.79	-0.43	
-0.43	-0.79	-0.79	-0.43	-0.43	
Map policy
v	v	v	v	<	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.17368198 -0.35453255  0.6435164   0.00259562]
mean reward
0.17	-0.35	0.17	0.17	0.17	
0.17	0.17	0.17	0.17	-0.35	
0.17	0.17	0.64	0.17	-0.35	
0.17	-0.35	-0.35	-0.35	0.17	
0.17	-0.35	-0.35	0.17	0.17	
mean policy
v	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	<	^	^	^	
Optimal Policy
v	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	<	^	^	^	
MAP policy loss 0.08340999999976267
Mean policy loss 7.731120396647917e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	>	>	^	^	
reward
-5.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [-0.43047389 -0.79059398  0.43536974  0.01032371]
map reward
-0.79	-0.43	-0.43	-0.79	-0.43	
-0.79	-0.43	-0.43	-0.43	-0.79	
-0.43	-0.43	0.44	-0.43	-0.43	
-0.79	0.01	0.01	-0.43	-0.43	
-0.43	-0.79	-0.43	-0.43	-0.43	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.17368198 -0.35453255  0.6435164   0.00259562]
mean reward
-0.35	0.17	0.17	-0.35	0.17	
-0.35	0.17	0.17	0.17	-0.35	
0.17	0.17	0.64	0.17	0.17	
-0.35	0.00	0.00	0.17	0.17	
0.17	-0.35	0.17	0.17	0.17	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	>	^	^	^	
features
1 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	^	<	
^	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	>	
>	v	v	v	<	
>	>	<	<	<	
^	^	^	^	^	
<	>	>	^	^	
-------- IRD Solution -------
ird reward
-13.96	-13.19	-13.19	-13.96	-13.19	
-13.96	-13.19	-13.19	-13.19	-13.96	
-13.19	-13.19	-13.13	-13.19	-13.19	
-13.96	-13.85	-13.85	-13.19	-13.19	
-13.19	-13.96	-13.19	-13.19	-13.19	
ird policy
>	v	v	v	>	
>	v	v	<	<	
>	>	>	<	<	
^	^	^	^	^	
<	>	>	^	^	
MAP policy loss 7.535209949594974
mean policy loss 7.195110078890728
robust policy loss 271.46545148761993
regret policy loss 1.0095619889343308
ird policy loss 1.0326257346254857
MAP lava occupancy 0.15600000000014289
Mean lava occupancy 0.15600000000014289
Robust lava occupancy 2.8234973891773447
Regret lava occupancy 0.08000000098156287
IRD lava occupancy 0.08000000078801822
##############
Trial  23
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	1 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (17, 2), (13, 0), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (12, 3), (5, 3)]
w_map [-0.34510698 -0.60700965  0.70332203  0.13333634] loglik -6.931471805641394
accepted/total = 1713/2000 = 0.8565
MAP Policy on Train MDP
map_weights [-0.34510698 -0.60700965  0.70332203  0.13333634]
map reward
-0.35	-0.61	-0.35	-0.35	-0.35	
-0.35	-0.61	-0.35	-0.35	-0.35	
-0.35	-0.35	0.70	-0.35	-0.35	
-0.35	-0.35	-0.35	-0.35	-0.35	
-0.35	-0.35	-0.35	-0.35	-0.61	
Map policy
v	>	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.03421624 -0.40914229  0.52775053 -0.06834219]
mean reward
0.03	-0.41	0.03	0.03	0.03	
0.03	-0.41	0.03	0.03	0.03	
0.03	0.03	0.53	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	-0.41	
mean policy
v	>	v	v	<	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	^	^	^	
Optimal Policy
v	>	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss -7.800672260915853e-11
Mean policy loss 3.0518708691272545e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
^	>	v	<	^	
^	>	^	<	^	
^	^	^	<	v	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-5.00	
-1.00	-5.00	1.00	-100.00	-5.00	
-1.00	-1.00	-5.00	-100.00	-100.00	
-100.00	-5.00	-1.00	-1.00	-5.00	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	2 	3 	1 	
0 	0 	1 	3 	3 	
3 	1 	0 	0 	1 	
MAP on testing env
map_weights [-0.34510698 -0.60700965  0.70332203  0.13333634]
map reward
-0.35	-0.35	-0.35	-0.35	-0.35	
-0.35	0.13	-0.35	0.13	-0.61	
-0.35	-0.61	0.70	0.13	-0.61	
-0.35	-0.35	-0.61	0.13	0.13	
0.13	-0.61	-0.35	-0.35	-0.61	
Map policy
>	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.03421624 -0.40914229  0.52775053 -0.06834219]
mean reward
0.03	0.03	0.03	0.03	0.03	
0.03	-0.07	0.03	-0.07	-0.41	
0.03	-0.41	0.53	-0.07	-0.41	
0.03	0.03	-0.41	-0.07	-0.07	
-0.07	-0.41	0.03	0.03	-0.41	
mean policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	<	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	2 	3 	1 	
0 	0 	1 	3 	3 	
3 	1 	0 	0 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
^	>	v	<	^	
^	>	^	<	<	
^	<	^	<	v	
^	^	^	<	<	
-------- IRD Solution -------
ird reward
-13.04	-13.04	-13.04	-13.04	-13.04	
-13.04	-13.69	-13.04	-13.69	-13.64	
-13.04	-13.64	-12.87	-13.69	-13.64	
-13.04	-13.04	-13.64	-13.69	-13.69	
-13.69	-13.64	-13.04	-13.04	-13.64	
ird policy
>	>	v	<	<	
^	>	v	<	^	
^	>	^	<	<	
^	<	^	<	^	
^	^	^	<	<	
MAP policy loss 263.05058465956347
mean policy loss 35.519632527142505
robust policy loss 422.59497977929993
regret policy loss 4.341175097527611
ird policy loss 6.9337556665871665
MAP lava occupancy 2.7253335963355534
Mean lava occupancy 2.7253335963355534
Robust lava occupancy 4.520446986751011
Regret lava occupancy 0.28636889850197833
IRD lava occupancy 0.31410000016109363
##############
Trial  24
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
features
0 	1 	0 	1 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	1 	0 	
1 	0 	1 	0 	0 	
1 	1 	0 	0 	1 	
demonstration
[(12, 2), (7, 3), (12, 0), (3, 0), (6, 1), (11, 1), (2, 3), (5, 1), (0, 3), (4, 0)]
w_map [ 0.12405271 -0.11354768  0.92931378  0.32877612] loglik -4.158878984422017
accepted/total = 1743/2000 = 0.8715
MAP Policy on Train MDP
map_weights [ 0.12405271 -0.11354768  0.92931378  0.32877612]
map reward
0.12	-0.11	0.12	-0.11	0.12	
0.12	0.12	0.12	0.12	-0.11	
0.12	0.12	0.93	-0.11	0.12	
-0.11	0.12	-0.11	0.12	0.12	
-0.11	-0.11	0.12	0.12	-0.11	
Map policy
v	v	v	<	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.02200545 -0.51916898  0.51979446 -0.18609901]
mean reward
-0.02	-0.52	-0.02	-0.52	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.52	
-0.02	-0.02	0.52	-0.52	-0.02	
-0.52	-0.02	-0.52	-0.02	-0.02	
-0.52	-0.52	-0.02	-0.02	-0.52	
mean policy
v	v	v	<	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	<	<	
^	^	^	^	<	
Optimal Policy
v	v	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	^	^	
MAP policy loss 9.580039233222926e-07
Mean policy loss 2.09446226612231e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	^	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-100.00	-5.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	3 	1 	0 	
1 	0 	0 	0 	0 	
3 	0 	2 	0 	1 	
0 	0 	0 	3 	1 	
1 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.12405271 -0.11354768  0.92931378  0.32877612]
map reward
0.12	-0.11	0.33	-0.11	0.12	
-0.11	0.12	0.12	0.12	0.12	
0.33	0.12	0.93	0.12	-0.11	
0.12	0.12	0.12	0.33	-0.11	
-0.11	0.12	0.12	-0.11	0.12	
Map policy
>	>	v	<	v	
v	>	v	v	<	
>	>	v	<	<	
^	^	^	^	<	
^	^	^	^	<	
MEAN policy on test env
mean_weights [-0.02200545 -0.51916898  0.51979446 -0.18609901]
mean reward
-0.02	-0.52	-0.19	-0.52	-0.02	
-0.52	-0.02	-0.02	-0.02	-0.02	
-0.19	-0.02	0.52	-0.02	-0.52	
-0.02	-0.02	-0.02	-0.19	-0.52	
-0.52	-0.02	-0.02	-0.52	-0.02	
mean policy
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	<	<	
features
0 	1 	3 	1 	0 	
1 	0 	0 	0 	0 	
3 	0 	2 	0 	1 	
0 	0 	0 	3 	1 	
1 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	^	<	v	
v	v	v	v	<	
<	>	^	<	<	
^	>	^	<	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	<	^	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-12.76	-13.47	-13.47	-13.47	-12.76	
-13.47	-12.76	-12.76	-12.76	-12.76	
-13.47	-12.76	-12.64	-12.76	-13.47	
-12.76	-12.76	-12.76	-13.47	-13.47	
-13.47	-12.76	-12.76	-13.47	-12.76	
ird policy
v	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	<	^	
>	^	^	<	<	
MAP policy loss 16.504845940972597
mean policy loss 3.6100010704684693
robust policy loss 107.43757214299367
regret policy loss 3.231061578012895e-09
ird policy loss 3.4699155215811572e-09
MAP lava occupancy 0.2863563084790079
Mean lava occupancy 0.2863563084790079
Robust lava occupancy 1.1994492574890392
Regret lava occupancy 0.12000000003270131
IRD lava occupancy 0.12000000004683291
##############
Trial  25
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	<	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	<	
>	^	^	^	^	
reward
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
1 	0 	0 	1 	0 	
0 	0 	1 	0 	1 	
1 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
demonstration
[(9, 0), (8, 3), (12, 1), (12, 0), (6, 3), (17, 2), (13, 0), (11, 1), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [-0.04905542 -0.27005349  0.93237977  0.2352289 ] loglik -5.37527514361193
accepted/total = 1663/2000 = 0.8315
MAP Policy on Train MDP
map_weights [-0.04905542 -0.27005349  0.93237977  0.2352289 ]
map reward
-0.27	-0.05	-0.05	-0.27	-0.05	
-0.05	-0.05	-0.27	-0.05	-0.27	
-0.27	-0.05	0.93	-0.05	-0.05	
-0.27	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.05	-0.05	-0.27	-0.05	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.21571588 -0.59184905  0.26214899  0.32259016]
mean reward
-0.59	-0.22	-0.22	-0.59	-0.22	
-0.22	-0.22	-0.59	-0.22	-0.59	
-0.59	-0.22	0.26	-0.22	-0.22	
-0.59	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.59	-0.22	
mean policy
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	^	^	^	^	
Optimal Policy
v	v	<	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	<	
>	^	^	^	^	
MAP policy loss 0.0798008956709444
Mean policy loss 0.07979999932804094
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	^	^	
reward
-100.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-5.00	-1.00	-100.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-100.00	-5.00	-100.00	-100.00	-100.00	
features
3 	0 	0 	1 	3 	
0 	0 	1 	0 	3 	
0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	
3 	1 	3 	3 	3 	
MAP on testing env
map_weights [-0.04905542 -0.27005349  0.93237977  0.2352289 ]
map reward
0.24	-0.05	-0.05	-0.27	0.24	
-0.05	-0.05	-0.27	-0.05	0.24	
-0.05	-0.27	0.93	-0.05	-0.05	
-0.05	-0.05	-0.27	-0.05	-0.05	
0.24	-0.27	0.24	0.24	0.24	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	>	^	<	<	
MEAN policy on test env
mean_weights [-0.21571588 -0.59184905  0.26214899  0.32259016]
mean reward
0.32	-0.22	-0.22	-0.59	0.32	
-0.22	-0.22	-0.59	-0.22	0.32	
-0.22	-0.59	0.26	-0.22	-0.22	
-0.22	-0.22	-0.59	-0.22	-0.22	
0.32	-0.59	0.32	0.32	0.32	
mean policy
<	<	<	>	>	
^	<	>	>	^	
^	>	v	>	^	
v	<	v	v	v	
<	<	>	>	>	
features
3 	0 	0 	1 	3 	
0 	0 	1 	0 	3 	
0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	
3 	1 	3 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
<	<	v	>	v	
^	>	v	v	>	
>	>	>	<	<	
v	>	^	^	v	
<	>	>	v	<	
------ Regret Solution ---------
expert u_sa [10.61340254  0.975       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	>	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
^	^	>	^	^	
-------- IRD Solution -------
ird reward
-12.62	-12.03	-12.03	-12.70	-12.62	
-12.03	-12.03	-12.70	-12.03	-12.62	
-12.03	-12.70	-11.76	-12.03	-12.03	
-12.03	-12.03	-12.70	-12.03	-12.03	
-12.62	-12.70	-12.62	-12.62	-12.62	
ird policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	^	^	
MAP policy loss 30.146567174803636
mean policy loss 1863.8702528133883
robust policy loss 580.5504203765913
regret policy loss 9.682913113621634
ird policy loss 3.6100001847930763
MAP lava occupancy 0.3478909455330004
Mean lava occupancy 0.3478909455330004
Robust lava occupancy 6.10243002098072
Regret lava occupancy 0.38040077763765767
IRD lava occupancy 0.3180000018066157
##############
Trial  26
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	<	v	v	
v	v	v	v	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	>	^	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
1 	0 	1 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (9, 0), (1, 3), (12, 1), (8, 3), (12, 0), (6, 3), (11, 1), (4, 3), (13, 0)]
w_map [ 0.13313766 -0.27022268  0.92719043 -0.22264763] loglik -1.3862943611197807
accepted/total = 1643/2000 = 0.8215
MAP Policy on Train MDP
map_weights [ 0.13313766 -0.27022268  0.92719043 -0.22264763]
map reward
0.13	0.13	0.13	-0.27	0.13	
-0.27	0.13	-0.27	0.13	0.13	
0.13	0.13	0.93	0.13	-0.27	
0.13	0.13	-0.27	0.13	0.13	
0.13	0.13	0.13	0.13	0.13	
Map policy
>	v	v	v	v	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.16611362 -0.59232619  0.29883229  0.11431046]
mean reward
-0.17	-0.17	-0.17	-0.59	-0.17	
-0.59	-0.17	-0.59	-0.17	-0.17	
-0.17	-0.17	0.30	-0.17	-0.59	
-0.17	-0.17	-0.59	-0.17	-0.17	
-0.17	-0.17	-0.17	-0.17	-0.17	
mean policy
>	v	v	v	v	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	<	v	v	
v	v	v	v	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	>	^	^	
MAP policy loss 0.15960008789433433
Mean policy loss 0.15960002180453062
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	v	<	
^	>	^	<	<	
v	>	^	<	<	
>	>	^	^	^	
reward
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-5.00	1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-100.00	-100.00	
features
1 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	2 	0 	0 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	3 	
MAP on testing env
map_weights [ 0.13313766 -0.27022268  0.92719043 -0.22264763]
map reward
-0.27	0.13	-0.27	0.13	0.13	
0.13	0.13	0.13	0.13	-0.27	
-0.22	-0.27	0.93	0.13	0.13	
0.13	-0.22	0.13	0.13	-0.22	
0.13	0.13	0.13	-0.22	-0.22	
Map policy
>	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-0.16611362 -0.59232619  0.29883229  0.11431046]
mean reward
-0.59	-0.17	-0.59	-0.17	-0.17	
-0.17	-0.17	-0.17	-0.17	-0.59	
0.11	-0.59	0.30	-0.17	-0.17	
-0.17	0.11	-0.17	-0.17	0.11	
-0.17	-0.17	-0.17	0.11	0.11	
mean policy
v	v	v	v	v	
v	<	v	v	v	
<	<	v	v	v	
^	<	<	v	v	
^	^	>	v	>	
features
1 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	2 	0 	0 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
v	>	v	v	v	
<	>	^	<	v	
>	>	^	>	v	
^	^	>	>	>	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	>	v	<	<	
^	>	>	<	<	
v	>	^	^	<	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-13.77	-13.06	-13.77	-13.06	-13.06	
-13.06	-13.06	-13.06	-13.06	-13.77	
-13.72	-13.77	-12.91	-13.06	-13.06	
-13.06	-13.72	-13.06	-13.06	-13.72	
-13.06	-13.06	-13.06	-13.72	-13.72	
ird policy
>	v	v	v	<	
>	>	v	v	v	
^	>	^	<	<	
v	>	^	<	<	
>	>	^	^	<	
MAP policy loss 31.55794942899307
mean policy loss 1801.212290542099
robust policy loss 662.5972413882935
regret policy loss -1.4051611263443675e-08
ird policy loss -1.034657620690993e-08
MAP lava occupancy 0.36951557572329197
Mean lava occupancy 0.36951557572329197
Robust lava occupancy 6.869987317928787
Regret lava occupancy 0.23800000000106053
IRD lava occupancy 0.23800000003450256
##############
Trial  27
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
1 	0 	0 	1 	1 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	1 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (12, 0), (3, 0), (6, 1), (12, 3), (17, 2), (11, 1), (2, 3), (13, 0), (4, 0)]
w_map [ 0.30726321 -0.53198988  0.78007474  0.11857268] loglik -6.931470644621982
accepted/total = 1655/2000 = 0.8275
MAP Policy on Train MDP
map_weights [ 0.30726321 -0.53198988  0.78007474  0.11857268]
map reward
0.31	0.31	0.31	0.31	-0.53	
-0.53	0.31	0.31	-0.53	-0.53	
0.31	0.31	0.78	0.31	-0.53	
0.31	0.31	0.31	0.31	-0.53	
-0.53	0.31	0.31	0.31	0.31	
Map policy
>	v	v	<	<	
>	>	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.05696336 -0.49562864  0.33268039  0.02177154]
mean reward
-0.06	-0.06	-0.06	-0.06	-0.50	
-0.50	-0.06	-0.06	-0.50	-0.50	
-0.06	-0.06	0.33	-0.06	-0.50	
-0.06	-0.06	-0.06	-0.06	-0.50	
-0.50	-0.06	-0.06	-0.06	-0.06	
mean policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
>	v	v	<	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	^	^	^	<	
MAP policy loss 7.93912399550678e-07
Mean policy loss 2.1932045214667628e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-100.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-100.00	
-100.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.30726321 -0.53198988  0.78007474  0.11857268]
map reward
0.31	0.12	-0.53	0.31	0.31	
0.31	-0.53	0.31	0.31	0.12	
0.12	0.31	0.78	0.31	-0.53	
0.31	0.31	0.31	0.31	0.31	
0.31	0.31	0.31	0.31	0.31	
Map policy
v	<	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [-0.05696336 -0.49562864  0.33268039  0.02177154]
mean reward
-0.06	0.02	-0.50	-0.06	-0.06	
-0.06	-0.50	-0.06	-0.06	0.02	
0.02	-0.06	0.33	-0.06	-0.50	
-0.06	-0.06	-0.06	-0.06	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
mean policy
v	<	v	v	v	
v	v	v	v	<	
>	>	v	<	<	
^	^	^	^	<	
^	^	^	^	<	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	v	v	v	
v	^	v	<	>	
<	>	^	<	<	
^	>	^	<	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-12.90	-13.64	-13.64	-12.90	-12.90	
-12.90	-13.64	-12.90	-12.90	-13.64	
-13.64	-12.90	-12.83	-12.90	-13.64	
-12.90	-12.90	-12.90	-12.90	-12.90	
-12.90	-12.90	-12.90	-12.90	-12.90	
ird policy
v	<	v	v	<	
^	v	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 8.18985164562325
mean policy loss 21.449195119657944
robust policy loss 531.7475896088412
regret policy loss 1.0884453598841715e-08
ird policy loss 1.6268894921643735
MAP lava occupancy 0.19976404297968806
Mean lava occupancy 0.19976404297968806
Robust lava occupancy 5.4466024610873545
Regret lava occupancy 0.12000000028079409
IRD lava occupancy 0.12000002115761604
##############
Trial  28
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	<	<	
>	>	v	v	v	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
1 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (12, 0), (6, 1), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (4, 3), (13, 0)]
w_map [-0.16334264 -0.45523569  0.73248394 -0.47911056] loglik -8.317766078184945
accepted/total = 1651/2000 = 0.8255
MAP Policy on Train MDP
map_weights [-0.16334264 -0.45523569  0.73248394 -0.47911056]
map reward
-0.16	-0.46	-0.16	-0.16	-0.16	
-0.46	-0.16	-0.16	-0.46	-0.16	
-0.16	-0.16	0.73	-0.16	-0.16	
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.46	-0.16	-0.16	-0.16	-0.16	
Map policy
v	v	v	<	<	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.13154769 -0.32674565  0.60504095 -0.03527831]
mean reward
0.13	-0.33	0.13	0.13	0.13	
-0.33	0.13	0.13	-0.33	0.13	
0.13	0.13	0.61	0.13	0.13	
0.13	0.13	0.13	0.13	0.13	
-0.33	0.13	0.13	0.13	0.13	
mean policy
v	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	<	
Optimal Policy
v	v	v	<	<	
>	>	v	v	v	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	^	<	
MAP policy loss 1.2849769030071068e-08
Mean policy loss 2.12119617912232e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	<	<	v	
v	v	v	>	v	
>	>	v	<	v	
>	^	^	<	<	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.16334264 -0.45523569  0.73248394 -0.47911056]
map reward
-0.16	-0.16	-0.16	-0.46	-0.16	
-0.16	-0.46	-0.46	-0.16	-0.16	
-0.16	-0.16	0.73	-0.46	-0.16	
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.46	-0.16	-0.16	-0.16	-0.16	
Map policy
v	>	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.13154769 -0.32674565  0.60504095 -0.03527831]
mean reward
0.13	0.13	0.13	-0.33	0.13	
0.13	-0.33	-0.33	0.13	0.13	
0.13	0.13	0.61	-0.33	0.13	
0.13	0.13	0.13	0.13	0.13	
-0.33	0.13	0.13	0.13	0.13	
mean policy
v	<	v	v	v	
v	v	v	v	v	
>	>	<	<	v	
>	^	^	<	<	
^	^	^	<	<	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	<	v	v	v	
v	v	v	v	v	
>	>	<	<	v	
>	^	^	<	<	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-12.85	-12.85	-12.85	-13.43	-12.85	
-12.85	-13.43	-13.43	-12.85	-12.85	
-12.85	-12.85	-12.72	-13.43	-12.85	
-12.85	-12.85	-12.85	-12.85	-12.85	
-13.43	-12.85	-12.85	-12.85	-12.85	
ird policy
v	<	<	<	v	
v	v	v	>	v	
>	>	<	<	v	
>	>	^	<	<	
>	^	^	<	<	
MAP policy loss 18.37833003264837
mean policy loss 0.043186528643720024
robust policy loss 0.3466260705535151
regret policy loss 0.043186540410172725
ird policy loss 5.969408187374292e-09
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  29
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	<	
>	>	v	<	v	
v	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
1 	0 	0 	1 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 0), (6, 1), (12, 3), (17, 2), (2, 3), (13, 0), (4, 0)]
w_map [ 0.27988415 -0.01165899  0.66323405  0.69400975] loglik -4.682130854579555
accepted/total = 1673/2000 = 0.8365
MAP Policy on Train MDP
map_weights [ 0.27988415 -0.01165899  0.66323405  0.69400975]
map reward
0.28	0.28	0.28	0.28	-0.01	
-0.01	0.28	0.28	-0.01	0.28	
0.28	-0.01	0.66	0.28	0.28	
0.28	0.28	0.28	0.28	0.28	
0.28	0.28	-0.01	0.28	0.28	
Map policy
>	v	v	<	v	
>	>	v	v	v	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.04936475 -0.40591603  0.54348147  0.09389019]
mean reward
0.05	0.05	0.05	0.05	-0.41	
-0.41	0.05	0.05	-0.41	0.05	
0.05	-0.41	0.54	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	-0.41	0.05	0.05	
mean policy
>	v	v	<	v	
>	>	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
Optimal Policy
>	v	v	<	<	
>	>	v	<	v	
v	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
MAP policy loss 0.07980047358426248
Mean policy loss 0.0798000046135165
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	v	
>	>	v	v	<	
^	>	^	<	<	
>	v	^	^	^	
>	>	>	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-100.00	1.00	-1.00	-1.00	
-100.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	2 	0 	0 	
3 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.27988415 -0.01165899  0.66323405  0.69400975]
map reward
0.28	0.28	0.28	0.28	0.28	
0.28	0.28	0.28	0.28	0.28	
-0.01	0.69	0.66	0.28	0.28	
0.69	0.28	-0.01	0.28	-0.01	
0.28	0.28	0.28	0.28	0.28	
Map policy
v	v	v	v	<	
>	v	v	<	<	
v	>	<	<	<	
<	<	^	^	^	
^	<	<	^	<	
MEAN policy on test env
mean_weights [ 0.04936475 -0.40591603  0.54348147  0.09389019]
mean reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
-0.41	0.09	0.54	0.05	0.05	
0.09	0.05	-0.41	0.05	-0.41	
0.05	0.05	0.05	0.05	0.05	
mean policy
>	v	v	v	v	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	^	
^	^	^	^	<	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	2 	0 	0 	
3 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	^	^	
^	^	<	^	<	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	v	v	
>	>	v	v	<	
^	>	^	<	<	
>	^	^	^	^	
>	>	>	^	<	
-------- IRD Solution -------
ird reward
-12.87	-12.87	-12.87	-12.87	-12.87	
-12.87	-12.87	-12.87	-12.87	-12.87	
-13.40	-13.53	-12.79	-12.87	-12.87	
-13.53	-12.87	-13.40	-12.87	-13.40	
-12.87	-12.87	-12.87	-12.87	-12.87	
ird policy
>	>	v	v	v	
>	>	v	v	<	
^	>	>	<	<	
v	v	^	^	^	
>	>	>	^	<	
MAP policy loss 17.618285715638
mean policy loss 889.8886017046207
robust policy loss 835.8262326472748
regret policy loss 4.604086675739379
ird policy loss 2.7459051699096904e-09
MAP lava occupancy 0.1366028349851512
Mean lava occupancy 0.1366028349851512
Robust lava occupancy 8.527475112331729
Regret lava occupancy 0.12816357503588752
IRD lava occupancy 0.08000000002173993
##############
Trial  30
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	<	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	<	^	
^	<	<	<	<	
reward
-1.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	1 	0 	1 	
0 	1 	0 	0 	1 	
0 	0 	2 	1 	0 	
0 	1 	1 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (3, 3), (12, 0), (8, 0), (4, 0), (11, 1), (10, 1), (0, 3), (5, 3)]
w_map [-0.09287981 -0.47753589  0.67669421  0.55264614] loglik -1.3862943611197807
accepted/total = 1693/2000 = 0.8465
MAP Policy on Train MDP
map_weights [-0.09287981 -0.47753589  0.67669421  0.55264614]
map reward
-0.09	-0.48	-0.48	-0.09	-0.48	
-0.09	-0.48	-0.09	-0.09	-0.48	
-0.09	-0.09	0.68	-0.48	-0.09	
-0.09	-0.48	-0.48	-0.48	-0.09	
-0.09	-0.09	-0.09	-0.09	-0.09	
Map policy
v	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	>	^	<	^	
MEAN policy on Train MDP
mean_weights [-0.06563466 -0.54270857  0.41283867 -0.02322556]
mean reward
-0.07	-0.54	-0.54	-0.07	-0.54	
-0.07	-0.54	-0.07	-0.07	-0.54	
-0.07	-0.07	0.41	-0.54	-0.07	
-0.07	-0.54	-0.54	-0.54	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
mean policy
v	<	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	<	^	<	<	
Optimal Policy
v	<	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	<	^	
^	<	<	<	<	
MAP policy loss 0.20097963670482746
Mean policy loss 0.04175918443115623
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	<	
>	>	v	<	<	
>	>	<	<	v	
>	^	^	<	<	
>	^	^	^	^	
reward
-100.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	1.00	-5.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-1.00	
features
3 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	2 	1 	1 	
3 	0 	0 	0 	0 	
0 	1 	1 	1 	0 	
MAP on testing env
map_weights [-0.09287981 -0.47753589  0.67669421  0.55264614]
map reward
0.55	0.55	-0.09	-0.09	-0.09	
-0.09	-0.09	-0.09	-0.09	-0.48	
-0.48	-0.09	0.68	-0.48	-0.48	
0.55	-0.09	-0.09	-0.09	-0.09	
-0.09	-0.48	-0.48	-0.48	-0.09	
Map policy
<	<	<	<	<	
^	^	<	<	<	
v	v	<	<	<	
<	<	<	<	<	
^	<	^	^	^	
MEAN policy on test env
mean_weights [-0.06563466 -0.54270857  0.41283867 -0.02322556]
mean reward
-0.02	-0.02	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.54	
-0.54	-0.07	0.41	-0.54	-0.54	
-0.02	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.54	-0.54	-0.54	-0.07	
mean policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	v	
>	^	^	<	<	
^	^	^	^	^	
features
3 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	2 	1 	1 	
3 	0 	0 	0 	0 	
0 	1 	1 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	v	<	<	
^	v	v	<	<	
>	>	<	<	<	
<	^	^	<	<	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	v	v	<	<	
>	>	<	<	v	
>	>	^	<	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-13.68	-13.68	-12.95	-12.95	-12.95	
-12.95	-12.95	-12.95	-12.95	-13.60	
-13.60	-12.95	-12.91	-13.60	-13.60	
-13.68	-12.95	-12.95	-12.95	-12.95	
-12.95	-13.60	-13.60	-13.60	-12.95	
ird policy
v	>	v	<	<	
>	v	v	<	<	
>	>	<	<	v	
>	>	^	<	<	
<	^	^	^	^	
MAP policy loss 5.24832050194375
mean policy loss 7.37200126730595
robust policy loss 342.48485621574645
regret policy loss -8.213133784185089e-10
ird policy loss 0.5163128261275322
MAP lava occupancy 0.15800003205963364
Mean lava occupancy 0.15800003205963364
Robust lava occupancy 3.5488236868232015
Regret lava occupancy 0.12000000000049393
IRD lava occupancy 0.12000000003630965
##############
Trial  31
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	<	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	<	^	
>	^	^	<	^	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
-5.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
features
0 	0 	1 	0 	0 	
1 	0 	1 	0 	1 	
1 	0 	2 	1 	0 	
0 	0 	0 	1 	0 	
0 	0 	1 	0 	1 	
demonstration
[(0, 1), (8, 3), (1, 3), (3, 3), (12, 0), (6, 3), (17, 2), (11, 1), (13, 0), (12, 3), (4, 0)]
w_map [ 0.18279067 -0.35753319  0.74310613  0.53530446] loglik -2.0794413473289524
accepted/total = 1661/2000 = 0.8305
MAP Policy on Train MDP
map_weights [ 0.18279067 -0.35753319  0.74310613  0.53530446]
map reward
0.18	0.18	-0.36	0.18	0.18	
-0.36	0.18	-0.36	0.18	-0.36	
-0.36	0.18	0.74	-0.36	0.18	
0.18	0.18	0.18	-0.36	0.18	
0.18	0.18	-0.36	0.18	-0.36	
Map policy
>	v	<	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	^	
^	^	^	<	^	
MEAN policy on Train MDP
mean_weights [-0.08001368 -0.50592126  0.38426703 -0.21469383]
mean reward
-0.08	-0.08	-0.51	-0.08	-0.08	
-0.51	-0.08	-0.51	-0.08	-0.51	
-0.51	-0.08	0.38	-0.51	-0.08	
-0.08	-0.08	-0.08	-0.51	-0.08	
-0.08	-0.08	-0.51	-0.08	-0.51	
mean policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	^	
^	^	^	<	^	
Optimal Policy
>	v	<	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	<	^	
>	^	^	<	^	
MAP policy loss 2.356532861312599e-07
Mean policy loss 0.07980000526825079
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	^	
>	^	^	<	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.18279067 -0.35753319  0.74310613  0.53530446]
map reward
0.18	-0.36	0.18	0.18	0.18	
0.18	0.18	0.18	0.18	0.54	
0.18	0.18	0.74	0.18	0.18	
-0.36	0.18	0.18	0.54	0.18	
0.18	0.18	0.18	0.18	0.18	
Map policy
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	^	>	^	
>	^	^	^	^	
MEAN policy on test env
mean_weights [-0.08001368 -0.50592126  0.38426703 -0.21469383]
mean reward
-0.08	-0.51	-0.08	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.21	
-0.08	-0.08	0.38	-0.08	-0.08	
-0.51	-0.08	-0.08	-0.21	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
mean policy
v	v	v	v	<	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	^	
>	^	^	<	<	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	v	v	>	>	
>	>	^	<	^	
>	^	^	^	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.15971504  0.4286875   8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	^	
>	^	^	<	^	
-------- IRD Solution -------
ird reward
-12.83	-13.51	-12.83	-12.83	-12.83	
-12.83	-12.83	-12.83	-12.83	-13.54	
-12.83	-12.83	-12.74	-12.83	-12.83	
-13.51	-12.83	-12.83	-13.54	-12.83	
-12.83	-12.83	-12.83	-12.83	-12.83	
ird policy
v	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	^	
>	^	^	<	<	
MAP policy loss 2.2692985816997204
mean policy loss 5.060449096427888e-07
robust policy loss 263.7372307734909
regret policy loss -8.234264589623841e-10
ird policy loss 7.26285934476989e-07
MAP lava occupancy 0.09833838028267917
Mean lava occupancy 0.09833838028267917
Robust lava occupancy 2.7202338034659235
Regret lava occupancy 0.08000000000029621
IRD lava occupancy 0.08000000531249873
##############
Trial  32
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	^	<	
^	<	^	^	^	
reward
-1.00	-1.00	-5.00	-5.00	-5.00	
-5.00	-5.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	1 	1 	1 	
1 	1 	1 	1 	0 	
0 	0 	2 	0 	1 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(0, 1), (1, 3), (12, 1), (12, 0), (9, 3), (6, 3), (12, 3), (17, 2), (11, 1), (14, 0), (4, 3), (13, 0)]
w_map [ 0.21566517 -0.82041812  0.52841797 -0.03430869] loglik -3.9969187195700897
accepted/total = 1690/2000 = 0.845
MAP Policy on Train MDP
map_weights [ 0.21566517 -0.82041812  0.52841797 -0.03430869]
map reward
0.22	0.22	-0.82	-0.82	-0.82	
-0.82	-0.82	-0.82	-0.82	0.22	
0.22	0.22	0.53	0.22	-0.82	
0.22	-0.82	0.22	0.22	0.22	
0.22	0.22	-0.82	0.22	0.22	
Map policy
>	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
^	^	^	^	<	
^	<	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.07400352 -0.54936815  0.44412939 -0.03261231]
mean reward
-0.07	-0.07	-0.55	-0.55	-0.55	
-0.55	-0.55	-0.55	-0.55	-0.07	
-0.07	-0.07	0.44	-0.07	-0.55	
-0.07	-0.55	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.55	-0.07	-0.07	
mean policy
>	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
^	^	^	^	<	
^	<	^	^	^	
Optimal Policy
>	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	^	<	
^	<	^	^	^	
MAP policy loss 6.69072511207125e-07
Mean policy loss -7.699114453613154e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	v	
>	>	v	<	v	
v	>	v	<	v	
v	>	^	<	<	
>	>	^	<	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	2 	3 	0 	
0 	3 	0 	0 	0 	
1 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.21566517 -0.82041812  0.52841797 -0.03430869]
map reward
0.22	0.22	0.22	-0.03	0.22	
-0.03	0.22	0.22	-0.82	0.22	
0.22	-0.03	0.53	-0.03	0.22	
0.22	-0.03	0.22	0.22	0.22	
-0.82	0.22	0.22	0.22	0.22	
Map policy
>	v	v	<	v	
>	>	v	<	v	
>	>	^	<	<	
^	>	^	<	<	
>	>	^	<	<	
MEAN policy on test env
mean_weights [-0.07400352 -0.54936815  0.44412939 -0.03261231]
mean reward
-0.07	-0.07	-0.07	-0.03	-0.07	
-0.03	-0.07	-0.07	-0.55	-0.07	
-0.07	-0.03	0.44	-0.03	-0.07	
-0.07	-0.03	-0.07	-0.07	-0.07	
-0.55	-0.07	-0.07	-0.07	-0.07	
mean policy
v	v	v	<	<	
v	v	v	v	v	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	^	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	2 	3 	0 	
0 	3 	0 	0 	0 	
1 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	>	v	v	v	
>	>	<	<	<	
>	>	^	<	<	
>	>	^	^	<	
------ Regret Solution ---------
expert u_sa [10.18590254  1.4025      8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	v	
>	>	v	<	<	
>	>	^	<	v	
^	>	^	<	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-12.27	-12.27	-12.27	-12.85	-12.27	
-12.85	-12.27	-12.27	-12.73	-12.27	
-12.27	-12.85	-12.11	-12.85	-12.27	
-12.27	-12.85	-12.27	-12.27	-12.27	
-12.73	-12.27	-12.27	-12.27	-12.27	
ird policy
>	>	v	<	v	
>	>	v	<	v	
>	>	^	<	v	
^	>	^	<	<	
>	>	^	<	<	
MAP policy loss 616.1644077433965
mean policy loss 929.837107764968
robust policy loss 470.9546997410286
regret policy loss 6.082779778106064
ird policy loss 6.833549556791164
MAP lava occupancy 6.425972109260226
Mean lava occupancy 6.425972109260226
Robust lava occupancy 4.964272734872345
Regret lava occupancy 0.26426660151158665
IRD lava occupancy 0.274100000614421
##############
Trial  33
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
v	>	v	<	<	
>	>	v	<	^	
>	^	^	<	^	
>	^	^	<	<	
reward
-5.00	-5.00	-5.00	-5.00	-5.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
1 	1 	1 	1 	1 	
1 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	0 	0 	1 	1 	
0 	0 	0 	0 	1 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 0), (8, 0), (17, 2), (11, 1), (10, 1), (4, 3), (0, 3), (12, 3), (5, 3)]
w_map [ 0.25666591 -0.10114322  0.95958027  0.05566304] loglik -3.295836866004038
accepted/total = 1649/2000 = 0.8245
MAP Policy on Train MDP
map_weights [ 0.25666591 -0.10114322  0.95958027  0.05566304]
map reward
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	0.26	0.26	0.26	
0.26	0.26	0.96	-0.10	0.26	
-0.10	0.26	0.26	-0.10	-0.10	
0.26	0.26	0.26	0.26	-0.10	
Map policy
v	v	v	v	v	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	<	^	
>	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.12515507 -0.5311795   0.30993252 -0.12435816]
mean reward
-0.53	-0.53	-0.53	-0.53	-0.53	
-0.53	-0.53	-0.13	-0.13	-0.13	
-0.13	-0.13	0.31	-0.53	-0.13	
-0.53	-0.13	-0.13	-0.53	-0.53	
-0.13	-0.13	-0.13	-0.13	-0.53	
mean policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	<	^	
>	^	^	<	<	
Optimal Policy
v	v	v	v	v	
v	>	v	<	<	
>	>	v	<	^	
>	^	^	<	^	
>	^	^	<	<	
MAP policy loss 0.1556100835052794
Mean policy loss 0.1556100705516027
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	v	
>	^	^	<	<	
^	^	^	<	<	
reward
-100.00	-5.00	-100.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-100.00	-1.00	-1.00	-1.00	
features
3 	1 	3 	1 	0 	
0 	1 	0 	0 	0 	
1 	0 	2 	3 	0 	
1 	0 	0 	0 	0 	
1 	3 	0 	0 	0 	
MAP on testing env
map_weights [ 0.25666591 -0.10114322  0.95958027  0.05566304]
map reward
0.06	-0.10	0.06	-0.10	0.26	
0.26	-0.10	0.26	0.26	0.26	
-0.10	0.26	0.96	0.06	0.26	
-0.10	0.26	0.26	0.26	0.26	
-0.10	0.06	0.26	0.26	0.26	
Map policy
v	>	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [-0.12515507 -0.5311795   0.30993252 -0.12435816]
mean reward
-0.12	-0.53	-0.12	-0.53	-0.13	
-0.13	-0.53	-0.13	-0.13	-0.13	
-0.53	-0.13	0.31	-0.12	-0.13	
-0.53	-0.13	-0.13	-0.13	-0.13	
-0.53	-0.12	-0.13	-0.13	-0.13	
mean policy
v	>	v	<	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
features
3 	1 	3 	1 	0 	
0 	1 	0 	0 	0 	
1 	0 	2 	3 	0 	
1 	0 	0 	0 	0 	
1 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [10.11340254  1.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	>	v	<	<	
>	>	<	<	v	
>	^	^	<	<	
^	^	^	<	<	
-------- IRD Solution -------
ird reward
-12.98	-12.77	-12.98	-12.77	-12.16	
-12.16	-12.77	-12.16	-12.16	-12.16	
-12.77	-12.16	-12.08	-12.98	-12.16	
-12.77	-12.16	-12.16	-12.16	-12.16	
-12.77	-12.98	-12.16	-12.16	-12.16	
ird policy
v	v	v	v	v	
>	>	v	<	<	
>	>	<	<	^	
>	^	^	<	<	
^	>	^	<	<	
MAP policy loss 12.477429178089743
mean policy loss 893.8565051956248
robust policy loss 342.70102458366785
regret policy loss 6.685505442305395e-08
ird policy loss 9.445161337356955e-07
MAP lava occupancy 0.2882648393459246
Mean lava occupancy 0.2882648393459246
Robust lava occupancy 3.625426510852252
Regret lava occupancy 0.16000000030388498
IRD lava occupancy 0.16000000761681346
##############
Trial  34
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	<	<	
^	v	v	v	v	
v	>	v	<	<	
>	>	^	^	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-5.00	-5.00	-5.00	-5.00	
-5.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	1 	
1 	1 	1 	1 	1 	
1 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
demonstration
[(0, 1), (7, 3), (12, 1), (3, 0), (17, 2), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [ 0.29241006 -0.73267996  0.61366961 -0.03295516] loglik -1.3944554074523694
accepted/total = 1701/2000 = 0.8505
MAP Policy on Train MDP
map_weights [ 0.29241006 -0.73267996  0.61366961 -0.03295516]
map reward
0.29	0.29	0.29	0.29	-0.73	
-0.73	-0.73	-0.73	-0.73	-0.73	
-0.73	-0.73	0.61	0.29	0.29	
0.29	0.29	0.29	0.29	0.29	
0.29	0.29	0.29	-0.73	0.29	
Map policy
>	>	v	<	<	
^	v	v	v	v	
v	>	v	<	<	
>	>	^	^	<	
>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.10840647 -0.41517124  0.6119242  -0.01717071]
mean reward
0.11	0.11	0.11	0.11	-0.42	
-0.42	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.42	0.61	0.11	0.11	
0.11	0.11	0.11	0.11	0.11	
0.11	0.11	0.11	-0.42	0.11	
mean policy
>	>	v	<	<	
^	v	v	v	v	
v	>	>	<	<	
>	>	^	<	<	
>	>	^	<	^	
Optimal Policy
>	>	v	<	<	
^	v	v	v	v	
v	>	v	<	<	
>	>	^	^	<	
>	>	^	^	^	
MAP policy loss 1.0218594692634753e-07
Mean policy loss 1.2902999779740298e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
^	>	^	^	<	
^	<	^	^	^	
reward
-1.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-100.00	-1.00	-5.00	-1.00	
-5.00	-5.00	-100.00	-1.00	-1.00	
features
0 	1 	1 	0 	1 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	1 	
0 	3 	0 	1 	0 	
1 	1 	3 	0 	0 	
MAP on testing env
map_weights [ 0.29241006 -0.73267996  0.61366961 -0.03295516]
map reward
0.29	-0.73	-0.73	0.29	-0.73	
0.29	0.29	0.29	-0.73	0.29	
0.29	0.29	0.61	0.29	-0.73	
0.29	-0.03	0.29	-0.73	0.29	
-0.73	-0.73	-0.03	0.29	0.29	
Map policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
^	>	^	^	v	
^	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.10840647 -0.41517124  0.6119242  -0.01717071]
mean reward
0.11	-0.42	-0.42	0.11	-0.42	
0.11	0.11	0.11	-0.42	0.11	
0.11	0.11	0.61	0.11	-0.42	
0.11	-0.02	0.11	-0.42	0.11	
-0.42	-0.42	-0.02	0.11	0.11	
mean policy
v	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	<	<	
^	^	^	<	<	
features
0 	1 	1 	0 	1 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	1 	
0 	3 	0 	1 	0 	
1 	1 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	^	v	
^	>	v	<	<	
------ Regret Solution ---------
expert u_sa [10.23102754  1.357375    8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
^	^	^	<	<	
^	<	^	^	^	
-------- IRD Solution -------
ird reward
-12.31	-12.89	-12.89	-12.31	-12.89	
-12.31	-12.31	-12.31	-12.89	-12.31	
-12.31	-12.31	-12.17	-12.31	-12.89	
-12.31	-12.91	-12.31	-12.89	-12.31	
-12.89	-12.89	-12.91	-12.31	-12.31	
ird policy
v	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	<	<	
^	^	^	^	^	
MAP policy loss 17.141030926577827
mean policy loss 10.580909985155706
robust policy loss 252.44306965637819
regret policy loss 1.3982868968809292e-06
ird policy loss 3.541409971105354
MAP lava occupancy 0.24628970908155198
Mean lava occupancy 0.24628970908155198
Robust lava occupancy 2.6132326565406307
Regret lava occupancy 0.08000001376648613
IRD lava occupancy 0.1180000000229669
##############
Trial  35
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	<	v	
>	>	v	v	<	
^	>	^	<	<	
>	>	^	^	<	
^	^	^	^	^	
reward
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-5.00	-5.00	-5.00	-5.00	
features
1 	0 	0 	1 	0 	
0 	0 	0 	1 	0 	
0 	1 	2 	0 	1 	
0 	0 	0 	0 	0 	
1 	1 	1 	1 	1 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (9, 0), (6, 1), (8, 0), (12, 3), (17, 2), (4, 3), (13, 0)]
w_map [ 0.26997992 -0.73534992  0.58648858 -0.20591864] loglik -6.0772153733194045
accepted/total = 1656/2000 = 0.828
MAP Policy on Train MDP
map_weights [ 0.26997992 -0.73534992  0.58648858 -0.20591864]
map reward
-0.74	0.27	0.27	-0.74	0.27	
0.27	0.27	0.27	-0.74	0.27	
0.27	-0.74	0.59	0.27	-0.74	
0.27	0.27	0.27	0.27	0.27	
-0.74	-0.74	-0.74	-0.74	-0.74	
Map policy
>	>	v	<	v	
>	>	v	v	<	
v	>	^	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.01614503 -0.47745639  0.43924181 -0.15551175]
mean reward
-0.48	-0.02	-0.02	-0.48	-0.02	
-0.02	-0.02	-0.02	-0.48	-0.02	
-0.02	-0.48	0.44	-0.02	-0.48	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.48	-0.48	-0.48	-0.48	-0.48	
mean policy
>	>	v	<	v	
>	>	v	<	<	
^	>	>	<	<	
>	>	^	<	<	
^	^	^	^	^	
Optimal Policy
>	>	v	<	v	
>	>	v	v	<	
^	>	^	<	<	
>	>	^	^	<	
^	^	^	^	^	
MAP policy loss 2.112750877025782e-10
Mean policy loss 2.7481947228319425e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	>	v	
>	v	v	<	v	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-100.00	-5.00	
-5.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	3 	0 	0 	
0 	0 	1 	3 	1 	
1 	0 	2 	1 	1 	
0 	0 	0 	0 	1 	
3 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.26997992 -0.73534992  0.58648858 -0.20591864]
map reward
0.27	-0.74	-0.21	0.27	0.27	
0.27	0.27	-0.74	-0.21	-0.74	
-0.74	0.27	0.59	-0.74	-0.74	
0.27	0.27	0.27	0.27	-0.74	
-0.21	0.27	0.27	-0.74	0.27	
Map policy
v	v	v	<	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
MEAN policy on test env
mean_weights [-0.01614503 -0.47745639  0.43924181 -0.15551175]
mean reward
-0.02	-0.48	-0.16	-0.02	-0.02	
-0.02	-0.02	-0.48	-0.16	-0.48	
-0.48	-0.02	0.44	-0.48	-0.48	
-0.02	-0.02	-0.02	-0.02	-0.48	
-0.16	-0.02	-0.02	-0.48	-0.02	
mean policy
v	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
features
0 	1 	3 	0 	0 	
0 	0 	1 	3 	1 	
1 	0 	2 	1 	1 	
0 	0 	0 	0 	1 	
3 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	^	<	<	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	<	<	
<	^	^	<	<	
------ Regret Solution ---------
expert u_sa [10.63715254  0.95125     8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	>	v	
>	v	v	<	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-11.90	-12.52	-12.79	-11.90	-11.90	
-11.90	-11.90	-12.52	-12.79	-12.52	
-12.52	-11.90	-11.75	-12.52	-12.52	
-11.90	-11.90	-11.90	-11.90	-12.52	
-12.79	-11.90	-11.90	-12.52	-11.90	
ird policy
v	v	v	>	<	
>	v	v	<	v	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	<	<	
MAP policy loss 34.651565172272484
mean policy loss 10.313410212386207
robust policy loss 125.66481673998513
regret policy loss 8.7988052954735e-10
ird policy loss 0.457729008803341
MAP lava occupancy 0.23103417263749113
Mean lava occupancy 0.23103417263749113
Robust lava occupancy 1.387422959045738
Regret lava occupancy 0.12000000000744196
IRD lava occupancy 0.12000000008401089
##############
Trial  36
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
>	^	^	^	^	
reward
-1.00	-1.00	-5.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	1 	1 	0 	
0 	1 	0 	0 	1 	
0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (9, 3), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [ 0.24444185 -0.80422497  0.54071347  0.03315608] loglik -2.789976103449817
accepted/total = 1603/2000 = 0.8015
MAP Policy on Train MDP
map_weights [ 0.24444185 -0.80422497  0.54071347  0.03315608]
map reward
0.24	0.24	-0.80	-0.80	0.24	
0.24	-0.80	0.24	0.24	-0.80	
0.24	-0.80	0.54	0.24	0.24	
0.24	0.24	-0.80	0.24	0.24	
0.24	0.24	0.24	-0.80	0.24	
Map policy
v	>	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.07651912 -0.54761452  0.44653687 -0.06184849]
mean reward
-0.08	-0.08	-0.55	-0.55	-0.08	
-0.08	-0.55	-0.08	-0.08	-0.55	
-0.08	-0.55	0.45	-0.08	-0.08	
-0.08	-0.08	-0.55	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.55	-0.08	
mean policy
v	>	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
>	^	^	^	^	
Optimal Policy
v	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
>	^	^	^	^	
MAP policy loss 1.8201320513278052e-08
Mean policy loss 1.3325701113744981e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
v	>	v	v	<	
>	>	<	<	<	
>	^	^	^	^	
^	^	<	<	^	
reward
-100.00	-1.00	-100.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-100.00	
features
3 	0 	3 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	3 	
MAP on testing env
map_weights [ 0.24444185 -0.80422497  0.54071347  0.03315608]
map reward
0.03	0.24	0.03	-0.80	0.24	
-0.80	-0.80	0.24	0.24	0.24	
-0.80	0.24	0.54	0.24	0.24	
0.24	0.24	-0.80	0.03	0.24	
0.24	0.24	0.24	0.03	0.03	
Map policy
>	>	v	v	v	
^	>	v	<	<	
>	>	>	<	<	
>	^	^	^	^	
^	^	<	^	^	
MEAN policy on test env
mean_weights [-0.07651912 -0.54761452  0.44653687 -0.06184849]
mean reward
-0.06	-0.08	-0.06	-0.55	-0.08	
-0.55	-0.55	-0.08	-0.08	-0.08	
-0.55	-0.08	0.45	-0.08	-0.08	
-0.08	-0.08	-0.55	-0.06	-0.08	
-0.08	-0.08	-0.08	-0.06	-0.06	
mean policy
>	>	v	<	v	
^	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	>	^	<	
features
3 	0 	3 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
<	>	^	<	v	
^	v	v	v	<	
>	>	^	<	<	
>	^	^	v	<	
>	>	>	^	<	
------ Regret Solution ---------
expert u_sa [10.68471504  0.9036875   8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	<	<	^	
-------- IRD Solution -------
ird reward
-13.09	-12.48	-13.09	-13.14	-12.48	
-13.14	-13.14	-12.48	-12.48	-12.48	
-13.14	-12.48	-12.33	-12.48	-12.48	
-12.48	-12.48	-13.14	-13.09	-12.48	
-12.48	-12.48	-12.48	-13.09	-13.09	
ird policy
>	>	v	v	v	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	^	
^	^	<	<	^	
MAP policy loss 9.813542143841014
mean policy loss 40.00250207254042
robust policy loss 406.6411416389527
regret policy loss 1.7477468674392815e-07
ird policy loss 7.0395000619367405
MAP lava occupancy 0.2933121235853735
Mean lava occupancy 0.2933121235853735
Robust lava occupancy 4.276643891761758
Regret lava occupancy 0.20000000177909766
IRD lava occupancy 0.2741000006534303
##############
Trial  37
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	1 	1 	1 	0 	
demonstration
[(7, 3), (12, 2), (9, 0), (12, 1), (8, 3), (12, 0), (6, 1), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.05324903 -0.23559881  0.36579323 -0.89880646] loglik -6.068424641878238
accepted/total = 1685/2000 = 0.8425
MAP Policy on Train MDP
map_weights [-0.05324903 -0.23559881  0.36579323 -0.89880646]
map reward
-0.05	-0.24	-0.05	-0.24	-0.05	
-0.05	-0.05	-0.05	-0.05	-0.24	
-0.05	-0.05	0.37	-0.05	-0.24	
-0.05	-0.05	-0.24	-0.05	-0.05	
-0.05	-0.24	-0.24	-0.24	-0.05	
Map policy
v	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.09460745 -0.55847591  0.36706286  0.05166491]
mean reward
-0.09	-0.56	-0.09	-0.56	-0.09	
-0.09	-0.09	-0.09	-0.09	-0.56	
-0.09	-0.09	0.37	-0.09	-0.56	
-0.09	-0.09	-0.56	-0.09	-0.09	
-0.09	-0.56	-0.56	-0.56	-0.09	
mean policy
v	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
Optimal Policy
v	v	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	^	^	
MAP policy loss 5.972382689650724e-07
Mean policy loss 3.436137155365948e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	v	
v	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
>	^	^	^	^	
reward
-1.00	-5.00	-1.00	-100.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-5.00	
features
0 	1 	0 	3 	0 	
1 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	3 	1 	
MAP on testing env
map_weights [-0.05324903 -0.23559881  0.36579323 -0.89880646]
map reward
-0.05	-0.24	-0.05	-0.90	-0.05	
-0.24	-0.24	-0.05	-0.05	-0.05	
-0.05	-0.05	0.37	-0.05	-0.05	
-0.24	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.05	-0.05	-0.90	-0.24	
Map policy
>	>	v	v	v	
v	>	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	^	
MEAN policy on test env
mean_weights [-0.09460745 -0.55847591  0.36706286  0.05166491]
mean reward
-0.09	-0.56	-0.09	0.05	-0.09	
-0.56	-0.56	-0.09	-0.09	-0.09	
-0.09	-0.09	0.37	-0.09	-0.09	
-0.56	-0.09	-0.09	-0.09	-0.09	
-0.09	-0.09	-0.09	0.05	-0.56	
mean policy
>	>	v	v	<	
v	>	v	<	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	<	
features
0 	1 	0 	3 	0 	
1 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	3 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	v	^	<	
v	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	v	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-12.53	-13.29	-12.53	-13.23	-12.53	
-13.29	-13.29	-12.53	-12.53	-12.53	
-12.53	-12.53	-12.38	-12.53	-12.53	
-13.29	-12.53	-12.53	-12.53	-12.53	
-12.53	-12.53	-12.53	-13.23	-13.29	
ird policy
>	>	v	v	v	
v	>	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
>	^	^	^	^	
MAP policy loss 2.589927882082395
mean policy loss 7.5239999985404555
robust policy loss 180.29360178926754
regret policy loss 4.100032757053951e-08
ird policy loss 3.2081746900031183e-07
MAP lava occupancy 0.10186728332020394
Mean lava occupancy 0.10186728332020394
Robust lava occupancy 1.884981670942136
Regret lava occupancy 0.0800000002347792
IRD lava occupancy 0.08000000235144958
##############
Trial  38
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	<	
reward
-5.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-5.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
features
1 	0 	0 	0 	1 	
1 	0 	0 	0 	0 	
1 	1 	2 	0 	1 	
0 	0 	1 	1 	0 	
1 	1 	0 	0 	1 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 0), (6, 1), (2, 3), (13, 0), (4, 0)]
w_map [ 0.0370559  -0.40024939  0.63048574  0.66401432] loglik -3.465734386400527
accepted/total = 1665/2000 = 0.8325
MAP Policy on Train MDP
map_weights [ 0.0370559  -0.40024939  0.63048574  0.66401432]
map reward
-0.40	0.04	0.04	0.04	-0.40	
-0.40	0.04	0.04	0.04	0.04	
-0.40	-0.40	0.63	0.04	-0.40	
0.04	0.04	-0.40	-0.40	0.04	
-0.40	-0.40	0.04	0.04	-0.40	
Map policy
>	>	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.20619897 -0.67468476  0.22575378  0.09555719]
mean reward
-0.67	-0.21	-0.21	-0.21	-0.67	
-0.67	-0.21	-0.21	-0.21	-0.21	
-0.67	-0.67	0.23	-0.21	-0.67	
-0.21	-0.21	-0.67	-0.67	-0.21	
-0.67	-0.67	-0.21	-0.21	-0.67	
mean policy
>	>	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	^	
^	^	^	<	<	
Optimal Policy
>	>	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	<	
MAP policy loss 1.0159146714393275e-06
Mean policy loss -1.5121638143045235e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	^	
^	>	^	<	^	
^	v	^	^	^	
>	v	>	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	1.00	-5.00	-1.00	
-100.00	-100.00	-100.00	-1.00	-1.00	
-100.00	-1.00	-100.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	2 	1 	0 	
3 	3 	3 	0 	0 	
3 	0 	3 	0 	0 	
MAP on testing env
map_weights [ 0.0370559  -0.40024939  0.63048574  0.66401432]
map reward
-0.40	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	-0.40	0.04	
0.04	0.66	0.63	-0.40	0.04	
0.66	0.66	0.66	0.04	0.04	
0.66	0.04	0.66	0.04	0.04	
Map policy
v	v	v	<	<	
v	v	v	<	v	
v	v	<	<	<	
v	<	<	<	<	
<	<	^	<	<	
MEAN policy on test env
mean_weights [-0.20619897 -0.67468476  0.22575378  0.09555719]
mean reward
-0.67	-0.21	-0.21	-0.21	-0.21	
-0.21	-0.21	-0.21	-0.67	-0.21	
-0.21	0.10	0.23	-0.67	-0.21	
0.10	0.10	0.10	-0.21	-0.21	
0.10	-0.21	0.10	-0.21	-0.21	
mean policy
v	v	v	<	<	
v	v	v	<	v	
>	>	<	<	v	
>	>	^	<	<	
^	^	^	<	<	
features
1 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	2 	1 	0 	
3 	3 	3 	0 	0 	
3 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
v	v	v	<	v	
>	>	^	<	<	
>	^	^	<	<	
^	^	^	<	<	
------ Regret Solution ---------
expert u_sa [10.58840254  1.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	<	^	
^	>	^	<	<	
^	^	^	^	^	
^	>	>	^	^	
-------- IRD Solution -------
ird reward
-11.92	-11.21	-11.21	-11.21	-11.21	
-11.21	-11.21	-11.21	-11.92	-11.21	
-11.21	-11.66	-10.82	-11.92	-11.21	
-11.66	-11.66	-11.66	-11.21	-11.21	
-11.66	-11.21	-11.66	-11.21	-11.21	
ird policy
>	>	v	<	<	
>	>	v	<	^	
^	>	^	<	<	
^	^	^	<	<	
^	^	^	^	^	
MAP policy loss 66.03392479197042
mean policy loss 923.7049067381975
robust policy loss 602.5840357435922
regret policy loss 9.520603278215054
ird policy loss 30.13415186820302
MAP lava occupancy 0.7550899558211863
Mean lava occupancy 0.7550899558211863
Robust lava occupancy 6.356231593142582
Regret lava occupancy 0.3544394633013731
IRD lava occupancy 0.5725950001476651
##############
Trial  39
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	<	<	<	<	
v	v	v	v	v	
>	>	>	<	<	
^	^	^	^	<	
^	>	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-5.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	1 	1 	1 	
0 	0 	2 	0 	1 	
0 	1 	1 	0 	0 	
1 	0 	0 	0 	0 	
demonstration
[(12, 1), (12, 0), (3, 0), (1, 0), (4, 0), (2, 0), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [ 0.20790822 -0.70904331  0.62245241 -0.25804023] loglik -1.386296653833142
accepted/total = 1083/2000 = 0.5415
MAP Policy on Train MDP
map_weights [ 0.20790822 -0.70904331  0.62245241 -0.25804023]
map reward
0.21	0.21	0.21	0.21	0.21	
0.21	-0.71	-0.71	-0.71	-0.71	
0.21	0.21	0.62	0.21	-0.71	
0.21	-0.71	-0.71	0.21	0.21	
-0.71	0.21	0.21	0.21	0.21	
Map policy
v	<	<	<	<	
v	v	v	v	v	
>	>	>	<	<	
^	^	^	^	<	
^	>	>	^	^	
MEAN policy on Train MDP
mean_weights [ 0.07667631 -0.59648563  0.26965216 -0.34771197]
mean reward
0.08	0.08	0.08	0.08	0.08	
0.08	-0.60	-0.60	-0.60	-0.60	
0.08	0.08	0.27	0.08	-0.60	
0.08	-0.60	-0.60	0.08	0.08	
-0.60	0.08	0.08	0.08	0.08	
mean policy
v	<	<	<	<	
v	v	v	v	^	
>	>	>	<	<	
^	^	^	^	<	
^	>	>	^	^	
Optimal Policy
v	<	<	<	<	
v	v	v	v	v	
>	>	>	<	<	
^	^	^	^	<	
^	>	>	^	^	
MAP policy loss 1.120297387843805e-08
Mean policy loss 0.03435964515500456
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	<	<	<	
v	v	v	^	^	
>	>	v	<	v	
>	>	^	<	v	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-100.00	-5.00	-1.00	
-1.00	-5.00	1.00	-100.00	-100.00	
-100.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	3 	3 	1 	0 	
0 	1 	2 	3 	3 	
3 	0 	0 	3 	0 	
3 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.20790822 -0.70904331  0.62245241 -0.25804023]
map reward
0.21	0.21	0.21	0.21	0.21	
0.21	-0.26	-0.26	-0.71	0.21	
0.21	-0.71	0.62	-0.26	-0.26	
-0.26	0.21	0.21	-0.26	0.21	
-0.26	0.21	0.21	0.21	0.21	
Map policy
>	>	v	<	<	
^	>	v	v	^	
v	>	v	<	<	
>	>	^	<	v	
>	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.07667631 -0.59648563  0.26965216 -0.34771197]
mean reward
0.08	0.08	0.08	0.08	0.08	
0.08	-0.35	-0.35	-0.60	0.08	
0.08	-0.60	0.27	-0.35	-0.35	
-0.35	0.08	0.08	-0.35	0.08	
-0.35	0.08	0.08	0.08	0.08	
mean policy
>	>	v	<	<	
^	>	v	v	^	
v	>	v	<	v	
>	>	^	<	v	
>	^	^	<	<	
features
0 	0 	0 	0 	0 	
0 	3 	3 	1 	0 	
0 	1 	2 	3 	3 	
3 	0 	0 	3 	0 	
3 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
------ Regret Solution ---------
expert u_sa [12.36320161  0.          7.51838781  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	<	<	<	<	
v	v	v	^	^	
>	>	v	<	v	
>	>	^	<	v	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-13.12	-13.12	-13.12	-13.12	-13.12	
-13.12	-13.88	-13.88	-13.80	-13.12	
-13.12	-13.80	-13.06	-13.88	-13.88	
-13.88	-13.12	-13.12	-13.88	-13.12	
-13.88	-13.12	-13.12	-13.12	-13.12	
ird policy
<	<	^	<	<	
^	^	v	^	^	
^	>	v	<	v	
>	>	^	<	v	
>	^	^	<	<	
MAP policy loss 487.75939431779443
mean policy loss 33.51660633044978
robust policy loss 87.43506911825428
regret policy loss 1.8647449621269545
ird policy loss 4.982752393368238
MAP lava occupancy 5.040997364103043
Mean lava occupancy 5.040997364103043
Robust lava occupancy 1.186047385820858
Regret lava occupancy 0.2915998082441701
IRD lava occupancy 0.2800000068452362
##############
Trial  40
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	^	
^	^	^	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-5.00	
features
0 	1 	0 	1 	0 	
1 	0 	1 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	1 	1 	1 	1 	
demonstration
[(9, 0), (8, 3), (12, 1), (12, 0), (11, 1), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [ 0.23346366  0.03380438  0.96891016 -0.0745995 ] loglik -3.4657301045272106
accepted/total = 1640/2000 = 0.82
MAP Policy on Train MDP
map_weights [ 0.23346366  0.03380438  0.96891016 -0.0745995 ]
map reward
0.23	0.03	0.23	0.03	0.23	
0.03	0.23	0.03	0.23	0.23	
0.23	0.23	0.97	0.23	0.23	
0.23	0.23	0.03	0.23	0.23	
0.23	0.03	0.03	0.03	0.03	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.09525624 -0.50689612  0.38595542  0.02022566]
mean reward
-0.10	-0.51	-0.10	-0.51	-0.10	
-0.51	-0.10	-0.51	-0.10	-0.10	
-0.10	-0.10	0.39	-0.10	-0.10	
-0.10	-0.10	-0.51	-0.10	-0.10	
-0.10	-0.51	-0.51	-0.51	-0.51	
mean policy
v	v	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	^	
Optimal Policy
v	v	v	v	v	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 1.3180744264876948e-06
Mean policy loss 2.5496780676776548e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	<	<	
^	>	^	<	^	
>	>	^	<	<	
^	>	^	<	<	
reward
-100.00	-1.00	-100.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
3 	0 	3 	0 	1 	
0 	0 	0 	0 	1 	
0 	3 	2 	3 	0 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [ 0.23346366  0.03380438  0.96891016 -0.0745995 ]
map reward
-0.07	0.23	-0.07	0.23	0.03	
0.23	0.23	0.23	0.23	0.03	
0.23	-0.07	0.97	-0.07	0.23	
0.23	0.23	0.23	0.23	-0.07	
0.23	-0.07	0.23	0.23	0.23	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
^	>	^	^	<	
MEAN policy on test env
mean_weights [-0.09525624 -0.50689612  0.38595542  0.02022566]
mean reward
0.02	-0.10	0.02	-0.10	-0.51	
-0.10	-0.10	-0.10	-0.10	-0.51	
-0.10	0.02	0.39	0.02	-0.10	
-0.10	-0.10	-0.10	-0.10	0.02	
-0.10	0.02	-0.10	-0.10	-0.10	
mean policy
>	>	v	<	<	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	^	
>	^	^	^	^	
features
3 	0 	3 	0 	1 	
0 	0 	0 	0 	1 	
0 	3 	2 	3 	0 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	^	<	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	>	v	<	<	
^	>	^	<	^	
>	>	^	<	<	
^	>	^	<	<	
-------- IRD Solution -------
ird reward
-13.16	-12.46	-13.16	-12.46	-13.20	
-12.46	-12.46	-12.46	-12.46	-13.20	
-12.46	-13.16	-12.30	-13.16	-12.46	
-12.46	-12.46	-12.46	-12.46	-13.16	
-12.46	-13.16	-12.46	-12.46	-12.46	
ird policy
>	v	v	v	<	
>	>	v	<	<	
v	>	^	<	<	
>	>	^	<	<	
^	>	^	<	<	
MAP policy loss 922.5102167547548
mean policy loss 926.8538053695283
robust policy loss 306.45462903129294
regret policy loss 2.801425135062763e-07
ird policy loss 3.537800368235968
MAP lava occupancy 9.559743599856224
Mean lava occupancy 9.559743599856224
Robust lava occupancy 3.3384952427456036
Regret lava occupancy 0.2400000026413637
IRD lava occupancy 0.2780000033610034
##############
Trial  41
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	<	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-5.00	
-5.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	1 	0 	0 	1 	
1 	0 	2 	1 	0 	
0 	0 	0 	0 	1 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (1, 1), (12, 3), (4, 0)]
w_map [ 0.01560905 -0.25387379  0.95380814  0.15985773] loglik -3.9889828806501555
accepted/total = 1692/2000 = 0.846
MAP Policy on Train MDP
map_weights [ 0.01560905 -0.25387379  0.95380814  0.15985773]
map reward
-0.25	0.02	0.02	0.02	0.02	
0.02	-0.25	0.02	0.02	-0.25	
-0.25	0.02	0.95	-0.25	0.02	
0.02	0.02	0.02	0.02	-0.25	
-0.25	0.02	0.02	0.02	0.02	
Map policy
>	>	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.05623693 -0.38410706  0.57577407 -0.03221448]
mean reward
-0.38	0.06	0.06	0.06	0.06	
0.06	-0.38	0.06	0.06	-0.38	
-0.38	0.06	0.58	-0.38	0.06	
0.06	0.06	0.06	0.06	-0.38	
-0.38	0.06	0.06	0.06	0.06	
mean policy
>	>	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	<	
Optimal Policy
>	>	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	<	
MAP policy loss 1.2269604289061146e-06
Mean policy loss 4.637324999001713e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	<	^	
>	^	^	<	<	
reward
-5.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-100.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	1 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.01560905 -0.25387379  0.95380814  0.15985773]
map reward
-0.25	-0.25	0.16	0.02	0.02	
0.02	0.02	0.16	0.16	0.02	
0.16	0.02	0.95	0.02	0.02	
0.02	0.02	0.02	-0.25	0.02	
0.02	0.02	0.02	0.02	0.02	
Map policy
v	>	v	<	<	
v	>	v	<	<	
>	>	^	<	<	
^	>	^	<	^	
^	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.05623693 -0.38410706  0.57577407 -0.03221448]
mean reward
-0.38	-0.38	-0.03	0.06	0.06	
0.06	0.06	-0.03	-0.03	0.06	
-0.03	0.06	0.58	0.06	0.06	
0.06	0.06	0.06	-0.38	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
v	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	<	^	
>	^	^	<	<	
features
1 	1 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	v	v	v	v	
>	>	^	<	<	
>	>	^	<	^	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	>	>	v	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-13.49	-13.49	-13.46	-12.78	-12.78	
-12.78	-12.78	-13.46	-13.46	-12.78	
-13.46	-12.78	-12.65	-12.78	-12.78	
-12.78	-12.78	-12.78	-13.49	-12.78	
-12.78	-12.78	-12.78	-12.78	-12.78	
ird policy
v	v	>	>	v	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	<	^	
>	^	^	<	<	
MAP policy loss 328.4202916344293
mean policy loss 7.3180500069894805
robust policy loss 279.9479777000753
regret policy loss 7.360690997340602e-08
ird policy loss 1.5108981951846012e-09
MAP lava occupancy 3.4748548311238436
Mean lava occupancy 3.4748548311238436
Robust lava occupancy 2.9898376586484714
Regret lava occupancy 0.16000000052944366
IRD lava occupancy 0.16000000006285506
##############
Trial  42
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	<	v	<	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	^	<	
^	^	>	^	<	
reward
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-5.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
features
1 	0 	1 	0 	0 	
0 	0 	1 	1 	1 	
1 	0 	2 	0 	1 	
1 	0 	1 	0 	1 	
0 	1 	1 	0 	0 	
demonstration
[(8, 3), (12, 1), (3, 3), (12, 0), (6, 3), (11, 1), (5, 1), (0, 3), (13, 0), (4, 0)]
w_map [ 0.0295183  -0.29394217  0.91124697 -0.2869767 ] loglik -2.07944048625734
accepted/total = 1619/2000 = 0.8095
MAP Policy on Train MDP
map_weights [ 0.0295183  -0.29394217  0.91124697 -0.2869767 ]
map reward
-0.29	0.03	-0.29	0.03	0.03	
0.03	0.03	-0.29	-0.29	-0.29	
-0.29	0.03	0.91	0.03	-0.29	
-0.29	0.03	-0.29	0.03	-0.29	
0.03	-0.29	-0.29	0.03	0.03	
Map policy
v	v	v	v	<	
>	v	v	v	v	
>	>	>	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.03058241 -0.40388139  0.46163924 -0.08468297]
mean reward
-0.40	-0.03	-0.40	-0.03	-0.03	
-0.03	-0.03	-0.40	-0.40	-0.40	
-0.40	-0.03	0.46	-0.03	-0.40	
-0.40	-0.03	-0.40	-0.03	-0.40	
-0.03	-0.40	-0.40	-0.03	-0.03	
mean policy
v	v	v	v	<	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
v	v	<	v	<	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	^	<	
^	^	>	^	<	
MAP policy loss 0.159601057966249
Mean policy loss 0.1596000005288739
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	^	
>	>	<	<	v	
>	^	^	<	v	
>	^	^	<	>	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-100.00	-1.00	1.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	2 	3 	0 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
MAP on testing env
map_weights [ 0.0295183  -0.29394217  0.91124697 -0.2869767 ]
map reward
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	-0.29	-0.29	
-0.29	0.03	0.91	-0.29	0.03	
-0.29	0.03	0.03	-0.29	0.03	
0.03	-0.29	0.03	-0.29	0.03	
Map policy
>	v	v	<	<	
>	v	v	<	v	
>	>	v	<	<	
>	^	^	<	^	
>	^	^	<	^	
MEAN policy on test env
mean_weights [-0.03058241 -0.40388139  0.46163924 -0.08468297]
mean reward
-0.03	-0.03	-0.03	-0.03	-0.03	
-0.03	-0.03	-0.03	-0.40	-0.08	
-0.08	-0.03	0.46	-0.08	-0.03	
-0.40	-0.03	-0.03	-0.08	-0.03	
-0.03	-0.40	-0.03	-0.08	-0.03	
mean policy
>	v	v	<	<	
>	v	v	<	v	
>	>	<	<	<	
>	^	^	<	^	
>	^	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	2 	3 	0 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	<	
>	>	v	<	v	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [10.63715254  0.95125     8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
>	>	^	<	^	
>	^	^	<	^	
-------- IRD Solution -------
ird reward
-12.58	-12.58	-12.58	-12.58	-12.58	
-12.58	-12.58	-12.58	-13.09	-13.22	
-13.22	-12.58	-12.46	-13.22	-12.58	
-13.09	-12.58	-12.58	-13.22	-12.58	
-12.58	-13.09	-12.58	-13.22	-12.58	
ird policy
>	v	v	<	<	
>	v	v	<	^	
>	>	<	<	<	
>	^	^	<	^	
>	^	^	<	^	
MAP policy loss 473.2649515631725
mean policy loss 12.124102601719015
robust policy loss 354.8085688476594
regret policy loss 8.7022035381423
ird policy loss 8.618792874307747
MAP lava occupancy 4.998771743564348
Mean lava occupancy 4.998771743564348
Robust lava occupancy 3.805954255106302
Regret lava occupancy 0.30839501160473815
IRD lava occupancy 0.3083950059856191
##############
Trial  43
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	<	<	v	
v	v	v	v	v	
>	>	v	<	v	
>	^	^	<	<	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
1 	0 	1 	0 	1 	
0 	0 	2 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (1, 3), (12, 0), (3, 0), (6, 3), (17, 2), (2, 0), (11, 1), (12, 3), (4, 0)]
w_map [-0.02120109 -0.79764623  0.58286482 -0.15355654] loglik -2.0794412499025157
accepted/total = 1340/2000 = 0.67
MAP Policy on Train MDP
map_weights [-0.02120109 -0.79764623  0.58286482 -0.15355654]
map reward
-0.02	-0.02	-0.02	-0.80	-0.02	
-0.80	-0.02	-0.80	-0.02	-0.80	
-0.02	-0.02	0.58	-0.80	-0.02	
-0.80	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
Map policy
>	v	<	<	v	
v	v	v	v	v	
>	>	<	<	v	
>	>	^	<	<	
>	^	^	<	<	
MEAN policy on Train MDP
mean_weights [ 0.20972507 -0.40021808  0.51283609  0.35920375]
mean reward
0.21	0.21	0.21	-0.40	0.21	
-0.40	0.21	-0.40	0.21	-0.40	
0.21	0.21	0.51	-0.40	0.21	
-0.40	0.21	0.21	0.21	0.21	
0.21	0.21	0.21	0.21	0.21	
mean policy
>	v	<	<	v	
v	v	v	v	v	
>	>	<	<	v	
>	>	^	<	<	
>	^	^	<	<	
Optimal Policy
>	v	<	<	v	
v	v	v	v	v	
>	>	v	<	v	
>	^	^	<	<	
>	^	^	<	<	
MAP policy loss 3.890077921245294e-08
Mean policy loss 6.589322851247026e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	v	
^	^	^	<	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-5.00	-1.00	1.00	-100.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	1 	3 	
0 	0 	0 	0 	3 	
1 	0 	2 	3 	0 	
0 	1 	1 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.02120109 -0.79764623  0.58286482 -0.15355654]
map reward
-0.02	-0.02	-0.02	-0.80	-0.15	
-0.02	-0.02	-0.02	-0.02	-0.15	
-0.80	-0.02	0.58	-0.15	-0.02	
-0.02	-0.80	-0.80	-0.02	-0.02	
-0.02	-0.02	-0.15	-0.02	-0.02	
Map policy
>	v	v	<	v	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	^	^	
MEAN policy on test env
mean_weights [ 0.20972507 -0.40021808  0.51283609  0.35920375]
mean reward
0.21	0.21	0.21	-0.40	0.36	
0.21	0.21	0.21	0.21	0.36	
-0.40	0.21	0.51	0.36	0.21	
0.21	-0.40	-0.40	0.21	0.21	
0.21	0.21	0.36	0.21	0.21	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
v	^	^	^	^	
>	>	>	^	^	
features
0 	0 	0 	1 	3 	
0 	0 	0 	0 	3 	
1 	0 	2 	3 	0 	
0 	1 	1 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
v	^	^	^	^	
>	>	>	^	^	
------ Regret Solution ---------
expert u_sa [11.52065566  0.475       7.88593375  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	v	
^	^	^	<	<	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-12.88	-12.88	-12.88	-13.51	-13.51	
-12.88	-12.88	-12.88	-12.88	-13.51	
-13.51	-12.88	-12.80	-13.51	-12.88	
-12.88	-13.51	-13.51	-12.88	-12.88	
-12.88	-12.88	-13.51	-12.88	-12.88	
ird policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
^	^	^	^	^	
MAP policy loss 28.10755056892548
mean policy loss 896.5849489041331
robust policy loss 592.2559856949936
regret policy loss -1.2486992428256682e-07
ird policy loss 3.552621652175231
MAP lava occupancy 0.26830439428693814
Mean lava occupancy 0.26830439428693814
Robust lava occupancy 6.1545384054022
Regret lava occupancy 0.16000000009656593
IRD lava occupancy 0.1980000179755185
##############
Trial  44
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	<	<	
^	v	v	v	v	
>	>	^	<	<	
>	>	^	<	^	
>	^	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-5.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	1 	1 	
1 	0 	2 	0 	0 	
1 	0 	0 	1 	0 	
1 	0 	0 	1 	1 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [ 0.07333741 -0.37181728  0.89753872 -0.22538361] loglik -5.54517744447935
accepted/total = 1714/2000 = 0.857
MAP Policy on Train MDP
map_weights [ 0.07333741 -0.37181728  0.89753872 -0.22538361]
map reward
0.07	0.07	0.07	0.07	0.07	
0.07	-0.37	0.07	-0.37	-0.37	
-0.37	0.07	0.90	0.07	0.07	
-0.37	0.07	0.07	-0.37	0.07	
-0.37	0.07	0.07	-0.37	-0.37	
Map policy
>	>	v	<	<	
>	v	v	<	v	
>	>	<	<	<	
>	>	^	<	^	
>	^	^	<	^	
MEAN policy on Train MDP
mean_weights [-0.11746882 -0.58805516  0.36311107  0.11133191]
mean reward
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.59	-0.12	-0.59	-0.59	
-0.59	-0.12	0.36	-0.12	-0.12	
-0.59	-0.12	-0.12	-0.59	-0.12	
-0.59	-0.12	-0.12	-0.59	-0.59	
mean policy
>	>	v	<	<	
^	>	v	<	v	
>	>	<	<	<	
>	>	^	<	^	
>	^	^	<	^	
Optimal Policy
>	>	v	<	<	
^	v	v	v	v	
>	>	^	<	<	
>	>	^	<	^	
>	^	^	<	^	
MAP policy loss 0.08341010412701244
Mean policy loss -2.3846044930331267e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	>	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.07333741 -0.37181728  0.89753872 -0.22538361]
map reward
0.07	0.07	0.07	-0.23	0.07	
0.07	0.07	0.07	0.07	-0.23	
0.07	0.07	0.90	0.07	0.07	
-0.37	0.07	0.07	0.07	0.07	
-0.23	0.07	0.07	0.07	0.07	
Map policy
v	v	v	v	<	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [-0.11746882 -0.58805516  0.36311107  0.11133191]
mean reward
-0.12	-0.12	-0.12	0.11	-0.12	
-0.12	-0.12	-0.12	-0.12	0.11	
-0.12	-0.12	0.36	-0.12	-0.12	
-0.59	-0.12	-0.12	-0.12	-0.12	
0.11	-0.12	-0.12	-0.12	-0.12	
mean policy
>	v	v	^	<	
>	>	v	<	>	
>	>	>	<	<	
v	^	^	^	<	
<	<	^	^	^	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	^	<	
>	v	v	<	>	
>	>	<	<	<	
v	^	^	<	<	
<	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-12.68	-12.68	-12.68	-13.20	-12.68	
-12.68	-12.68	-12.68	-12.68	-13.20	
-12.68	-12.68	-12.53	-12.68	-12.68	
-13.51	-12.68	-12.68	-12.68	-12.68	
-13.20	-12.68	-12.68	-12.68	-12.68	
ird policy
v	v	v	v	<	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	<	
MAP policy loss 3.0936877750787204
mean policy loss 454.9222612801488
robust policy loss 189.0966844188705
regret policy loss 3.0936871772030643
ird policy loss 3.0936872489765643
MAP lava occupancy 0.15800000547569376
Mean lava occupancy 0.15800000547569376
Robust lava occupancy 2.0197661061287926
Regret lava occupancy 0.15800000000420206
IRD lava occupancy 0.15800000061284725
##############
Trial  45
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
>	^	^	v	<	
^	^	<	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
1 	0 	2 	1 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 0), (9, 3), (11, 1), (2, 3), (14, 0), (4, 3), (1, 1), (13, 0)]
w_map [ 0.16635525 -0.80546461  0.56834158 -0.02324973] loglik -0.7173210792916507
accepted/total = 1505/2000 = 0.7525
MAP Policy on Train MDP
map_weights [ 0.16635525 -0.80546461  0.56834158 -0.02324973]
map reward
0.17	0.17	0.17	-0.81	0.17	
0.17	-0.81	-0.81	0.17	0.17	
-0.81	0.17	0.57	-0.81	0.17	
0.17	0.17	-0.81	0.17	0.17	
0.17	0.17	0.17	0.17	0.17	
Map policy
>	>	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
>	^	^	v	<	
^	^	<	<	<	
MEAN policy on Train MDP
mean_weights [-0.08031261 -0.49484509  0.42539386 -0.00722322]
mean reward
-0.08	-0.08	-0.08	-0.49	-0.08	
-0.08	-0.49	-0.49	-0.08	-0.08	
-0.49	-0.08	0.43	-0.49	-0.08	
-0.08	-0.08	-0.49	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
mean policy
>	>	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
Optimal Policy
>	>	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
>	^	^	v	<	
^	^	<	<	<	
MAP policy loss 2.9166372534673857e-07
Mean policy loss 0.2561765757736475
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-5.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	1 	
MAP on testing env
map_weights [ 0.16635525 -0.80546461  0.56834158 -0.02324973]
map reward
0.17	0.17	0.17	0.17	-0.81	
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	0.57	0.17	-0.81	
0.17	0.17	0.17	0.17	0.17	
0.17	-0.81	0.17	-0.81	-0.81	
Map policy
>	v	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.08031261 -0.49484509  0.42539386 -0.00722322]
mean reward
-0.08	-0.08	-0.08	-0.08	-0.49	
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.08	0.43	-0.08	-0.49	
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.49	-0.08	-0.49	-0.49	
mean policy
>	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	^	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
^	>	^	^	^	
------ Regret Solution ---------
expert u_sa [10.73102754  0.857375    8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
^	^	^	<	^	
-------- IRD Solution -------
ird reward
-12.31	-12.31	-12.31	-12.31	-12.86	
-12.31	-12.31	-12.31	-12.31	-12.31	
-12.31	-12.31	-12.09	-12.31	-12.86	
-12.31	-12.31	-12.31	-12.31	-12.31	
-12.31	-12.86	-12.31	-12.86	-12.86	
ird policy
v	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MAP policy loss 1.894516447754498
mean policy loss 1.3699174121473146e-09
robust policy loss 2.6514298302054984e-07
regret policy loss 5.373896119764687e-10
ird policy loss 1.3689302304220607e-07
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  46
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	<	<	<	
v	v	v	v	^	
>	>	>	<	<	
>	>	^	^	v	
^	^	^	<	<	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (1, 3), (12, 1), (12, 0), (3, 0), (6, 3), (17, 2), (2, 0), (11, 1), (13, 0), (12, 3), (4, 0)]
w_map [ 0.22623537 -0.63020619  0.69678431  0.25719515] loglik -3.9889814132811807
accepted/total = 1459/2000 = 0.7295
MAP Policy on Train MDP
map_weights [ 0.22623537 -0.63020619  0.69678431  0.25719515]
map reward
-0.63	0.23	0.23	0.23	0.23	
0.23	0.23	-0.63	-0.63	0.23	
0.23	0.23	0.70	0.23	-0.63	
0.23	0.23	0.23	-0.63	0.23	
0.23	0.23	0.23	0.23	0.23	
Map policy
v	v	<	<	<	
v	v	v	v	^	
>	>	v	<	<	
>	^	^	<	v	
^	^	^	<	<	
MEAN policy on Train MDP
mean_weights [ 0.01855776 -0.62756707  0.29977304  0.09533747]
mean reward
-0.63	0.02	0.02	0.02	0.02	
0.02	0.02	-0.63	-0.63	0.02	
0.02	0.02	0.30	0.02	-0.63	
0.02	0.02	0.02	-0.63	0.02	
0.02	0.02	0.02	0.02	0.02	
mean policy
v	v	<	<	<	
v	v	v	v	^	
>	>	v	<	<	
>	^	^	<	v	
^	^	^	<	<	
Optimal Policy
v	v	<	<	<	
v	v	v	v	^	
>	>	>	<	<	
>	>	^	^	v	
^	^	^	<	<	
MAP policy loss 8.559397306606015e-07
Mean policy loss 2.120118483550648e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	<	
>	>	v	v	<	
v	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
features
0 	0 	0 	0 	1 	
3 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
MAP on testing env
map_weights [ 0.22623537 -0.63020619  0.69678431  0.25719515]
map reward
0.23	0.23	0.23	0.23	-0.63	
0.26	0.23	0.23	0.23	0.23	
0.23	-0.63	0.70	0.23	0.23	
0.23	0.23	0.23	0.23	0.23	
0.23	0.23	0.23	-0.63	0.26	
Map policy
v	>	v	v	<	
>	>	v	<	<	
^	>	v	<	<	
>	>	^	^	<	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.01855776 -0.62756707  0.29977304  0.09533747]
mean reward
0.02	0.02	0.02	0.02	-0.63	
0.10	0.02	0.02	0.02	0.02	
0.02	-0.63	0.30	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	-0.63	0.10	
mean policy
v	>	v	v	<	
>	>	v	<	<	
^	>	v	<	<	
>	>	^	^	<	
>	>	^	^	^	
features
0 	0 	0 	0 	1 	
3 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	v	
<	>	v	<	<	
^	>	v	<	<	
>	>	^	<	v	
>	>	^	>	>	
------ Regret Solution ---------
expert u_sa [11.49565566  0.5         7.88593375  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	>	v	<	<	
v	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-12.80	-12.80	-12.80	-12.80	-13.57	
-13.49	-12.80	-12.80	-12.80	-12.80	
-12.80	-13.57	-12.74	-12.80	-12.80	
-12.80	-12.80	-12.80	-12.80	-12.80	
-12.80	-12.80	-12.80	-13.57	-13.49	
ird policy
>	v	v	v	v	
>	>	v	<	<	
v	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
MAP policy loss 19.546476280156263
mean policy loss 7.524000024143026
robust policy loss 304.56378629231597
regret policy loss 0.7651316184956078
ird policy loss -5.550124346220819e-09
MAP lava occupancy 0.14131234013473476
Mean lava occupancy 0.14131234013473476
Robust lava occupancy 3.1289602693947787
Regret lava occupancy 0.08772860232150462
IRD lava occupancy 0.08000000004863454
##############
Trial  47
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
v	^	^	^	<	
>	>	>	^	<	
reward
-1.00	-5.00	-5.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	1 	1 	1 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	1 	
0 	1 	1 	0 	1 	
0 	0 	0 	0 	1 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (12, 0), (8, 0), (6, 3), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.48701936 -0.82687582  0.27733741  0.04660997] loglik -4.682131227124557
accepted/total = 1655/2000 = 0.8275
MAP Policy on Train MDP
map_weights [-0.48701936 -0.82687582  0.27733741  0.04660997]
map reward
-0.49	-0.83	-0.83	-0.83	-0.49	
-0.49	-0.49	-0.49	-0.49	-0.49	
-0.83	-0.49	0.28	-0.49	-0.83	
-0.49	-0.83	-0.83	-0.49	-0.83	
-0.49	-0.49	-0.49	-0.49	-0.83	
Map policy
v	v	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
>	>	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.00729157 -0.37858101  0.55319234  0.02859583]
mean reward
0.01	-0.38	-0.38	-0.38	0.01	
0.01	0.01	0.01	0.01	0.01	
-0.38	0.01	0.55	0.01	-0.38	
0.01	-0.38	-0.38	0.01	-0.38	
0.01	0.01	0.01	0.01	-0.38	
mean policy
v	v	v	v	v	
>	>	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
>	>	^	^	<	
Optimal Policy
v	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
v	^	^	^	<	
>	>	>	^	<	
MAP policy loss 0.24913702500797996
Mean policy loss 0.24913704545000603
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	v	
>	v	v	<	v	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	>	^	
reward
-5.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	1 	3 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [-0.48701936 -0.82687582  0.27733741  0.04660997]
map reward
-0.83	-0.49	-0.49	-0.83	0.05	
-0.49	-0.49	-0.49	-0.83	-0.49	
-0.49	-0.49	0.28	-0.49	-0.49	
-0.49	-0.49	0.05	0.05	-0.49	
-0.49	-0.83	-0.49	-0.49	-0.49	
Map policy
v	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	^	<	
MEAN policy on test env
mean_weights [ 0.00729157 -0.37858101  0.55319234  0.02859583]
mean reward
-0.38	0.01	0.01	-0.38	0.03	
0.01	0.01	0.01	-0.38	0.01	
0.01	0.01	0.55	0.01	0.01	
0.01	0.01	0.03	0.03	0.01	
0.01	-0.38	0.01	0.01	0.01	
mean policy
v	v	v	<	v	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
features
1 	0 	0 	1 	3 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	^	
^	^	^	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	>	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
^	^	>	>	^	
-------- IRD Solution -------
ird reward
-13.78	-13.13	-13.13	-13.78	-13.78	
-13.13	-13.13	-13.13	-13.78	-13.13	
-13.13	-13.13	-13.01	-13.13	-13.13	
-13.13	-13.13	-13.78	-13.78	-13.13	
-13.13	-13.78	-13.13	-13.13	-13.13	
ird policy
v	v	v	<	v	
>	v	v	<	v	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	>	^	
MAP policy loss 22.223454499926888
mean policy loss 897.4125547923297
robust policy loss 239.73280870973872
regret policy loss 5.183954838935967e-07
ird policy loss 1.04034300271727e-09
MAP lava occupancy 0.34049499999935995
Mean lava occupancy 0.34049499999935995
Robust lava occupancy 2.543623829793494
Regret lava occupancy 0.12000000399800709
IRD lava occupancy 0.12000000000852284
##############
Trial  48
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
^	v	v	v	<	
>	>	^	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-5.00	-5.00	
features
0 	0 	0 	0 	1 	
0 	1 	0 	0 	0 	
1 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
1 	0 	1 	1 	1 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (8, 0), (17, 2), (11, 1), (2, 3), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.36447565 -0.77070191  0.46337038 -0.24179321] loglik -6.93147180560797
accepted/total = 1699/2000 = 0.8495
MAP Policy on Train MDP
map_weights [-0.36447565 -0.77070191  0.46337038 -0.24179321]
map reward
-0.36	-0.36	-0.36	-0.36	-0.77	
-0.36	-0.77	-0.36	-0.36	-0.36	
-0.77	-0.36	0.46	-0.36	-0.77	
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.77	-0.36	-0.77	-0.77	-0.77	
Map policy
>	>	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.01547657 -0.40775849  0.52421065 -0.00189601]
mean reward
0.02	0.02	0.02	0.02	-0.41	
0.02	-0.41	0.02	0.02	0.02	
-0.41	0.02	0.52	0.02	-0.41	
0.02	0.02	0.02	0.02	0.02	
-0.41	0.02	-0.41	-0.41	-0.41	
mean policy
>	>	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	^	
Optimal Policy
>	>	v	v	<	
^	v	v	v	<	
>	>	^	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 0.08340999937975346
Mean policy loss 0.08341000522965437
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	>	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	^	^	^	<	
reward
-1.00	-5.00	-5.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	1 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	2 	0 	1 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.36447565 -0.77070191  0.46337038 -0.24179321]
map reward
-0.36	-0.77	-0.77	-0.24	-0.36	
-0.77	-0.36	-0.36	-0.36	-0.24	
-0.36	-0.36	0.46	-0.36	-0.77	
-0.36	-0.36	-0.36	-0.77	-0.77	
-0.36	-0.36	-0.77	-0.36	-0.36	
Map policy
v	v	v	v	v	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
^	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.01547657 -0.40775849  0.52421065 -0.00189601]
mean reward
0.02	-0.41	-0.41	-0.00	0.02	
-0.41	0.02	0.02	0.02	-0.00	
0.02	0.02	0.52	0.02	-0.41	
0.02	0.02	0.02	-0.41	-0.41	
0.02	0.02	-0.41	0.02	0.02	
mean policy
v	v	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	<	
features
0 	1 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	2 	0 	1 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	>	^	<	
>	v	v	^	>	
>	>	>	<	^	
>	^	^	<	^	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-12.90	-13.46	-13.46	-13.57	-12.90	
-13.46	-12.90	-12.90	-12.90	-13.57	
-12.90	-12.90	-12.84	-12.90	-13.46	
-12.90	-12.90	-12.90	-13.46	-13.46	
-12.90	-12.90	-13.46	-12.90	-12.90	
ird policy
v	v	v	v	>	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
^	^	^	^	<	
MAP policy loss 3.632431311166521
mean policy loss 3.0936871878312324
robust policy loss 508.031715198567
regret policy loss 3.0936871794312903
ird policy loss 2.7890495114812897e-08
MAP lava occupancy 0.11800000000009864
Mean lava occupancy 0.11800000000009864
Robust lava occupancy 5.169767861651179
Regret lava occupancy 0.11799999999934804
IRD lava occupancy 0.080000000050354
##############
Trial  49
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	<	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	>	^	^	<	
reward
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	1 	
0 	1 	0 	0 	1 	
0 	0 	2 	1 	0 	
1 	1 	1 	0 	1 	
0 	1 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (3, 3), (12, 0), (8, 0), (4, 0), (11, 1), (10, 1), (0, 3), (5, 3)]
w_map [-0.15950983 -0.716006    0.67705634 -0.0590485 ] loglik -1.3862943611197807
accepted/total = 1653/2000 = 0.8265
MAP Policy on Train MDP
map_weights [-0.15950983 -0.716006    0.67705634 -0.0590485 ]
map reward
-0.16	-0.16	-0.72	-0.16	-0.72	
-0.16	-0.72	-0.16	-0.16	-0.72	
-0.16	-0.16	0.68	-0.72	-0.16	
-0.72	-0.72	-0.72	-0.16	-0.72	
-0.16	-0.72	-0.16	-0.16	-0.16	
Map policy
v	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	>	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.14687278 -0.58078099  0.37408539  0.3149519 ]
mean reward
-0.15	-0.15	-0.58	-0.15	-0.58	
-0.15	-0.58	-0.15	-0.15	-0.58	
-0.15	-0.15	0.37	-0.58	-0.15	
-0.58	-0.58	-0.58	-0.15	-0.58	
-0.15	-0.58	-0.15	-0.15	-0.15	
mean policy
v	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	>	^	^	<	
Optimal Policy
v	<	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	>	^	^	<	
MAP policy loss 0.08341022635352374
Mean policy loss 0.08341000005527982
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
v	>	v	<	<	
>	>	v	<	v	
^	>	^	<	<	
>	^	^	^	<	
reward
-5.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
features
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.15950983 -0.716006    0.67705634 -0.0590485 ]
map reward
-0.72	-0.06	-0.06	-0.16	-0.16	
-0.16	-0.72	-0.16	-0.16	-0.16	
-0.16	-0.16	0.68	-0.72	-0.16	
-0.72	-0.72	-0.16	-0.16	-0.16	
-0.72	-0.16	-0.72	-0.16	-0.16	
Map policy
>	>	v	<	<	
v	>	v	<	<	
>	>	<	<	<	
^	>	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [-0.14687278 -0.58078099  0.37408539  0.3149519 ]
mean reward
-0.58	0.31	0.31	-0.15	-0.15	
-0.15	-0.58	-0.15	-0.15	-0.15	
-0.15	-0.15	0.37	-0.58	-0.15	
-0.58	-0.58	-0.15	-0.15	-0.15	
-0.58	-0.15	-0.58	-0.15	-0.15	
mean policy
>	>	<	<	<	
^	^	^	^	<	
>	>	^	<	<	
^	^	^	<	<	
>	^	^	^	^	
features
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	<	<	<	
v	^	v	^	<	
>	>	^	<	<	
^	>	^	<	<	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.08840254  0.5         8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	<	
v	>	v	<	<	
>	>	<	<	^	
^	>	^	<	<	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-13.31	-13.24	-13.24	-12.65	-12.65	
-12.65	-13.31	-12.65	-12.65	-12.65	
-12.65	-12.65	-12.44	-13.31	-12.65	
-13.31	-13.31	-12.65	-12.65	-12.65	
-13.31	-12.65	-13.31	-12.65	-12.65	
ird policy
v	>	v	v	<	
v	>	v	<	<	
>	>	v	<	^	
^	>	^	<	<	
>	^	^	^	<	
MAP policy loss 1.859053598661497
mean policy loss 1710.2550633020714
robust policy loss 870.9101476043369
regret policy loss 5.5652778850891185
ird policy loss 3.6099999997114764
MAP lava occupancy 0.09463309525607896
Mean lava occupancy 0.09463309525607896
Robust lava occupancy 8.791722929632206
Regret lava occupancy 0.1377502816672134
IRD lava occupancy 0.11800000000049557
