##############
Trial  0
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	v	^	>	^	
>	>	>	>	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	1 	0 	0 	2 	
0 	0 	1 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (13, 1), (8, 3), (7, 1), (14, 1), (2, 3), (1, 1)]
w_map [ 0.41946663 -2.7950776   4.30736965  1.64710092] loglik -2.079414773260396
accepted/total = 1927/2000 = 0.9635
MAP Policy on Train MDP
map_weights [ 0.41946663 -2.7950776   4.30736965  1.64710092]
map reward
-2.80	0.42	0.42	0.42	0.42	
0.42	-2.80	0.42	0.42	0.42	
0.42	-2.80	0.42	0.42	4.31	
0.42	0.42	-2.80	-2.80	0.42	
0.42	0.42	0.42	0.42	0.42	
Map policy
>	>	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 0.61905436 -2.72377482  3.51617305  1.99774581]
mean reward
-2.72	0.62	0.62	0.62	0.62	
0.62	-2.72	0.62	0.62	0.62	
0.62	-2.72	0.62	0.62	3.52	
0.62	0.62	-2.72	-2.72	0.62	
0.62	0.62	0.62	0.62	0.62	
mean policy
>	>	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	^	^	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	v	^	>	^	
>	>	>	>	^	
MAP policy loss 0.048628210040797426
Mean policy loss 0.04862726788941077
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	>	v	
v	v	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-5.00	-100.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
1 	3 	0 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	0 	3 	2 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.41946663 -2.7950776   4.30736965  1.64710092]
map reward
-2.80	1.65	0.42	-2.80	0.42	
0.42	-2.80	1.65	0.42	0.42	
0.42	0.42	0.42	1.65	4.31	
-2.80	0.42	-2.80	0.42	0.42	
0.42	0.42	0.42	-2.80	0.42	
Map policy
>	>	v	v	v	
v	>	>	v	v	
>	>	>	>	>	
^	^	>	^	^	
>	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.61905436 -2.72377482  3.51617305  1.99774581]
mean reward
-2.72	2.00	0.62	-2.72	0.62	
0.62	-2.72	2.00	0.62	0.62	
0.62	0.62	0.62	2.00	3.52	
-2.72	0.62	-2.72	0.62	0.62	
0.62	0.62	0.62	-2.72	0.62	
mean policy
>	>	v	v	v	
v	>	>	v	v	
>	>	>	>	>	
^	^	>	^	^	
>	^	^	^	^	
features
1 	3 	0 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	0 	3 	2 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	v	>	v	
v	v	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
>	^	<	>	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	^	>	v	
v	v	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
>	^	<	>	^	
-------- IRD Solution -------
ird reward
-92.51	-88.36	-86.88	-92.51	-86.88	
-86.88	-92.51	-88.36	-86.88	-86.88	
-86.88	-86.88	-86.88	-88.36	-84.60	
-92.51	-86.88	-92.51	-86.88	-86.88	
-86.88	-86.88	-86.88	-92.51	-86.88	
ird policy
v	>	v	>	v	
v	v	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
>	^	<	>	^	
MAP policy loss 46.667812443622104
mean policy loss 80.9534377245178
robust policy loss 41.238013404478494
regret policy loss 38.830617528637895
ird policy loss 41.238013205989844
MAP lava occupancy 0.6094208852682905
Mean lava occupancy 0.6094208852682905
Robust lava occupancy 0.5616794615324563
Regret lava occupancy 0.5244737494382038
IRD lava occupancy 0.5616794626272229
##############
Trial  1
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	1 	
demonstration
[(13, 1), (12, 1), (14, 1), (11, 1), (10, 1), (0, 3), (5, 3)]
w_map [ 0.7527498  -0.12896817  4.3028971   2.24945947] loglik -1.386292414997115
accepted/total = 1895/2000 = 0.9475
MAP Policy on Train MDP
map_weights [ 0.7527498  -0.12896817  4.3028971   2.24945947]
map reward
0.75	0.75	0.75	0.75	-0.13	
0.75	0.75	0.75	0.75	0.75	
0.75	0.75	0.75	0.75	4.30	
0.75	0.75	0.75	0.75	0.75	
0.75	0.75	-0.13	0.75	-0.13	
Map policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	>	^	^	
MEAN policy on Train MDP
mean_weights [ 2.00362732 -0.33775442  5.01655973  1.29546866]
mean reward
2.00	2.00	2.00	2.00	-0.34	
2.00	2.00	2.00	2.00	2.00	
2.00	2.00	2.00	2.00	5.02	
2.00	2.00	2.00	2.00	2.00	
2.00	2.00	-0.34	2.00	-0.34	
mean policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	>	^	^	
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	^	^	^	
MAP policy loss 3.434299770815885e-07
Mean policy loss 3.2469701372050963e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	v	v	
>	^	>	>	v	
^	>	>	>	>	
>	>	^	^	^	
^	^	>	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-100.00	-100.00	-5.00	-1.00	1.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-100.00	-100.00	-1.00	-100.00	
features
1 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	1 	0 	2 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	3 	
MAP on testing env
map_weights [ 0.7527498  -0.12896817  4.3028971   2.24945947]
map reward
-0.13	0.75	0.75	0.75	0.75	
0.75	0.75	2.25	0.75	0.75	
2.25	2.25	-0.13	0.75	4.30	
-0.13	0.75	0.75	-0.13	-0.13	
0.75	2.25	2.25	0.75	2.25	
Map policy
v	>	v	>	v	
v	>	>	>	v	
>	>	>	>	>	
^	^	>	^	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 2.00362732 -0.33775442  5.01655973  1.29546866]
mean reward
-0.34	2.00	2.00	2.00	2.00	
2.00	2.00	1.30	2.00	2.00	
1.30	1.30	-0.34	2.00	5.02	
-0.34	2.00	2.00	-0.34	-0.34	
2.00	1.30	1.30	2.00	1.30	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	^	>	^	^	
features
1 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	1 	0 	2 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	^	>	>	v	
^	>	>	>	>	
>	>	^	^	^	
^	^	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	^	>	v	v	
^	^	>	>	>	
>	^	<	^	^	
<	^	^	v	^	
-------- IRD Solution -------
ird reward
-139.85	-133.26	-133.26	-133.26	-133.26	
-133.26	-133.26	-139.24	-133.26	-133.26	
-139.24	-139.24	-139.85	-133.26	-131.12	
-139.85	-133.26	-133.26	-139.85	-139.85	
-133.26	-139.24	-139.24	-133.26	-139.24	
ird policy
>	>	>	v	v	
>	^	>	>	v	
^	>	>	>	>	
>	>	>	^	^	
>	^	>	^	^	
MAP policy loss 28.994956052499553
mean policy loss 14.36278988339171
robust policy loss 1.4244826035784974e-06
regret policy loss 20.145869181926795
ird policy loss 3.609999994935535
MAP lava occupancy 0.5292696599313932
Mean lava occupancy 0.5292696599313932
Robust lava occupancy 0.24000001207679142
Regret lava occupancy 0.4205949999987319
IRD lava occupancy 0.2780000000661037
##############
Trial  2
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	<	
>	>	>	v	v	
^	^	>	>	>	
>	>	>	^	^	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	1 	1 	0 	2 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
demonstration
[(13, 1), (8, 3), (7, 1), (6, 1), (14, 1), (5, 1), (0, 3)]
w_map [-0.44218995 -9.69918079  7.61494494 -3.20404423] loglik -0.6931408317541354
accepted/total = 1962/2000 = 0.981
MAP Policy on Train MDP
map_weights [-0.44218995 -9.69918079  7.61494494 -3.20404423]
map reward
-0.44	-0.44	-0.44	-0.44	-0.44	
-0.44	-0.44	-0.44	-0.44	-9.70	
-0.44	-9.70	-9.70	-0.44	7.61	
-0.44	-0.44	-0.44	-0.44	-9.70	
-0.44	-0.44	-0.44	-9.70	-0.44	
Map policy
>	>	>	v	v	
>	>	>	v	v	
^	>	>	>	>	
>	>	>	^	^	
>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.59574315 -4.73926016  6.65235453 -1.56486124]
mean reward
-0.60	-0.60	-0.60	-0.60	-0.60	
-0.60	-0.60	-0.60	-0.60	-4.74	
-0.60	-4.74	-4.74	-0.60	6.65	
-0.60	-0.60	-0.60	-0.60	-4.74	
-0.60	-0.60	-0.60	-4.74	-0.60	
mean policy
>	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	^	
Optimal Policy
>	>	>	v	<	
>	>	>	v	v	
^	^	>	>	>	
>	>	>	^	^	
>	>	^	^	^	
MAP policy loss 0.029460183650146576
Mean policy loss 0.19879652603484033
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	v	
>	v	v	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-100.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-100.00	
features
0 	0 	0 	0 	0 	
3 	0 	0 	3 	1 	
0 	0 	0 	0 	2 	
0 	1 	0 	0 	0 	
1 	0 	0 	0 	3 	
MAP on testing env
map_weights [-0.44218995 -9.69918079  7.61494494 -3.20404423]
map reward
-0.44	-0.44	-0.44	-0.44	-0.44	
-3.20	-0.44	-0.44	-3.20	-9.70	
-0.44	-0.44	-0.44	-0.44	7.61	
-0.44	-9.70	-0.44	-0.44	-0.44	
-9.70	-0.44	-0.44	-0.44	-3.20	
Map policy
>	v	v	v	v	
>	>	v	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	^	^	
MEAN policy on test env
mean_weights [-0.59574315 -4.73926016  6.65235453 -1.56486124]
mean reward
-0.60	-0.60	-0.60	-0.60	-0.60	
-1.56	-0.60	-0.60	-1.56	-4.74	
-0.60	-0.60	-0.60	-0.60	6.65	
-0.60	-4.74	-0.60	-0.60	-0.60	
-4.74	-0.60	-0.60	-0.60	-1.56	
mean policy
>	>	v	v	v	
>	>	v	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	^	^	
features
0 	0 	0 	0 	0 	
3 	0 	0 	3 	1 	
0 	0 	0 	0 	2 	
0 	1 	0 	0 	0 	
1 	0 	0 	0 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	>	v	
>	>	v	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	^	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	>	v	
>	>	v	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-134.44	-134.44	-134.44	-134.44	-134.44	
-131.83	-134.44	-134.44	-131.83	-138.46	
-134.44	-134.44	-134.44	-134.44	-123.26	
-134.44	-138.46	-134.44	-134.44	-134.44	
-138.46	-134.44	-134.44	-134.44	-131.83	
ird policy
v	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
MAP policy loss 29.81636627134133
mean policy loss 3.6282495059414517
robust policy loss 0.010650903622582773
regret policy loss 0.01064949982034051
ird policy loss 42.27245398637318
MAP lava occupancy 0.41751677795446757
Mean lava occupancy 0.41751677795446757
Robust lava occupancy 0.12000001080360273
Regret lava occupancy 0.12000000000340288
IRD lava occupancy 0.5483454998192361
##############
Trial  3
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	>	>	v	
v	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	1 	0 	1 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
demonstration
[(13, 1), (12, 1), (14, 1), (11, 1), (10, 1), (0, 3), (5, 3)]
w_map [0.41274056 0.08059965 0.84897904 2.10411814] loglik 0.0
accepted/total = 1913/2000 = 0.9565
MAP Policy on Train MDP
map_weights [0.41274056 0.08059965 0.84897904 2.10411814]
map reward
0.41	0.08	0.41	0.41	0.41	
0.41	0.08	0.41	0.08	0.41	
0.41	0.41	0.41	0.41	0.85	
0.41	0.41	0.41	0.41	0.08	
0.41	0.41	0.41	0.41	0.41	
Map policy
v	>	>	>	v	
v	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [ 0.59988991 -2.18079078  5.11048661  0.33036962]
mean reward
0.60	-2.18	0.60	0.60	0.60	
0.60	-2.18	0.60	-2.18	0.60	
0.60	0.60	0.60	0.60	5.11	
0.60	0.60	0.60	0.60	-2.18	
0.60	0.60	0.60	0.60	0.60	
mean policy
v	>	>	>	v	
v	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	>	^	^	
Optimal Policy
v	>	>	>	v	
v	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
MAP policy loss 0.01121006734956001
Mean policy loss 0.011210780603036777
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	>	v	
>	>	>	>	v	
>	v	^	>	>	
>	>	>	^	^	
>	^	^	^	>	
reward
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	1 	2 	
0 	0 	0 	0 	3 	
0 	0 	1 	3 	0 	
MAP on testing env
map_weights [0.41274056 0.08059965 0.84897904 2.10411814]
map reward
0.41	0.41	2.10	2.10	0.41	
0.41	0.08	0.41	0.41	0.41	
0.41	0.41	0.08	0.08	0.85	
0.41	0.41	0.41	0.41	2.10	
0.41	0.41	0.08	2.10	0.41	
Map policy
>	>	>	^	<	
^	>	^	^	v	
^	^	^	>	v	
>	>	>	v	>	
>	>	>	v	<	
MEAN policy on test env
mean_weights [ 0.59988991 -2.18079078  5.11048661  0.33036962]
mean reward
0.60	0.60	0.33	0.33	0.60	
0.60	-2.18	0.60	0.60	0.60	
0.60	0.60	-2.18	-2.18	5.11	
0.60	0.60	0.60	0.60	0.33	
0.60	0.60	-2.18	0.33	0.60	
mean policy
>	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	^	>	^	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	1 	2 	
0 	0 	0 	0 	3 	
0 	0 	1 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	^	<	
^	>	^	^	v	
^	v	^	>	>	
>	>	>	>	^	
>	^	>	v	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
^	^	>	>	v	
>	v	v	>	>	
>	>	>	>	^	
>	^	>	>	^	
-------- IRD Solution -------
ird reward
-153.18	-153.18	-157.89	-157.89	-153.18	
-153.18	-160.69	-153.18	-153.18	-153.18	
-153.18	-153.18	-160.69	-160.69	-147.87	
-153.18	-153.18	-153.18	-153.18	-157.89	
-153.18	-153.18	-160.69	-157.89	-153.18	
ird policy
>	>	v	>	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	^	^	>	^	
MAP policy loss 15.896218521941925
mean policy loss 35.54741730131383
robust policy loss 998.0697887359406
regret policy loss 80.51419809155264
ird policy loss 41.51986627632361
MAP lava occupancy 0.3195132402606448
Mean lava occupancy 0.3195132402606448
Robust lava occupancy 10.100466745314133
Regret lava occupancy 1.0022170406380813
IRD lava occupancy 0.6105332236822094
##############
Trial  4
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	>	>	v	
>	>	^	v	v	
>	>	>	>	>	
>	>	v	>	^	
>	>	>	>	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	1 	0 	2 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(6, 1), (3, 1), (9, 3), (2, 1), (14, 1), (4, 3), (5, 1), (0, 3), (7, 2)]
w_map [ 0.39652454 -0.84542387  0.86200128 -0.15538738] loglik 0.0
accepted/total = 1825/2000 = 0.9125
MAP Policy on Train MDP
map_weights [ 0.39652454 -0.84542387  0.86200128 -0.15538738]
map reward
0.40	-0.85	0.40	0.40	0.40	
0.40	0.40	0.40	-0.85	0.40	
0.40	0.40	-0.85	0.40	0.86	
0.40	0.40	0.40	-0.85	0.40	
0.40	0.40	0.40	0.40	0.40	
Map policy
v	>	>	>	v	
>	>	^	v	v	
>	>	>	>	>	
>	>	v	^	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 0.87406333 -3.48562246  1.56316701 -2.06231358]
mean reward
0.87	-3.49	0.87	0.87	0.87	
0.87	0.87	0.87	-3.49	0.87	
0.87	0.87	-3.49	0.87	1.56	
0.87	0.87	0.87	-3.49	0.87	
0.87	0.87	0.87	0.87	0.87	
mean policy
v	>	>	>	v	
>	>	^	v	v	
v	v	>	>	>	
>	>	v	^	^	
>	>	>	>	^	
Optimal Policy
v	>	>	>	v	
>	>	^	v	v	
>	>	>	>	>	
>	>	v	>	^	
>	>	>	>	^	
MAP policy loss -8.510008243662465e-09
Mean policy loss 0.19979784003730455
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	v	v	
>	v	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
reward
-100.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
3 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	0 	0 	2 	
0 	1 	0 	0 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [ 0.39652454 -0.84542387  0.86200128 -0.15538738]
map reward
-0.16	0.40	0.40	-0.16	0.40	
0.40	0.40	-0.85	0.40	-0.85	
-0.85	0.40	0.40	0.40	0.86	
0.40	-0.85	0.40	0.40	0.40	
0.40	-0.16	0.40	0.40	0.40	
Map policy
v	v	>	v	v	
>	v	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 0.87406333 -3.48562246  1.56316701 -2.06231358]
mean reward
-2.06	0.87	0.87	-2.06	0.87	
0.87	0.87	-3.49	0.87	-3.49	
-3.49	0.87	0.87	0.87	1.56	
0.87	-3.49	0.87	0.87	0.87	
0.87	-2.06	0.87	0.87	0.87	
mean policy
v	v	<	v	<	
>	v	>	v	v	
>	>	>	>	>	
v	>	>	>	^	
>	>	>	>	^	
features
3 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	0 	0 	2 	
0 	1 	0 	0 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	<	v	<	
>	v	>	v	v	
>	>	>	>	>	
v	>	>	>	^	
>	>	>	^	^	
------ Regret Solution ---------
expert u_sa [ 6.73159137  0.         13.14999804  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	<	v	>	
>	v	>	v	v	
>	>	>	>	>	
v	>	>	>	^	
<	>	>	>	^	
-------- IRD Solution -------
ird reward
-76.22	-72.51	-72.51	-76.22	-72.51	
-72.51	-72.51	-78.08	-72.51	-78.08	
-78.08	-72.51	-72.51	-72.51	-70.79	
-72.51	-78.08	-72.51	-72.51	-72.51	
-72.51	-76.22	-72.51	-72.51	-72.51	
ird policy
v	v	<	v	v	
>	v	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 30.27415668343215
mean policy loss 10.910999818612327
robust policy loss 10.911001988633027
regret policy loss 3.409796525021426
ird policy loss 3.617600011444843
MAP lava occupancy 0.4061132769287524
Mean lava occupancy 0.4061132769287524
Robust lava occupancy 0.23210001074031134
Regret lava occupancy 0.12000000000267579
IRD lava occupancy 0.15800000010431692
##############
Trial  5
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	1 	
0 	0 	0 	0 	1 	
0 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (13, 1), (8, 3), (1, 3), (7, 1), (6, 1), (14, 1)]
w_map [ 0.35512615 -1.6376226   5.33024285 -8.26962827] loglik -2.772575175034035
accepted/total = 1956/2000 = 0.978
MAP Policy on Train MDP
map_weights [ 0.35512615 -1.6376226   5.33024285 -8.26962827]
map reward
-1.64	0.36	0.36	0.36	-1.64	
0.36	0.36	0.36	0.36	-1.64	
0.36	0.36	0.36	0.36	5.33	
0.36	0.36	-1.64	0.36	0.36	
0.36	-1.64	0.36	0.36	0.36	
Map policy
>	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
MEAN policy on Train MDP
mean_weights [ 1.593319   -0.6644276   4.83912523 -5.4877872 ]
mean reward
-0.66	1.59	1.59	1.59	-0.66	
1.59	1.59	1.59	1.59	-0.66	
1.59	1.59	1.59	1.59	4.84	
1.59	1.59	-0.66	1.59	1.59	
1.59	-0.66	1.59	1.59	1.59	
mean policy
>	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
Optimal Policy
>	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
MAP policy loss 0.01121196267139439
Mean policy loss 0.011210079931929497
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	v	v	
>	>	>	>	v	
>	v	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-5.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	3 	
0 	0 	0 	0 	0 	
0 	0 	3 	1 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.35512615 -1.6376226   5.33024285 -8.26962827]
map reward
0.36	-1.64	0.36	0.36	-8.27	
0.36	0.36	0.36	0.36	0.36	
0.36	0.36	-8.27	-1.64	5.33	
0.36	0.36	0.36	0.36	0.36	
0.36	0.36	0.36	0.36	0.36	
Map policy
v	>	>	v	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 1.593319   -0.6644276   4.83912523 -5.4877872 ]
mean reward
1.59	-0.66	1.59	1.59	-5.49	
1.59	1.59	1.59	1.59	1.59	
1.59	1.59	-5.49	-0.66	4.84	
1.59	1.59	1.59	1.59	1.59	
1.59	1.59	1.59	1.59	1.59	
mean policy
v	>	>	v	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
features
0 	1 	0 	0 	3 	
0 	0 	0 	0 	0 	
0 	0 	3 	1 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	>	v	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	>	v	v	
>	>	>	>	v	
>	^	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-125.99	-128.54	-125.99	-125.99	-135.39	
-125.99	-125.99	-125.99	-125.99	-125.99	
-125.99	-125.99	-135.39	-128.54	-120.86	
-125.99	-125.99	-125.99	-125.99	-125.99	
-125.99	-125.99	-125.99	-125.99	-125.99	
ird policy
v	>	>	v	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 31.694379869997142
mean policy loss 0.011210000832040379
robust policy loss 0.01121717751583029
regret policy loss 0.011210065323894458
ird policy loss 0.01121002130394455
MAP lava occupancy 0.3809002437636163
Mean lava occupancy 0.3809002437636163
Robust lava occupancy 0.08000006567838196
Regret lava occupancy 0.08000000047876911
IRD lava occupancy 0.08000000016810259
##############
Trial  6
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	1 	0 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (3, 1), (9, 3), (2, 1), (14, 1), (4, 3), (1, 1)]
w_map [-0.47935467  0.48826263  2.38023264 -2.83292386] loglik -2.772588722240471
accepted/total = 1938/2000 = 0.969
MAP Policy on Train MDP
map_weights [-0.47935467  0.48826263  2.38023264 -2.83292386]
map reward
-0.48	-0.48	-0.48	-0.48	-0.48	
-0.48	-0.48	-0.48	-0.48	-0.48	
-0.48	-0.48	-0.48	-0.48	2.38	
-0.48	-0.48	-0.48	0.49	-0.48	
0.49	-0.48	-0.48	-0.48	-0.48	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [ 0.0184533   0.76704472  2.0093941  -3.09122765]
mean reward
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	2.01	
0.02	0.02	0.02	0.77	0.02	
0.77	0.02	0.02	0.02	0.02	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MAP policy loss 0.997480997661407
Mean policy loss 0.9974810169544799
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	>	v	<	
v	v	>	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	^	^	
reward
-1.00	-5.00	-100.00	-1.00	-100.00	
-1.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	0 	3 	
0 	1 	1 	0 	1 	
0 	0 	0 	0 	2 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.47935467  0.48826263  2.38023264 -2.83292386]
map reward
-0.48	0.49	-2.83	-0.48	-2.83	
-0.48	0.49	0.49	-0.48	0.49	
-0.48	-0.48	-0.48	-0.48	2.38	
0.49	0.49	-0.48	-0.48	-0.48	
-0.48	-0.48	-0.48	-0.48	-0.48	
Map policy
>	v	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	>	>	^	
MEAN policy on test env
mean_weights [ 0.0184533   0.76704472  2.0093941  -3.09122765]
mean reward
0.02	0.77	-3.09	0.02	-3.09	
0.02	0.77	0.77	0.02	0.77	
0.02	0.02	0.02	0.02	2.01	
0.77	0.77	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
mean policy
>	v	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	>	>	^	
features
0 	1 	3 	0 	3 	
0 	1 	1 	0 	1 	
0 	0 	0 	0 	2 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	>	v	
>	>	>	>	v	
^	^	>	>	>	
>	^	>	^	^	
^	^	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	<	>	v	<	
v	v	>	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-59.84	-60.75	-64.15	-59.84	-64.15	
-59.84	-60.75	-60.75	-59.84	-60.75	
-59.84	-59.84	-59.84	-59.84	-57.70	
-60.75	-60.75	-59.84	-59.84	-59.84	
-59.84	-59.84	-59.84	-59.84	-59.84	
ird policy
v	v	>	v	v	
v	v	>	v	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	^	^	
MAP policy loss 9.712051271233918
mean policy loss 2.920887189100969
robust policy loss 9.80043952878196
regret policy loss 2.6334061598021208e-09
ird policy loss 0.0425003468008307
MAP lava occupancy 0.16372777438080083
Mean lava occupancy 0.16372777438080083
Robust lava occupancy 0.11799999780532605
Regret lava occupancy 0.08000000004777581
IRD lava occupancy 0.0800000012239456
##############
Trial  7
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
features
0 	0 	0 	0 	0 	
1 	0 	1 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
1 	0 	0 	1 	1 	
demonstration
[(0, 1), (13, 1), (1, 3), (12, 1), (6, 3), (14, 1), (11, 1)]
w_map [ 0.28837521 -0.54595012  2.91846258 -4.44840543] loglik -0.6931359664049523
accepted/total = 1886/2000 = 0.943
MAP Policy on Train MDP
map_weights [ 0.28837521 -0.54595012  2.91846258 -4.44840543]
map reward
0.29	0.29	0.29	0.29	0.29	
-0.55	0.29	-0.55	0.29	0.29	
0.29	0.29	0.29	0.29	2.92	
0.29	0.29	0.29	0.29	0.29	
-0.55	0.29	0.29	-0.55	-0.55	
Map policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.10667451 -1.70093046  1.46557069 -4.58132125]
mean reward
0.11	0.11	0.11	0.11	0.11	
-1.70	0.11	-1.70	0.11	0.11	
0.11	0.11	0.11	0.11	1.47	
0.11	0.11	0.11	0.11	0.11	
-1.70	0.11	0.11	-1.70	-1.70	
mean policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
MAP policy loss 1.4445409969345013e-06
Mean policy loss -6.013225019134427e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	>	v	
>	>	^	>	v	
>	>	>	>	>	
>	>	>	>	^	
<	^	^	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-100.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	1 	0 	2 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	3 	
MAP on testing env
map_weights [ 0.28837521 -0.54595012  2.91846258 -4.44840543]
map reward
0.29	-0.55	0.29	0.29	0.29	
0.29	0.29	0.29	-0.55	0.29	
0.29	0.29	-0.55	0.29	2.92	
-4.45	0.29	0.29	-0.55	0.29	
0.29	-4.45	0.29	-4.45	-4.45	
Map policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.10667451 -1.70093046  1.46557069 -4.58132125]
mean reward
0.11	-1.70	0.11	0.11	0.11	
0.11	0.11	0.11	-1.70	0.11	
0.11	0.11	-1.70	0.11	1.47	
-4.58	0.11	0.11	-1.70	0.11	
0.11	-4.58	0.11	-4.58	-4.58	
mean policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	^	^	^	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	1 	0 	2 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	>	>	v	
>	>	^	v	v	
^	^	>	>	>	
^	^	<	>	^	
<	^	^	<	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	>	>	v	
>	>	^	v	v	
^	^	>	>	>	
^	^	<	>	^	
<	^	^	<	^	
-------- IRD Solution -------
ird reward
-70.31	-74.96	-70.31	-70.31	-70.31	
-70.31	-70.31	-70.31	-74.96	-70.31	
-70.31	-70.31	-74.96	-70.31	-69.10	
-76.95	-70.31	-70.31	-74.96	-70.31	
-70.31	-76.95	-70.31	-76.95	-76.95	
ird policy
v	>	>	>	v	
>	>	^	v	v	
^	^	>	>	>	
^	^	>	>	^	
^	^	^	^	^	
MAP policy loss 3.1541115195847125
mean policy loss 2.783877555390696
robust policy loss 1.2040275104283857
regret policy loss 1.2040274998410787
ird policy loss 3.2860967073421294
MAP lava occupancy 0.198000189857054
Mean lava occupancy 0.198000189857054
Robust lava occupancy 0.16000000006749399
Regret lava occupancy 0.16000000000257789
IRD lava occupancy 0.1980000000358933
##############
Trial  8
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (1, 3), (7, 1), (8, 1), (6, 1), (9, 3), (14, 1)]
w_map [-0.37332381  1.21916669  0.86904229 -0.88884258] loglik -3.465732410365945
accepted/total = 1991/2000 = 0.9955
MAP Policy on Train MDP
map_weights [-0.37332381  1.21916669  0.86904229 -0.88884258]
map reward
-0.37	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	0.87	
-0.37	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	-0.37	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-3.74512746  0.84535869  1.36182524 -2.29515074]
mean reward
-3.75	-3.75	-3.75	-3.75	-3.75	
-3.75	-3.75	-3.75	-3.75	-3.75	
-3.75	-3.75	-3.75	-3.75	1.36	
-3.75	-3.75	-3.75	-3.75	-3.75	
-3.75	-3.75	-3.75	-3.75	-3.75	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 2.4490440614019504e-07
Mean policy loss -1.8401073720308858e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
^	^	>	>	v	
^	>	>	>	>	
v	v	>	>	^	
>	>	>	>	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-5.00	1.00	
-5.00	-5.00	-100.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	0 	1 	2 	
1 	1 	3 	0 	0 	
1 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.37332381  1.21916669  0.86904229 -0.88884258]
map reward
-0.37	1.22	-0.37	1.22	-0.37	
-0.37	-0.37	-0.89	-0.37	-0.37	
-0.37	-0.89	-0.37	1.22	0.87	
1.22	1.22	-0.89	-0.37	-0.37	
1.22	-0.37	-0.37	-0.37	-0.37	
Map policy
>	^	<	^	<	
v	^	^	^	^	
v	v	>	^	<	
v	<	<	^	^	
<	<	<	^	^	
MEAN policy on test env
mean_weights [-3.74512746  0.84535869  1.36182524 -2.29515074]
mean reward
-3.75	0.85	-3.75	0.85	-3.75	
-3.75	-3.75	-2.30	-3.75	-3.75	
-3.75	-2.30	-3.75	0.85	1.36	
0.85	0.85	-2.30	-3.75	-3.75	
0.85	-3.75	-3.75	-3.75	-3.75	
mean policy
>	^	>	v	v	
v	>	>	v	v	
>	>	>	>	>	
>	>	>	^	^	
^	^	^	^	^	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	0 	1 	2 	
1 	1 	3 	0 	0 	
1 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	^	v	v	v	
v	>	>	>	>	
>	<	>	^	^	
^	^	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	^	>	>	v	
>	>	>	>	>	
>	v	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-36.81	-30.21	-36.81	-30.21	-36.81	
-36.81	-36.81	-34.23	-36.81	-36.81	
-36.81	-34.23	-36.81	-30.21	-27.43	
-30.21	-30.21	-34.23	-36.81	-36.81	
-30.21	-36.81	-36.81	-36.81	-36.81	
ird policy
>	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	^	^	
^	^	^	^	^	
MAP policy loss 28.858004891679865
mean policy loss 38.19402059791359
robust policy loss 21.222504282837956
regret policy loss 3.3964577277216614
ird policy loss 30.38753002584515
MAP lava occupancy 0.4177561511198898
Mean lava occupancy 0.4177561511198898
Robust lava occupancy 0.2043108938105797
Regret lava occupancy 0.15799999999887698
IRD lava occupancy 0.4145950000016574
##############
Trial  9
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
^	>	>	>	v	
>	>	>	>	>	
v	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	2 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (3, 1), (9, 3), (2, 1), (14, 1), (4, 3), (1, 1)]
w_map [ 0.48792226 -1.84689284  1.09774286  1.87433   ] loglik -1.3862934140615835
accepted/total = 1861/2000 = 0.9305
MAP Policy on Train MDP
map_weights [ 0.48792226 -1.84689284  1.09774286  1.87433   ]
map reward
0.49	0.49	0.49	0.49	0.49	
0.49	-1.85	0.49	0.49	0.49	
0.49	0.49	-1.85	0.49	1.10	
0.49	-1.85	0.49	0.49	0.49	
0.49	0.49	0.49	0.49	0.49	
Map policy
>	>	>	>	v	
^	>	>	>	v	
^	>	>	>	>	
v	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-0.88258431 -2.11475405  0.51761665  4.10380767]
mean reward
-0.88	-0.88	-0.88	-0.88	-0.88	
-0.88	-2.11	-0.88	-0.88	-0.88	
-0.88	-0.88	-2.11	-0.88	0.52	
-0.88	-2.11	-0.88	-0.88	-0.88	
-0.88	-0.88	-0.88	-0.88	-0.88	
mean policy
>	>	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
^	>	>	>	v	
>	>	>	>	>	
v	>	>	>	^	
>	>	>	>	^	
MAP policy loss 0.09733750251188479
Mean policy loss 0.032940347517613544
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	v	>	v	
v	v	v	>	v	
>	>	>	>	>	
>	^	>	^	^	
^	>	>	^	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-100.00	-5.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-100.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	0 	0 	2 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [ 0.48792226 -1.84689284  1.09774286  1.87433   ]
map reward
0.49	0.49	0.49	1.87	0.49	
0.49	1.87	-1.85	-1.85	0.49	
0.49	0.49	0.49	0.49	1.10	
0.49	0.49	1.87	0.49	1.87	
0.49	1.87	0.49	0.49	0.49	
Map policy
>	>	>	^	<	
>	v	^	^	v	
v	v	v	>	v	
v	v	v	>	>	
>	v	<	<	^	
MEAN policy on test env
mean_weights [-0.88258431 -2.11475405  0.51761665  4.10380767]
mean reward
-0.88	-0.88	-0.88	4.10	-0.88	
-0.88	4.10	-2.11	-2.11	-0.88	
-0.88	-0.88	-0.88	-0.88	0.52	
-0.88	-0.88	4.10	-0.88	4.10	
-0.88	4.10	-0.88	-0.88	-0.88	
mean policy
>	>	>	^	<	
>	v	^	^	v	
v	v	v	>	v	
v	v	>	>	>	
>	v	<	>	^	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	0 	0 	2 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	^	v	
>	v	v	>	v	
>	>	>	>	>	
>	v	>	>	^	
>	v	<	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	>	>	v	
v	v	v	>	v	
>	>	>	>	>	
>	^	>	^	^	
^	>	>	^	^	
-------- IRD Solution -------
ird reward
-36.36	-36.36	-36.36	-32.91	-36.36	
-36.36	-32.91	-37.74	-37.74	-36.36	
-36.36	-36.36	-36.36	-36.36	-32.63	
-36.36	-36.36	-32.91	-36.36	-32.91	
-36.36	-32.91	-36.36	-36.36	-36.36	
ird policy
>	>	>	^	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	v	^	>	^	
MAP policy loss 51.347769148596726
mean policy loss 1851.7673372355946
robust policy loss 746.9923017749602
regret policy loss 10.684401199478991
ird policy loss 486.97957762988744
MAP lava occupancy 0.7068361719315979
Mean lava occupancy 0.7068361719315979
Robust lava occupancy 7.624405540262873
Regret lava occupancy 0.31210000969339696
IRD lava occupancy 5.0473704439155735
##############
Trial  10
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	<	
v	v	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	1 	0 	1 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (13, 1), (1, 3), (12, 1), (6, 3), (14, 1), (11, 1)]
w_map [ 0.61372371 -4.37265075  3.71782502 -2.20196523] loglik -1.3861757606082392
accepted/total = 1892/2000 = 0.946
MAP Policy on Train MDP
map_weights [ 0.61372371 -4.37265075  3.71782502 -2.20196523]
map reward
-4.37	0.61	0.61	0.61	0.61	
0.61	0.61	-4.37	0.61	-4.37	
0.61	0.61	0.61	0.61	3.72	
0.61	0.61	0.61	0.61	0.61	
0.61	0.61	0.61	0.61	0.61	
Map policy
>	>	>	v	v	
>	v	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 0.77278148 -2.2738643   2.09233798 -3.64961863]
mean reward
-2.27	0.77	0.77	0.77	0.77	
0.77	0.77	-2.27	0.77	-2.27	
0.77	0.77	0.77	0.77	2.09	
0.77	0.77	0.77	0.77	0.77	
0.77	0.77	0.77	0.77	0.77	
mean policy
>	>	>	v	<	
v	v	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	v	<	
v	v	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 0.0112117998046877
Mean policy loss 7.314627896437553e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
^	^	>	>	v	
^	^	>	>	>	
^	v	>	>	^	
>	^	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-100.00	1.00	
-100.00	-1.00	-100.00	-1.00	-1.00	
-5.00	-5.00	-100.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	0 	3 	2 	
3 	0 	3 	0 	0 	
1 	1 	3 	1 	0 	
MAP on testing env
map_weights [ 0.61372371 -4.37265075  3.71782502 -2.20196523]
map reward
0.61	0.61	0.61	0.61	0.61	
0.61	0.61	-2.20	0.61	0.61	
0.61	-2.20	0.61	-2.20	3.72	
-2.20	0.61	-2.20	0.61	0.61	
-4.37	-4.37	-2.20	-4.37	0.61	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	^	>	^	
MEAN policy on test env
mean_weights [ 0.77278148 -2.2738643   2.09233798 -3.64961863]
mean reward
0.77	0.77	0.77	0.77	0.77	
0.77	0.77	-3.65	0.77	0.77	
0.77	-3.65	0.77	-3.65	2.09	
-3.65	0.77	-3.65	0.77	0.77	
-2.27	-2.27	-3.65	-2.27	0.77	
mean policy
>	>	>	>	v	
^	^	>	>	v	
^	>	>	>	>	
^	>	>	>	^	
>	^	>	>	^	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	0 	3 	2 	
3 	0 	3 	0 	0 	
1 	1 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
^	^	>	>	v	
^	^	>	>	>	
^	>	>	>	^	
>	^	>	>	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
^	^	>	>	v	
^	^	>	>	>	
^	>	>	>	^	
^	^	>	>	^	
-------- IRD Solution -------
ird reward
-96.41	-96.41	-96.41	-96.41	-96.41	
-96.41	-96.41	-102.82	-96.41	-96.41	
-96.41	-102.82	-96.41	-102.82	-93.63	
-102.82	-96.41	-102.82	-96.41	-96.41	
-106.46	-106.46	-102.82	-106.46	-96.41	
ird policy
>	>	>	>	v	
>	^	>	>	v	
^	>	>	>	>	
>	>	>	>	^	
^	^	^	>	^	
MAP policy loss 72.1672746889294
mean policy loss 5.886170406626269
robust policy loss 2.5667310449160192
regret policy loss 2.9996870402345066
ird policy loss 16.450413427208325
MAP lava occupancy 1.0993938788929096
Mean lava occupancy 1.0993938788929096
Robust lava occupancy 0.386395053152857
Regret lava occupancy 0.39010000000505696
IRD lava occupancy 0.5345950007813484
##############
Trial  11
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	>	v	
>	>	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 1), (8, 1), (9, 3), (14, 1), (2, 3), (1, 1)]
w_map [ 0.0157843  -4.87237289  4.47109945  1.79661917] loglik -1.3862027170489455
accepted/total = 1909/2000 = 0.9545
MAP Policy on Train MDP
map_weights [ 0.0157843  -4.87237289  4.47109945  1.79661917]
map reward
0.02	0.02	0.02	-4.87	0.02	
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	-4.87	4.47	
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
Map policy
>	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 0.68564442 -3.1290508   2.72824683 -1.79269241]
mean reward
0.69	0.69	0.69	-3.13	0.69	
0.69	0.69	0.69	0.69	0.69	
0.69	0.69	0.69	-3.13	2.73	
0.69	0.69	0.69	0.69	0.69	
0.69	0.69	0.69	0.69	0.69	
mean policy
>	>	v	>	v	
>	>	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	v	>	v	
>	>	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 0.031978545723360036
Mean policy loss 2.497803579717267e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
^	^	^	<	^	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-100.00	-1.00	-1.00	-5.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	0 	0 	2 	
0 	0 	0 	1 	0 	
1 	3 	0 	0 	1 	
MAP on testing env
map_weights [ 0.0157843  -4.87237289  4.47109945  1.79661917]
map reward
0.02	0.02	0.02	1.80	0.02	
0.02	0.02	0.02	1.80	0.02	
1.80	0.02	0.02	0.02	4.47	
0.02	0.02	0.02	-4.87	0.02	
-4.87	1.80	0.02	0.02	-4.87	
Map policy
>	>	>	v	v	
v	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
>	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.68564442 -3.1290508   2.72824683 -1.79269241]
mean reward
0.69	0.69	0.69	-1.79	0.69	
0.69	0.69	0.69	-1.79	0.69	
-1.79	0.69	0.69	0.69	2.73	
0.69	0.69	0.69	-3.13	0.69	
-3.13	-1.79	0.69	0.69	-3.13	
mean policy
>	>	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
^	^	^	<	^	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	0 	0 	2 	
0 	0 	0 	1 	0 	
1 	3 	0 	0 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
^	^	^	<	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	^	^	>	^	
^	^	^	<	^	
-------- IRD Solution -------
ird reward
-104.44	-104.44	-104.44	-102.43	-104.44	
-104.44	-104.44	-104.44	-102.43	-104.44	
-102.43	-104.44	-104.44	-104.44	-98.14	
-104.44	-104.44	-104.44	-106.22	-104.44	
-106.22	-102.43	-104.44	-104.44	-106.22	
ird policy
>	>	>	v	v	
v	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
>	^	^	^	^	
MAP policy loss 25.984677974444082
mean policy loss 1.2358465362183324e-09
robust policy loss 2.1727360715595445e-06
regret policy loss 1.1228999952095364e-09
ird policy loss 43.327804230641604
MAP lava occupancy 0.41374692030852767
Mean lava occupancy 0.41374692030852767
Robust lava occupancy 0.1600000120936192
Regret lava occupancy 0.16000000004029336
IRD lava occupancy 0.5974702499925976
##############
Trial  12
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (13, 1), (8, 3), (7, 1), (14, 1), (2, 3), (1, 1)]
w_map [-0.73086741  1.38741812  4.70543418 -1.45455972] loglik -3.465598379819312
accepted/total = 1918/2000 = 0.959
MAP Policy on Train MDP
map_weights [-0.73086741  1.38741812  4.70543418 -1.45455972]
map reward
1.39	-0.73	-0.73	-0.73	-0.73	
-0.73	-0.73	-0.73	-0.73	-0.73	
-0.73	-0.73	-0.73	-0.73	4.71	
-0.73	-0.73	-0.73	-0.73	-0.73	
-0.73	-0.73	-0.73	-0.73	-0.73	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [0.6866833  1.2016661  3.14222978 0.39090441]
mean reward
1.20	0.69	0.69	0.69	0.69	
0.69	0.69	0.69	0.69	0.69	
0.69	0.69	0.69	0.69	3.14	
0.69	0.69	0.69	0.69	0.69	
0.69	0.69	0.69	0.69	0.69	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 1.1518645780007886e-06
Mean policy loss 6.724339141062075e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
^	v	v	>	v	
>	>	>	>	>	
v	^	^	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-1.00	1.00	
-1.00	-100.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	0 	0 	2 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.73086741  1.38741812  4.70543418 -1.45455972]
map reward
-0.73	-0.73	-0.73	-0.73	-0.73	
-0.73	1.39	-0.73	-1.45	-0.73	
-1.45	-0.73	-0.73	-0.73	4.71	
-0.73	-1.45	1.39	-1.45	-0.73	
-0.73	-0.73	-0.73	1.39	-0.73	
Map policy
>	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [0.6866833  1.2016661  3.14222978 0.39090441]
mean reward
0.69	0.69	0.69	0.69	0.69	
0.69	1.20	0.69	0.39	0.69	
0.39	0.69	0.69	0.69	3.14	
0.69	0.39	1.20	0.39	0.69	
0.69	0.69	0.69	1.20	0.69	
mean policy
>	v	>	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	0 	0 	2 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	>	v	v	
v	>	>	>	v	
<	<	>	>	>	
^	^	>	>	^	
^	^	<	^	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
v	^	^	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-102.85	-102.85	-102.85	-102.85	-102.85	
-102.85	-98.92	-102.85	-102.22	-102.85	
-102.22	-102.85	-102.85	-102.85	-96.81	
-102.85	-102.22	-98.92	-102.22	-102.85	
-102.85	-102.85	-102.85	-98.92	-102.85	
ird policy
>	v	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	^	^	
MAP policy loss 33.95925928345116
mean policy loss 4.135059586322736
robust policy loss 672.822414092101
regret policy loss 0.03129018215758306
ird policy loss 60.84626882199103
MAP lava occupancy 0.5032856084820333
Mean lava occupancy 0.5032856084820333
Robust lava occupancy 6.853033306606322
Regret lava occupancy 0.16000000003652656
IRD lava occupancy 0.7708404997298699
##############
Trial  13
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	2 	
0 	1 	0 	0 	0 	
1 	0 	1 	0 	1 	
demonstration
[(0, 1), (7, 3), (13, 1), (12, 1), (14, 1), (2, 3), (1, 1)]
w_map [-0.48549493 -5.46964557  2.30285833  0.12442344] loglik -2.0794402227238606
accepted/total = 1927/2000 = 0.9635
MAP Policy on Train MDP
map_weights [-0.48549493 -5.46964557  2.30285833  0.12442344]
map reward
-0.49	-0.49	-0.49	-0.49	-0.49	
-0.49	-0.49	-0.49	-5.47	-0.49	
-0.49	-0.49	-0.49	-0.49	2.30	
-0.49	-5.47	-0.49	-0.49	-0.49	
-5.47	-0.49	-5.47	-0.49	-5.47	
Map policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	^	^	
MEAN policy on Train MDP
mean_weights [-1.3890675  -3.70728333  1.51100508  0.88467515]
mean reward
-1.39	-1.39	-1.39	-1.39	-1.39	
-1.39	-1.39	-1.39	-3.71	-1.39	
-1.39	-1.39	-1.39	-1.39	1.51	
-1.39	-3.71	-1.39	-1.39	-1.39	
-3.71	-1.39	-3.71	-1.39	-3.71	
mean policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	^	^	
Optimal Policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	^	^	
MAP policy loss 2.1327054009953866e-07
Mean policy loss 1.1610613512291756e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	0 	0 	2 	
3 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.48549493 -5.46964557  2.30285833  0.12442344]
map reward
-0.49	-0.49	-0.49	-0.49	-0.49	
-0.49	-0.49	-5.47	-5.47	-0.49	
-5.47	-0.49	-0.49	-0.49	2.30	
0.12	-0.49	-0.49	-0.49	-0.49	
-0.49	-0.49	0.12	-0.49	-0.49	
Map policy
>	>	>	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
MEAN policy on test env
mean_weights [-1.3890675  -3.70728333  1.51100508  0.88467515]
mean reward
-1.39	-1.39	-1.39	-1.39	-1.39	
-1.39	-1.39	-3.71	-3.71	-1.39	
-3.71	-1.39	-1.39	-1.39	1.51	
0.88	-1.39	-1.39	-1.39	-1.39	
-1.39	-1.39	0.88	-1.39	-1.39	
mean policy
>	>	>	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	0 	0 	2 	
3 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	v	v	>	v	
v	>	>	>	>	
<	<	>	>	^	
^	>	v	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
-------- IRD Solution -------
ird reward
-55.57	-55.57	-55.57	-55.57	-55.57	
-55.57	-55.57	-57.40	-57.40	-55.57	
-57.40	-55.57	-55.57	-55.57	-50.58	
-53.19	-55.57	-55.57	-55.57	-55.57	
-55.57	-55.57	-53.19	-55.57	-55.57	
ird policy
>	>	>	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
MAP policy loss 6.486808393239357
mean policy loss 7.52400024017178
robust policy loss 370.54497324879367
regret policy loss -3.786160385210735e-09
ird policy loss 7.52400079959261
MAP lava occupancy 0.13874358095249362
Mean lava occupancy 0.13874358095249362
Robust lava occupancy 3.7617085465906563
Regret lava occupancy 0.08000000000079426
IRD lava occupancy 0.1560000068841018
##############
Trial  14
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	^	^	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
1 	0 	1 	1 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 1), (8, 1), (9, 3), (14, 1), (2, 3), (1, 1)]
w_map [ 0.14510329 -4.0880898   6.41819535 -4.21678868] loglik -1.3861166277856682
accepted/total = 1939/2000 = 0.9695
MAP Policy on Train MDP
map_weights [ 0.14510329 -4.0880898   6.41819535 -4.21678868]
map reward
0.15	0.15	0.15	0.15	0.15	
-4.09	0.15	0.15	0.15	0.15	
-4.09	0.15	-4.09	-4.09	6.42	
0.15	0.15	0.15	0.15	0.15	
0.15	0.15	0.15	0.15	0.15	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 1.1119658  -4.77227134  4.41113361 -4.66846913]
mean reward
1.11	1.11	1.11	1.11	1.11	
-4.77	1.11	1.11	1.11	1.11	
-4.77	1.11	-4.77	-4.77	4.41	
1.11	1.11	1.11	1.11	1.11	
1.11	1.11	1.11	1.11	1.11	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	^	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	^	^	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 0.3283790310610795
Mean policy loss 0.01121025004217839
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	v	v	
v	<	>	v	v	
v	v	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-5.00	
-1.00	-5.00	-100.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	3 	0 	1 	
0 	1 	3 	1 	2 	
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.14510329 -4.0880898   6.41819535 -4.21678868]
map reward
0.15	0.15	-4.22	0.15	0.15	
0.15	0.15	-4.22	0.15	-4.09	
0.15	-4.09	-4.22	-4.09	6.42	
0.15	0.15	0.15	0.15	0.15	
-4.22	0.15	0.15	0.15	0.15	
Map policy
>	v	>	>	v	
>	>	>	>	v	
v	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 1.1119658  -4.77227134  4.41113361 -4.66846913]
mean reward
1.11	1.11	-4.67	1.11	1.11	
1.11	1.11	-4.67	1.11	-4.77	
1.11	-4.77	-4.67	-4.77	4.41	
1.11	1.11	1.11	1.11	1.11	
-4.67	1.11	1.11	1.11	1.11	
mean policy
v	v	>	v	v	
v	<	>	v	v	
v	v	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
features
0 	0 	3 	0 	0 	
0 	0 	3 	0 	1 	
0 	1 	3 	1 	2 	
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	<	<	v	v	
v	<	<	v	v	
v	v	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	<	<	v	>	
v	<	<	^	v	
v	v	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-123.52	-123.52	-131.14	-123.52	-123.52	
-123.52	-123.52	-131.14	-123.52	-130.92	
-123.52	-130.92	-131.14	-130.92	-119.21	
-123.52	-123.52	-123.52	-123.52	-123.52	
-131.14	-123.52	-123.52	-123.52	-123.52	
ird policy
v	v	>	v	v	
v	<	>	v	v	
v	v	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 24.456894284555432
mean policy loss 0.011212241578403948
robust policy loss 0.42706708064092735
regret policy loss 2.2137317592455785
ird policy loss 0.011211442997917764
MAP lava occupancy 0.4070511117948869
Mean lava occupancy 0.4070511117948869
Robust lava occupancy 0.1600001295114253
Regret lava occupancy 0.1600000000005114
IRD lava occupancy 0.16000001357591145
##############
Trial  15
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	v	v	
>	^	>	>	>	
>	v	>	>	^	
>	>	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	2 	
0 	0 	1 	0 	0 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (13, 1), (8, 3), (3, 3), (2, 1), (14, 1), (1, 1)]
w_map [ 0.09417744 -0.32511695  2.70459783  1.03997692] loglik -3.465673743827665
accepted/total = 1853/2000 = 0.9265
MAP Policy on Train MDP
map_weights [ 0.09417744 -0.32511695  2.70459783  1.03997692]
map reward
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	-0.33	0.09	2.70	
0.09	0.09	-0.33	0.09	0.09	
-0.33	0.09	0.09	0.09	0.09	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [-1.12421349 -2.65880554  1.10680398  1.89749762]
mean reward
-1.12	-1.12	-1.12	-1.12	-1.12	
-1.12	-1.12	-1.12	-1.12	-1.12	
-1.12	-1.12	-2.66	-1.12	1.11	
-1.12	-1.12	-2.66	-1.12	-1.12	
-2.66	-1.12	-1.12	-1.12	-1.12	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
Optimal Policy
>	>	>	>	v	
>	>	>	v	v	
>	^	>	>	>	
>	v	>	>	^	
>	>	>	^	^	
MAP policy loss 0.06939843608727703
Mean policy loss 0.06939372377572633
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
>	>	>	>	v	
^	^	>	>	>	
v	>	>	^	^	
>	>	>	^	<	
reward
-100.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-5.00	-5.00	-1.00	1.00	
-5.00	-100.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
3 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	1 	0 	2 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [ 0.09417744 -0.32511695  2.70459783  1.03997692]
map reward
1.04	0.09	-0.33	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
1.04	-0.33	-0.33	0.09	2.70	
-0.33	1.04	0.09	0.09	-0.33	
0.09	-0.33	0.09	0.09	0.09	
Map policy
v	v	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	^	>	^	^	
MEAN policy on test env
mean_weights [-1.12421349 -2.65880554  1.10680398  1.89749762]
mean reward
1.90	-1.12	-2.66	-1.12	-1.12	
-1.12	-1.12	-1.12	-1.12	-1.12	
1.90	-2.66	-2.66	-1.12	1.11	
-2.66	1.90	-1.12	-1.12	-2.66	
-1.12	-2.66	-1.12	-1.12	-1.12	
mean policy
<	<	<	<	<	
v	<	<	<	<	
<	<	<	<	<	
^	<	<	<	<	
^	^	^	<	<	
features
3 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	1 	0 	2 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
<	<	<	v	v	
^	<	<	>	v	
<	<	>	>	>	
^	^	>	^	^	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	>	>	v	
>	>	>	>	v	
^	^	>	>	>	
>	>	>	^	^	
<	>	>	^	<	
-------- IRD Solution -------
ird reward
-54.57	-54.15	-54.71	-54.15	-54.15	
-54.15	-54.15	-54.15	-54.15	-54.15	
-54.57	-54.71	-54.71	-54.15	-51.00	
-54.71	-54.57	-54.15	-54.15	-54.71	
-54.15	-54.71	-54.15	-54.15	-54.15	
ird policy
>	v	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	>	^	^	
MAP policy loss 5.0393150864143905
mean policy loss 1773.0910740343356
robust policy loss 865.6909503936232
regret policy loss 4.469911886885289
ird policy loss 3.6956867136383584
MAP lava occupancy 0.15800048398509217
Mean lava occupancy 0.15800048398509217
Robust lava occupancy 8.717813646301805
Regret lava occupancy 0.15799999999608985
IRD lava occupancy 0.15800000014054805
##############
Trial  16
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
^	>	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
1 	0 	0 	1 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	1 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (13, 1), (12, 1), (14, 1), (2, 3), (1, 1)]
w_map [ 0.27383092 -5.69401516  2.44486881 -5.13674158] loglik -0.6931468009761375
accepted/total = 1903/2000 = 0.9515
MAP Policy on Train MDP
map_weights [ 0.27383092 -5.69401516  2.44486881 -5.13674158]
map reward
0.27	0.27	0.27	0.27	-5.69	
-5.69	0.27	0.27	-5.69	0.27	
0.27	0.27	0.27	0.27	2.44	
0.27	0.27	0.27	0.27	-5.69	
0.27	-5.69	0.27	0.27	0.27	
Map policy
>	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
^	>	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.16378151 -4.68265299  1.25029007 -4.92150382]
mean reward
0.16	0.16	0.16	0.16	-4.68	
-4.68	0.16	0.16	-4.68	0.16	
0.16	0.16	0.16	0.16	1.25	
0.16	0.16	0.16	0.16	-4.68	
0.16	-4.68	0.16	0.16	0.16	
mean policy
>	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
^	>	^	^	<	
Optimal Policy
>	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	^	^	
^	>	^	^	<	
MAP policy loss 4.4848219382729974e-07
Mean policy loss 2.528458116837773e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	^	^	
reward
-1.00	-100.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-5.00	-1.00	-1.00	
features
0 	3 	0 	0 	1 	
0 	1 	1 	0 	0 	
1 	0 	0 	0 	2 	
0 	3 	0 	0 	0 	
3 	0 	1 	0 	0 	
MAP on testing env
map_weights [ 0.27383092 -5.69401516  2.44486881 -5.13674158]
map reward
0.27	-5.14	0.27	0.27	-5.69	
0.27	-5.69	-5.69	0.27	0.27	
-5.69	0.27	0.27	0.27	2.44	
0.27	-5.14	0.27	0.27	0.27	
-5.14	0.27	-5.69	0.27	0.27	
Map policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
MEAN policy on test env
mean_weights [ 0.16378151 -4.68265299  1.25029007 -4.92150382]
mean reward
0.16	-4.92	0.16	0.16	-4.68	
0.16	-4.68	-4.68	0.16	0.16	
-4.68	0.16	0.16	0.16	1.25	
0.16	-4.92	0.16	0.16	0.16	
-4.92	0.16	-4.68	0.16	0.16	
mean policy
v	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
features
0 	3 	0 	0 	1 	
0 	1 	1 	0 	0 	
1 	0 	0 	0 	2 	
0 	3 	0 	0 	0 	
3 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
v	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
<	>	>	v	v	
^	v	>	>	v	
>	>	>	>	>	
<	>	>	>	^	
>	v	>	>	^	
-------- IRD Solution -------
ird reward
-44.18	-49.24	-44.18	-44.18	-50.36	
-44.18	-50.36	-50.36	-44.18	-44.18	
-50.36	-44.18	-44.18	-44.18	-41.15	
-44.18	-49.24	-44.18	-44.18	-44.18	
-49.24	-44.18	-50.36	-44.18	-44.18	
ird policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
MAP policy loss 10.953596895908392
mean policy loss 5.8686701021049e-07
robust policy loss 14.267099999759012
regret policy loss 7.096478234965833
ird policy loss 14.267100066568297
MAP lava occupancy 0.2239908278714229
Mean lava occupancy 0.2239908278714229
Robust lava occupancy 0.27010000000714524
Regret lava occupancy 0.148347991420629
IRD lava occupancy 0.2701000003642726
##############
Trial  17
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	v	v	
>	>	>	>	v	
>	^	>	>	>	
>	^	>	^	^	
>	>	>	^	<	
reward
-1.00	-5.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	1 	
0 	0 	0 	0 	0 	
1 	0 	1 	0 	2 	
0 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
demonstration
[(7, 1), (8, 1), (6, 1), (9, 3), (14, 1), (5, 1), (0, 3)]
w_map [-0.67238487 -2.88372228  0.2417952   4.04585937] loglik -0.6931471805664273
accepted/total = 1942/2000 = 0.971
MAP Policy on Train MDP
map_weights [-0.67238487 -2.88372228  0.2417952   4.04585937]
map reward
-0.67	-2.88	-0.67	-2.88	-2.88	
-0.67	-0.67	-0.67	-0.67	-0.67	
-2.88	-0.67	-2.88	-0.67	0.24	
-0.67	-0.67	-2.88	-0.67	-2.88	
-0.67	-0.67	-0.67	-0.67	-0.67	
Map policy
v	>	v	v	v	
>	>	>	>	v	
>	^	>	>	>	
>	^	>	^	^	
>	>	>	^	<	
MEAN policy on Train MDP
mean_weights [ 0.04870498 -2.88177454  2.68015555  2.15658767]
mean reward
0.05	-2.88	0.05	-2.88	-2.88	
0.05	0.05	0.05	0.05	0.05	
-2.88	0.05	-2.88	0.05	2.68	
0.05	0.05	-2.88	0.05	-2.88	
0.05	0.05	0.05	0.05	0.05	
mean policy
v	>	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	^	^	
>	>	>	^	^	
Optimal Policy
v	>	v	v	v	
>	>	>	>	v	
>	^	>	>	>	
>	^	>	^	^	
>	>	>	^	<	
MAP policy loss -9.16825551242062e-10
Mean policy loss 0.08060375176696066
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
^	^	^	^	^	
^	<	>	^	<	
reward
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-5.00	-100.00	-5.00	-100.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	1 	3 	1 	3 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.67238487 -2.88372228  0.2417952   4.04585937]
map reward
-0.67	4.05	-0.67	-0.67	-0.67	
-0.67	-0.67	-0.67	-0.67	-0.67	
-0.67	-0.67	-0.67	-0.67	0.24	
-0.67	-2.88	4.05	-2.88	4.05	
-0.67	-0.67	4.05	-0.67	-0.67	
Map policy
>	^	<	<	v	
^	^	v	v	v	
>	>	v	>	v	
>	>	v	<	>	
>	>	v	<	^	
MEAN policy on test env
mean_weights [ 0.04870498 -2.88177454  2.68015555  2.15658767]
mean reward
0.05	2.16	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	2.68	
0.05	-2.88	2.16	-2.88	2.16	
0.05	0.05	2.16	0.05	0.05	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
>	>	^	>	^	
features
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	1 	3 	1 	3 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	^	<	v	v	
>	^	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
>	>	^	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
^	^	^	^	^	
^	<	<	>	^	
-------- IRD Solution -------
ird reward
-88.81	-87.15	-88.81	-88.81	-88.81	
-88.81	-88.81	-88.81	-88.81	-88.81	
-88.81	-88.81	-88.81	-88.81	-83.78	
-88.81	-91.48	-87.15	-91.48	-87.15	
-88.81	-88.81	-87.15	-88.81	-88.81	
ird policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
>	>	^	>	^	
MAP policy loss 12.23834722148929
mean policy loss 35.98660589232185
robust policy loss 367.02815489506526
regret policy loss 6.996047309944794
ird policy loss 35.98660523813106
MAP lava occupancy 0.2731042261042732
Mean lava occupancy 0.2731042261042732
Robust lava occupancy 3.8187788328822574
Regret lava occupancy 0.23409999999785475
IRD lava occupancy 0.5305950005846193
##############
Trial  18
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 3), (13, 1), (12, 1), (6, 1), (14, 1), (5, 1), (0, 3)]
w_map [-2.62893464 -7.38720313 -0.35915613  2.89859722] loglik -2.0794402793094378
accepted/total = 1903/2000 = 0.9515
MAP Policy on Train MDP
map_weights [-2.62893464 -7.38720313 -0.35915613  2.89859722]
map reward
-2.63	-7.39	-2.63	-2.63	-2.63	
-2.63	-2.63	-2.63	-2.63	-2.63	
-2.63	-2.63	-2.63	-2.63	-0.36	
-2.63	-2.63	-7.39	-2.63	-2.63	
-2.63	-2.63	-2.63	-2.63	-2.63	
Map policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [-1.63213273 -3.40253789 -0.53189519  2.99357504]
mean reward
-1.63	-3.40	-1.63	-1.63	-1.63	
-1.63	-1.63	-1.63	-1.63	-1.63	
-1.63	-1.63	-1.63	-1.63	-0.53	
-1.63	-1.63	-3.40	-1.63	-1.63	
-1.63	-1.63	-1.63	-1.63	-1.63	
mean policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
Optimal Policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MAP policy loss 3.8039810695486564e-08
Mean policy loss 5.455960618190592e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
v	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-2.62893464 -7.38720313 -0.35915613  2.89859722]
map reward
-2.63	-2.63	2.90	-2.63	-2.63	
-2.63	-7.39	-2.63	-2.63	-2.63	
-2.63	-2.63	-7.39	-2.63	-0.36	
-2.63	-2.63	-2.63	-2.63	-2.63	
-2.63	-2.63	-7.39	-2.63	-2.63	
Map policy
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-1.63213273 -3.40253789 -0.53189519  2.99357504]
mean reward
-1.63	-1.63	2.99	-1.63	-1.63	
-1.63	-3.40	-1.63	-1.63	-1.63	
-1.63	-1.63	-3.40	-1.63	-0.53	
-1.63	-1.63	-1.63	-1.63	-1.63	
-1.63	-1.63	-3.40	-1.63	-1.63	
mean policy
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	^	<	<	
^	>	^	<	v	
^	v	^	>	>	
^	>	>	^	^	
^	^	>	>	^	
-------- IRD Solution -------
ird reward
-7.47	-7.47	-4.61	-7.47	-7.47	
-7.47	-8.64	-7.47	-7.47	-7.47	
-7.47	-7.47	-8.64	-7.47	-5.23	
-7.47	-7.47	-7.47	-7.47	-7.47	
-7.47	-7.47	-8.64	-7.47	-7.47	
ird policy
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 3.442719967062238
mean policy loss 1715.900459157631
robust policy loss 1715.9004546005287
regret policy loss 893.6056831280168
ird policy loss 1715.9004558873355
MAP lava occupancy 0.06607583145735325
Mean lava occupancy 0.06607583145735325
Robust lava occupancy 17.029771012165302
Regret lava occupancy 8.905917467729433
IRD lava occupancy 17.02977102500056
##############
Trial  19
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
>	^	<	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	0 	0 	0 	
1 	0 	1 	0 	0 	
0 	0 	0 	0 	2 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
demonstration
[(13, 1), (12, 1), (6, 3), (14, 1), (11, 1), (5, 1), (0, 3)]
w_map [ 1.02820889 -1.28799717  2.18397061 -2.53327609] loglik -1.3862888657904477
accepted/total = 1822/2000 = 0.911
MAP Policy on Train MDP
map_weights [ 1.02820889 -1.28799717  2.18397061 -2.53327609]
map reward
1.03	-1.29	1.03	1.03	1.03	
-1.29	1.03	-1.29	1.03	1.03	
1.03	1.03	1.03	1.03	2.18	
-1.29	1.03	-1.29	1.03	1.03	
1.03	1.03	1.03	-1.29	1.03	
Map policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
>	^	<	^	^	
MEAN policy on Train MDP
mean_weights [-1.64869971 -2.92137993 -0.64091614 -4.08000349]
mean reward
-1.65	-2.92	-1.65	-1.65	-1.65	
-2.92	-1.65	-2.92	-1.65	-1.65	
-1.65	-1.65	-1.65	-1.65	-0.64	
-2.92	-1.65	-2.92	-1.65	-1.65	
-1.65	-1.65	-1.65	-2.92	-1.65	
mean policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
>	^	>	^	^	
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
>	^	<	^	^	
MAP policy loss 4.969090744108041e-07
Mean policy loss 0.024937024103899516
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	v	v	
>	>	>	>	v	
^	>	v	>	>	
>	>	>	>	^	
>	^	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	2 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [ 1.02820889 -1.28799717  2.18397061 -2.53327609]
map reward
1.03	1.03	1.03	1.03	-1.29	
1.03	1.03	1.03	1.03	1.03	
1.03	-1.29	1.03	-1.29	2.18	
1.03	1.03	1.03	1.03	1.03	
1.03	1.03	-2.53	1.03	1.03	
Map policy
>	>	>	v	v	
>	>	>	>	v	
^	>	v	>	>	
>	>	>	>	^	
>	^	>	>	^	
MEAN policy on test env
mean_weights [-1.64869971 -2.92137993 -0.64091614 -4.08000349]
mean reward
-1.65	-1.65	-1.65	-1.65	-2.92	
-1.65	-1.65	-1.65	-1.65	-1.65	
-1.65	-2.92	-1.65	-2.92	-0.64	
-1.65	-1.65	-1.65	-1.65	-1.65	
-1.65	-1.65	-4.08	-1.65	-1.65	
mean policy
>	>	>	v	v	
>	>	>	>	v	
^	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	2 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	v	
^	>	^	>	>	
>	>	>	>	^	
>	^	>	>	^	
------ Regret Solution ---------
expert u_sa [ 4.34816219  0.95       14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	v	
^	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
-------- IRD Solution -------
ird reward
-37.02	-37.02	-37.02	-37.02	-39.19	
-37.02	-37.02	-37.02	-37.02	-37.02	
-37.02	-39.19	-37.02	-39.19	-35.93	
-37.02	-37.02	-37.02	-37.02	-37.02	
-37.02	-37.02	-40.70	-37.02	-37.02	
ird policy
>	>	>	v	v	
>	>	>	>	v	
^	>	^	>	>	
>	>	>	>	^	
>	^	>	>	^	
MAP policy loss 3.122063400548691
mean policy loss 0.021859503406963077
robust policy loss 1.908493341140627e-08
regret policy loss 0.07349556274406477
ird policy loss 7.403244411952059e-08
MAP lava occupancy 0.04000001430123957
Mean lava occupancy 0.04000001430123957
Robust lava occupancy 0.04000000006778313
Regret lava occupancy 0.0400000000000675
IRD lava occupancy 0.04000000054176219
##############
Trial  20
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	1 	
0 	0 	0 	0 	1 	
demonstration
[(0, 1), (7, 1), (8, 1), (9, 3), (14, 1), (2, 3), (1, 1)]
w_map [-2.19814775 -2.19097801 -0.16803881  1.94699343] loglik -3.329917047422441
accepted/total = 1915/2000 = 0.9575
MAP Policy on Train MDP
map_weights [-2.19814775 -2.19097801 -0.16803881  1.94699343]
map reward
-2.20	-2.20	-2.20	-2.20	-2.19	
-2.20	-2.20	-2.20	-2.20	-2.20	
-2.20	-2.20	-2.20	-2.20	-0.17	
-2.20	-2.20	-2.20	-2.20	-2.19	
-2.20	-2.20	-2.20	-2.20	-2.19	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-2.90873012 -5.03404315  3.11304734  3.07374133]
mean reward
-2.91	-2.91	-2.91	-2.91	-5.03	
-2.91	-2.91	-2.91	-2.91	-2.91	
-2.91	-2.91	-2.91	-2.91	3.11	
-2.91	-2.91	-2.91	-2.91	-5.03	
-2.91	-2.91	-2.91	-2.91	-5.03	
mean policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	>	^	^	
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
MAP policy loss 2.238618158739232
Mean policy loss 0.011210020407430993
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
^	>	^	v	v	
^	^	>	>	>	
^	^	>	^	^	
^	^	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-100.00	-1.00	1.00	
-1.00	-1.00	-100.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
0 	0 	3 	0 	2 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-2.19814775 -2.19097801 -0.16803881  1.94699343]
map reward
-2.20	-2.20	-2.20	-2.20	-2.20	
-2.20	-2.20	-2.20	1.95	-2.20	
-2.20	-2.20	1.95	-2.20	-0.17	
-2.20	-2.20	1.95	-2.19	-2.19	
-2.20	-2.20	-2.19	-2.20	-2.20	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-2.90873012 -5.03404315  3.11304734  3.07374133]
mean reward
-2.91	-2.91	-2.91	-2.91	-2.91	
-2.91	-2.91	-2.91	3.07	-2.91	
-2.91	-2.91	3.07	-2.91	3.11	
-2.91	-2.91	3.07	-5.03	-5.03	
-2.91	-2.91	-5.03	-2.91	-2.91	
mean policy
>	v	v	v	v	
>	v	v	v	v	
>	>	v	>	>	
>	>	^	<	^	
>	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
0 	0 	3 	0 	2 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
>	v	v	v	v	
>	>	v	<	>	
>	>	^	<	^	
^	^	^	<	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	v	
>	>	v	>	>	
>	>	^	<	^	
^	^	^	>	^	
-------- IRD Solution -------
ird reward
-65.21	-65.21	-65.21	-65.21	-65.21	
-65.21	-65.21	-65.21	-59.47	-65.21	
-65.21	-65.21	-59.47	-65.21	-55.39	
-65.21	-65.21	-59.47	-70.78	-70.78	
-65.21	-65.21	-70.78	-65.21	-65.21	
ird policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	^	^	
>	^	^	>	^	
MAP policy loss 25.00199722304788
mean policy loss 1247.6484329485766
robust policy loss 1435.3992487486519
regret policy loss 198.31711188891984
ird policy loss 68.9301074467541
MAP lava occupancy 0.3765951277071015
Mean lava occupancy 0.3765951277071015
Robust lava occupancy 14.378223997590593
Regret lava occupancy 2.119664580251242
IRD lava occupancy 0.8373355000499368
##############
Trial  21
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(7, 3), (13, 1), (12, 1), (6, 1), (14, 1), (5, 1), (0, 3)]
w_map [ 0.31700774  0.38104466  2.52056052 -1.18471735] loglik -2.772567564509245
accepted/total = 1918/2000 = 0.959
MAP Policy on Train MDP
map_weights [ 0.31700774  0.38104466  2.52056052 -1.18471735]
map reward
0.32	0.32	0.32	0.32	0.32	
0.32	0.32	0.32	0.32	0.32	
0.32	0.32	0.32	0.32	2.52	
0.32	0.32	0.32	0.32	0.32	
0.32	0.38	0.32	0.32	0.32	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [2.14107568 1.37221265 3.48399785 3.56264334]
mean reward
2.14	2.14	2.14	2.14	2.14	
2.14	2.14	2.14	2.14	2.14	
2.14	2.14	2.14	2.14	3.48	
2.14	2.14	2.14	2.14	2.14	
2.14	1.37	2.14	2.14	2.14	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
MAP policy loss 0.15199968053508858
Mean policy loss 1.4184131148266277e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	v	v	
>	>	>	>	>	
^	^	>	>	^	
^	^	>	^	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [ 0.31700774  0.38104466  2.52056052 -1.18471735]
map reward
0.32	0.32	-1.18	0.32	0.32	
0.32	0.32	0.32	0.38	-1.18	
0.32	0.32	0.32	0.32	2.52	
0.32	0.32	0.38	0.32	0.32	
0.32	0.32	0.38	0.32	0.32	
Map policy
v	v	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
MEAN policy on test env
mean_weights [2.14107568 1.37221265 3.48399785 3.56264334]
mean reward
2.14	2.14	3.56	2.14	2.14	
2.14	2.14	2.14	1.37	3.56	
2.14	2.14	2.14	2.14	3.48	
2.14	2.14	1.37	2.14	2.14	
2.14	2.14	1.37	2.14	2.14	
mean policy
>	>	^	<	v	
>	^	^	>	>	
^	^	^	>	^	
^	^	^	^	^	
^	^	>	^	^	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	>	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	^	<	
>	>	v	v	v	
>	>	>	>	>	
^	^	>	>	^	
^	^	>	^	^	
-------- IRD Solution -------
ird reward
-129.69	-129.69	-126.55	-129.69	-129.69	
-129.69	-129.69	-129.69	-132.16	-126.55	
-129.69	-129.69	-129.69	-129.69	-127.67	
-129.69	-129.69	-132.16	-129.69	-129.69	
-129.69	-129.69	-132.16	-129.69	-129.69	
ird policy
>	>	^	<	v	
^	^	^	>	>	
^	^	^	>	^	
^	^	^	^	^	
^	^	>	^	^	
MAP policy loss 23.852117747126726
mean policy loss 1767.5390154674164
robust policy loss 1.8718629080496696
regret policy loss 1.279622409230891
ird policy loss 1767.539017384447
MAP lava occupancy 0.31428156542203856
Mean lava occupancy 0.31428156542203856
Robust lava occupancy 0.08000000299169933
Regret lava occupancy 0.08000000115943356
IRD lava occupancy 17.599462784490488
##############
Trial  22
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 1), (8, 1), (6, 1), (9, 3), (14, 1), (5, 1), (0, 3)]
w_map [ 2.71615169 -5.21962855  6.96075998 -2.15458114] loglik -2.7725848877107637
accepted/total = 1881/2000 = 0.9405
MAP Policy on Train MDP
map_weights [ 2.71615169 -5.21962855  6.96075998 -2.15458114]
map reward
2.72	-5.22	2.72	2.72	2.72	
2.72	2.72	2.72	2.72	2.72	
2.72	2.72	2.72	2.72	6.96	
2.72	2.72	-5.22	2.72	2.72	
2.72	2.72	2.72	2.72	2.72	
Map policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [ 2.6368718  -2.24475128  4.63531312 -2.27814153]
mean reward
2.64	-2.24	2.64	2.64	2.64	
2.64	2.64	2.64	2.64	2.64	
2.64	2.64	2.64	2.64	4.64	
2.64	2.64	-2.24	2.64	2.64	
2.64	2.64	2.64	2.64	2.64	
mean policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
Optimal Policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MAP policy loss 1.6744891208689272e-07
Mean policy loss 4.993498331842816e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
^	^	>	>	^	
>	>	>	^	^	
reward
-5.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	0 	0 	2 	
0 	3 	3 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 2.71615169 -5.21962855  6.96075998 -2.15458114]
map reward
-5.22	2.72	2.72	-5.22	2.72	
-5.22	2.72	2.72	2.72	-5.22	
2.72	2.72	2.72	2.72	6.96	
2.72	-2.15	-2.15	2.72	2.72	
2.72	2.72	2.72	2.72	2.72	
Map policy
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
^	^	>	>	^	
>	>	>	^	^	
MEAN policy on test env
mean_weights [ 2.6368718  -2.24475128  4.63531312 -2.27814153]
mean reward
-2.24	2.64	2.64	-2.24	2.64	
-2.24	2.64	2.64	2.64	-2.24	
2.64	2.64	2.64	2.64	4.64	
2.64	-2.28	-2.28	2.64	2.64	
2.64	2.64	2.64	2.64	2.64	
mean policy
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
^	^	>	>	^	
>	>	>	^	^	
features
1 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	0 	0 	2 	
0 	3 	3 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
^	^	>	>	^	
>	>	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	>	
>	>	>	v	v	
>	>	>	>	>	
^	^	>	>	^	
>	>	>	^	^	
-------- IRD Solution -------
ird reward
-155.47	-148.98	-148.98	-155.47	-148.98	
-155.47	-148.98	-148.98	-148.98	-155.47	
-148.98	-148.98	-148.98	-148.98	-144.25	
-148.98	-155.46	-155.46	-148.98	-148.98	
-148.98	-148.98	-148.98	-148.98	-148.98	
ird policy
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
^	^	>	>	^	
>	>	>	^	^	
MAP policy loss 5.093520958814844
mean policy loss 6.65250550047243e-07
robust policy loss 3.154621243933037e-07
regret policy loss 1.2920000038135546
ird policy loss 7.180554951486534e-07
MAP lava occupancy 0.1229855712046882
Mean lava occupancy 0.1229855712046882
Robust lava occupancy 0.0800000011722965
Regret lava occupancy 0.0800000000001076
IRD lava occupancy 0.08000000619630984
##############
Trial  23
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	>	>	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
demonstration
[(13, 1), (8, 3), (7, 1), (6, 1), (14, 1), (5, 1), (0, 3)]
w_map [1.64317749 0.68271591 5.48082475 3.26301408] loglik -0.6931427763101965
accepted/total = 1937/2000 = 0.9685
MAP Policy on Train MDP
map_weights [1.64317749 0.68271591 5.48082475 3.26301408]
map reward
1.64	0.68	1.64	1.64	1.64	
1.64	1.64	1.64	1.64	1.64	
1.64	1.64	0.68	1.64	5.48	
1.64	1.64	1.64	1.64	1.64	
1.64	1.64	1.64	1.64	0.68	
Map policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [3.11393151 1.1288958  5.11651645 2.11167603]
mean reward
3.11	1.13	3.11	3.11	3.11	
3.11	3.11	3.11	3.11	3.11	
3.11	3.11	1.13	3.11	5.12	
3.11	3.11	3.11	3.11	3.11	
3.11	3.11	3.11	3.11	1.13	
mean policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
Optimal Policy
v	>	>	>	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MAP policy loss 0.035587115100606165
Mean policy loss 0.03558652510245297
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
^	^	^	>	v	
^	>	^	>	>	
^	>	^	<	^	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-5.00	
-1.00	-5.00	-1.00	-100.00	1.00	
-1.00	-1.00	-1.00	-100.00	-100.00	
-100.00	-1.00	-1.00	-1.00	-5.00	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	0 	3 	2 	
0 	0 	0 	3 	3 	
3 	0 	0 	0 	1 	
MAP on testing env
map_weights [1.64317749 0.68271591 5.48082475 3.26301408]
map reward
1.64	1.64	1.64	1.64	1.64	
1.64	3.26	1.64	3.26	0.68	
1.64	0.68	1.64	3.26	5.48	
1.64	1.64	1.64	3.26	3.26	
3.26	1.64	1.64	1.64	0.68	
Map policy
>	v	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MEAN policy on test env
mean_weights [3.11393151 1.1288958  5.11651645 2.11167603]
mean reward
3.11	3.11	3.11	3.11	3.11	
3.11	2.11	3.11	2.11	1.13	
3.11	1.13	3.11	2.11	5.12	
3.11	3.11	3.11	2.11	2.11	
2.11	3.11	3.11	3.11	1.13	
mean policy
>	>	v	>	v	
>	>	v	v	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	^	^	^	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	0 	3 	2 	
0 	0 	0 	3 	3 	
3 	0 	0 	0 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	<	
^	>	v	v	v	
v	>	>	>	>	
>	>	^	>	^	
>	>	^	<	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	<	
^	>	v	v	v	
v	>	>	>	>	
>	>	^	>	^	
>	>	^	<	^	
-------- IRD Solution -------
ird reward
-123.77	-123.77	-123.77	-123.77	-123.77	
-123.77	-128.65	-123.77	-128.65	-126.21	
-123.77	-126.21	-123.77	-128.65	-121.71	
-123.77	-123.77	-123.77	-128.65	-128.65	
-128.65	-123.77	-123.77	-123.77	-126.21	
ird policy
>	>	>	>	v	
^	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	^	>	^	
MAP policy loss 100.75249914359878
mean policy loss 66.99168375671978
robust policy loss 65.09805962092844
regret policy loss 65.09805911651253
ird policy loss 43.729047439294355
MAP lava occupancy 1.3118355281672371
Mean lava occupancy 1.3118355281672371
Robust lava occupancy 0.9535898792378904
Regret lava occupancy 0.9535898747146986
IRD lava occupancy 0.731691739489626
##############
Trial  24
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	>	v	
>	>	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 1), (8, 1), (6, 1), (9, 3), (14, 1), (5, 1), (0, 3)]
w_map [-0.92706463 -1.3487284  -0.1618748   0.25495127] loglik 0.0
accepted/total = 1933/2000 = 0.9665
MAP Policy on Train MDP
map_weights [-0.92706463 -1.3487284  -0.1618748   0.25495127]
map reward
-0.93	-1.35	-0.93	-1.35	-0.93	
-0.93	-0.93	-0.93	-0.93	-0.93	
-0.93	-0.93	-0.93	-1.35	-0.16	
-0.93	-0.93	-0.93	-0.93	-0.93	
-0.93	-0.93	-0.93	-0.93	-0.93	
Map policy
v	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-1.55558495 -4.03536202  0.96691439 -1.48932304]
mean reward
-1.56	-4.04	-1.56	-4.04	-1.56	
-1.56	-1.56	-1.56	-1.56	-1.56	
-1.56	-1.56	-1.56	-4.04	0.97	
-1.56	-1.56	-1.56	-1.56	-1.56	
-1.56	-1.56	-1.56	-1.56	-1.56	
mean policy
v	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
v	>	v	>	v	
>	>	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 0.031976523618955824
Mean policy loss 0.03197652136968074
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	^	^	^	
>	^	^	<	^	
reward
-1.00	-5.00	-100.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-100.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	3 	1 	0 	
0 	0 	0 	0 	0 	
3 	0 	0 	0 	2 	
0 	0 	0 	3 	1 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.92706463 -1.3487284  -0.1618748   0.25495127]
map reward
-0.93	-1.35	0.25	-1.35	-0.93	
-0.93	-0.93	-0.93	-0.93	-0.93	
0.25	-0.93	-0.93	-0.93	-0.16	
-0.93	-0.93	-0.93	0.25	-1.35	
-0.93	-0.93	-0.93	-1.35	-0.93	
Map policy
v	>	^	<	<	
v	<	^	<	<	
<	<	<	<	^	
^	<	<	<	<	
^	^	<	^	^	
MEAN policy on test env
mean_weights [-1.55558495 -4.03536202  0.96691439 -1.48932304]
mean reward
-1.56	-4.04	-1.49	-4.04	-1.56	
-1.56	-1.56	-1.56	-1.56	-1.56	
-1.49	-1.56	-1.56	-1.56	0.97	
-1.56	-1.56	-1.56	-1.49	-4.04	
-1.56	-1.56	-1.56	-4.04	-1.56	
mean policy
v	>	v	>	v	
v	>	>	>	v	
>	>	>	>	>	
^	>	>	^	^	
^	>	^	^	^	
features
0 	1 	3 	1 	0 	
0 	0 	0 	0 	0 	
3 	0 	0 	0 	2 	
0 	0 	0 	3 	1 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	^	<	v	
v	<	^	<	v	
<	<	^	>	>	
^	^	>	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	^	^	^	
>	^	^	<	>	
-------- IRD Solution -------
ird reward
-66.57	-76.76	-69.76	-76.76	-66.57	
-66.57	-66.57	-66.57	-66.57	-66.57	
-69.76	-66.57	-66.57	-66.57	-63.90	
-66.57	-66.57	-66.57	-69.76	-76.76	
-66.57	-66.57	-66.57	-76.76	-66.57	
ird policy
v	v	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	^	^	
>	^	^	^	^	
MAP policy loss 22.72302907021438
mean policy loss 36.36705444451134
robust policy loss 1159.0612611988636
regret policy loss 1.2920001778095545
ird policy loss 3.628249500310759
MAP lava occupancy 0.3374969757465741
Mean lava occupancy 0.3374969757465741
Robust lava occupancy 11.62554071365088
Regret lava occupancy 0.12000000097534663
IRD lava occupancy 0.15800000000332337
##############
Trial  25
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(13, 1), (12, 1), (6, 3), (14, 1), (11, 1), (5, 1), (0, 3)]
w_map [-4.86709861 -5.94562753 -2.70377235  5.09347901] loglik -1.3862889719821396
accepted/total = 1855/2000 = 0.9275
MAP Policy on Train MDP
map_weights [-4.86709861 -5.94562753 -2.70377235  5.09347901]
map reward
-4.87	-4.87	-4.87	-4.87	-4.87	
-4.87	-4.87	-5.95	-4.87	-4.87	
-4.87	-4.87	-4.87	-4.87	-2.70	
-4.87	-4.87	-4.87	-4.87	-4.87	
-4.87	-4.87	-4.87	-4.87	-4.87	
Map policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-2.01848193 -3.62602463 -0.64574188  3.467779  ]
mean reward
-2.02	-2.02	-2.02	-2.02	-2.02	
-2.02	-2.02	-3.63	-2.02	-2.02	
-2.02	-2.02	-2.02	-2.02	-0.65	
-2.02	-2.02	-2.02	-2.02	-2.02	
-2.02	-2.02	-2.02	-2.02	-2.02	
mean policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 1.1212542076755039e-07
Mean policy loss 1.619176505157291e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	v	<	
v	>	>	v	v	
v	v	>	>	>	
>	>	>	^	^	
^	^	^	^	^	
reward
-100.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-5.00	-1.00	-100.00	
-1.00	-5.00	-5.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-5.00	-100.00	-100.00	-100.00	
features
3 	0 	0 	1 	3 	
0 	0 	1 	0 	3 	
0 	1 	1 	0 	2 	
0 	0 	0 	0 	0 	
3 	1 	3 	3 	3 	
MAP on testing env
map_weights [-4.86709861 -5.94562753 -2.70377235  5.09347901]
map reward
5.09	-4.87	-4.87	-5.95	5.09	
-4.87	-4.87	-5.95	-4.87	5.09	
-4.87	-5.95	-5.95	-4.87	-2.70	
-4.87	-4.87	-4.87	-4.87	-4.87	
5.09	-5.95	5.09	5.09	5.09	
Map policy
<	<	<	>	>	
^	<	>	>	^	
v	v	v	>	^	
v	>	v	v	v	
<	>	>	>	>	
MEAN policy on test env
mean_weights [-2.01848193 -3.62602463 -0.64574188  3.467779  ]
mean reward
3.47	-2.02	-2.02	-3.63	3.47	
-2.02	-2.02	-3.63	-2.02	3.47	
-2.02	-3.63	-3.63	-2.02	-0.65	
-2.02	-2.02	-2.02	-2.02	-2.02	
3.47	-3.63	3.47	3.47	3.47	
mean policy
<	<	<	>	>	
^	<	>	>	^	
v	v	v	>	^	
v	>	v	v	v	
<	>	>	>	>	
features
3 	0 	0 	1 	3 	
0 	0 	1 	0 	3 	
0 	1 	1 	0 	2 	
0 	0 	0 	0 	0 	
3 	1 	3 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
<	<	<	>	>	
^	<	>	>	^	
v	v	v	>	^	
v	>	v	v	v	
<	>	>	>	>	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
<	<	<	>	v	
^	<	>	>	^	
^	^	v	>	^	
v	>	v	v	v	
<	>	>	>	<	
-------- IRD Solution -------
ird reward
-26.68	-28.25	-28.25	-28.93	-26.68	
-28.25	-28.25	-28.93	-28.25	-26.68	
-28.25	-28.93	-28.93	-28.25	-26.84	
-28.25	-28.25	-28.25	-28.25	-28.25	
-26.68	-28.93	-26.68	-26.68	-26.68	
ird policy
<	<	<	>	>	
^	<	>	>	^	
v	v	v	>	^	
v	>	v	v	v	
<	>	>	>	>	
MAP policy loss 34.216059298807366
mean policy loss 1879.9962544296664
robust policy loss 1879.9962538593122
regret policy loss 1879.996235632767
ird policy loss 1879.9962546791485
MAP lava occupancy 0.6157304097991494
Mean lava occupancy 0.6157304097991494
Robust lava occupancy 18.939899994359372
Regret lava occupancy 18.939899812650765
IRD lava occupancy 18.939900002553088
##############
Trial  26
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(13, 1), (12, 1), (6, 3), (14, 1), (11, 1), (5, 1), (0, 3)]
w_map [-6.5631197  -1.01875854 -3.45017476 -1.15549485] loglik -2.0794380880251992
accepted/total = 1950/2000 = 0.975
MAP Policy on Train MDP
map_weights [-6.5631197  -1.01875854 -3.45017476 -1.15549485]
map reward
-6.56	-6.56	-6.56	-6.56	-6.56	
-6.56	-6.56	-6.56	-6.56	-6.56	
-6.56	-6.56	-6.56	-6.56	-3.45	
-6.56	-6.56	-1.02	-6.56	-6.56	
-6.56	-6.56	-6.56	-6.56	-6.56	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
MEAN policy on Train MDP
mean_weights [-6.4917165  -3.8233615  -2.56630483 -1.14044158]
mean reward
-6.49	-6.49	-6.49	-6.49	-6.49	
-6.49	-6.49	-6.49	-6.49	-6.49	
-6.49	-6.49	-6.49	-6.49	-2.57	
-6.49	-6.49	-3.82	-6.49	-6.49	
-6.49	-6.49	-6.49	-6.49	-6.49	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MAP policy loss 0.7299805596198591
Mean policy loss 0.72998000170887
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	>	v	<	
>	>	>	v	v	
^	>	>	>	>	
v	>	>	^	^	
>	>	^	^	^	
reward
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-5.00	-1.00	-1.00	1.00	
-1.00	-100.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-100.00	-100.00	
features
1 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	0 	0 	2 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	3 	
MAP on testing env
map_weights [-6.5631197  -1.01875854 -3.45017476 -1.15549485]
map reward
-1.02	-6.56	-1.02	-6.56	-6.56	
-6.56	-6.56	-6.56	-6.56	-1.02	
-1.16	-1.02	-6.56	-6.56	-3.45	
-6.56	-1.16	-6.56	-6.56	-1.16	
-6.56	-6.56	-6.56	-1.16	-1.16	
Map policy
<	<	^	<	v	
^	v	^	>	>	
>	<	<	>	^	
>	^	<	>	^	
^	^	>	>	^	
MEAN policy on test env
mean_weights [-6.4917165  -3.8233615  -2.56630483 -1.14044158]
mean reward
-3.82	-6.49	-3.82	-6.49	-6.49	
-6.49	-6.49	-6.49	-6.49	-3.82	
-1.14	-3.82	-6.49	-6.49	-2.57	
-6.49	-1.14	-6.49	-6.49	-1.14	
-6.49	-6.49	-6.49	-1.14	-1.14	
mean policy
v	<	<	v	v	
v	v	v	>	v	
<	<	<	>	v	
^	^	<	>	v	
^	^	>	v	<	
features
1 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	0 	0 	2 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	<	^	>	v	
v	v	>	>	v	
<	<	<	>	>	
^	^	<	>	^	
^	^	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	>	>	v	
v	v	>	v	v	
<	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
11.43	9.92	11.43	9.92	9.92	
9.92	9.92	9.92	9.92	11.43	
11.88	11.43	9.92	9.92	11.43	
9.92	11.88	9.92	9.92	11.88	
9.92	9.92	9.92	11.88	11.88	
ird policy
v	<	<	>	v	
v	v	v	>	v	
<	<	<	>	v	
^	^	<	>	v	
^	^	>	v	>	
MAP policy loss 24.649987884352683
mean policy loss 1827.8103792536929
robust policy loss 912.7652292252025
regret policy loss 281.57426896480285
ird policy loss 1827.8103783050205
MAP lava occupancy 0.47931753464765514
Mean lava occupancy 0.47931753464765514
Robust lava occupancy 9.247519342811767
Regret lava occupancy 3.0422720228064626
IRD lava occupancy 18.34860998854255
##############
Trial  27
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	1 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (1, 3), (7, 1), (8, 1), (6, 1), (9, 3), (14, 1)]
w_map [ 1.99422339 -2.35998273  3.75596014  4.51087845] loglik -3.465731686435902
accepted/total = 1912/2000 = 0.956
MAP Policy on Train MDP
map_weights [ 1.99422339 -2.35998273  3.75596014  4.51087845]
map reward
1.99	1.99	1.99	1.99	-2.36	
1.99	1.99	1.99	1.99	1.99	
1.99	1.99	1.99	1.99	3.76	
1.99	1.99	1.99	1.99	-2.36	
-2.36	1.99	1.99	1.99	1.99	
Map policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.33672843 -3.84767288  1.91249458  3.92352342]
mean reward
0.34	0.34	0.34	0.34	-3.85	
0.34	0.34	0.34	0.34	0.34	
0.34	0.34	0.34	0.34	1.91	
0.34	0.34	0.34	0.34	-3.85	
-3.85	0.34	0.34	0.34	0.34	
mean policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	^	^	<	
MAP policy loss 2.98107708995507e-07
Mean policy loss 1.1689454884611372e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-100.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-100.00	
-100.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 1.99422339 -2.35998273  3.75596014  4.51087845]
map reward
1.99	4.51	-2.36	1.99	1.99	
1.99	-2.36	1.99	1.99	4.51	
4.51	1.99	1.99	1.99	3.76	
1.99	1.99	1.99	1.99	1.99	
1.99	1.99	1.99	1.99	1.99	
Map policy
>	^	<	>	v	
v	^	>	>	>	
<	<	<	>	^	
^	<	<	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.33672843 -3.84767288  1.91249458  3.92352342]
mean reward
0.34	3.92	-3.85	0.34	0.34	
0.34	-3.85	0.34	0.34	3.92	
3.92	0.34	0.34	0.34	1.91	
0.34	0.34	0.34	0.34	0.34	
0.34	0.34	0.34	0.34	0.34	
mean policy
>	^	<	>	v	
v	^	>	>	>	
<	<	<	>	^	
^	<	<	^	^	
^	^	<	^	^	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	^	<	>	v	
v	^	>	>	>	
<	<	<	>	^	
^	<	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	<	>	v	v	
v	>	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-82.41	-84.10	-89.39	-82.41	-82.41	
-82.41	-89.39	-82.41	-82.41	-84.10	
-84.10	-82.41	-82.41	-82.41	-81.46	
-82.41	-82.41	-82.41	-82.41	-82.41	
-82.41	-82.41	-82.41	-82.41	-82.41	
ird policy
v	<	v	v	v	
v	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 21.898612855536275
mean policy loss 1836.8855520851403
robust policy loss 1836.884691109261
regret policy loss 30.60881187815115
ird policy loss 14.024625306564815
MAP lava occupancy 0.34189662149426897
Mean lava occupancy 0.34189662149426897
Robust lava occupancy 18.340801446661352
Regret lava occupancy 0.43391204096083635
IRD lava occupancy 0.26639500433440905
##############
Trial  28
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (1, 3), (7, 1), (8, 1), (6, 1), (9, 3), (14, 1)]
w_map [ 0.574118   -4.16799733  2.86919097 -1.51039976] loglik -3.4656259733383195
accepted/total = 1939/2000 = 0.9695
MAP Policy on Train MDP
map_weights [ 0.574118   -4.16799733  2.86919097 -1.51039976]
map reward
0.57	-4.17	0.57	0.57	0.57	
-4.17	0.57	0.57	0.57	0.57	
0.57	0.57	0.57	0.57	2.87	
0.57	0.57	0.57	0.57	0.57	
-4.17	0.57	0.57	0.57	0.57	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 1.50043534 -2.76641103  2.79997081  0.12554209]
mean reward
1.50	-2.77	1.50	1.50	1.50	
-2.77	1.50	1.50	1.50	1.50	
1.50	1.50	1.50	1.50	2.80	
1.50	1.50	1.50	1.50	1.50	
-2.77	1.50	1.50	1.50	1.50	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 1.8234370152692136e-06
Mean policy loss 7.596598558534673e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	>	>	v	
v	v	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	0 	1 	2 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.574118   -4.16799733  2.86919097 -1.51039976]
map reward
0.57	0.57	0.57	-4.17	0.57	
0.57	-4.17	-4.17	0.57	0.57	
0.57	0.57	0.57	-4.17	2.87	
0.57	0.57	0.57	0.57	0.57	
-4.17	0.57	0.57	0.57	0.57	
Map policy
v	>	>	>	v	
v	v	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 1.50043534 -2.76641103  2.79997081  0.12554209]
mean reward
1.50	1.50	1.50	-2.77	1.50	
1.50	-2.77	-2.77	1.50	1.50	
1.50	1.50	1.50	-2.77	2.80	
1.50	1.50	1.50	1.50	1.50	
-2.77	1.50	1.50	1.50	1.50	
mean policy
v	<	>	>	v	
v	v	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	0 	1 	2 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	<	>	>	v	
v	v	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 4.34816219  0.95       14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	<	>	>	v	
v	v	>	>	v	
v	v	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-90.13	-90.13	-90.13	-94.86	-90.13	
-90.13	-94.86	-94.86	-90.13	-90.13	
-90.13	-90.13	-90.13	-94.86	-88.74	
-90.13	-90.13	-90.13	-90.13	-90.13	
-94.86	-90.13	-90.13	-90.13	-90.13	
ird policy
v	<	>	>	v	
v	v	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 1.275728430867756
mean policy loss 0.08525048535619403
robust policy loss 0.08525260859852614
regret policy loss 0.0852504440181343
ird policy loss 0.08525048781127403
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  29
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
^	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(0, 1), (7, 3), (13, 1), (12, 1), (14, 1), (2, 3), (1, 1)]
w_map [0.87636289 0.52542914 4.74542443 2.65323825] loglik -2.7725616630123113
accepted/total = 1873/2000 = 0.9365
MAP Policy on Train MDP
map_weights [0.87636289 0.52542914 4.74542443 2.65323825]
map reward
0.88	0.88	0.88	0.88	0.53	
0.88	0.88	0.88	0.88	0.88	
0.88	0.53	0.88	0.88	4.75	
0.88	0.88	0.88	0.88	0.88	
0.88	0.88	0.53	0.88	0.88	
Map policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
MEAN policy on Train MDP
mean_weights [ 0.30027854 -1.02548502  2.4158115   2.63629902]
mean reward
0.30	0.30	0.30	0.30	-1.03	
0.30	0.30	0.30	0.30	0.30	
0.30	-1.03	0.30	0.30	2.42	
0.30	0.30	0.30	0.30	0.30	
0.30	0.30	-1.03	0.30	0.30	
mean policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
^	>	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
MAP policy loss 0.024938643387790574
Mean policy loss 0.024937050962956217
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
>	>	>	>	v	
^	v	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-100.00	-5.00	-1.00	1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	1 	0 	2 	
3 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [0.87636289 0.52542914 4.74542443 2.65323825]
map reward
0.88	0.88	0.88	0.88	0.88	
0.88	0.88	0.88	0.88	0.88	
0.53	2.65	0.53	0.88	4.75	
2.65	0.88	0.88	0.88	0.88	
0.88	0.88	0.88	0.88	0.88	
Map policy
>	v	>	>	v	
>	v	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
^	^	>	>	^	
MEAN policy on test env
mean_weights [ 0.30027854 -1.02548502  2.4158115   2.63629902]
mean reward
0.30	0.30	0.30	0.30	0.30	
0.30	0.30	0.30	0.30	0.30	
-1.03	2.64	-1.03	0.30	2.42	
2.64	0.30	0.30	0.30	0.30	
0.30	0.30	0.30	0.30	0.30	
mean policy
v	v	<	v	v	
v	v	<	v	v	
v	v	<	>	>	
<	<	<	<	^	
^	<	<	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	1 	0 	2 	
3 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	<	<	<	
v	v	<	<	<	
v	v	<	<	<	
<	<	<	<	<	
^	<	<	<	<	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	>	>	>	v	
>	^	>	>	>	
<	>	>	>	^	
^	>	>	>	^	
-------- IRD Solution -------
ird reward
-125.13	-125.13	-125.13	-125.13	-125.13	
-125.13	-125.13	-125.13	-125.13	-125.13	
-126.59	-125.25	-126.59	-125.13	-117.82	
-125.25	-125.13	-125.13	-125.13	-125.13	
-125.13	-125.13	-125.13	-125.13	-125.13	
ird policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 5.873963377266569
mean policy loss 1221.5009688413356
robust policy loss 1738.0755334828882
regret policy loss 39.46183954096903
ird policy loss 3.797586539532678
MAP lava occupancy 0.1347147018971886
Mean lava occupancy 0.1347147018971886
Robust lava occupancy 17.28973742321881
Regret lava occupancy 0.4733820143255423
IRD lava occupancy 0.11800000022306817
##############
Trial  30
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (1, 3), (7, 1), (8, 1), (6, 1), (9, 3), (14, 1)]
w_map [ 1.22105074 -2.8921326   3.50243196  2.26880149] loglik -3.465733137474672
accepted/total = 1967/2000 = 0.9835
MAP Policy on Train MDP
map_weights [ 1.22105074 -2.8921326   3.50243196  2.26880149]
map reward
1.22	1.22	1.22	1.22	-2.89	
1.22	1.22	1.22	1.22	1.22	
1.22	1.22	1.22	1.22	3.50	
1.22	1.22	1.22	-2.89	1.22	
1.22	1.22	1.22	1.22	1.22	
Map policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-1.10912619 -4.61223269  2.12369988  0.76335912]
mean reward
-1.11	-1.11	-1.11	-1.11	-4.61	
-1.11	-1.11	-1.11	-1.11	-1.11	
-1.11	-1.11	-1.11	-1.11	2.12	
-1.11	-1.11	-1.11	-4.61	-1.11	
-1.11	-1.11	-1.11	-1.11	-1.11	
mean policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MAP policy loss 2.6930033556436805e-07
Mean policy loss -1.2005472205776913e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	v	
>	v	v	v	v	
>	>	v	>	>	
>	>	>	>	^	
>	>	^	>	^	
reward
-100.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-5.00	1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
3 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	0 	1 	2 	
3 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 1.22105074 -2.8921326   3.50243196  2.26880149]
map reward
2.27	2.27	1.22	1.22	1.22	
1.22	1.22	1.22	1.22	-2.89	
-2.89	1.22	1.22	-2.89	3.50	
2.27	1.22	1.22	1.22	1.22	
1.22	1.22	1.22	-2.89	1.22	
Map policy
>	>	>	v	v	
>	>	>	>	v	
v	>	>	>	>	
>	>	>	>	^	
^	>	^	>	^	
MEAN policy on test env
mean_weights [-1.10912619 -4.61223269  2.12369988  0.76335912]
mean reward
0.76	0.76	-1.11	-1.11	-1.11	
-1.11	-1.11	-1.11	-1.11	-4.61	
-4.61	-1.11	-1.11	-4.61	2.12	
0.76	-1.11	-1.11	-1.11	-1.11	
-1.11	-1.11	-1.11	-4.61	-1.11	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	^	>	^	
features
3 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	0 	1 	2 	
3 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
<	<	<	<	v	
^	^	<	>	v	
v	^	>	>	>	
<	<	>	>	^	
^	<	^	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	<	
>	v	v	<	v	
>	v	v	>	>	
>	>	>	>	^	
>	>	^	>	^	
-------- IRD Solution -------
ird reward
-67.86	-67.86	-71.08	-71.08	-71.08	
-71.08	-71.08	-71.08	-71.08	-75.13	
-75.13	-71.08	-71.08	-75.13	-64.32	
-67.86	-71.08	-71.08	-71.08	-71.08	
-71.08	-71.08	-71.08	-75.13	-71.08	
ird policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	^	>	^	
MAP policy loss 4.940662566656087
mean policy loss 7.615213369897321
robust policy loss 896.0408752078706
regret policy loss 0.456440659235145
ird policy loss 7.6152130388216115
MAP lava occupancy 0.15836836328035328
Mean lava occupancy 0.15836836328035328
Robust lava occupancy 9.028477825921202
Regret lava occupancy 0.12000000010965306
IRD lava occupancy 0.1960000000121266
##############
Trial  31
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 3), (13, 1), (12, 1), (6, 1), (14, 1), (5, 1), (0, 3)]
w_map [-0.18497261 -0.18295346  0.27474365 -0.39455636] loglik -2.706177726471253
accepted/total = 1917/2000 = 0.9585
MAP Policy on Train MDP
map_weights [-0.18497261 -0.18295346  0.27474365 -0.39455636]
map reward
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	0.27	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 2.50862216e-04 -2.23409318e+00  1.14568872e+00 -1.96161042e-01]
mean reward
0.00	0.00	0.00	0.00	0.00	
0.00	0.00	0.00	0.00	-2.23	
0.00	0.00	0.00	0.00	1.15	
0.00	0.00	0.00	0.00	0.00	
0.00	0.00	0.00	0.00	0.00	
mean policy
>	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 1.1108168682635138
Mean policy loss 3.390583815784595e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	0 	0 	2 	
0 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.18497261 -0.18295346  0.27474365 -0.39455636]
map reward
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.39	
-0.18	-0.18	-0.18	-0.18	0.27	
-0.18	-0.18	-0.18	-0.39	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
Map policy
>	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 2.50862216e-04 -2.23409318e+00  1.14568872e+00 -1.96161042e-01]
mean reward
0.00	-2.23	0.00	0.00	0.00	
0.00	0.00	0.00	0.00	-0.20	
0.00	0.00	0.00	0.00	1.15	
0.00	0.00	0.00	-0.20	0.00	
0.00	0.00	0.00	0.00	0.00	
mean policy
v	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	0 	0 	2 	
0 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-40.45	-42.40	-40.45	-40.45	-40.45	
-40.45	-40.45	-40.45	-40.45	-41.46	
-40.45	-40.45	-40.45	-40.45	-39.08	
-40.45	-40.45	-40.45	-41.46	-40.45	
-40.45	-40.45	-40.45	-40.45	-40.45	
ird policy
v	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MAP policy loss 42.97438310758092
mean policy loss 3.621210239498426
robust policy loss 69.9195188607036
regret policy loss 1.0644205655774286e-08
ird policy loss 3.6212103914129594
MAP lava occupancy 0.5145189611255233
Mean lava occupancy 0.5145189611255233
Robust lava occupancy 0.7848005230853199
Regret lava occupancy 0.08000000003667734
IRD lava occupancy 0.1180000036425262
##############
Trial  32
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	<	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	<	>	>	^	
reward
-1.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	2 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(13, 1), (12, 1), (6, 3), (14, 1), (11, 1), (5, 1), (0, 3)]
w_map [-0.49403219 -4.18925909  1.84816442 -3.93526701] loglik -1.386283122596069
accepted/total = 1947/2000 = 0.9735
MAP Policy on Train MDP
map_weights [-0.49403219 -4.18925909  1.84816442 -3.93526701]
map reward
-0.49	-0.49	-0.49	-4.19	-4.19	
-0.49	-0.49	-4.19	-0.49	-0.49	
-0.49	-0.49	-0.49	-0.49	1.85	
-0.49	-4.19	-0.49	-0.49	-0.49	
-0.49	-0.49	-4.19	-0.49	-0.49	
Map policy
v	v	v	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-1.58892221 -4.56499311 -0.00810717 -2.87475397]
mean reward
-1.59	-1.59	-1.59	-4.56	-4.56	
-1.59	-1.59	-4.56	-1.59	-1.59	
-1.59	-1.59	-1.59	-1.59	-0.01	
-1.59	-4.56	-1.59	-1.59	-1.59	
-1.59	-1.59	-4.56	-1.59	-1.59	
mean policy
v	v	<	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	<	>	>	^	
Optimal Policy
v	v	<	v	v	
>	v	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	<	>	>	^	
MAP policy loss 0.05622724196679137
Mean policy loss -4.963674326077805e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	>	v	
>	>	v	>	v	
v	>	v	>	>	
v	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-100.00	1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	2 	
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.49403219 -4.18925909  1.84816442 -3.93526701]
map reward
-0.49	-0.49	-0.49	-3.94	-0.49	
-3.94	-0.49	-0.49	-4.19	-0.49	
-0.49	-3.94	-0.49	-3.94	1.85	
-0.49	-3.94	-0.49	-0.49	-0.49	
-0.49	-0.49	-0.49	-0.49	-0.49	
Map policy
>	>	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [-1.58892221 -4.56499311 -0.00810717 -2.87475397]
mean reward
-1.59	-1.59	-1.59	-2.87	-1.59	
-2.87	-1.59	-1.59	-4.56	-1.59	
-1.59	-2.87	-1.59	-2.87	-0.01	
-1.59	-2.87	-1.59	-1.59	-1.59	
-1.59	-1.59	-1.59	-1.59	-1.59	
mean policy
>	>	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	2 	
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	>	>	v	
<	v	v	v	v	
>	>	>	>	>	
>	^	>	^	^	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	>	v	
>	>	v	>	v	
v	>	v	>	>	
v	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-27.12	-27.12	-27.12	-29.56	-27.12	
-29.56	-27.12	-27.12	-30.62	-27.12	
-27.12	-29.56	-27.12	-29.56	-23.43	
-27.12	-29.56	-27.12	-27.12	-27.12	
-27.12	-27.12	-27.12	-27.12	-27.12	
ird policy
>	>	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 76.9983783008314
mean policy loss 36.75351895122513
robust policy loss 192.27723565201507
regret policy loss 5.364312636235366e-08
ird policy loss 36.75351896091937
MAP lava occupancy 0.9881813966063502
Mean lava occupancy 0.9881813966063502
Robust lava occupancy 2.1409828479396795
Regret lava occupancy 0.2000000003154153
IRD lava occupancy 0.5851967377131851
##############
Trial  33
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	>	v	
>	>	>	>	v	
>	>	^	>	>	
>	>	v	>	^	
>	>	>	>	^	
reward
-5.00	-5.00	-5.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	1 	1 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	2 	
1 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 1), (8, 1), (6, 1), (9, 3), (14, 1), (5, 1), (0, 3)]
w_map [ 0.14183066 -0.51694579  0.72360697  0.02427993] loglik 0.0
accepted/total = 1833/2000 = 0.9165
MAP Policy on Train MDP
map_weights [ 0.14183066 -0.51694579  0.72360697  0.02427993]
map reward
-0.52	-0.52	-0.52	-0.52	0.14	
0.14	0.14	0.14	0.14	0.14	
0.14	0.14	0.14	-0.52	0.72	
-0.52	0.14	0.14	-0.52	0.14	
0.14	0.14	0.14	0.14	0.14	
Map policy
v	v	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 1.44328196 -0.50064323  2.69137428 -6.09991376]
mean reward
-0.50	-0.50	-0.50	-0.50	1.44	
1.44	1.44	1.44	1.44	1.44	
1.44	1.44	1.44	-0.50	2.69	
-0.50	1.44	1.44	-0.50	1.44	
1.44	1.44	1.44	1.44	1.44	
mean policy
v	v	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
Optimal Policy
v	v	v	>	v	
>	>	>	>	v	
>	>	^	>	>	
>	>	v	>	^	
>	>	>	>	^	
MAP policy loss 0.062354219438011504
Mean policy loss 0.062354219407359635
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
>	>	>	>	v	
>	>	^	>	>	
>	>	>	>	^	
^	>	>	>	^	
reward
-100.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-100.00	1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-100.00	-1.00	-1.00	-1.00	
features
3 	1 	3 	0 	0 	
0 	1 	0 	0 	0 	
1 	0 	0 	3 	2 	
1 	0 	0 	0 	0 	
1 	3 	0 	0 	0 	
MAP on testing env
map_weights [ 0.14183066 -0.51694579  0.72360697  0.02427993]
map reward
0.02	-0.52	0.02	0.14	0.14	
0.14	-0.52	0.14	0.14	0.14	
-0.52	0.14	0.14	0.02	0.72	
-0.52	0.14	0.14	0.14	0.14	
-0.52	0.02	0.14	0.14	0.14	
Map policy
v	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 1.44328196 -0.50064323  2.69137428 -6.09991376]
mean reward
-6.10	-0.50	-6.10	1.44	1.44	
1.44	-0.50	1.44	1.44	1.44	
-0.50	1.44	1.44	-6.10	2.69	
-0.50	1.44	1.44	1.44	1.44	
-0.50	-6.10	1.44	1.44	1.44	
mean policy
v	v	>	>	v	
>	>	>	>	v	
>	>	^	>	>	
>	>	>	>	^	
^	>	>	>	^	
features
3 	1 	3 	0 	0 	
0 	1 	0 	0 	0 	
1 	0 	0 	3 	2 	
1 	0 	0 	0 	0 	
1 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	>	>	v	
>	>	>	>	v	
>	>	v	>	>	
>	>	>	>	^	
^	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	>	>	v	
>	>	>	>	v	
>	>	^	>	>	
>	>	>	>	^	
^	>	>	>	^	
-------- IRD Solution -------
ird reward
-92.20	-82.67	-92.20	-79.48	-79.48	
-79.48	-82.67	-79.48	-79.48	-79.48	
-82.67	-79.48	-79.48	-92.20	-78.73	
-82.67	-79.48	-79.48	-79.48	-79.48	
-82.67	-92.20	-79.48	-79.48	-79.48	
ird policy
v	v	>	>	v	
>	>	>	>	v	
>	>	^	>	>	
>	>	>	>	^	
^	>	>	>	^	
MAP policy loss 24.193583264575434
mean policy loss 1.5886254670638422e-06
robust policy loss 1.3281731947589748e-07
regret policy loss -9.554054603011508e-09
ird policy loss 5.140909227763224e-07
MAP lava occupancy 0.4093702499929917
Mean lava occupancy 0.4093702499929917
Robust lava occupancy 0.16000000076026635
Regret lava occupancy 0.1600000000001548
IRD lava occupancy 0.1600000032664706
##############
Trial  34
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
v	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	1 	0 	0 	1 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
demonstration
[(0, 1), (13, 1), (8, 3), (7, 1), (14, 1), (2, 3), (1, 1)]
w_map [-0.12912083 -1.74298712  6.09615286 -2.57020172] loglik -1.3860355808974418
accepted/total = 1988/2000 = 0.994
MAP Policy on Train MDP
map_weights [-0.12912083 -1.74298712  6.09615286 -2.57020172]
map reward
-0.13	-0.13	-0.13	-0.13	-0.13	
-1.74	-1.74	-0.13	-0.13	-1.74	
-0.13	-0.13	-0.13	-0.13	6.10	
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.13	-1.74	-0.13	
Map policy
>	>	v	v	v	
v	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
MEAN policy on Train MDP
mean_weights [ 1.72457079 -3.43563355  6.13415055 -2.11844062]
mean reward
1.72	1.72	1.72	1.72	1.72	
-3.44	-3.44	1.72	1.72	-3.44	
1.72	1.72	1.72	1.72	6.13	
1.72	1.72	1.72	1.72	1.72	
1.72	1.72	1.72	-3.44	1.72	
mean policy
>	>	v	v	v	
v	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
Optimal Policy
>	>	v	v	<	
v	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
MAP policy loss 0.01121948762392605
Mean policy loss 0.011210123698170676
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	<	>	>	^	
reward
-1.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-5.00	-5.00	-100.00	-1.00	-1.00	
features
0 	1 	0 	0 	1 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	2 	
0 	3 	0 	0 	0 	
1 	1 	3 	0 	0 	
MAP on testing env
map_weights [-0.12912083 -1.74298712  6.09615286 -2.57020172]
map reward
-0.13	-1.74	-0.13	-0.13	-1.74	
-0.13	-0.13	-0.13	-1.74	-0.13	
-0.13	-0.13	-0.13	-0.13	6.10	
-0.13	-2.57	-0.13	-0.13	-0.13	
-1.74	-1.74	-2.57	-0.13	-0.13	
Map policy
v	>	v	v	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	>	^	
MEAN policy on test env
mean_weights [ 1.72457079 -3.43563355  6.13415055 -2.11844062]
mean reward
1.72	-3.44	1.72	1.72	-3.44	
1.72	1.72	1.72	-3.44	1.72	
1.72	1.72	1.72	1.72	6.13	
1.72	-2.12	1.72	1.72	1.72	
-3.44	-3.44	-2.12	1.72	1.72	
mean policy
v	>	v	v	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	>	^	
features
0 	1 	0 	0 	1 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	2 	
0 	3 	0 	0 	0 	
1 	1 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	>	^	
-------- IRD Solution -------
ird reward
-192.44	-200.74	-192.44	-192.44	-200.74	
-192.44	-192.44	-192.44	-200.74	-192.44	
-192.44	-192.44	-192.44	-192.44	-184.71	
-192.44	-198.83	-192.44	-192.44	-192.44	
-200.74	-200.74	-198.83	-192.44	-192.44	
ird policy
v	v	v	v	v	
>	>	v	>	v	
>	>	>	>	>	
^	>	>	>	^	
^	>	>	>	^	
MAP policy loss 9.720526843575866
mean policy loss 3.5075431698852446
robust policy loss 3.5075405894107305
regret policy loss 3.4892901751376777
ird policy loss 3.5075396736944935
MAP lava occupancy 0.17364486705985166
Mean lava occupancy 0.17364486705985166
Robust lava occupancy 0.1180000063327409
Regret lava occupancy 0.11800000001151889
IRD lava occupancy 0.1180000000055612
##############
Trial  35
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	v	>	v	
v	>	>	>	>	
>	>	>	>	^	
^	>	>	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
features
1 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	1 	0 	0 	2 	
0 	0 	0 	0 	0 	
1 	1 	0 	0 	1 	
demonstration
[(0, 1), (7, 3), (13, 1), (1, 3), (12, 1), (6, 1), (14, 1)]
w_map [ 2.26362081 -5.25168466  8.78954425 -5.44183461] loglik -1.3861952383449534
accepted/total = 1951/2000 = 0.9755
MAP Policy on Train MDP
map_weights [ 2.26362081 -5.25168466  8.78954425 -5.44183461]
map reward
-5.25	2.26	2.26	2.26	2.26	
2.26	2.26	2.26	-5.25	2.26	
2.26	-5.25	2.26	2.26	8.79	
2.26	2.26	2.26	2.26	2.26	
-5.25	-5.25	2.26	2.26	-5.25	
Map policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	^	^	
MEAN policy on Train MDP
mean_weights [ 3.0526809  -5.37021564  6.10562223 -5.17558984]
mean reward
-5.37	3.05	3.05	3.05	3.05	
3.05	3.05	3.05	-5.37	3.05	
3.05	-5.37	3.05	3.05	6.11	
3.05	3.05	3.05	3.05	3.05	
-5.37	-5.37	3.05	3.05	-5.37	
mean policy
>	>	>	>	v	
>	>	v	>	v	
v	>	>	>	>	
>	>	>	>	^	
^	>	>	^	^	
Optimal Policy
>	>	>	>	v	
>	>	v	>	v	
v	>	>	>	>	
>	>	>	>	^	
^	>	>	^	^	
MAP policy loss 0.024938796997032414
Mean policy loss 2.3103028631930173e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-5.00	
-5.00	-1.00	-1.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	0 	0 	
0 	0 	0 	3 	1 	
1 	0 	0 	1 	2 	
0 	0 	0 	0 	1 	
3 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 2.26362081 -5.25168466  8.78954425 -5.44183461]
map reward
2.26	-5.25	-5.44	2.26	2.26	
2.26	2.26	2.26	-5.44	-5.25	
-5.25	2.26	2.26	-5.25	8.79	
2.26	2.26	2.26	2.26	-5.25	
-5.44	2.26	2.26	2.26	2.26	
Map policy
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [ 3.0526809  -5.37021564  6.10562223 -5.17558984]
mean reward
3.05	-5.37	-5.18	3.05	3.05	
3.05	3.05	3.05	-5.18	-5.37	
-5.37	3.05	3.05	-5.37	6.11	
3.05	3.05	3.05	3.05	-5.37	
-5.18	3.05	3.05	3.05	3.05	
mean policy
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
features
0 	1 	3 	0 	0 	
0 	0 	0 	3 	1 	
1 	0 	0 	1 	2 	
0 	0 	0 	0 	1 	
3 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-138.80	-146.46	-147.57	-138.80	-138.80	
-138.80	-138.80	-138.80	-147.57	-146.46	
-146.46	-138.80	-138.80	-146.46	-132.22	
-138.80	-138.80	-138.80	-138.80	-146.46	
-147.57	-138.80	-138.80	-138.80	-138.80	
ird policy
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 4.610052782219101
mean policy loss 5.267266577763108e-09
robust policy loss 1.6151730952707055e-08
regret policy loss 1.2178749555590818
ird policy loss 1.2784516500996546e-11
MAP lava occupancy 0.16554345670988846
Mean lava occupancy 0.16554345670988846
Robust lava occupancy 0.12000000014241864
Regret lava occupancy 0.12000000000014881
IRD lava occupancy 0.12000000000126085
##############
Trial  36
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	>	v	
^	>	>	>	v	
>	>	>	>	>	
>	v	>	>	^	
>	>	>	^	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	0 	0 	0 	
0 	1 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (13, 1), (12, 1), (14, 1), (2, 3), (1, 1)]
w_map [-0.042543   -1.81621174  5.63120505 -0.76565062] loglik -0.6931067238911055
accepted/total = 1920/2000 = 0.96
MAP Policy on Train MDP
map_weights [-0.042543   -1.81621174  5.63120505 -0.76565062]
map reward
-0.04	-0.04	-0.04	-1.82	-0.04	
-0.04	-1.82	-0.04	-0.04	-0.04	
-0.04	-1.82	-0.04	-0.04	5.63	
-0.04	-0.04	-1.82	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
Map policy
>	>	v	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [-0.03199829 -1.33621536  3.79232918 -1.37404666]
mean reward
-0.03	-0.03	-0.03	-1.34	-0.03	
-0.03	-1.34	-0.03	-0.03	-0.03	
-0.03	-1.34	-0.03	-0.03	3.79	
-0.03	-0.03	-1.34	-0.03	-0.03	
-0.03	-0.03	-0.03	-0.03	-0.03	
mean policy
>	>	v	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
Optimal Policy
>	>	v	>	v	
^	>	>	>	v	
>	>	>	>	>	
>	v	>	>	^	
>	>	>	^	^	
MAP policy loss 0.07232526145595042
Mean policy loss 0.07231747845857422
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	>	>	v	
>	>	>	>	v	
>	^	>	>	>	
^	^	<	>	^	
^	^	<	<	^	
reward
-100.00	-1.00	-100.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	1.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-100.00	
features
3 	0 	3 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	3 	0 	2 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	3 	
MAP on testing env
map_weights [-0.042543   -1.81621174  5.63120505 -0.76565062]
map reward
-0.77	-0.04	-0.77	-0.04	-0.04	
-1.82	-1.82	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.77	-0.04	5.63	
-0.04	-0.04	-1.82	-0.77	-0.04	
-0.04	-0.04	-0.04	-0.77	-0.77	
Map policy
>	>	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	^	>	^	^	
MEAN policy on test env
mean_weights [-0.03199829 -1.33621536  3.79232918 -1.37404666]
mean reward
-1.37	-0.03	-1.37	-0.03	-0.03	
-1.34	-1.34	-0.03	-0.03	-0.03	
-0.03	-0.03	-1.37	-0.03	3.79	
-0.03	-0.03	-1.34	-1.37	-0.03	
-0.03	-0.03	-0.03	-1.37	-1.37	
mean policy
>	v	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
^	^	^	>	^	
features
3 	0 	3 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	3 	0 	2 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	^	>	>	v	
v	>	>	>	v	
>	>	>	>	>	
^	^	>	>	^	
^	^	<	^	^	
-------- IRD Solution -------
ird reward
-93.90	-92.70	-93.90	-92.70	-92.70	
-93.72	-93.72	-92.70	-92.70	-92.70	
-92.70	-92.70	-93.90	-92.70	-87.19	
-92.70	-92.70	-93.72	-93.90	-92.70	
-92.70	-92.70	-92.70	-93.90	-93.90	
ird policy
>	v	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
^	^	^	>	^	
MAP policy loss 55.25382387502573
mean policy loss 32.57841295324353
robust policy loss 29.335212260951728
regret policy loss 32.92448205479158
ird policy loss 29.335198781413816
MAP lava occupancy 0.8295283664440438
Mean lava occupancy 0.8295283664440438
Robust lava occupancy 0.5634703718168721
Regret lava occupancy 0.5942454999852573
IRD lava occupancy 0.5634702500328843
##############
Trial  37
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
demonstration
[(0, 1), (3, 3), (8, 1), (9, 3), (2, 1), (14, 1), (1, 1)]
w_map [ 0.04401102 -6.65298581  4.05644467  0.04316078] loglik -3.4656297101773816
accepted/total = 1972/2000 = 0.986
MAP Policy on Train MDP
map_weights [ 0.04401102 -6.65298581  4.05644467  0.04316078]
map reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	4.06	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	-6.65	0.04	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
MEAN policy on Train MDP
mean_weights [-0.71899992 -3.46715764  3.70891845  2.22190944]
mean reward
-0.72	-0.72	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	-0.72	3.71	
-0.72	-0.72	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	-3.47	-0.72	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
MAP policy loss 1.0454181782488386e-06
Mean policy loss 1.5643120688775894e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
reward
-1.00	-5.00	-1.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-5.00	
features
0 	1 	0 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	3 	1 	
MAP on testing env
map_weights [ 0.04401102 -6.65298581  4.05644467  0.04316078]
map reward
0.04	-6.65	0.04	0.04	0.04	
0.04	-6.65	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	4.06	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	-6.65	
Map policy
v	>	v	>	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-0.71899992 -3.46715764  3.70891845  2.22190944]
mean reward
-0.72	-3.47	-0.72	2.22	-0.72	
-0.72	-3.47	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	-0.72	3.71	
-0.72	-0.72	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	2.22	-3.47	
mean policy
v	>	>	v	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
features
0 	1 	0 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	3 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
v	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
-------- IRD Solution -------
ird reward
-94.54	-100.59	-94.54	-91.84	-94.54	
-94.54	-100.59	-94.54	-94.54	-94.54	
-94.54	-94.54	-94.54	-94.54	-88.19	
-94.54	-94.54	-94.54	-94.54	-94.54	
-94.54	-94.54	-94.54	-91.84	-100.59	
ird policy
v	>	>	v	v	
v	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MAP policy loss 5.250097200350101
mean policy loss 18.06700560246173
robust policy loss 21.614210041476298
regret policy loss 21.61421138330177
ird policy loss 18.06700498966256
MAP lava occupancy 0.1289813099184624
Mean lava occupancy 0.1289813099184624
Robust lava occupancy 0.296790000545993
Regret lava occupancy 0.29679001386512116
IRD lava occupancy 0.2624950000518174
##############
Trial  38
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	^	>	^	
^	^	^	<	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
features
1 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
1 	0 	0 	0 	2 	
0 	0 	0 	1 	0 	
1 	1 	0 	0 	1 	
demonstration
[(0, 1), (7, 3), (13, 1), (12, 1), (14, 1), (2, 3), (1, 1)]
w_map [-0.17804514 -0.44382673  0.72375481 -2.17985678] loglik -2.079437498561788
accepted/total = 1889/2000 = 0.9445
MAP Policy on Train MDP
map_weights [-0.17804514 -0.44382673  0.72375481 -2.17985678]
map reward
-0.44	-0.18	-0.18	-0.18	-0.18	
-0.44	-0.18	-0.18	-0.18	-0.18	
-0.44	-0.18	-0.18	-0.18	0.72	
-0.18	-0.18	-0.18	-0.44	-0.18	
-0.44	-0.44	-0.18	-0.18	-0.44	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
^	>	^	^	^	
MEAN policy on Train MDP
mean_weights [-2.87591963 -4.80316487 -1.48875817 -4.43550092]
mean reward
-4.80	-2.88	-2.88	-2.88	-2.88	
-4.80	-2.88	-2.88	-2.88	-2.88	
-4.80	-2.88	-2.88	-2.88	-1.49	
-2.88	-2.88	-2.88	-4.80	-2.88	
-4.80	-4.80	-2.88	-2.88	-4.80	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
^	^	^	^	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	^	>	^	
^	^	^	<	^	
MAP policy loss 0.01825039193459918
Mean policy loss 0.01824942988997555
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
>	>	^	>	v	
^	>	>	>	>	
^	v	>	>	^	
>	v	>	>	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-5.00	1.00	
-100.00	-100.00	-100.00	-1.00	-1.00	
-100.00	-1.00	-100.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	0 	1 	2 	
3 	3 	3 	0 	0 	
3 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.17804514 -0.44382673  0.72375481 -2.17985678]
map reward
-0.44	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.44	-0.18	
-0.18	-2.18	-0.18	-0.44	0.72	
-2.18	-2.18	-2.18	-0.18	-0.18	
-2.18	-0.18	-2.18	-0.18	-0.18	
Map policy
>	>	>	>	v	
>	>	v	>	v	
^	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [-2.87591963 -4.80316487 -1.48875817 -4.43550092]
mean reward
-4.80	-2.88	-2.88	-2.88	-2.88	
-2.88	-2.88	-2.88	-4.80	-2.88	
-2.88	-4.44	-2.88	-4.80	-1.49	
-4.44	-4.44	-4.44	-2.88	-2.88	
-4.44	-2.88	-4.44	-2.88	-2.88	
mean policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
features
1 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	0 	1 	2 	
3 	3 	3 	0 	0 	
3 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	>	>	v	
v	v	v	>	v	
v	v	v	>	>	
>	>	>	>	^	
^	^	^	>	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	>	v	>	v	
^	>	>	>	>	
^	>	>	>	^	
>	v	>	>	^	
-------- IRD Solution -------
ird reward
-16.59	-15.56	-15.56	-15.56	-15.56	
-15.56	-15.56	-15.56	-16.59	-15.56	
-15.56	-17.87	-15.56	-16.59	-14.78	
-17.87	-17.87	-17.87	-15.56	-15.56	
-17.87	-15.56	-17.87	-15.56	-15.56	
ird policy
>	>	>	>	v	
>	>	v	>	v	
^	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
MAP policy loss 36.90426614799403
mean policy loss 18.174563430178083
robust policy loss 319.2934365289889
regret policy loss 1.5965863175804411
ird policy loss 7.428675962893161
MAP lava occupancy 0.6373815154680175
Mean lava occupancy 0.6373815154680175
Robust lava occupancy 3.463541235267421
Regret lava occupancy 0.2639302043916608
IRD lava occupancy 0.35210000081346504
##############
Trial  39
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	<	v	
v	>	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	1 	1 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (13, 1), (12, 1), (14, 1), (2, 3), (1, 1)]
w_map [-0.09486493 -3.07357456  0.85840267 -5.82600619] loglik -0.6931444834799549
accepted/total = 1833/2000 = 0.9165
MAP Policy on Train MDP
map_weights [-0.09486493 -3.07357456  0.85840267 -5.82600619]
map reward
-0.09	-0.09	-0.09	-0.09	-0.09	
-0.09	-3.07	-0.09	-3.07	-3.07	
-0.09	-0.09	-0.09	-0.09	0.86	
-0.09	-0.09	-0.09	-0.09	-0.09	
-3.07	-0.09	-0.09	-0.09	-0.09	
Map policy
v	>	v	<	v	
v	v	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-0.67449169 -2.13740266  0.91956523 -3.84249252]
mean reward
-0.67	-0.67	-0.67	-0.67	-0.67	
-0.67	-2.14	-0.67	-2.14	-2.14	
-0.67	-0.67	-0.67	-0.67	0.92	
-0.67	-0.67	-0.67	-0.67	-0.67	
-2.14	-0.67	-0.67	-0.67	-0.67	
mean policy
>	>	v	>	v	
v	>	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
v	>	v	<	v	
v	>	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 2.4518605173766417e-07
Mean policy loss 0.010649556914658631
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	v	
^	^	>	>	v	
^	v	v	>	>	
>	v	v	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-100.00	1.00	
-100.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	3 	3 	0 	0 	
0 	1 	0 	3 	2 	
3 	0 	0 	3 	0 	
3 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.09486493 -3.07357456  0.85840267 -5.82600619]
map reward
-0.09	-0.09	-0.09	-0.09	-0.09	
-0.09	-5.83	-5.83	-0.09	-0.09	
-0.09	-3.07	-0.09	-5.83	0.86	
-5.83	-0.09	-0.09	-5.83	-0.09	
-5.83	-0.09	-0.09	-0.09	-0.09	
Map policy
>	>	>	>	v	
^	^	>	>	v	
^	v	v	>	>	
>	v	v	>	^	
>	>	>	>	^	
MEAN policy on test env
mean_weights [-0.67449169 -2.13740266  0.91956523 -3.84249252]
mean reward
-0.67	-0.67	-0.67	-0.67	-0.67	
-0.67	-3.84	-3.84	-0.67	-0.67	
-0.67	-2.14	-0.67	-3.84	0.92	
-3.84	-0.67	-0.67	-3.84	-0.67	
-3.84	-0.67	-0.67	-0.67	-0.67	
mean policy
>	>	>	>	v	
^	^	>	>	v	
>	>	>	>	>	
>	>	v	>	^	
>	>	>	>	^	
features
0 	0 	0 	0 	0 	
0 	3 	3 	0 	0 	
0 	1 	0 	3 	2 	
3 	0 	0 	3 	0 	
3 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
^	^	>	>	v	
^	v	v	>	>	
>	v	v	>	^	
>	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
^	^	>	>	v	
^	v	v	>	>	
>	>	v	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-54.67	-54.67	-54.67	-54.67	-54.67	
-54.67	-57.52	-57.52	-54.67	-54.67	
-54.67	-54.91	-54.67	-57.52	-51.24	
-57.52	-54.67	-54.67	-57.52	-54.67	
-57.52	-54.67	-54.67	-54.67	-54.67	
ird policy
>	>	>	>	v	
v	v	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
MAP policy loss 79.68270962046064
mean policy loss 10.119054388142803
robust policy loss 8.51325036421019e-09
regret policy loss 2.3070154604384885e-08
ird policy loss 26.601357805323136
MAP lava occupancy 1.051661926495967
Mean lava occupancy 1.051661926495967
Robust lava occupancy 0.24000000006648137
Regret lava occupancy 0.24000000014959652
IRD lava occupancy 0.5182455001787493
##############
Trial  40
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-5.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	1 	0 	0 	1 	
demonstration
[(0, 1), (3, 1), (9, 3), (2, 1), (14, 1), (4, 3), (1, 1)]
w_map [-1.5588964  -3.60248964  0.28428569 -0.97017835] loglik -2.079441541697733
accepted/total = 1910/2000 = 0.955
MAP Policy on Train MDP
map_weights [-1.5588964  -3.60248964  0.28428569 -0.97017835]
map reward
-1.56	-1.56	-1.56	-1.56	-1.56	
-3.60	-1.56	-1.56	-1.56	-1.56	
-1.56	-1.56	-1.56	-1.56	0.28	
-1.56	-1.56	-3.60	-1.56	-1.56	
-1.56	-3.60	-1.56	-1.56	-3.60	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
MEAN policy on Train MDP
mean_weights [-3.07897115 -4.50934801 -1.31693986 -3.00067639]
mean reward
-3.08	-3.08	-3.08	-3.08	-3.08	
-4.51	-3.08	-3.08	-3.08	-3.08	
-3.08	-3.08	-3.08	-3.08	-1.32	
-3.08	-3.08	-4.51	-3.08	-3.08	
-3.08	-4.51	-3.08	-3.08	-4.51	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
Optimal Policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
^	>	>	^	^	
MAP policy loss -1.7524161202014943e-09
Mean policy loss 2.7783149903967685e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	>	>	v	
>	>	>	>	v	
^	^	^	>	>	
^	<	<	<	^	
^	<	^	^	<	
reward
-100.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-100.00	-100.00	-100.00	1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
3 	0 	3 	0 	0 	
0 	0 	0 	0 	1 	
0 	3 	3 	3 	2 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-1.5588964  -3.60248964  0.28428569 -0.97017835]
map reward
-0.97	-1.56	-0.97	-1.56	-1.56	
-1.56	-1.56	-1.56	-1.56	-3.60	
-1.56	-0.97	-0.97	-0.97	0.28	
-1.56	-1.56	-1.56	-1.56	-0.97	
-1.56	-0.97	-1.56	-1.56	-1.56	
Map policy
>	>	v	v	v	
>	v	v	v	v	
>	>	>	>	>	
>	^	^	>	^	
>	^	^	^	^	
MEAN policy on test env
mean_weights [-3.07897115 -4.50934801 -1.31693986 -3.00067639]
mean reward
-3.00	-3.08	-3.00	-3.08	-3.08	
-3.08	-3.08	-3.08	-3.08	-4.51	
-3.08	-3.00	-3.00	-3.00	-1.32	
-3.08	-3.08	-3.08	-3.08	-3.00	
-3.08	-3.00	-3.08	-3.08	-3.08	
mean policy
>	>	v	v	v	
>	v	v	v	v	
>	>	>	>	>	
>	^	^	>	^	
>	^	^	^	^	
features
3 	0 	3 	0 	0 	
0 	0 	0 	0 	1 	
0 	3 	3 	3 	2 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	>	>	v	
>	>	>	>	v	
^	v	>	>	>	
>	>	>	>	^	
^	>	>	>	^	
-------- IRD Solution -------
ird reward
-26.56	-27.68	-26.56	-27.68	-27.68	
-27.68	-27.68	-27.68	-27.68	-29.09	
-27.68	-26.56	-26.56	-26.56	-24.31	
-27.68	-27.68	-27.68	-27.68	-26.56	
-27.68	-26.56	-27.68	-27.68	-27.68	
ird policy
>	>	v	v	v	
>	v	v	v	v	
>	>	>	>	>	
>	^	^	>	^	
>	^	^	^	^	
MAP policy loss 102.25192936065395
mean policy loss 148.83471574361005
robust policy loss 74.24461783325728
regret policy loss 45.67588674816332
ird policy loss 148.83471586743246
MAP lava occupancy 1.3658372864909867
Mean lava occupancy 1.3658372864909867
Robust lava occupancy 1.088298631391972
Regret lava occupancy 0.7898387481591682
IRD lava occupancy 1.8417339750162054
##############
Trial  41
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
^	>	>	>	v	
^	v	^	>	>	
>	>	>	^	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
1 	0 	1 	1 	2 	
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (3, 1), (9, 3), (2, 1), (14, 1), (4, 3), (1, 1)]
w_map [-0.60739817 -1.05609737  2.16472519  1.34187364] loglik -1.3862914790643117
accepted/total = 1920/2000 = 0.96
MAP Policy on Train MDP
map_weights [-0.60739817 -1.05609737  2.16472519  1.34187364]
map reward
-0.61	-0.61	-0.61	-0.61	-0.61	
-0.61	-1.06	-0.61	-0.61	-0.61	
-1.06	-0.61	-1.06	-1.06	2.16	
-0.61	-0.61	-0.61	-0.61	-1.06	
-0.61	-0.61	-0.61	-0.61	-0.61	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	^	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 0.7944571  -1.03044954  6.25250223  3.79010812]
mean reward
0.79	0.79	0.79	0.79	0.79	
0.79	-1.03	0.79	0.79	0.79	
-1.03	0.79	-1.03	-1.03	6.25	
0.79	0.79	0.79	0.79	-1.03	
0.79	0.79	0.79	0.79	0.79	
mean policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
^	>	>	>	v	
^	v	^	>	>	
>	>	>	^	^	
>	>	>	>	^	
MAP policy loss 0.11467205434176775
Mean policy loss 0.11467134497108358
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	0 	0 	2 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.60739817 -1.05609737  2.16472519  1.34187364]
map reward
-0.61	-0.61	1.34	-0.61	-0.61	
-0.61	-0.61	1.34	1.34	-0.61	
1.34	-0.61	-0.61	-0.61	2.16	
-0.61	-0.61	-0.61	-1.06	-0.61	
-0.61	-0.61	-0.61	-0.61	-0.61	
Map policy
>	>	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
^	>	>	>	^	
MEAN policy on test env
mean_weights [ 0.7944571  -1.03044954  6.25250223  3.79010812]
mean reward
0.79	0.79	3.79	0.79	0.79	
0.79	0.79	3.79	3.79	0.79	
3.79	0.79	0.79	0.79	6.25	
0.79	0.79	0.79	-1.03	0.79	
0.79	0.79	0.79	0.79	0.79	
mean policy
>	>	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
^	>	>	>	^	
features
0 	0 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	0 	0 	2 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
^	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
^	>	>	>	^	
-------- IRD Solution -------
ird reward
-141.97	-141.97	-139.65	-141.97	-141.97	
-141.97	-141.97	-139.65	-139.65	-141.97	
-139.65	-141.97	-141.97	-141.97	-135.26	
-141.97	-141.97	-141.97	-144.88	-141.97	
-141.97	-141.97	-141.97	-141.97	-141.97	
ird policy
>	>	v	v	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	^	>	^	
^	>	>	>	^	
MAP policy loss 37.551979733904275
mean policy loss 57.426460540758086
robust policy loss 57.42646858997334
regret policy loss 57.426459854237514
ird policy loss 57.4264597492545
MAP lava occupancy 0.53126198583378
Mean lava occupancy 0.53126198583378
Robust lava occupancy 0.7400653351507626
Regret lava occupancy 0.7400652508848521
IRD lava occupancy 0.7400652500012046
##############
Trial  42
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
1 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(13, 1), (12, 1), (6, 3), (14, 1), (11, 1), (5, 1), (0, 3)]
w_map [  0.6123096   -3.79948128   2.42678562 -10.60709687] loglik -1.3862395480919076
accepted/total = 1911/2000 = 0.9555
MAP Policy on Train MDP
map_weights [  0.6123096   -3.79948128   2.42678562 -10.60709687]
map reward
0.61	0.61	-3.80	0.61	0.61	
0.61	0.61	0.61	-3.80	0.61	
-3.80	0.61	0.61	0.61	2.43	
0.61	0.61	0.61	0.61	0.61	
0.61	0.61	0.61	0.61	0.61	
Map policy
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 1.56741361 -0.62534623  3.13672397 -5.07893956]
mean reward
1.57	1.57	-0.63	1.57	1.57	
1.57	1.57	1.57	-0.63	1.57	
-0.63	1.57	1.57	1.57	3.14	
1.57	1.57	1.57	1.57	1.57	
1.57	1.57	1.57	1.57	1.57	
mean policy
v	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	v	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 1.5311824897168425e-06
Mean policy loss -2.5918392682500874e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
<	<	<	>	>	
^	^	^	<	v	
>	^	^	>	>	
>	^	^	>	^	
<	^	^	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-100.00	-1.00	-1.00	-100.00	1.00	
-5.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	0 	3 	2 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
MAP on testing env
map_weights [  0.6123096   -3.79948128   2.42678562 -10.60709687]
map reward
0.61	0.61	0.61	0.61	0.61	
0.61	0.61	0.61	-3.80	-10.61	
-10.61	0.61	0.61	-10.61	2.43	
-3.80	0.61	0.61	-10.61	0.61	
0.61	-3.80	0.61	-10.61	0.61	
Map policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	^	^	>	^	
MEAN policy on test env
mean_weights [ 1.56741361 -0.62534623  3.13672397 -5.07893956]
mean reward
1.57	1.57	1.57	1.57	1.57	
1.57	1.57	1.57	-0.63	-5.08	
-5.08	1.57	1.57	-5.08	3.14	
-0.63	1.57	1.57	-5.08	1.57	
1.57	-0.63	1.57	-5.08	1.57	
mean policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	^	^	>	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	0 	3 	2 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	>	
^	^	^	<	v	
^	^	^	>	>	
>	^	^	>	^	
<	^	^	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	>	v	v	v	
>	>	>	>	>	
>	^	^	>	^	
<	^	^	>	^	
-------- IRD Solution -------
ird reward
-96.38	-96.38	-96.38	-96.38	-96.38	
-96.38	-96.38	-96.38	-98.53	-103.44	
-103.44	-96.38	-96.38	-103.44	-92.91	
-98.53	-96.38	-96.38	-103.44	-96.38	
-96.38	-98.53	-96.38	-103.44	-96.38	
ird policy
>	>	>	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	^	>	^	
MAP policy loss 49.978395636355714
mean policy loss 38.10000729676683
robust policy loss 4.156201095782945
regret policy loss 31.22869734092132
ird policy loss 38.10000700462476
MAP lava occupancy 0.9420821819635707
Mean lava occupancy 0.9420821819635707
Robust lava occupancy 0.26813444418691695
Regret lava occupancy 0.7119458579330353
IRD lava occupancy 0.8220984752689433
##############
Trial  43
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	<	v	v	
v	v	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
1 	0 	1 	0 	1 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (13, 1), (1, 3), (12, 1), (6, 3), (14, 1), (11, 1)]
w_map [-0.19745498 -0.56203676  0.27887381 -1.28679569] loglik 0.0
accepted/total = 1922/2000 = 0.961
MAP Policy on Train MDP
map_weights [-0.19745498 -0.56203676  0.27887381 -1.28679569]
map reward
-0.20	-0.20	-0.20	-0.56	-0.20	
-0.56	-0.20	-0.56	-0.20	-0.56	
-0.20	-0.20	-0.20	-0.20	0.28	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
Map policy
>	v	v	v	v	
v	v	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 1.80002119 -1.11804734  6.28712878  7.34895045]
mean reward
1.80	1.80	1.80	-1.12	1.80	
-1.12	1.80	-1.12	1.80	-1.12	
1.80	1.80	1.80	1.80	6.29	
1.80	1.80	1.80	1.80	1.80	
1.80	1.80	1.80	1.80	1.80	
mean policy
>	v	v	v	v	
>	v	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	v	<	v	v	
v	v	v	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 0.024937164763740936
Mean policy loss 0.024937296683597884
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	<	<	
>	v	<	<	v	
v	v	v	>	>	
>	>	>	>	^	
>	^	>	>	^	
reward
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-5.00	-1.00	-100.00	-100.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	1 	3 	
0 	0 	0 	0 	3 	
1 	0 	3 	3 	2 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.19745498 -0.56203676  0.27887381 -1.28679569]
map reward
-0.20	-0.20	-0.20	-0.56	-1.29	
-0.20	-0.20	-0.20	-0.20	-1.29	
-0.56	-0.20	-1.29	-1.29	0.28	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-1.29	-0.20	-0.20	
Map policy
>	v	v	v	v	
>	v	>	v	v	
v	v	v	>	>	
>	>	>	>	^	
>	^	>	>	^	
MEAN policy on test env
mean_weights [ 1.80002119 -1.11804734  6.28712878  7.34895045]
mean reward
1.80	1.80	1.80	-1.12	7.35	
1.80	1.80	1.80	1.80	7.35	
-1.12	1.80	7.35	7.35	6.29	
1.80	1.80	1.80	1.80	1.80	
1.80	1.80	7.35	1.80	1.80	
mean policy
>	v	v	>	>	
>	v	v	>	^	
>	>	>	<	^	
>	>	v	^	^	
>	>	v	<	<	
features
0 	0 	0 	1 	3 	
0 	0 	0 	0 	3 	
1 	0 	3 	3 	2 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	>	v	
>	v	v	>	v	
>	>	>	>	>	
>	>	^	^	^	
>	>	v	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
>	v	>	>	v	
v	v	v	>	>	
>	>	>	>	^	
>	^	>	>	^	
-------- IRD Solution -------
ird reward
-186.90	-186.90	-186.90	-193.35	-182.44	
-186.90	-186.90	-186.90	-186.90	-182.44	
-193.35	-186.90	-182.44	-182.44	-179.54	
-186.90	-186.90	-186.90	-186.90	-186.90	
-186.90	-186.90	-182.44	-186.90	-186.90	
ird policy
>	>	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	>	^	^	^	
>	>	^	^	^	
MAP policy loss 88.51711928234639
mean policy loss 1853.0658336487581
robust policy loss 287.2077156033005
regret policy loss 16.186856415282328
ird policy loss 126.44385130016495
MAP lava occupancy 1.1198526639593973
Mean lava occupancy 1.1198526639593973
Robust lava occupancy 3.09859913650045
Regret lava occupancy 0.38249498041730123
IRD lava occupancy 1.5051389750058313
##############
Trial  44
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (13, 1), (8, 3), (7, 1), (14, 1), (2, 3), (1, 1)]
w_map [ 0.30514246 -3.8144183   4.19668376  4.03478134] loglik -2.7724143820014433
accepted/total = 1950/2000 = 0.975
MAP Policy on Train MDP
map_weights [ 0.30514246 -3.8144183   4.19668376  4.03478134]
map reward
0.31	0.31	0.31	0.31	0.31	
0.31	0.31	0.31	0.31	-3.81	
-3.81	0.31	0.31	0.31	4.20	
0.31	0.31	0.31	0.31	0.31	
0.31	0.31	0.31	0.31	0.31	
Map policy
>	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-0.48804865 -3.33320389  2.86223438  3.4175638 ]
mean reward
-0.49	-0.49	-0.49	-0.49	-0.49	
-0.49	-0.49	-0.49	-0.49	-3.33	
-3.33	-0.49	-0.49	-0.49	2.86	
-0.49	-0.49	-0.49	-0.49	-0.49	
-0.49	-0.49	-0.49	-0.49	-0.49	
mean policy
>	>	>	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	v	v	<	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 0.011211705187739368
Mean policy loss 0.01121000041888226
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	>	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	0 	0 	2 	
1 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.30514246 -3.8144183   4.19668376  4.03478134]
map reward
0.31	0.31	0.31	4.03	0.31	
0.31	0.31	0.31	0.31	4.03	
0.31	0.31	0.31	0.31	4.20	
-3.81	0.31	0.31	0.31	0.31	
4.03	0.31	0.31	0.31	0.31	
Map policy
>	>	>	^	v	
>	>	>	>	v	
v	>	>	>	>	
v	v	>	>	^	
<	<	<	^	^	
MEAN policy on test env
mean_weights [-0.48804865 -3.33320389  2.86223438  3.4175638 ]
mean reward
-0.49	-0.49	-0.49	3.42	-0.49	
-0.49	-0.49	-0.49	-0.49	3.42	
-0.49	-0.49	-0.49	-0.49	2.86	
-3.33	-0.49	-0.49	-0.49	-0.49	
3.42	-0.49	-0.49	-0.49	-0.49	
mean policy
>	>	>	^	v	
v	>	>	>	>	
v	v	>	>	^	
v	v	v	^	^	
<	<	<	<	^	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	0 	0 	2 	
1 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
v	>	>	>	^	
<	<	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
-------- IRD Solution -------
ird reward
-76.85	-76.85	-76.85	-73.77	-76.85	
-76.85	-76.85	-76.85	-76.85	-73.77	
-76.85	-76.85	-76.85	-76.85	-70.84	
-78.18	-76.85	-76.85	-76.85	-76.85	
-73.77	-76.85	-76.85	-76.85	-76.85	
ird policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 6.133015648024715
mean policy loss 1821.1062420300539
robust policy loss 291.347432314357
regret policy loss 2.318000193025271
ird policy loss 40.26437716455767
MAP lava occupancy 0.1960125007268139
Mean lava occupancy 0.1960125007268139
Robust lava occupancy 3.036671065711352
Regret lava occupancy 0.15800000166977762
IRD lava occupancy 0.5412967389347455
##############
Trial  45
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	v	
^	>	>	>	v	
^	v	^	>	>	
>	v	>	>	^	
>	>	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	1 	2 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (3, 1), (9, 3), (2, 1), (14, 1), (4, 3), (1, 1)]
w_map [ 2.88705652 -2.44429491  6.7353786   3.16839147] loglik -1.3862774389763217
accepted/total = 1940/2000 = 0.97
MAP Policy on Train MDP
map_weights [ 2.88705652 -2.44429491  6.7353786   3.16839147]
map reward
2.89	2.89	2.89	2.89	2.89	
2.89	-2.44	2.89	2.89	2.89	
2.89	2.89	-2.44	-2.44	6.74	
2.89	2.89	-2.44	2.89	2.89	
2.89	2.89	2.89	2.89	2.89	
Map policy
>	>	>	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [ 3.33065277 -1.91899144  5.99818132  1.26397862]
mean reward
3.33	3.33	3.33	3.33	3.33	
3.33	-1.92	3.33	3.33	3.33	
3.33	3.33	-1.92	-1.92	6.00	
3.33	3.33	-1.92	3.33	3.33	
3.33	3.33	3.33	3.33	3.33	
mean policy
>	>	>	>	v	
^	>	>	>	v	
^	v	^	>	>	
v	v	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	>	v	
^	>	>	>	v	
^	v	^	>	>	
>	v	>	>	^	
>	>	>	>	^	
MAP policy loss 0.17290998630283044
Mean policy loss -1.2153911106704829e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	v	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
MAP on testing env
map_weights [ 2.88705652 -2.44429491  6.7353786   3.16839147]
map reward
2.89	2.89	2.89	2.89	-2.44	
2.89	2.89	2.89	2.89	2.89	
2.89	2.89	-2.44	2.89	6.74	
2.89	2.89	2.89	2.89	2.89	
2.89	2.89	2.89	2.89	-2.44	
Map policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MEAN policy on test env
mean_weights [ 3.33065277 -1.91899144  5.99818132  1.26397862]
mean reward
3.33	3.33	3.33	3.33	-1.92	
3.33	3.33	3.33	3.33	3.33	
3.33	3.33	-1.92	3.33	6.00	
3.33	3.33	3.33	3.33	3.33	
3.33	3.33	3.33	3.33	-1.92	
mean policy
>	>	>	v	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
features
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
-------- IRD Solution -------
ird reward
-152.07	-152.07	-152.07	-152.07	-152.89	
-152.07	-152.07	-152.07	-152.07	-152.07	
-152.07	-152.07	-152.89	-152.07	-144.80	
-152.07	-152.07	-152.07	-152.07	-152.07	
-152.07	-152.07	-152.07	-152.07	-152.89	
ird policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	^	^	
MAP policy loss 0.6566471719205036
mean policy loss 2.2690195555785664e-10
robust policy loss 0.035587054944976715
regret policy loss 2.652970385041975e-08
ird policy loss 0.03558652513553909
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  46
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (3, 1), (9, 3), (2, 1), (14, 1), (4, 3), (1, 1)]
w_map [ 0.71305401 -3.17914094  1.90352797 -1.31377054] loglik -2.0794336903618387
accepted/total = 1907/2000 = 0.9535
MAP Policy on Train MDP
map_weights [ 0.71305401 -3.17914094  1.90352797 -1.31377054]
map reward
-3.18	0.71	0.71	0.71	0.71	
0.71	0.71	-3.18	0.71	0.71	
0.71	0.71	0.71	0.71	1.90	
0.71	0.71	0.71	0.71	0.71	
0.71	0.71	0.71	0.71	0.71	
Map policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-0.23615994 -4.68632305  1.52515775 -1.71972467]
mean reward
-4.69	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-4.69	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	1.53	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
mean policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	>	>	v	v	
>	v	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	>	>	^	
MAP policy loss 4.83897223685012e-07
Mean policy loss -5.350007659110645e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	v	v	
>	>	>	>	v	
v	^	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-100.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
features
0 	0 	0 	0 	1 	
3 	0 	0 	0 	0 	
0 	1 	3 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
MAP on testing env
map_weights [ 0.71305401 -3.17914094  1.90352797 -1.31377054]
map reward
0.71	0.71	0.71	0.71	-3.18	
-1.31	0.71	0.71	0.71	0.71	
0.71	-3.18	-1.31	0.71	1.90	
0.71	0.71	0.71	0.71	0.71	
0.71	0.71	0.71	-3.18	-1.31	
Map policy
>	>	>	v	v	
>	>	>	>	v	
v	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-0.23615994 -4.68632305  1.52515775 -1.71972467]
mean reward
-0.24	-0.24	-0.24	-0.24	-4.69	
-1.72	-0.24	-0.24	-0.24	-0.24	
-0.24	-4.69	-1.72	-0.24	1.53	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-4.69	-1.72	
mean policy
>	>	>	v	v	
>	>	>	>	v	
v	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
features
0 	0 	0 	0 	1 	
3 	0 	0 	0 	0 	
0 	1 	3 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	v	
^	>	>	>	>	
>	>	>	>	^	
>	>	^	>	^	
------ Regret Solution ---------
expert u_sa [ 4.29816219  1.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	v	
v	^	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-44.76	-44.76	-44.76	-44.76	-48.69	
-46.71	-44.76	-44.76	-44.76	-44.76	
-44.76	-48.69	-46.71	-44.76	-39.88	
-44.76	-44.76	-44.76	-44.76	-44.76	
-44.76	-44.76	-44.76	-48.69	-46.71	
ird policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	>	^	^	^	
MAP policy loss 32.34728744338553
mean policy loss 3.6282497700252305
robust policy loss 24.831119210136876
regret policy loss 5.854878332156588e-08
ird policy loss 7.227086519967971
MAP lava occupancy 0.43810263167529206
Mean lava occupancy 0.43810263167529206
Robust lava occupancy 0.3721704008423759
Regret lava occupancy 0.12000000029886841
IRD lava occupancy 0.1941000000546467
##############
Trial  47
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	0 	0 	0 	
1 	0 	0 	0 	2 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(13, 1), (8, 3), (7, 1), (6, 1), (14, 1), (5, 1), (0, 3)]
w_map [ 0.14590401 -1.17478531  4.22467053  4.72044865] loglik -2.079398940801184
accepted/total = 1819/2000 = 0.9095
MAP Policy on Train MDP
map_weights [ 0.14590401 -1.17478531  4.22467053  4.72044865]
map reward
0.15	-1.17	0.15	-1.17	0.15	
0.15	0.15	0.15	0.15	0.15	
-1.17	0.15	0.15	0.15	4.22	
0.15	0.15	-1.17	0.15	0.15	
0.15	0.15	0.15	0.15	0.15	
Map policy
v	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [ 0.59066847 -0.45498637  2.33435898  2.31598822]
mean reward
0.59	-0.45	0.59	-0.45	0.59	
0.59	0.59	0.59	0.59	0.59	
-0.45	0.59	0.59	0.59	2.33	
0.59	0.59	-0.45	0.59	0.59	
0.59	0.59	0.59	0.59	0.59	
mean policy
v	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
Optimal Policy
v	>	v	>	v	
>	>	>	>	v	
>	>	>	>	>	
>	^	>	>	^	
>	>	>	^	^	
MAP policy loss 3.6241135596033758e-06
Mean policy loss 6.754342555745363e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
>	^	^	>	^	
^	>	>	>	^	
reward
-5.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	1 	3 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	2 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [ 0.14590401 -1.17478531  4.22467053  4.72044865]
map reward
-1.17	0.15	0.15	-1.17	4.72	
0.15	0.15	0.15	-1.17	0.15	
0.15	0.15	0.15	0.15	4.22	
0.15	0.15	4.72	4.72	0.15	
0.15	-1.17	0.15	0.15	0.15	
Map policy
>	>	>	>	>	
>	v	v	>	^	
>	>	v	v	^	
>	>	>	<	<	
^	>	^	^	<	
MEAN policy on test env
mean_weights [ 0.59066847 -0.45498637  2.33435898  2.31598822]
mean reward
-0.45	0.59	0.59	-0.45	2.32	
0.59	0.59	0.59	-0.45	0.59	
0.59	0.59	0.59	0.59	2.33	
0.59	0.59	2.32	2.32	0.59	
0.59	-0.45	0.59	0.59	0.59	
mean policy
>	>	>	>	>	
>	v	v	v	v	
>	>	v	>	>	
>	>	>	<	^	
^	>	^	^	^	
features
1 	0 	0 	1 	3 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	2 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	>	v	
>	>	v	>	v	
>	>	>	>	>	
>	^	^	>	^	
^	>	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	v	
>	>	v	>	v	
>	>	>	>	>	
>	^	^	>	^	
^	>	>	>	^	
-------- IRD Solution -------
ird reward
-64.32	-63.04	-63.04	-64.32	-58.48	
-63.04	-63.04	-63.04	-64.32	-63.04	
-63.04	-63.04	-63.04	-63.04	-59.03	
-63.04	-63.04	-58.48	-58.48	-63.04	
-63.04	-64.32	-63.04	-63.04	-63.04	
ird policy
>	>	>	>	>	
>	v	v	>	^	
>	>	v	v	^	
>	>	>	<	<	
^	>	^	^	<	
MAP policy loss 10.968657501411029
mean policy loss 1385.2555779183253
robust policy loss 3.628250131072755
regret policy loss 9.873470430132869e-09
ird policy loss 1834.377561506484
MAP lava occupancy 0.2252066710265841
Mean lava occupancy 0.2252066710265841
Robust lava occupancy 0.15799999336803652
Regret lava occupancy 0.12000000003724295
IRD lava occupancy 18.30280999956468
##############
Trial  48
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
^	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
features
0 	0 	0 	0 	1 	
0 	1 	0 	0 	0 	
1 	0 	0 	0 	2 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	1 	
demonstration
[(0, 1), (13, 1), (8, 3), (3, 3), (2, 1), (14, 1), (1, 1)]
w_map [-0.56586141 -1.66644484 -0.203744   -0.67498457] loglik -1.3862943362865394
accepted/total = 1866/2000 = 0.933
MAP Policy on Train MDP
map_weights [-0.56586141 -1.66644484 -0.203744   -0.67498457]
map reward
-0.57	-0.57	-0.57	-0.57	-1.67	
-0.57	-1.67	-0.57	-0.57	-0.57	
-1.67	-0.57	-0.57	-0.57	-0.20	
-0.57	-0.57	-0.57	-0.57	-0.57	
-0.57	-0.57	-1.67	-0.57	-1.67	
Map policy
>	>	v	v	v	
^	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
^	^	>	^	^	
MEAN policy on Train MDP
mean_weights [ 0.55780897 -0.70680816  2.62202352 -0.26885934]
mean reward
0.56	0.56	0.56	0.56	-0.71	
0.56	-0.71	0.56	0.56	0.56	
-0.71	0.56	0.56	0.56	2.62	
0.56	0.56	0.56	0.56	0.56	
0.56	0.56	-0.71	0.56	-0.71	
mean policy
>	>	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	^	^	
Optimal Policy
>	>	v	v	v	
^	>	>	>	v	
>	>	>	>	>	
>	>	>	>	^	
>	^	>	^	^	
MAP policy loss 5.242138456651446e-08
Mean policy loss 0.031290201763276085
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	>	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
^	^	^	>	^	
reward
-1.00	-1.00	-5.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	0 	0 	2 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.56586141 -1.66644484 -0.203744   -0.67498457]
map reward
-0.57	-0.57	-1.67	-0.67	-0.57	
-1.67	-0.57	-0.57	-0.57	-0.67	
-0.57	-0.57	-0.57	-0.57	-0.20	
-0.57	-0.57	-0.57	-1.67	-1.67	
-0.57	-0.57	-1.67	-0.57	-0.57	
Map policy
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
^	^	^	>	^	
MEAN policy on test env
mean_weights [ 0.55780897 -0.70680816  2.62202352 -0.26885934]
mean reward
0.56	0.56	-0.71	-0.27	0.56	
-0.71	0.56	0.56	0.56	-0.27	
0.56	0.56	0.56	0.56	2.62	
0.56	0.56	0.56	-0.71	-0.71	
0.56	0.56	-0.71	0.56	0.56	
mean policy
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
^	^	^	>	^	
features
0 	0 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	0 	0 	2 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
^	^	^	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
^	^	^	>	>	
-------- IRD Solution -------
ird reward
-103.89	-103.89	-107.43	-106.19	-103.89	
-107.43	-103.89	-103.89	-103.89	-106.19	
-103.89	-103.89	-103.89	-103.89	-100.17	
-103.89	-103.89	-103.89	-107.43	-107.43	
-103.89	-103.89	-107.43	-103.89	-103.89	
ird policy
>	v	v	v	v	
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
^	^	^	>	^	
MAP policy loss 23.843191505093415
mean policy loss 2.3180003695311746
robust policy loss 2.318001832225463
regret policy loss 4.400596342029776
ird policy loss 2.318000881986784
MAP lava occupancy 0.3195688367384807
Mean lava occupancy 0.3195688367384807
Robust lava occupancy 0.11800001510434771
Regret lava occupancy 0.11800000000277439
IRD lava occupancy 0.11800000772901156
##############
Trial  49
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	>	v	v	
>	>	>	>	v	
^	^	^	>	>	
^	^	>	>	^	
^	>	>	>	^	
reward
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	1 	2 	
0 	1 	1 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(7, 1), (8, 1), (6, 1), (9, 3), (14, 1), (5, 1), (0, 3)]
w_map [-3.11127422 -5.06137495 -1.67178968  6.52997351] loglik -0.6931442159766448
accepted/total = 1800/2000 = 0.9
MAP Policy on Train MDP
map_weights [-3.11127422 -5.06137495 -1.67178968  6.52997351]
map reward
-3.11	-3.11	-5.06	-3.11	-5.06	
-3.11	-3.11	-3.11	-3.11	-3.11	
-3.11	-3.11	-5.06	-5.06	-1.67	
-3.11	-5.06	-5.06	-3.11	-3.11	
-3.11	-5.06	-3.11	-3.11	-3.11	
Map policy
>	v	>	v	v	
>	>	>	>	v	
>	^	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
MEAN policy on Train MDP
mean_weights [-2.42703157 -3.251989   -1.40622426  2.59067219]
mean reward
-2.43	-2.43	-3.25	-2.43	-3.25	
-2.43	-2.43	-2.43	-2.43	-2.43	
-2.43	-2.43	-3.25	-3.25	-1.41	
-2.43	-3.25	-3.25	-2.43	-2.43	
-2.43	-3.25	-2.43	-2.43	-2.43	
mean policy
>	v	>	v	v	
>	>	>	>	v	
>	>	>	>	>	
^	>	>	>	^	
>	>	>	>	^	
Optimal Policy
>	v	>	v	v	
>	>	>	>	v	
^	^	^	>	>	
^	^	>	>	^	
^	>	>	>	^	
MAP policy loss 0.07347281675802914
Mean policy loss 0.537430388743789
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	>	v	
>	>	>	>	v	
>	v	v	>	>	
>	>	>	>	^	
>	^	>	>	^	
reward
-5.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
features
1 	3 	3 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	1 	1 	2 	
0 	0 	0 	0 	0 	
1 	0 	1 	0 	0 	
MAP on testing env
map_weights [-3.11127422 -5.06137495 -1.67178968  6.52997351]
map reward
-5.06	6.53	6.53	-3.11	-3.11	
-3.11	-3.11	-3.11	-3.11	-3.11	
-3.11	-3.11	-5.06	-5.06	-1.67	
-3.11	-3.11	-3.11	-3.11	-3.11	
-5.06	-3.11	-5.06	-3.11	-3.11	
Map policy
>	^	<	<	<	
>	^	^	^	<	
^	^	^	^	^	
^	^	^	<	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-2.42703157 -3.251989   -1.40622426  2.59067219]
mean reward
-3.25	2.59	2.59	-2.43	-2.43	
-2.43	-2.43	-2.43	-2.43	-2.43	
-2.43	-2.43	-3.25	-3.25	-1.41	
-2.43	-2.43	-2.43	-2.43	-2.43	
-3.25	-2.43	-3.25	-2.43	-2.43	
mean policy
>	^	<	<	<	
>	^	^	^	<	
^	^	^	^	^	
^	^	^	<	^	
^	^	^	^	^	
features
1 	3 	3 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	1 	1 	2 	
0 	0 	0 	0 	0 	
1 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	^	<	v	
>	^	^	>	v	
>	^	^	>	>	
>	>	>	>	^	
>	^	>	>	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         14.58342723  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	>	>	v	
>	>	>	>	v	
>	v	v	>	>	
>	>	>	>	^	
^	^	>	>	^	
-------- IRD Solution -------
ird reward
-3.01	-4.74	-4.74	-2.28	-2.28	
-2.28	-2.28	-2.28	-2.28	-2.28	
-2.28	-2.28	-3.01	-3.01	-1.69	
-2.28	-2.28	-2.28	-2.28	-2.28	
-3.01	-2.28	-3.01	-2.28	-2.28	
ird policy
v	v	>	>	v	
>	>	>	>	v	
>	v	>	>	>	
>	>	>	>	^	
>	^	>	>	^	
MAP policy loss 2.400525545103685
mean policy loss 1746.9723220767396
robust policy loss 615.4689637311944
regret policy loss 2.4213242522108658e-11
ird policy loss 0.0112100001056607
MAP lava occupancy 0.0999212683064709
Mean lava occupancy 0.0999212683064709
Robust lava occupancy 6.192075498967579
Regret lava occupancy 0.08000000000026863
IRD lava occupancy 0.08000000000096986
