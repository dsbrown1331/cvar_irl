##############
Trial  0
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	v	v	<	
>	v	>	v	<	<	
>	v	>	v	<	v	
>	>	>	v	<	v	
>	>	^	^	<	<	
>	>	^	^	<	<	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	-5.00	
features
0 	1 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	0 	
1 	0 	1 	0 	1 	0 	
1 	0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	1 	
demonstration
[(7, 3), (21, 0), (13, 3), (15, 3), (6, 1), (20, 1), (21, 2), (21, 3), (27, 2), (0, 3), (19, 1)]
w_map [-1.89613519 -6.62447902  4.04736766 -0.39200828] loglik -3.9889839577713246
accepted/total = 1879/2000 = 0.9395
MAP Policy on Train MDP
map_weights [-1.89613519 -6.62447902  4.04736766 -0.39200828]
map reward
-1.90	-6.62	-1.90	-6.62	-1.90	-6.62	
-1.90	-1.90	-1.90	-1.90	-1.90	-1.90	
-6.62	-1.90	-6.62	-1.90	-6.62	-1.90	
-6.62	-1.90	-1.90	4.05	-6.62	-1.90	
-1.90	-1.90	-1.90	-1.90	-1.90	-1.90	
-6.62	-1.90	-1.90	-1.90	-1.90	-6.62	
Map policy
v	>	v	v	v	v	
>	v	>	v	<	<	
>	v	>	v	<	v	
>	>	>	v	<	<	
>	>	^	^	<	<	
>	>	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-1.89162612 -3.61295417  2.7222384   3.06535515]
mean reward
-1.89	-3.61	-1.89	-3.61	-1.89	-3.61	
-1.89	-1.89	-1.89	-1.89	-1.89	-1.89	
-3.61	-1.89	-3.61	-1.89	-3.61	-1.89	
-3.61	-1.89	-1.89	2.72	-3.61	-1.89	
-1.89	-1.89	-1.89	-1.89	-1.89	-1.89	
-3.61	-1.89	-1.89	-1.89	-1.89	-3.61	
mean policy
v	>	v	v	v	v	
>	v	>	v	<	<	
>	v	v	v	<	v	
>	>	>	v	<	<	
>	>	>	^	<	<	
>	>	^	^	<	<	
Optimal Policy
v	>	v	v	v	<	
>	v	>	v	<	<	
>	v	>	v	<	v	
>	>	>	v	<	v	
>	>	^	^	<	<	
>	>	^	^	<	<	
MAP policy loss 0.10806254423077803
Mean policy loss 0.10806251457204245
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	v	<	
>	>	v	v	v	<	
>	>	>	v	v	<	
^	>	>	<	<	<	
^	<	^	^	^	<	
^	>	>	>	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-5.00	-1.00	-1.00	-1.00	-100.00	
-5.00	-100.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-100.00	-100.00	-1.00	-100.00	
-1.00	-100.00	-5.00	-5.00	-1.00	-100.00	
features
0 	0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	1 	
1 	1 	0 	0 	0 	3 	
1 	3 	0 	2 	0 	1 	
0 	1 	3 	3 	0 	3 	
0 	3 	1 	1 	0 	3 	
MAP on testing env
map_weights [-1.89613519 -6.62447902  4.04736766 -0.39200828]
map reward
-1.90	-1.90	-1.90	-1.90	-1.90	-1.90	
-0.39	-1.90	-1.90	-1.90	-1.90	-6.62	
-6.62	-6.62	-1.90	-1.90	-1.90	-0.39	
-6.62	-0.39	-1.90	4.05	-1.90	-6.62	
-1.90	-6.62	-0.39	-0.39	-1.90	-0.39	
-1.90	-0.39	-6.62	-6.62	-1.90	-0.39	
Map policy
v	>	v	v	v	<	
>	>	v	v	v	v	
v	v	v	v	v	<	
>	>	>	v	<	<	
>	>	>	^	<	<	
>	>	^	^	^	^	
MEAN policy on test env
mean_weights [-1.89162612 -3.61295417  2.7222384   3.06535515]
mean reward
-1.89	-1.89	-1.89	-1.89	-1.89	-1.89	
3.07	-1.89	-1.89	-1.89	-1.89	-3.61	
-3.61	-3.61	-1.89	-1.89	-1.89	3.07	
-3.61	3.07	-1.89	2.72	-1.89	-3.61	
-1.89	-3.61	3.07	3.07	-1.89	3.07	
-1.89	3.07	-3.61	-3.61	-1.89	3.07	
mean policy
v	<	<	v	v	v	
<	<	<	v	v	v	
^	v	v	v	>	>	
>	>	v	v	<	v	
v	>	>	<	>	v	
>	v	^	^	>	>	
features
0 	0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	1 	
1 	1 	0 	0 	0 	3 	
1 	3 	0 	2 	0 	1 	
0 	1 	3 	3 	0 	3 	
0 	3 	1 	1 	0 	3 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	v	
>	v	v	v	v	v	
v	v	>	v	v	<	
>	>	>	v	<	<	
>	>	>	^	<	<	
>	>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [10.36924393  0.          5.33798079  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	v	<	
>	>	v	v	v	v	
>	v	v	v	v	<	
>	>	>	v	<	<	
>	>	>	^	<	<	
>	>	^	^	^	^	
-------- IRD Solution -------
ird reward
-29.23	-29.23	-29.23	-29.23	-29.23	-29.23	
-28.23	-29.23	-29.23	-29.23	-29.23	-33.05	
-33.05	-33.05	-29.23	-29.23	-29.23	-28.23	
-33.05	-28.23	-29.23	-28.97	-29.23	-33.05	
-29.23	-33.05	-28.23	-28.23	-29.23	-28.23	
-29.23	-28.23	-33.05	-33.05	-29.23	-28.23	
ird policy
v	<	<	v	v	<	
<	<	<	v	v	v	
^	v	v	v	>	>	
>	>	v	v	<	^	
v	>	>	<	<	>	
>	v	^	^	>	>	
MAP policy loss 885.3668682841428
mean policy loss 1845.3993101058618
robust policy loss 897.7930017456567
regret policy loss 885.3667383202188
ird policy loss 1840.4540988330855
MAP lava occupancy 9.179953481581345
Mean lava occupancy 18.70493402819369
Robust lava occupancy 9.3003494700671
Regret lava occupancy 9.179952160016077
IRD lava occupancy 18.656048616071075
##############
Trial  1
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	>	>	v	
>	>	>	>	>	>	
^	^	^	>	^	^	
^	>	^	>	^	<	
^	>	>	>	^	^	
^	^	^	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	2 	
0 	1 	0 	1 	0 	1 	
0 	0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	0 	
demonstration
[(0, 1), (9, 1), (8, 1), (11, 1), (2, 3), (10, 1), (1, 1)]
w_map [-7.17216505 -6.60591231  6.98741553  0.63598378] loglik -2.079419268538004
accepted/total = 1948/2000 = 0.974
MAP Policy on Train MDP
map_weights [-7.17216505 -6.60591231  6.98741553  0.63598378]
map reward
-7.17	-7.17	-7.17	-7.17	-7.17	-7.17	
-7.17	-7.17	-7.17	-7.17	-7.17	6.99	
-7.17	-6.61	-7.17	-6.61	-7.17	-6.61	
-7.17	-7.17	-7.17	-6.61	-7.17	-7.17	
-7.17	-7.17	-7.17	-7.17	-7.17	-7.17	
-7.17	-7.17	-7.17	-6.61	-7.17	-7.17	
Map policy
>	>	>	>	>	v	
>	>	>	>	>	>	
>	>	>	>	>	^	
^	^	>	^	^	^	
^	^	^	^	^	^	
>	>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [-3.98300988 -8.25048851  4.98531901 -3.1355237 ]
mean reward
-3.98	-3.98	-3.98	-3.98	-3.98	-3.98	
-3.98	-3.98	-3.98	-3.98	-3.98	4.99	
-3.98	-8.25	-3.98	-8.25	-3.98	-8.25	
-3.98	-3.98	-3.98	-8.25	-3.98	-3.98	
-3.98	-3.98	-3.98	-3.98	-3.98	-3.98	
-3.98	-3.98	-3.98	-8.25	-3.98	-3.98	
mean policy
>	>	>	>	>	v	
>	>	>	>	>	>	
^	>	^	>	^	^	
^	>	^	>	^	^	
^	>	>	>	^	^	
^	^	^	>	^	^	
Optimal Policy
>	>	>	>	>	v	
>	>	>	>	>	>	
^	^	^	>	^	^	
^	>	^	>	^	<	
^	>	>	>	^	^	
^	^	^	>	^	^	
MAP policy loss 4.703508523080556
Mean policy loss 0.022205921755897307
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	>	>	>	
^	^	>	>	^	^	
>	>	>	^	^	^	
^	^	^	^	>	^	
^	^	>	>	^	^	
>	^	>	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	1.00	
-100.00	-5.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-5.00	-100.00	-1.00	-1.00	
-5.00	-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	2 	
3 	1 	3 	0 	0 	0 	
0 	0 	0 	0 	3 	0 	
0 	0 	1 	3 	0 	0 	
1 	1 	1 	0 	0 	0 	
0 	0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-7.17216505 -6.60591231  6.98741553  0.63598378]
map reward
-7.17	-7.17	-7.17	-7.17	-7.17	6.99	
0.64	-6.61	0.64	-7.17	-7.17	-7.17	
-7.17	-7.17	-7.17	-7.17	0.64	-7.17	
-7.17	-7.17	-6.61	0.64	-7.17	-7.17	
-6.61	-6.61	-6.61	-7.17	-7.17	-7.17	
-7.17	-7.17	0.64	-7.17	-7.17	-7.17	
Map policy
>	>	>	>	>	>	
>	>	>	>	^	^	
^	^	^	>	^	^	
^	>	>	^	^	^	
>	>	^	^	^	^	
>	>	^	^	^	^	
MEAN policy on test env
mean_weights [-3.98300988 -8.25048851  4.98531901 -3.1355237 ]
mean reward
-3.98	-3.98	-3.98	-3.98	-3.98	4.99	
-3.14	-8.25	-3.14	-3.98	-3.98	-3.98	
-3.98	-3.98	-3.98	-3.98	-3.14	-3.98	
-3.98	-3.98	-8.25	-3.14	-3.98	-3.98	
-8.25	-8.25	-8.25	-3.98	-3.98	-3.98	
-3.98	-3.98	-3.14	-3.98	-3.98	-3.98	
mean policy
>	>	>	>	>	>	
^	>	>	>	>	^	
^	>	^	>	^	^	
^	^	>	^	^	^	
^	^	>	^	^	^	
>	>	>	^	^	^	
features
0 	0 	0 	0 	0 	2 	
3 	1 	3 	0 	0 	0 	
0 	0 	0 	0 	3 	0 	
0 	0 	1 	3 	0 	0 	
1 	1 	1 	0 	0 	0 	
0 	0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	>	>	>	>	
^	>	>	>	>	^	
^	>	^	>	^	^	
^	^	>	^	^	^	
^	^	>	^	^	^	
>	>	>	^	^	^	
------ Regret Solution ---------
expert u_sa [ 5.29816219  0.         10.40906253  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	>	>	>	>	
^	^	>	>	^	^	
>	>	>	^	^	^	
^	^	^	^	>	^	
^	^	>	>	^	^	
>	^	>	>	>	^	
-------- IRD Solution -------
ird reward
-96.17	-96.17	-96.17	-96.17	-96.17	-81.20	
-95.98	-104.40	-95.98	-96.17	-96.17	-96.17	
-96.17	-96.17	-96.17	-96.17	-95.98	-96.17	
-96.17	-96.17	-104.40	-95.98	-96.17	-96.17	
-104.40	-104.40	-104.40	-96.17	-96.17	-96.17	
-96.17	-96.17	-95.98	-96.17	-96.17	-96.17	
ird policy
>	>	>	>	>	>	
^	>	>	>	>	^	
^	>	^	>	^	^	
^	^	>	^	^	^	
^	^	>	^	^	^	
>	>	>	^	^	^	
MAP policy loss 84.67604347536697
mean policy loss 69.37127332248802
robust policy loss 69.37096258508411
regret policy loss 4.152852010278624e-08
ird policy loss 69.37127323476975
MAP lava occupancy 0.9789903314098869
Mean lava occupancy 0.8416879460196771
Robust lava occupancy 0.8416847903298127
Regret lava occupancy 0.1388888892359104
IRD lava occupancy 0.841687945134018
##############
Trial  2
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	v	v	v	
>	>	>	v	v	<	
^	^	>	>	<	<	
>	^	>	^	<	<	
^	>	>	^	^	^	
^	>	>	^	^	^	
reward
-5.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	-1.00	
features
1 	0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	0 	
1 	0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	0 	
0 	1 	0 	0 	0 	0 	
1 	1 	0 	0 	0 	0 	
demonstration
[(0, 1), (1, 3), (15, 2), (7, 1), (8, 1), (15, 3), (9, 3), (21, 2), (15, 1), (16, 0)]
w_map [ 0.79081783 -4.13708168  1.33394415 11.65599181] loglik -4.682118952460087
accepted/total = 1867/2000 = 0.9335
MAP Policy on Train MDP
map_weights [ 0.79081783 -4.13708168  1.33394415 11.65599181]
map reward
-4.14	0.79	0.79	0.79	0.79	0.79	
0.79	0.79	0.79	0.79	0.79	0.79	
-4.14	0.79	-4.14	1.33	0.79	0.79	
0.79	0.79	-4.14	0.79	0.79	0.79	
0.79	-4.14	0.79	0.79	0.79	0.79	
-4.14	-4.14	0.79	0.79	0.79	0.79	
Map policy
>	>	>	v	v	<	
>	>	>	v	<	<	
^	^	>	>	<	<	
>	^	>	^	^	^	
^	>	>	^	^	^	
^	>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.73461803 -4.63522603  0.603191    7.65872227]
mean reward
-4.64	-0.73	-0.73	-0.73	-0.73	-0.73	
-0.73	-0.73	-0.73	-0.73	-0.73	-0.73	
-4.64	-0.73	-4.64	0.60	-0.73	-0.73	
-0.73	-0.73	-4.64	-0.73	-0.73	-0.73	
-0.73	-4.64	-0.73	-0.73	-0.73	-0.73	
-4.64	-4.64	-0.73	-0.73	-0.73	-0.73	
mean policy
>	>	>	v	v	v	
>	>	>	v	v	<	
^	^	>	>	<	<	
>	^	>	^	<	<	
^	>	>	^	^	^	
^	>	>	^	^	^	
Optimal Policy
>	>	>	v	v	v	
>	>	>	v	v	<	
^	^	>	>	<	<	
>	^	>	^	<	<	
^	>	>	^	^	^	
^	>	>	^	^	^	
MAP policy loss 3.421243371711652e-06
Mean policy loss 2.7249644381222016e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	^	<	^	<	
>	>	^	<	v	v	
v	^	v	v	v	v	
<	v	v	<	<	<	
>	>	<	<	^	^	
<	^	^	>	^	<	
reward
-100.00	-1.00	-1.00	-100.00	-5.00	-100.00	
-5.00	-1.00	-1.00	-1.00	-100.00	-100.00	
-5.00	-5.00	-100.00	-100.00	-1.00	-5.00	
-1.00	-100.00	-1.00	-1.00	-1.00	-1.00	
-100.00	1.00	-1.00	-5.00	-1.00	-100.00	
-1.00	-100.00	-5.00	-100.00	-1.00	-1.00	
features
3 	0 	0 	3 	1 	3 	
1 	0 	0 	0 	3 	3 	
1 	1 	3 	3 	0 	1 	
0 	3 	0 	0 	0 	0 	
3 	2 	0 	1 	0 	3 	
0 	3 	1 	3 	0 	0 	
MAP on testing env
map_weights [ 0.79081783 -4.13708168  1.33394415 11.65599181]
map reward
11.66	0.79	0.79	11.66	-4.14	11.66	
-4.14	0.79	0.79	0.79	11.66	11.66	
-4.14	-4.14	11.66	11.66	0.79	-4.14	
0.79	11.66	0.79	0.79	0.79	0.79	
11.66	1.33	0.79	-4.14	0.79	11.66	
0.79	11.66	-4.14	11.66	0.79	0.79	
Map policy
<	<	>	^	>	>	
^	>	v	v	>	^	
v	>	>	<	^	^	
v	v	^	^	v	v	
<	v	<	v	>	>	
>	v	>	v	<	^	
MEAN policy on test env
mean_weights [-0.73461803 -4.63522603  0.603191    7.65872227]
mean reward
7.66	-0.73	-0.73	7.66	-4.64	7.66	
-4.64	-0.73	-0.73	-0.73	7.66	7.66	
-4.64	-4.64	7.66	7.66	-0.73	-4.64	
-0.73	7.66	-0.73	-0.73	-0.73	-0.73	
7.66	0.60	-0.73	-4.64	-0.73	7.66	
-0.73	7.66	-4.64	7.66	-0.73	-0.73	
mean policy
<	<	>	^	>	>	
^	>	v	v	>	^	
v	>	>	<	^	^	
v	v	^	^	v	v	
<	v	<	v	>	>	
>	v	>	v	<	^	
features
3 	0 	0 	3 	1 	3 	
1 	0 	0 	0 	3 	3 	
1 	1 	3 	3 	0 	1 	
0 	3 	0 	0 	0 	0 	
3 	2 	0 	1 	0 	3 	
0 	3 	1 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
<	<	>	^	v	>	
^	^	v	>	>	^	
v	>	>	<	^	^	
v	v	^	^	^	v	
<	v	<	v	>	>	
>	v	<	v	<	^	
------ Regret Solution ---------
expert u_sa [8.86236204 1.         5.84486268 0.        ]
Policy for lambda=0.0 and alpha=0.95
<	<	v	^	>	v	
^	>	v	v	>	<	
v	v	v	<	<	^	
>	v	<	<	<	v	
>	v	<	<	>	>	
^	^	<	v	<	^	
-------- IRD Solution -------
ird reward
-17.00	-17.70	-17.70	-17.00	-22.45	-17.00	
-22.45	-17.70	-17.70	-17.70	-17.00	-17.00	
-22.45	-22.45	-17.00	-17.00	-17.70	-22.45	
-17.70	-17.00	-17.70	-17.70	-17.70	-17.70	
-17.00	-16.58	-17.70	-22.45	-17.70	-17.00	
-17.70	-17.00	-22.45	-17.00	-17.70	-17.70	
ird policy
>	>	v	v	<	v	
>	>	v	v	<	<	
v	v	v	<	<	<	
v	v	<	<	<	<	
>	v	<	<	^	^	
>	^	<	v	^	^	
MAP policy loss 1873.8235312867143
mean policy loss 1873.8235330158939
robust policy loss 1873.8235255322668
regret policy loss 1275.952315826301
ird policy loss 939.3449887133963
MAP lava occupancy 19.20138888977715
Mean lava occupancy 19.20138890672695
Robust lava occupancy 19.201388832094224
Regret lava occupancy 13.274489165099586
IRD lava occupancy 9.924478187389804
##############
Trial  3
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	>	^	<	<	
^	>	^	^	^	^	
>	^	^	^	^	^	
^	^	>	^	^	^	
^	^	^	^	^	^	
>	^	^	>	^	<	
reward
-1.00	-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	0 	
1 	0 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	0 	
demonstration
[(0, 1), (3, 2), (1, 1), (2, 1)]
w_map [ 0.05035573 -0.35498271  0.46496895  1.26299515] loglik 0.0
accepted/total = 1896/2000 = 0.948
MAP Policy on Train MDP
map_weights [ 0.05035573 -0.35498271  0.46496895  1.26299515]
map reward
0.05	0.05	0.05	0.46	0.05	0.05	
-0.35	0.05	0.05	0.05	0.05	0.05	
0.05	0.05	-0.35	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	0.05	
-0.35	0.05	0.05	-0.35	0.05	-0.35	
0.05	0.05	0.05	0.05	0.05	0.05	
Map policy
>	>	>	^	<	<	
^	>	^	^	^	^	
>	^	^	^	^	^	
^	^	>	^	^	^	
^	^	^	^	^	^	
>	^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [1.41918882 1.81881444 6.37505806 0.645403  ]
mean reward
1.42	1.42	1.42	6.38	1.42	1.42	
1.82	1.42	1.42	1.42	1.42	1.42	
1.42	1.42	1.82	1.42	1.42	1.42	
1.42	1.42	1.42	1.42	1.42	1.42	
1.82	1.42	1.42	1.82	1.42	1.82	
1.42	1.42	1.42	1.42	1.42	1.42	
mean policy
>	>	>	^	<	<	
>	>	^	^	^	^	
^	>	^	^	^	^	
^	^	^	^	^	^	
^	>	>	^	<	<	
^	>	^	^	^	^	
Optimal Policy
>	>	>	^	<	<	
^	>	^	^	^	^	
>	^	^	^	^	^	
^	^	>	^	^	^	
^	^	^	^	^	^	
>	^	^	>	^	<	
MAP policy loss 0.02172932909275855
Mean policy loss 1.7385751307159203
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	v	<	
>	v	v	>	v	v	
>	>	v	v	v	<	
>	>	>	v	<	<	
>	>	>	^	^	<	
>	>	^	^	^	<	
reward
-100.00	-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	-1.00	-100.00	-1.00	-1.00	
-100.00	-5.00	-1.00	1.00	-1.00	-5.00	
-100.00	-5.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-5.00	-1.00	-100.00	-1.00	-1.00	
features
3 	1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	1 	
1 	0 	0 	3 	0 	0 	
3 	1 	0 	2 	0 	1 	
3 	1 	0 	0 	0 	3 	
0 	1 	0 	3 	0 	0 	
MAP on testing env
map_weights [ 0.05035573 -0.35498271  0.46496895  1.26299515]
map reward
1.26	-0.35	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	-0.35	
-0.35	0.05	0.05	1.26	0.05	0.05	
1.26	-0.35	0.05	0.46	0.05	-0.35	
1.26	-0.35	0.05	0.05	0.05	1.26	
0.05	-0.35	0.05	1.26	0.05	0.05	
Map policy
<	<	<	<	<	v	
^	<	<	v	v	v	
v	<	>	v	<	v	
v	<	<	v	v	v	
<	<	v	v	>	>	
^	>	>	v	<	^	
MEAN policy on test env
mean_weights [1.41918882 1.81881444 6.37505806 0.645403  ]
mean reward
0.65	1.82	1.42	1.42	1.42	1.42	
1.42	1.42	1.42	1.42	1.42	1.82	
1.82	1.42	1.42	0.65	1.42	1.42	
0.65	1.82	1.42	6.38	1.42	1.82	
0.65	1.82	1.42	1.42	1.42	0.65	
1.42	1.82	1.42	0.65	1.42	1.42	
mean policy
>	v	v	v	v	v	
v	v	v	v	v	v	
>	v	v	v	v	v	
>	>	>	v	<	<	
>	^	>	^	<	^	
>	^	^	^	^	<	
features
3 	1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	1 	
1 	0 	0 	3 	0 	0 	
3 	1 	0 	2 	0 	1 	
3 	1 	0 	0 	0 	3 	
0 	1 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	v	v	v	v	
v	v	v	v	v	v	
v	v	v	v	v	v	
>	>	>	^	<	<	
>	^	>	^	<	^	