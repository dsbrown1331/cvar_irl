##############
Trial  0
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
<	>	v	<	<	
<	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (3, 3), (8, 0), (17, 2), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [ 0.14452985 -0.01000859  0.77312086 -0.07234071] loglik -5.3752783471520615
accepted/total = 1488/2000 = 0.744
MAP Policy on Train MDP
map_weights [ 0.14452985 -0.01000859  0.77312086 -0.07234071]
map reward
0.14	0.14	0.14	0.14	0.14	
0.14	-0.01	0.14	0.14	0.14	
0.14	-0.01	0.77	0.14	0.14	
0.14	0.14	0.14	-0.01	0.14	
0.14	0.14	0.14	0.14	0.14	
Map policy
>	>	v	v	v	
<	>	v	v	<	
>	>	>	<	<	
>	>	^	<	^	
>	^	^	<	^	
MEAN policy on Train MDP
mean_weights [ 0.05793634 -0.20938363  0.3919325   0.05599571]
mean reward
0.06	0.06	0.06	0.06	0.06	
0.06	-0.21	0.06	0.06	0.06	
0.06	-0.21	0.39	0.06	0.06	
0.06	0.06	0.06	-0.21	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
>	>	v	v	v	
<	>	v	v	<	
>	>	>	<	<	
>	>	^	<	^	
>	^	^	<	^	
Optimal Policy
>	>	v	v	v	
<	>	v	<	<	
<	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
MAP policy loss 1.0193460818026454e-06
Mean policy loss -1.267285505701654e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	^	v	v	
v	v	v	>	v	
>	>	<	<	v	
^	^	^	<	<	
>	^	^	<	^	
reward
-1.00	-100.00	-5.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	1.00	-100.00	-5.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	3 	1 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	2 	3 	1 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.14452985 -0.01000859  0.77312086 -0.07234071]
map reward
0.14	-0.07	-0.01	-0.01	0.14	
0.14	-0.01	-0.07	0.14	0.14	
0.14	0.14	0.77	-0.07	-0.01	
-0.01	0.14	-0.01	0.14	0.14	
0.14	0.14	0.14	-0.01	0.14	
Map policy
v	v	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	^	
MEAN policy on test env
mean_weights [ 0.05793634 -0.20938363  0.3919325   0.05599571]
mean reward
0.06	0.06	-0.21	-0.21	0.06	
0.06	-0.21	0.06	0.06	0.06	
0.06	0.06	0.39	0.06	-0.21	
-0.21	0.06	-0.21	0.06	0.06	
0.06	0.06	0.06	-0.21	0.06	
mean policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	^	
features
0 	3 	1 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	2 	3 	1 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	<	<	<	v	
v	v	v	<	<	
>	>	<	<	v	
^	^	^	<	<	
>	^	<	<	^	
-------- IRD Solution -------
ird reward
-9.03	-9.42	-9.41	-9.41	-9.03	
-9.03	-9.41	-9.42	-9.03	-9.03	
-9.03	-9.03	-8.84	-9.42	-9.41	
-9.41	-9.03	-9.41	-9.03	-9.03	
-9.03	-9.03	-9.03	-9.41	-9.03	
ird policy
v	<	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	<	<	^	
MAP policy loss 614.9485295011973
mean policy loss 39.49915342533517
robust policy loss 39.499154087681966
regret policy loss 19.13649716219163
ird policy loss 19.136500182466065
MAP lava occupancy 6.092095579591351
Mean lava occupancy 6.092095579591351
Robust lava occupancy 0.42868750622993834
Regret lava occupancy 0.21434374996414585
IRD lava occupancy 0.2143437799574109
##############
Trial  1
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (12, 3), (17, 2), (11, 1), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [-0.03800785 -0.04004018  0.8508199  -0.07113207] loglik -9.010913347278802
accepted/total = 1810/2000 = 0.905
MAP Policy on Train MDP
map_weights [-0.03800785 -0.04004018  0.8508199  -0.07113207]
map reward
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	0.85	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
Map policy
>	v	v	v	v	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.1625659   0.01436311  0.20048212  0.04746042]
mean reward
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.16	-0.16	0.20	-0.16	-0.16	
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.16	-0.16	-0.16	-0.16	-0.16	
mean policy
v	v	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.5125451416999293e-10
Mean policy loss 3.2560753189015985e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	v	v	
>	^	v	v	v	
v	v	>	<	<	
>	>	^	<	<	
^	<	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-100.00	-100.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-100.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	2 	0 	0 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	0 	
MAP on testing env
map_weights [-0.03800785 -0.04004018  0.8508199  -0.07113207]
map reward
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.07	-0.04	-0.04	
-0.07	-0.07	0.85	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.07	-0.07	-0.04	-0.04	
Map policy
>	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	^	<	
MEAN policy on test env
mean_weights [-0.1625659   0.01436311  0.20048212  0.04746042]
mean reward
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.16	-0.16	0.05	-0.16	-0.16	
0.05	0.05	0.20	-0.16	-0.16	
0.01	-0.16	-0.16	0.01	0.01	
-0.16	0.05	0.05	-0.16	-0.16	
mean policy
v	v	v	<	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	<	<	
^	^	<	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	2 	0 	0 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
<	^	^	<	<	
^	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	^	v	v	v	
^	>	v	<	<	
>	>	^	<	^	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-7.75	-7.75	-7.75	-7.75	-7.75	
-7.75	-7.75	-8.10	-7.75	-7.75	
-8.10	-8.10	-7.42	-7.75	-7.75	
-8.07	-7.75	-7.75	-8.07	-8.07	
-7.75	-8.10	-8.10	-7.75	-7.75	
ird policy
>	>	>	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
^	^	^	^	<	
MAP policy loss 465.1478581508449
mean policy loss 894.5618502428492
robust policy loss 547.3737168820419
regret policy loss 22.312383265700667
ird policy loss 1.569256391498186e-09
MAP lava occupancy 4.706433211894366
Mean lava occupancy 4.706433211894366
Robust lava occupancy 5.324001445166023
Regret lava occupancy 0.23486719206358506
IRD lava occupancy 1.6464872046031026e-11
##############
Trial  2
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
<	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
demonstration
[(7, 3), (12, 2), (9, 0), (12, 1), (8, 3), (6, 1), (17, 2), (13, 0), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [-0.23890795 -0.36328488  0.38732193  0.01048524] loglik -6.0684255883597515
accepted/total = 1315/2000 = 0.6575
MAP Policy on Train MDP
map_weights [-0.23890795 -0.36328488  0.38732193  0.01048524]
map reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.36	0.39	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.36	
-0.24	-0.24	-0.24	-0.36	-0.24	
Map policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.04433677 -0.26682838  0.34436191 -0.00106863]
mean reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	-0.27	0.34	0.04	0.04	
0.04	0.04	0.04	0.04	-0.27	
0.04	0.04	0.04	-0.27	0.04	
mean policy
>	>	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
<	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
MAP policy loss -2.5481086372847184e-08
Mean policy loss -2.5395434553876495e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	v	
v	v	v	<	v	
>	>	<	<	<	
^	<	^	^	<	
>	>	^	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-100.00	-1.00	-1.00	-100.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
3 	0 	0 	3 	1 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.23890795 -0.36328488  0.38732193  0.01048524]
map reward
-0.24	-0.36	-0.24	-0.36	-0.24	
0.01	-0.24	-0.24	0.01	-0.36	
-0.24	-0.24	0.39	-0.24	-0.24	
-0.24	-0.36	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
Map policy
v	>	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	^	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.04433677 -0.26682838  0.34436191 -0.00106863]
mean reward
0.04	-0.27	0.04	-0.27	0.04	
-0.00	0.04	0.04	-0.00	-0.27	
0.04	0.04	0.34	0.04	0.04	
0.04	-0.27	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
mean policy
v	>	v	<	v	
>	>	v	<	v	
>	>	^	<	<	
^	>	^	<	<	
>	>	^	^	^	
features
0 	1 	0 	1 	0 	
3 	0 	0 	3 	1 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
<	v	v	<	>	
v	v	v	v	v	
>	>	^	<	<	
^	^	^	<	<	
^	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	<	
>	>	v	v	v	
>	>	^	<	<	
^	^	^	^	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-8.85	-9.16	-8.85	-9.16	-8.85	
-9.18	-8.85	-8.85	-9.18	-9.16	
-8.85	-8.85	-8.49	-8.85	-8.85	
-8.85	-9.16	-8.85	-8.85	-8.85	
-8.85	-8.85	-8.85	-8.85	-8.85	
ird policy
>	v	v	<	v	
v	v	v	v	v	
>	>	<	<	<	
^	>	^	^	<	
>	>	^	^	^	
MAP policy loss 22.90292789753444
mean policy loss 22.56250130037179
robust policy loss 2.89582458849533
regret policy loss 22.593614352262826
ird policy loss 6.768683385352525e-06
MAP lava occupancy 0.228886024095239
Mean lava occupancy 0.228886024095239
Robust lava occupancy 2.8676466192334126e-10
Regret lava occupancy 0.23750000000220053
IRD lava occupancy 6.828806695706758e-08
##############
Trial  3
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	^	v	<	<	
>	v	v	v	v	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [ 0.04299048  0.04047688  0.50621554 -0.4103171 ] loglik -6.931046241002889
accepted/total = 1174/2000 = 0.587
MAP Policy on Train MDP
map_weights [ 0.04299048  0.04047688  0.50621554 -0.4103171 ]
map reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.51	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
Map policy
v	>	v	<	<	
>	>	v	v	v	
>	>	^	<	<	
>	>	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.05960873 -0.21709626  0.33496795  0.0095236 ]
mean reward
0.06	-0.22	0.06	0.06	0.06	
0.06	0.06	0.06	-0.22	0.06	
0.06	0.06	0.33	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
v	>	v	<	<	
>	>	v	>	v	
>	>	^	<	<	
>	>	^	^	<	
>	^	^	^	<	
Optimal Policy
v	^	v	<	<	
>	v	v	v	v	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 2.0336968505375863e-06
Mean policy loss 2.960863227825794e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	^	v	>	v	
v	>	v	<	<	
>	>	v	<	>	
>	>	^	<	^	
^	^	^	<	>	
reward
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	1 	3 	
0 	0 	1 	3 	0 	
MAP on testing env
map_weights [ 0.04299048  0.04047688  0.50621554 -0.4103171 ]
map reward
0.04	0.04	-0.41	-0.41	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.51	0.04	0.04	
0.04	0.04	0.04	0.04	-0.41	
0.04	0.04	0.04	-0.41	0.04	
Map policy
v	v	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	<	^	
^	^	^	<	^	
MEAN policy on test env
mean_weights [ 0.05960873 -0.21709626  0.33496795  0.0095236 ]
mean reward
0.06	0.06	0.01	0.01	0.06	
0.06	-0.22	0.06	0.06	0.06	
0.06	0.06	0.33	-0.22	0.06	
0.06	0.06	0.06	-0.22	0.01	
0.06	0.06	-0.22	0.01	0.06	
mean policy
v	>	v	v	v	
v	v	v	<	<	
>	>	v	<	^	
>	^	^	<	^	
^	^	^	<	^	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	1 	3 	
0 	0 	1 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	^	v	v	v	
v	^	v	<	<	
>	>	v	<	>	
>	>	^	<	^	
^	^	^	>	>	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
v	v	v	<	<	
>	>	v	<	^	
>	>	^	<	^	
^	^	^	<	^	
-------- IRD Solution -------
ird reward
-8.73	-8.73	-9.14	-9.14	-8.73	
-8.73	-9.14	-8.73	-8.73	-8.73	
-8.73	-8.73	-8.54	-9.14	-8.73	
-8.73	-8.73	-8.73	-9.14	-9.14	
-8.73	-8.73	-9.14	-9.14	-8.73	
ird policy
v	^	v	v	v	
v	v	v	<	<	
>	>	v	<	^	
>	^	^	^	^	
^	^	<	v	^	
MAP policy loss 53.57445493720355
mean policy loss 19.742798398153177
robust policy loss 1.417155636499721
regret policy loss 19.935350320546394
ird policy loss 19.742797997143036
MAP lava occupancy 0.47904617322540133
Mean lava occupancy 0.47904617322540133
Robust lava occupancy 3.6511033721450173e-09
Regret lava occupancy 0.23749999968334556
IRD lava occupancy 0.23750000000462415
##############
Trial  4
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	^	v	<	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (3, 0), (4, 0), (12, 3), (17, 2), (11, 1), (2, 3), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [-0.1223528  -0.1262146   0.65296504  0.09846757] loglik -7.6246185589535
accepted/total = 1476/2000 = 0.738
MAP Policy on Train MDP
map_weights [-0.1223528  -0.1262146   0.65296504  0.09846757]
map reward
-0.12	-0.13	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	0.65	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.13	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
Map policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
^	^	^	<	^	
MEAN policy on Train MDP
mean_weights [ 0.07926744 -0.23554799  0.38517302 -0.10659762]
mean reward
0.08	-0.24	0.08	0.08	0.08	
0.08	0.08	0.08	0.08	0.08	
0.08	0.08	0.39	0.08	0.08	
0.08	0.08	0.08	-0.24	0.08	
0.08	0.08	0.08	0.08	0.08	
mean policy
v	v	v	<	<	
>	>	v	<	<	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
Optimal Policy
v	^	v	<	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	<	
MAP policy loss 7.629388808598481e-07
Mean policy loss 3.7956277010309236e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	>	v	^	^	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.1223528  -0.1262146   0.65296504  0.09846757]
map reward
-0.12	-0.12	-0.12	0.10	-0.12	
-0.12	-0.12	-0.13	-0.12	-0.13	
-0.13	-0.12	0.65	-0.12	-0.12	
-0.12	-0.13	-0.13	-0.12	-0.12	
-0.12	0.10	-0.12	-0.12	-0.12	
Map policy
>	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.07926744 -0.23554799  0.38517302 -0.10659762]
mean reward
0.08	0.08	0.08	-0.11	0.08	
0.08	0.08	-0.24	0.08	-0.24	
-0.24	0.08	0.39	0.08	0.08	
0.08	-0.24	-0.24	0.08	0.08	
0.08	-0.11	0.08	0.08	0.08	
mean policy
v	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	^	
^	>	^	^	^	
features
0 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	^	^	
^	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	<	v	<	
>	v	<	v	<	
>	>	<	<	<	
>	^	>	^	^	
^	>	>	^	^	
-------- IRD Solution -------
ird reward
-9.12	-9.12	-9.12	-9.60	-9.12	
-9.12	-9.12	-9.57	-9.12	-9.57	
-9.57	-9.12	-9.09	-9.12	-9.12	
-9.12	-9.57	-9.57	-9.12	-9.12	
-9.12	-9.60	-9.12	-9.12	-9.12	
ird policy
>	v	<	v	>	
>	v	v	v	v	
>	>	<	<	<	
<	^	^	^	^	
<	>	v	^	^	
MAP policy loss 41.01892434974129
mean policy loss 22.56249999392131
robust policy loss 22.56250039042128
regret policy loss 21.877801363295163
ird policy loss 6.501411283101451
MAP lava occupancy 0.2410504782554605
Mean lava occupancy 0.2410504782554605
Robust lava occupancy 0.23750000382071373
Regret lava occupancy 0.23029264598865198
IRD lava occupancy 9.495966627658649e-09
##############
Trial  5
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 3), (8, 3), (12, 0), (6, 1), (17, 2), (11, 1), (13, 0), (12, 3), (4, 0)]
w_map [-0.45655061 -0.00826625  0.50042848 -0.03475466] loglik -9.01086382411226
accepted/total = 1368/2000 = 0.684
MAP Policy on Train MDP
map_weights [-0.45655061 -0.00826625  0.50042848 -0.03475466]
map reward
-0.46	-0.46	-0.46	-0.46	-0.46	
-0.46	-0.46	-0.46	-0.46	-0.01	
-0.46	-0.46	0.50	-0.46	-0.46	
-0.46	-0.46	-0.46	-0.46	-0.46	
-0.46	-0.46	-0.46	-0.46	-0.46	
Map policy
>	>	v	v	v	
>	>	v	v	>	
>	>	>	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.04826023 -0.16635456  0.41591742 -0.05789097]
mean reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	-0.17	
0.05	0.05	0.42	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
v	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	^	^	
>	^	^	^	^	
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 23.176955128182467
Mean policy loss 6.986836803557495e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	v	
^	^	^	v	<	
^	^	<	<	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.45655061 -0.00826625  0.50042848 -0.03475466]
map reward
-0.46	-0.01	-0.46	-0.46	-0.46	
-0.46	-0.46	-0.46	-0.46	-0.46	
-0.46	-0.46	0.50	-0.01	-0.01	
-0.46	-0.46	-0.01	-0.46	-0.46	
-0.46	-0.46	-0.46	-0.46	-0.46	
Map policy
>	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.04826023 -0.16635456  0.41591742 -0.05789097]
mean reward
0.05	-0.17	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.42	-0.17	-0.17	
0.05	0.05	-0.17	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	<	<	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	<	<	<	
-------- IRD Solution -------
ird reward
-8.70	-9.14	-8.70	-8.70	-8.70	
-8.70	-8.70	-8.70	-8.70	-8.70	
-8.70	-8.70	-8.64	-9.14	-9.14	
-8.70	-8.70	-9.14	-8.70	-8.70	
-8.70	-8.70	-8.70	-8.70	-8.70	
ird policy
v	v	v	<	<	
>	v	v	<	<	
>	>	^	<	v	
^	^	^	v	<	
^	^	<	<	<	
MAP policy loss 17.648547357823087
mean policy loss 0.4501219433721581
robust policy loss 4.888990944488501
regret policy loss 0.18386941289598835
ird policy loss 1.3820039497514092e-07
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  6
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (1, 1), (13, 0)]
w_map [-0.69216309  0.02847135  0.16080239 -0.11856317] loglik -8.31776615915794
accepted/total = 1802/2000 = 0.901
MAP Policy on Train MDP
map_weights [-0.69216309  0.02847135  0.16080239 -0.11856317]
map reward
-0.69	-0.69	-0.69	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.69	-0.69	
-0.69	-0.69	0.16	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.69	-0.69	
Map policy
>	v	v	v	v	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.17693827  0.08087892  0.20069414 -0.05714519]
mean reward
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	0.20	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
mean policy
v	v	v	v	v	
>	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.5088897172125206e-10
Mean policy loss 3.6638203454735126e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-5.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	0 	0 	
1 	1 	1 	0 	1 	
0 	0 	2 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.69216309  0.02847135  0.16080239 -0.11856317]
map reward
-0.69	0.03	-0.12	-0.69	-0.69	
0.03	0.03	0.03	-0.69	0.03	
-0.69	-0.69	0.16	-0.69	-0.69	
0.03	0.03	-0.69	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.69	-0.69	
Map policy
v	v	v	<	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.17693827  0.08087892  0.20069414 -0.05714519]
mean reward
-0.18	0.08	-0.06	-0.18	-0.18	
0.08	0.08	0.08	-0.18	0.08	
-0.18	-0.18	0.20	-0.18	-0.18	
0.08	0.08	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
mean policy
v	v	v	<	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
features
0 	1 	3 	0 	0 	
1 	1 	1 	0 	1 	
0 	0 	2 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	^	<	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	>	v	<	
v	v	v	v	<	
>	>	<	<	<	
^	>	^	^	^	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-8.73	-9.08	-9.04	-8.73	-8.73	
-9.08	-9.08	-9.08	-8.73	-9.08	
-8.73	-8.73	-8.42	-8.73	-8.73	
-9.08	-9.08	-8.73	-8.73	-8.73	
-8.73	-8.73	-8.73	-8.73	-8.73	
ird policy
v	>	v	v	<	
v	>	v	v	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	^	
MAP policy loss 20.23223126838012
mean policy loss 37.1646089774284
robust policy loss 500.56947668326256
regret policy loss 2.203690285806914e-10
ird policy loss 9.006981861736651e-09
MAP lava occupancy 0.09542463242356253
Mean lava occupancy 0.09542463242356253
Robust lava occupancy 4.81723622891173
Regret lava occupancy 3.0031386380289033e-12
IRD lava occupancy 6.469852483033659e-11
##############
Trial  7
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [-0.10807373 -0.09174802  0.74385752 -0.05632074] loglik -8.317766159158396
accepted/total = 1826/2000 = 0.913
MAP Policy on Train MDP
map_weights [-0.10807373 -0.09174802  0.74385752 -0.05632074]
map reward
-0.11	-0.11	-0.11	-0.11	-0.11	
-0.11	-0.11	-0.11	-0.11	-0.11	
-0.11	-0.11	0.74	-0.11	-0.11	
-0.11	-0.11	-0.11	-0.11	-0.11	
-0.11	-0.11	-0.11	-0.11	-0.11	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.24764558 -0.01908261  0.13326017 -0.10153839]
mean reward
-0.25	-0.25	-0.25	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
-0.25	-0.25	0.13	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
mean policy
v	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.511670351008655e-10
Mean policy loss 3.7499957071605427e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	<	<	
>	>	v	v	v	
>	>	v	<	<	
^	^	^	^	^	
<	^	v	<	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	0 	
MAP on testing env
map_weights [-0.10807373 -0.09174802  0.74385752 -0.05632074]
map reward
-0.11	-0.09	-0.11	-0.11	-0.11	
-0.11	-0.11	-0.11	-0.09	-0.11	
-0.11	-0.11	0.74	-0.11	-0.11	
-0.06	-0.11	-0.11	-0.09	-0.11	
-0.11	-0.06	-0.11	-0.06	-0.11	
Map policy
>	v	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
^	^	^	^	^	
^	>	^	^	<	
MEAN policy on test env
mean_weights [-0.24764558 -0.01908261  0.13326017 -0.10153839]
mean reward
-0.25	-0.02	-0.25	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.02	-0.25	
-0.25	-0.25	0.13	-0.25	-0.25	
-0.10	-0.25	-0.25	-0.02	-0.25	
-0.25	-0.10	-0.25	-0.10	-0.25	
mean policy
>	^	<	<	<	
^	^	<	^	<	
^	^	^	<	^	
^	^	^	^	<	
^	^	^	^	<	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	^	v	v	v	
>	>	v	<	<	
>	>	<	<	<	
<	>	^	v	<	
>	v	>	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	<	<	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	^	^	
>	^	^	<	^	
-------- IRD Solution -------
ird reward
-8.36	-8.85	-8.36	-8.36	-8.36	
-8.36	-8.36	-8.36	-8.85	-8.36	
-8.36	-8.36	-8.32	-8.36	-8.36	
-8.75	-8.36	-8.36	-8.85	-8.36	
-8.36	-8.75	-8.36	-8.75	-8.36	
ird policy
v	>	v	<	<	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	^	
<	>	^	<	^	
MAP policy loss 32.60840424184251
mean policy loss 125.6409769325652
robust policy loss 549.1450477408187
regret policy loss 19.33554491180996
ird policy loss 9.69872076747591e-09
MAP lava occupancy 0.3562500000644608
Mean lava occupancy 0.3562500000644608
Robust lava occupancy 5.285188143800163
Regret lava occupancy 0.23750000018465714
IRD lava occupancy 1.1014666418313089e-10
##############
Trial  8
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [-0.0632547   0.25108602  0.65773225 -0.02792703] loglik -9.704060527839829
accepted/total = 1820/2000 = 0.91
MAP Policy on Train MDP
map_weights [-0.0632547   0.25108602  0.65773225 -0.02792703]
map reward
-0.06	-0.06	-0.06	-0.06	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
-0.06	-0.06	0.66	-0.06	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
Map policy
v	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.19879753  0.07209451  0.1714532   0.01142337]
mean reward
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	0.17	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss -1.5804887237012063e-10
Mean policy loss 3.4619541677556164e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	^	<	v	
v	<	^	>	<	
<	<	>	<	v	
^	v	v	>	<	
>	>	>	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	1.00	-5.00	-100.00	
-5.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	1 	3 	
1 	1 	3 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.0632547   0.25108602  0.65773225 -0.02792703]
map reward
-0.06	0.25	-0.06	0.25	-0.06	
-0.06	-0.06	-0.03	-0.06	-0.06	
-0.06	-0.03	0.66	0.25	-0.03	
0.25	0.25	-0.03	-0.06	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
Map policy
>	v	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.19879753  0.07209451  0.1714532   0.01142337]
mean reward
-0.20	0.07	-0.20	0.07	-0.20	
-0.20	-0.20	0.01	-0.20	-0.20	
-0.20	0.01	0.17	0.07	0.01	
0.07	0.07	0.01	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
mean policy
>	v	v	v	<	
v	v	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	^	^	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	1 	3 	
1 	1 	3 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	<	
v	v	v	<	v	
>	>	>	<	<	
>	^	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	^	<	v	
>	<	v	>	<	
<	>	>	<	v	
v	>	^	>	<	
>	>	>	^	^	
-------- IRD Solution -------
ird reward
-8.86	-9.34	-8.86	-9.34	-8.86	
-8.86	-8.86	-9.29	-8.86	-8.86	
-8.86	-9.29	-8.79	-9.34	-9.29	
-9.34	-9.34	-9.29	-8.86	-8.86	
-8.86	-8.86	-8.86	-8.86	-8.86	
ird policy
v	>	^	<	v	
v	<	^	>	<	
<	>	^	<	<	
^	v	^	>	<	
>	>	>	^	^	
MAP policy loss 656.3836103304652
mean policy loss 85.28122596162024
robust policy loss 485.4219513939703
regret policy loss 283.1515527034094
ird policy loss -5.218696738640283e-09
MAP lava occupancy 6.690416474799608
Mean lava occupancy 6.690416474799608
Robust lava occupancy 4.8663201362572055
Regret lava occupancy 2.854415083317323
IRD lava occupancy 6.262473088643198e-11
##############
Trial  9
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
v	>	v	<	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (8, 0), (17, 2), (11, 1), (2, 3), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.07341829 -0.07847665  0.37209238  0.47601269] loglik -7.624614389604631
accepted/total = 1141/2000 = 0.5705
MAP Policy on Train MDP
map_weights [-0.07341829 -0.07847665  0.37209238  0.47601269]
map reward
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.08	-0.07	-0.07	-0.07	
-0.07	-0.07	0.37	-0.07	-0.08	
-0.07	-0.08	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
Map policy
>	>	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
>	>	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.03122616 -0.22607614  0.29365525  0.03413536]
mean reward
0.03	0.03	0.03	0.03	0.03	
0.03	-0.23	0.03	0.03	0.03	
0.03	0.03	0.29	0.03	-0.23	
0.03	-0.23	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
mean policy
>	>	v	v	<	
v	v	v	v	<	
>	>	^	<	<	
^	^	^	^	<	
>	>	^	^	<	
Optimal Policy
>	>	v	v	<	
v	>	v	<	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	<	
MAP policy loss 7.011396530205716e-08
Mean policy loss 3.896949429815777e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	^	>	v	
v	v	v	v	v	
>	>	<	<	<	
>	^	v	^	v	
^	>	v	^	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-100.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.07341829 -0.07847665  0.37209238  0.47601269]
map reward
-0.07	-0.07	-0.07	0.48	-0.07	
-0.07	0.48	-0.08	-0.08	-0.07	
-0.07	-0.07	0.37	-0.07	-0.07	
-0.07	-0.07	0.48	-0.07	0.48	
-0.07	0.48	-0.07	-0.07	-0.07	
Map policy
>	>	>	^	<	
v	>	>	^	v	
v	>	v	<	v	
>	v	^	>	>	
>	v	<	^	^	
MEAN policy on test env
mean_weights [ 0.03122616 -0.22607614  0.29365525  0.03413536]
mean reward
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	-0.23	-0.23	0.03	
0.03	0.03	0.29	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
mean policy
v	v	v	<	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	^	^	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	^	v	v	
v	>	v	<	v	
>	>	<	<	<	
>	^	^	^	v	
^	<	v	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	^	v	v	
v	>	>	<	v	
>	>	>	<	<	
>	^	^	^	>	
^	v	v	^	<	
-------- IRD Solution -------
ird reward
-8.80	-8.80	-8.80	-9.25	-8.80	
-8.80	-9.25	-9.23	-9.23	-8.80	
-8.80	-8.80	-8.68	-8.80	-8.80	
-8.80	-8.80	-9.25	-8.80	-9.25	
-8.80	-9.25	-8.80	-8.80	-8.80	
ird policy
v	^	^	<	v	
v	^	^	v	v	
>	>	<	<	<	
>	^	^	^	^	
^	>	v	^	<	
MAP policy loss 263.30455630293875
mean policy loss 897.4871807175006
robust policy loss 4.905663011035237
regret policy loss 0.05157173269777149
ird policy loss 3.7112720535636967e-10
MAP lava occupancy 2.5634932777589023
Mean lava occupancy 2.5634932777589023
Robust lava occupancy 2.628554392317653e-09
Regret lava occupancy 2.8712829016298272e-12
IRD lava occupancy 4.114578993136158e-12
##############
Trial  10
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	>	v	<	
v	v	v	v	>	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (8, 3), (1, 3), (12, 1), (3, 3), (12, 0), (6, 3), (17, 2), (11, 1), (13, 0), (12, 3), (4, 0)]
w_map [-0.21622342 -0.38671399  0.36625619 -0.0308064 ] loglik -3.988984046999967
accepted/total = 1470/2000 = 0.735
MAP Policy on Train MDP
map_weights [-0.21622342 -0.38671399  0.36625619 -0.0308064 ]
map reward
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.39	-0.22	-0.39	
-0.22	-0.22	0.37	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
Map policy
v	v	v	v	<	
v	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.08389194 -0.21676113  0.38250749  0.02081555]
mean reward
0.08	0.08	0.08	0.08	0.08	
0.08	0.08	-0.22	0.08	-0.22	
0.08	0.08	0.38	0.08	0.08	
0.08	0.08	0.08	0.08	0.08	
0.08	0.08	0.08	0.08	0.08	
mean policy
v	v	v	v	<	
v	v	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	^	
Optimal Policy
v	v	>	v	<	
v	v	v	v	>	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	^	^	
MAP policy loss -8.628436160007524e-11
Mean policy loss 7.577658292512446e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	<	<	
v	<	v	^	<	
<	^	v	<	^	
^	^	>	>	^	
<	v	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-100.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-100.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	3 	0 	
3 	0 	3 	0 	0 	
0 	1 	3 	1 	0 	
MAP on testing env
map_weights [-0.21622342 -0.38671399  0.36625619 -0.0308064 ]
map reward
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.03	-0.22	-0.22	
-0.22	-0.03	0.37	-0.03	-0.22	
-0.03	-0.22	-0.03	-0.22	-0.22	
-0.22	-0.39	-0.03	-0.39	-0.22	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	>	^	<	<	
MEAN policy on test env
mean_weights [ 0.08389194 -0.21676113  0.38250749  0.02081555]
mean reward
0.08	0.08	0.08	0.08	0.08	
0.08	0.08	0.02	0.08	0.08	
0.08	0.02	0.38	0.02	0.08	
0.02	0.08	0.02	0.08	0.08	
0.08	-0.22	0.02	-0.22	0.08	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	3 	0 	
3 	0 	3 	0 	0 	
0 	1 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	^	<	<	
v	<	v	^	<	
<	>	v	<	^	
^	>	^	>	<	
<	^	^	>	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	<	<	<	
v	<	v	^	<	
<	>	v	<	^	
>	v	^	>	<	
<	v	^	v	^	
-------- IRD Solution -------
ird reward
-8.91	-8.91	-8.91	-8.91	-8.91	
-8.91	-8.91	-9.23	-8.91	-8.91	
-8.91	-9.23	-8.59	-9.23	-8.91	
-9.23	-8.91	-9.23	-8.91	-8.91	
-8.91	-9.24	-9.23	-9.24	-8.91	
ird policy
v	v	<	<	<	
v	<	v	^	<	
<	>	v	<	^	
^	<	^	>	^	
<	<	<	>	^	
MAP policy loss 875.5576251091834
mean policy loss 877.3700650358006
robust policy loss 340.43855378353874
regret policy loss 405.9360929531077
ird policy loss 1.9103684717469704e-08
MAP lava occupancy 9.003322032078051
Mean lava occupancy 9.003322032078051
Robust lava occupancy 3.5060613159488327
Regret lava occupancy 4.180598279627941
IRD lava occupancy 1.9627159176845653e-10
##############
Trial  11
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	^	v	
>	>	v	<	<	
>	>	<	<	^	
>	>	^	<	<	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 0), (8, 0), (17, 2), (11, 1), (2, 3), (4, 3), (1, 1), (12, 3)]
w_map [-0.19396828 -0.36736786  0.40660954 -0.03205431] loglik -4.682131227316987
accepted/total = 723/2000 = 0.3615
MAP Policy on Train MDP
map_weights [-0.19396828 -0.36736786  0.40660954 -0.03205431]
map reward
-0.19	-0.19	-0.19	-0.37	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	0.41	-0.37	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
Map policy
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
MEAN policy on Train MDP
mean_weights [ 0.05416138 -0.12388207  0.12467285 -0.2703531 ]
mean reward
0.05	0.05	0.05	-0.12	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.12	-0.12	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
>	>	v	v	v	
>	>	v	<	<	
>	>	<	<	^	
>	>	^	<	<	
>	^	^	<	<	
Optimal Policy
>	>	v	^	v	
>	>	v	<	<	
>	>	<	<	^	
>	>	^	<	<	
^	^	^	<	<	
MAP policy loss -8.435451469279709e-11
Mean policy loss -1.846577404873706e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	>	
>	v	v	v	>	
>	>	^	<	<	
>	>	^	<	>	
^	<	^	<	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	2 	1 	3 	
0 	0 	0 	1 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.19396828 -0.36736786  0.40660954 -0.03205431]
map reward
-0.19	-0.19	-0.19	-0.03	-0.19	
-0.19	-0.19	-0.19	-0.03	-0.19	
-0.03	-0.19	0.41	-0.37	-0.03	
-0.19	-0.19	-0.19	-0.37	-0.19	
-0.19	-0.03	-0.19	-0.19	-0.19	
Map policy
v	v	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	<	^	
>	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.05416138 -0.12388207  0.12467285 -0.2703531 ]
mean reward
0.05	0.05	0.05	-0.27	0.05	
0.05	0.05	0.05	-0.27	0.05	
-0.27	0.05	0.12	-0.12	-0.27	
0.05	0.05	0.05	-0.12	0.05	
0.05	-0.27	0.05	0.05	0.05	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	v	
>	^	^	<	v	
^	^	^	<	<	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	2 	1 	3 	
0 	0 	0 	1 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	>	>	
>	>	v	>	>	
v	>	^	<	^	
>	^	^	<	>	
^	<	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	<	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	>	^	<	<	
-------- IRD Solution -------
ird reward
-8.91	-8.91	-8.91	-9.31	-8.91	
-8.91	-8.91	-8.91	-9.31	-8.91	
-9.31	-8.91	-8.69	-9.28	-9.31	
-8.91	-8.91	-8.91	-9.28	-8.91	
-8.91	-9.31	-8.91	-8.91	-8.91	
ird policy
>	v	v	<	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
MAP policy loss 39.5292441722016
mean policy loss 18.159919874766942
robust policy loss 2.9757780894719854
regret policy loss 16.986035789235007
ird policy loss 18.159919873036582
MAP lava occupancy 0.4352753319640488
Mean lava occupancy 0.4352753319640488
Robust lava occupancy 6.548800441881927e-11
Regret lava occupancy 0.21104026625660072
IRD lava occupancy 0.22562500001204897
##############
Trial  12
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (8, 3), (12, 1), (3, 3), (12, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.8667058   0.02635377 -0.03175392  0.07518651] loglik -9.010913347278688
accepted/total = 1814/2000 = 0.907
MAP Policy on Train MDP
map_weights [-0.8667058   0.02635377 -0.03175392  0.07518651]
map reward
-0.87	-0.87	-0.87	-0.87	-0.87	
-0.87	-0.87	-0.87	-0.87	-0.87	
-0.87	-0.87	-0.03	-0.87	-0.87	
-0.87	-0.87	-0.87	-0.87	-0.87	
-0.87	-0.87	-0.87	-0.87	-0.87	
Map policy
>	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.15256537  0.11872703  0.23879173 -0.0151933 ]
mean reward
-0.15	-0.15	-0.15	-0.15	-0.15	
-0.15	-0.15	-0.15	-0.15	-0.15	
-0.15	-0.15	0.24	-0.15	-0.15	
-0.15	-0.15	-0.15	-0.15	-0.15	
-0.15	-0.15	-0.15	-0.15	-0.15	
mean policy
v	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 5.86657440481515e-10
Mean policy loss 4.0214969004056904e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
<	>	v	>	>	
^	>	<	<	^	
<	<	^	^	>	
>	>	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
-100.00	-1.00	1.00	-1.00	-100.00	
-1.00	-100.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	2 	0 	3 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.8667058   0.02635377 -0.03175392  0.07518651]
map reward
-0.87	-0.87	-0.87	-0.87	-0.87	
-0.87	0.03	-0.87	0.08	-0.87	
0.08	-0.87	-0.03	-0.87	0.08	
-0.87	0.08	0.03	0.08	-0.87	
-0.87	-0.87	-0.87	0.03	-0.87	
Map policy
v	v	v	v	v	
v	v	>	v	v	
<	<	v	>	>	
^	>	<	<	<	
^	^	>	^	<	
MEAN policy on test env
mean_weights [-0.15256537  0.11872703  0.23879173 -0.0151933 ]
mean reward
-0.15	-0.15	-0.15	-0.15	-0.15	
-0.15	0.12	-0.15	-0.02	-0.15	
-0.02	-0.15	0.24	-0.15	-0.02	
-0.15	-0.02	0.12	-0.02	-0.15	
-0.15	-0.15	-0.15	0.12	-0.15	
mean policy
v	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	2 	0 	3 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
v	v	v	<	<	
<	>	v	<	>	
>	^	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
<	^	^	^	<	
>	>	^	<	^	
-------- IRD Solution -------
ird reward
-8.84	-8.84	-8.84	-8.84	-8.84	
-8.84	-9.25	-8.84	-9.28	-8.84	
-9.28	-8.84	-8.79	-8.84	-9.28	
-8.84	-9.28	-9.25	-9.28	-8.84	
-8.84	-8.84	-8.84	-9.25	-8.84	
ird policy
>	>	v	<	<	
^	>	v	<	^	
^	>	<	<	<	
<	^	^	^	<	
>	>	^	<	^	
MAP policy loss 64.68983673138622
mean policy loss 99.6579839746
robust policy loss 404.196357876808
regret policy loss 20.5294999989189
ird policy loss 20.529500004443772
MAP lava occupancy 0.5814659895755669
Mean lava occupancy 0.5814659895755669
Robust lava occupancy 3.89486709875344
Regret lava occupancy 0.22562500000097085
IRD lava occupancy 0.22562500001163277
##############
Trial  13
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [0.02963936 0.1981105  0.41132133 0.36092881] loglik -9.704053366117762
accepted/total = 1616/2000 = 0.808
MAP Policy on Train MDP
map_weights [0.02963936 0.1981105  0.41132133 0.36092881]
map reward
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.41	0.03	0.03	
0.03	0.20	0.03	0.03	0.03	
0.03	0.03	0.20	0.03	0.03	
Map policy
v	v	v	v	v	
v	v	v	v	v	
>	>	>	<	<	
>	>	^	<	<	
>	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.01219621 -0.12863929  0.40052588 -0.09249951]
mean reward
-0.01	-0.01	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.01	-0.01	-0.01	
-0.01	-0.01	0.40	-0.01	-0.01	
-0.01	-0.13	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.13	-0.01	-0.01	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
MAP policy loss 1.8050001396544417
Mean policy loss 3.777877649602246e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	<	<	
>	v	v	<	>	
>	>	v	<	v	
^	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	2 	0 	1 	
3 	0 	0 	0 	0 	
0 	0 	3 	1 	0 	
MAP on testing env
map_weights [0.02963936 0.1981105  0.41132133 0.36092881]
map reward
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.20	0.20	0.03	
0.20	0.03	0.41	0.03	0.20	
0.36	0.03	0.03	0.03	0.03	
0.03	0.03	0.36	0.20	0.03	
Map policy
v	v	v	v	v	
v	>	v	<	<	
v	<	v	<	<	
<	<	v	v	<	
^	>	v	<	<	
MEAN policy on test env
mean_weights [-0.01219621 -0.12863929  0.40052588 -0.09249951]
mean reward
-0.01	-0.01	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.13	-0.13	-0.01	
-0.13	-0.01	0.40	-0.01	-0.13	
-0.09	-0.01	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.09	-0.13	-0.01	
mean policy
>	v	v	<	<	
>	v	v	v	v	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	2 	0 	1 	
3 	0 	0 	0 	0 	
0 	0 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	<	v	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	<	<	
^	>	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	<	<	<	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.64	-8.64	-8.64	-8.64	-8.64	
-8.64	-8.64	-8.93	-8.93	-8.64	
-8.93	-8.64	-8.35	-8.64	-8.93	
-8.98	-8.64	-8.64	-8.64	-8.64	
-8.64	-8.64	-8.98	-8.93	-8.64	
ird policy
v	v	<	<	<	
>	v	v	v	>	
>	>	v	<	v	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 47.56204861694948
mean policy loss 0.45012188678504667
robust policy loss 49.78717769351544
regret policy loss 6.036757757653402
ird policy loss 3.499424604258561e-08
MAP lava occupancy 0.37658779261109804
Mean lava occupancy 0.37658779261109804
Robust lava occupancy 0.46312500779776133
Regret lava occupancy 0.059394094160527156
IRD lava occupancy 2.906988578134859e-10
##############
Trial  14
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	<	<	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 0), (3, 0), (6, 3), (17, 2), (11, 1), (2, 3), (12, 3), (4, 0)]
w_map [-0.03247479 -0.0897426   0.86502811 -0.0127545 ] loglik -6.0679990096658685
accepted/total = 1256/2000 = 0.628
MAP Policy on Train MDP
map_weights [-0.03247479 -0.0897426   0.86502811 -0.0127545 ]
map reward
-0.03	-0.03	-0.03	-0.03	-0.03	
-0.09	-0.03	-0.03	-0.03	-0.03	
-0.03	-0.03	0.87	-0.09	-0.09	
-0.03	-0.03	-0.03	-0.03	-0.03	
-0.03	-0.03	-0.03	-0.03	-0.03	
Map policy
>	v	v	v	<	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.06248295 -0.17447013  0.15337238 -0.03285429]
mean reward
-0.06	-0.06	-0.06	-0.06	-0.06	
-0.17	-0.06	-0.06	-0.06	-0.06	
-0.06	-0.06	0.15	-0.17	-0.17	
-0.06	-0.06	-0.06	-0.06	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
mean policy
>	v	v	<	<	
>	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
^	^	^	<	<	
Optimal Policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	<	<	
^	^	^	<	<	
MAP policy loss 3.285960538456506e-06
Mean policy loss 3.1143135304639857e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	<	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-5.00	-1.00	-100.00	-1.00	-5.00	
-1.00	-5.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
1 	0 	3 	0 	1 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.03247479 -0.0897426   0.86502811 -0.0127545 ]
map reward
-0.03	-0.03	-0.01	-0.03	-0.03	
-0.09	-0.03	-0.01	-0.03	-0.09	
-0.03	-0.09	0.87	-0.09	-0.03	
-0.03	-0.03	-0.03	-0.03	-0.03	
-0.03	-0.03	-0.03	-0.03	-0.03	
Map policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [-0.06248295 -0.17447013  0.15337238 -0.03285429]
mean reward
-0.06	-0.06	-0.03	-0.06	-0.06	
-0.17	-0.06	-0.03	-0.06	-0.17	
-0.06	-0.17	0.15	-0.17	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
-0.06	-0.06	-0.06	-0.06	-0.06	
mean policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
features
0 	0 	3 	0 	0 	
1 	0 	3 	0 	1 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	^	<	v	
<	v	v	v	>	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	<	v	<	
v	v	v	v	v	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
-------- IRD Solution -------
ird reward
-8.79	-8.79	-9.26	-8.79	-8.79	
-9.24	-8.79	-9.26	-8.79	-9.24	
-8.79	-9.24	-8.71	-9.24	-8.79	
-8.79	-8.79	-8.79	-8.79	-8.79	
-8.79	-8.79	-8.79	-8.79	-8.79	
ird policy
>	v	v	v	<	
v	v	v	v	v	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
MAP policy loss 314.9821377708507
mean policy loss 871.0843211431244
robust policy loss 527.6987530820256
regret policy loss 19.79050280978305
ird policy loss 1.938916998310558e-09
MAP lava occupancy 3.077297844342344
Mean lava occupancy 3.077297844342344
Robust lava occupancy 5.032166729860446
Regret lava occupancy 0.2083210822045062
IRD lava occupancy 1.5539727075092648e-11
##############
Trial  15
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 1), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.09263746 -0.14794327  0.72830924  0.03111004] loglik -7.454716427007213
accepted/total = 1476/2000 = 0.738
MAP Policy on Train MDP
map_weights [-0.09263746 -0.14794327  0.72830924  0.03111004]
map reward
-0.09	-0.09	-0.09	-0.09	-0.09	
-0.09	-0.09	-0.09	-0.09	-0.09	
-0.09	-0.09	0.73	-0.09	-0.09	
-0.09	-0.09	-0.15	-0.09	-0.09	
-0.09	-0.09	-0.09	-0.09	-0.09	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.05396477 -0.20364142  0.37567615  0.05490589]
mean reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.38	0.05	0.05	
0.05	0.05	-0.20	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 1.5497678224341005e-06
Mean policy loss 1.2709392566145894e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
v	^	^	<	<	
>	>	^	^	<	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-5.00	1.00	-1.00	-5.00	
-5.00	-100.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	2 	0 	1 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [-0.09263746 -0.14794327  0.72830924  0.03111004]
map reward
-0.09	-0.09	-0.15	-0.09	-0.09	
-0.09	-0.09	-0.09	-0.09	-0.09	
0.03	-0.15	0.73	-0.09	-0.15	
-0.15	0.03	-0.09	-0.09	-0.15	
-0.09	-0.15	-0.09	-0.09	-0.09	
Map policy
v	v	v	v	<	
v	>	v	v	<	
>	>	^	<	<	
>	>	^	^	^	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.05396477 -0.20364142  0.37567615  0.05490589]
mean reward
0.05	0.05	-0.20	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	-0.20	0.38	0.05	-0.20	
-0.20	0.05	0.05	0.05	-0.20	
0.05	-0.20	0.05	0.05	0.05	
mean policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	<	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	2 	0 	1 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	v	<	<	
<	>	>	<	<	
>	>	^	<	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	<	
>	>	v	v	<	
^	>	>	<	<	
>	>	^	^	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-8.92	-8.92	-9.24	-8.92	-8.92	
-8.92	-8.92	-8.92	-8.92	-8.92	
-9.24	-9.24	-8.59	-8.92	-9.24	
-9.24	-9.24	-8.92	-8.92	-9.24	
-8.92	-9.24	-8.92	-8.92	-8.92	
ird policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
^	^	^	^	^	
>	>	^	^	<	
MAP policy loss 39.91013234702686
mean policy loss 22.33687306152556
robust policy loss 22.33687517389873
regret policy loss -1.7653858236377218e-10
ird policy loss 1.6413089512501422e-08
MAP lava occupancy 0.27052274333870024
Mean lava occupancy 0.27052274333870024
Robust lava occupancy 0.2256250016861803
Regret lava occupancy 3.739057370185489e-13
IRD lava occupancy 1.361891032218441e-10
##############
Trial  16
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	v	
>	v	v	<	v	
>	>	>	<	<	
>	>	^	^	^	
^	>	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	1 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (12, 0), (3, 0), (6, 1), (17, 2), (11, 1), (2, 3), (13, 0), (12, 3), (4, 0)]
w_map [0.16082446 0.15722407 0.60880261 0.07314885] loglik -7.6245389560519925
accepted/total = 1155/2000 = 0.5775
MAP Policy on Train MDP
map_weights [0.16082446 0.15722407 0.60880261 0.07314885]
map reward
0.16	0.16	0.16	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
0.16	0.16	0.61	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
Map policy
>	v	v	<	v	
>	v	v	<	v	
>	>	>	<	<	
>	>	^	^	^	
^	>	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.12978046 -0.14314002  0.12232784 -0.03347471]
mean reward
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.14	-0.13	-0.13	-0.14	-0.13	
-0.13	-0.13	0.12	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.14	
-0.13	-0.14	-0.13	-0.13	-0.13	
mean policy
>	v	v	<	v	
>	v	v	<	v	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	^	<	
Optimal Policy
>	v	v	<	v	
>	v	v	<	v	
>	>	>	<	<	
>	>	^	^	^	
^	>	^	^	<	
MAP policy loss 1.8790669995515097e-06
Mean policy loss 9.861177434865309e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	^	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	^	^	^	<	
>	>	^	^	^	
reward
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	3 	0 	0 	0 	
0 	1 	1 	0 	0 	
1 	0 	2 	0 	0 	
0 	3 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [0.16082446 0.15722407 0.60880261 0.07314885]
map reward
0.16	0.07	0.16	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
0.16	0.16	0.61	0.16	0.16	
0.16	0.07	0.16	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
Map policy
v	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	^	<	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-0.12978046 -0.14314002  0.12232784 -0.03347471]
mean reward
-0.13	-0.03	-0.13	-0.13	-0.13	
-0.13	-0.14	-0.14	-0.13	-0.13	
-0.14	-0.13	0.12	-0.13	-0.13	
-0.13	-0.03	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.14	-0.13	-0.13	
mean policy
>	>	v	v	v	
v	>	v	v	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	^	
features
0 	3 	0 	0 	0 	
0 	1 	1 	0 	0 	
1 	0 	2 	0 	0 	
0 	3 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	^	v	<	<	
>	^	v	<	<	
<	>	^	<	<	
^	^	^	^	<	
^	^	v	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	>	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-9.01	-9.45	-9.01	-9.01	-9.01	
-9.01	-9.43	-9.43	-9.01	-9.01	
-9.43	-9.01	-8.87	-9.01	-9.01	
-9.01	-9.45	-9.01	-9.01	-9.01	
-9.01	-9.01	-9.43	-9.01	-9.01	
ird policy
v	>	>	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	^	^	<	<	
>	>	^	^	^	
MAP policy loss 45.50986802591048
mean policy loss 44.90175009459573
robust policy loss 539.7373349257701
regret policy loss 20.490148259658874
ird policy loss 1.0331468708324465e-08
MAP lava occupancy 0.37518067473045413
Mean lava occupancy 0.37518067473045413
Robust lava occupancy 5.101736302913593
Regret lava occupancy 0.21568577115369236
IRD lava occupancy 1.1032277270769226e-10
##############
Trial  17
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 3), (17, 2), (13, 0), (11, 1), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [ 0.09193736 -0.47835721  0.41761106  0.01209437] loglik -8.31776121203751
accepted/total = 1417/2000 = 0.7085
MAP Policy on Train MDP
map_weights [ 0.09193736 -0.47835721  0.41761106  0.01209437]
map reward
0.09	0.09	0.09	-0.48	0.09	
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.42	0.09	-0.48	
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
Map policy
>	>	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.06088632 -0.21506426  0.35905233  0.0520448 ]
mean reward
0.06	0.06	0.06	-0.22	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.36	0.06	-0.22	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	<	
MAP policy loss 5.064813690253134e-07
Mean policy loss 1.0088456459111939e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	<	v	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	^	v	
^	v	<	^	<	
reward
-1.00	-100.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-5.00	-100.00	
-1.00	-1.00	-100.00	-5.00	-1.00	
features
0 	3 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	1 	3 	1 	3 	
0 	0 	3 	1 	0 	
MAP on testing env
map_weights [ 0.09193736 -0.47835721  0.41761106  0.01209437]
map reward
0.09	0.01	0.09	-0.48	0.09	
-0.48	0.09	0.09	0.09	0.09	
0.09	0.09	0.42	-0.48	0.09	
0.09	-0.48	0.01	-0.48	0.01	
0.09	0.09	0.01	-0.48	0.09	
Map policy
>	v	v	v	v	
v	v	v	<	<	
>	>	^	<	^	
^	^	^	<	^	
^	>	^	<	^	
MEAN policy on test env
mean_weights [ 0.06088632 -0.21506426  0.35905233  0.0520448 ]
mean reward
0.06	0.05	0.06	-0.22	0.06	
-0.22	0.06	0.06	0.06	0.06	
0.06	0.06	0.36	-0.22	0.06	
0.06	-0.22	0.05	-0.22	0.05	
0.06	0.06	0.05	-0.22	0.06	
mean policy
>	v	v	<	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	<	^	
^	>	^	<	^	
features
0 	3 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	1 	3 	1 	3 	
0 	0 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	v	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	<	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	<	v	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	^	^	
^	v	<	<	^	
-------- IRD Solution -------
ird reward
-9.17	-9.60	-9.17	-9.60	-9.17	
-9.60	-9.17	-9.17	-9.17	-9.17	
-9.17	-9.17	-9.04	-9.60	-9.17	
-9.17	-9.60	-9.60	-9.60	-9.60	
-9.17	-9.17	-9.60	-9.60	-9.17	
ird policy
v	>	v	v	v	
>	v	v	<	<	
>	>	^	<	^	
^	^	^	^	^	
^	v	<	<	^	
MAP policy loss 256.7091336669412
mean policy loss 44.22250020854312
robust policy loss 27.808263316731505
regret policy loss 21.362092598676
ird policy loss 21.210505232344932
MAP lava occupancy 2.519097545661744
Mean lava occupancy 2.519097545661744
Robust lava occupancy 0.30668369583637267
Regret lava occupancy 0.23910225762041082
IRD lava occupancy 0.23750659839783164
##############
Trial  18
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	^	v	<	<	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 3), (12, 2), (12, 1), (12, 0), (6, 1), (9, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.03620517 -0.20175171  0.68310252 -0.0789406 ] loglik -6.06842297399578
accepted/total = 1326/2000 = 0.663
MAP Policy on Train MDP
map_weights [-0.03620517 -0.20175171  0.68310252 -0.0789406 ]
map reward
-0.04	-0.20	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	0.68	-0.04	-0.04	
-0.04	-0.04	-0.20	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
Map policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.056777   -0.23614166  0.3609566  -0.00624173]
mean reward
0.06	-0.24	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.36	0.06	0.06	
0.06	0.06	-0.24	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
v	^	v	<	<	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	^	^	
MAP policy loss 1.0203215422138756e-06
Mean policy loss 2.8846944277738373e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	^	^	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.03620517 -0.20175171  0.68310252 -0.0789406 ]
map reward
-0.04	-0.04	-0.08	-0.04	-0.04	
-0.04	-0.20	-0.04	-0.04	-0.04	
-0.04	-0.04	0.68	-0.04	-0.20	
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.20	-0.04	-0.04	
Map policy
v	v	v	v	<	
v	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.056777   -0.23614166  0.3609566  -0.00624173]
mean reward
0.06	0.06	-0.01	0.06	0.06	
0.06	-0.24	0.06	0.06	0.06	
0.06	0.06	0.36	0.06	-0.24	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	-0.24	0.06	0.06	
mean policy
v	>	v	v	<	
v	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	<	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	<	
v	>	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.94	-8.94	-9.34	-8.94	-8.94	
-8.94	-9.31	-8.94	-8.94	-8.94	
-8.94	-8.94	-8.76	-8.94	-9.31	
-8.94	-8.94	-8.94	-8.94	-8.94	
-8.94	-8.94	-9.31	-8.94	-8.94	
ird policy
v	^	v	v	v	
v	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	^	^	^	^	
MAP policy loss 7.197457331639928
mean policy loss 3.941283211600366e-08
robust policy loss 11.600630536270243
regret policy loss 0.05745165590082721
ird policy loss 3.8434793542263535e-10
MAP lava occupancy 0.0619478534267765
Mean lava occupancy 0.0619478534267765
Robust lava occupancy 0.11717808618845274
Regret lava occupancy 0.0005803197763175408
IRD lava occupancy 3.4396422714222986e-12
##############
Trial  19
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (6, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [ 0.05760616 -0.02818472  0.75856012 -0.155649  ] loglik -6.761572536696008
accepted/total = 1442/2000 = 0.721
MAP Policy on Train MDP
map_weights [ 0.05760616 -0.02818472  0.75856012 -0.155649  ]
map reward
0.06	-0.03	0.06	0.06	0.06	
-0.03	0.06	0.06	0.06	0.06	
0.06	0.06	0.76	0.06	0.06	
0.06	0.06	-0.03	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.03034888 -0.24600311  0.37783484 -0.05788162]
mean reward
0.03	-0.25	0.03	0.03	0.03	
-0.25	0.03	0.03	0.03	0.03	
0.03	0.03	0.38	0.03	0.03	
0.03	0.03	-0.25	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
mean policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
MAP policy loss 1.0954264429274135e-06
Mean policy loss -2.5425506804122033e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	^	
>	>	^	<	<	
^	^	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [ 0.05760616 -0.02818472  0.75856012 -0.155649  ]
map reward
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	-0.03	0.76	-0.03	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	-0.16	0.06	0.06	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.03034888 -0.24600311  0.37783484 -0.05788162]
mean reward
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
0.03	-0.25	0.38	-0.25	0.03	
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	-0.06	0.03	0.03	
mean policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	^	v	^	<	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	>	^	<	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.68	-8.68	-8.68	-8.68	-8.68	
-8.68	-8.68	-8.68	-8.68	-8.68	
-8.68	-8.98	-8.31	-8.98	-8.68	
-8.68	-8.68	-8.68	-8.68	-8.68	
-8.68	-8.68	-8.97	-8.68	-8.68	
ird policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	^	
MAP policy loss 23.478374862349877
mean policy loss 1.1046080981673423e-07
robust policy loss 39.25015005205318
regret policy loss -1.853863070000461e-11
ird policy loss 1.0014916014544628e-09
MAP lava occupancy 1.1474781510250494e-07
Mean lava occupancy 1.1474781510250494e-07
Robust lava occupancy 0.39646616211489644
Regret lava occupancy 5.361433231319518e-13
IRD lava occupancy 8.965350613407296e-12
##############
Trial  20
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [-0.54421304 -0.37194401 -0.00091016 -0.08293279] loglik -9.704060530925972
accepted/total = 1834/2000 = 0.917
MAP Policy on Train MDP
map_weights [-0.54421304 -0.37194401 -0.00091016 -0.08293279]
map reward
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	-0.00	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.54	-0.54	
Map policy
>	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.24087606 -0.0226011   0.17386024  0.02578508]
mean reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	0.17	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
mean policy
>	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 5.538905256625764e-10
Mean policy loss 4.5371791928736343e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	v	
>	v	v	<	v	
>	>	<	<	<	
>	^	^	^	<	
>	^	<	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
1 	0 	2 	0 	0 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.54421304 -0.37194401 -0.00091016 -0.08293279]
map reward
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.08	-0.54	
-0.37	-0.54	-0.00	-0.54	-0.54	
-0.54	-0.54	-0.08	-0.37	-0.37	
-0.54	-0.54	-0.37	-0.54	-0.54	
Map policy
v	v	v	v	<	
v	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
MEAN policy on test env
mean_weights [-0.24087606 -0.0226011   0.17386024  0.02578508]
mean reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	0.03	-0.24	
-0.02	-0.24	0.17	-0.24	-0.24	
-0.24	-0.24	0.03	-0.02	-0.02	
-0.24	-0.24	-0.02	-0.24	-0.24	
mean policy
v	v	v	v	v	
v	v	v	<	<	
>	>	v	<	v	
>	>	^	<	<	
>	>	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
1 	0 	2 	0 	0 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
v	>	v	v	v	
<	>	v	<	v	
^	>	^	>	<	
>	>	v	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	<	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-8.42	-8.42	-8.42	-8.42	-8.42	
-8.42	-8.42	-8.42	-8.93	-8.42	
-8.83	-8.42	-8.35	-8.42	-8.42	
-8.42	-8.42	-8.93	-8.83	-8.83	
-8.42	-8.42	-8.83	-8.42	-8.42	
ird policy
>	v	v	<	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	^	<	
>	^	<	^	<	
MAP policy loss 231.3982121208385
mean policy loss 853.2171970885005
robust policy loss 488.76150983573655
regret policy loss 3.2537248813110713
ird policy loss 5.709230171957458e-10
MAP lava occupancy 2.328706551303494
Mean lava occupancy 2.328706551303494
Robust lava occupancy 4.710002768367402
Regret lava occupancy 0.03286590789709598
IRD lava occupancy 8.139648283744874e-12
##############
Trial  21
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	v	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (6, 1), (8, 0), (17, 2), (13, 0), (11, 1), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [ 0.04604589  0.25681657  0.46037494 -0.2367626 ] loglik -9.704039228280976
accepted/total = 1626/2000 = 0.813
MAP Policy on Train MDP
map_weights [ 0.04604589  0.25681657  0.46037494 -0.2367626 ]
map reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.46	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.26	0.05	0.05	0.05	
Map policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	v	^	^	<	
>	v	<	<	<	
MEAN policy on Train MDP
mean_weights [-0.04826411 -0.13581706  0.36804574  0.07489927]
mean reward
-0.05	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.05	0.37	-0.05	-0.05	
-0.05	-0.05	-0.05	-0.05	-0.05	
-0.05	-0.14	-0.05	-0.05	-0.05	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
^	^	^	<	<	
Optimal Policy
>	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	v	^	^	<	
MAP policy loss 44.501410793215925
Mean policy loss 2.5168264347619207e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	v	v	<	v	
>	>	^	<	<	
^	^	^	^	<	
^	^	>	^	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [ 0.04604589  0.25681657  0.46037494 -0.2367626 ]
map reward
0.05	0.05	-0.24	0.05	0.05	
0.05	0.05	0.05	0.26	-0.24	
0.05	0.05	0.46	0.05	0.26	
0.05	0.05	0.26	0.05	0.05	
0.05	0.05	0.26	0.05	0.05	
Map policy
v	v	v	v	<	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
MEAN policy on test env
mean_weights [-0.04826411 -0.13581706  0.36804574  0.07489927]
mean reward
-0.05	-0.05	0.07	-0.05	-0.05	
-0.05	-0.05	-0.05	-0.14	0.07	
-0.05	-0.05	0.37	-0.05	-0.14	
-0.05	-0.05	-0.14	-0.05	-0.05	
-0.05	-0.05	-0.14	-0.05	-0.05	
mean policy
>	>	v	<	<	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	^	^	^	<	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	v	
>	>	v	v	>	
>	>	v	<	<	
^	^	^	^	<	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	<	
>	>	v	<	v	
>	>	^	<	<	
^	^	^	^	<	
^	^	<	^	^	
-------- IRD Solution -------
ird reward
-8.82	-8.82	-9.22	-8.82	-8.82	
-8.82	-8.82	-8.82	-9.26	-9.22	
-8.82	-8.82	-8.78	-8.82	-9.26	
-8.82	-8.82	-9.26	-8.82	-8.82	
-8.82	-8.82	-9.26	-8.82	-8.82	
ird policy
v	v	<	>	<	
>	>	v	v	v	
>	>	>	<	<	
^	^	^	^	<	
^	^	>	^	<	
MAP policy loss 35.49644450522415
mean policy loss 43.77125000266806
robust policy loss 494.6775714599472
regret policy loss 11.989792492987377
ird policy loss 3.274455142011289
MAP lava occupancy 0.21563345647077925
Mean lava occupancy 0.21563345647077925
Robust lava occupancy 4.764910190642122
Regret lava occupancy 0.12620834199876227
IRD lava occupancy 1.0633412178963157e-10
##############
Trial  22
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	>	
^	^	^	^	<	
^	^	>	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (11, 1), (2, 3), (1, 1), (13, 0), (4, 0)]
w_map [0.15770231 0.02778756 0.71950759 0.09500254] loglik -6.0681923059854626
accepted/total = 1282/2000 = 0.641
MAP Policy on Train MDP
map_weights [0.15770231 0.02778756 0.71950759 0.09500254]
map reward
0.16	0.16	0.16	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
0.16	0.16	0.72	0.16	0.03	
0.16	0.16	0.03	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
Map policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.07472693 -0.19461382  0.37044904 -0.05281862]
mean reward
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.37	0.07	-0.19	
0.07	0.07	-0.19	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
mean policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	>	
^	^	^	^	<	
^	^	>	^	<	
MAP policy loss 9.729999218711233e-07
Mean policy loss 1.3083450989270773e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	<	^	^	^	
^	>	v	^	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [0.15770231 0.02778756 0.71950759 0.09500254]
map reward
0.16	0.16	0.16	0.03	0.16	
0.03	0.16	0.16	0.16	0.03	
0.16	0.16	0.72	0.16	0.16	
0.03	0.10	0.10	0.16	0.16	
0.16	0.03	0.16	0.16	0.16	
Map policy
>	v	v	v	v	
v	v	v	v	<	
>	>	^	<	<	
^	>	^	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.07472693 -0.19461382  0.37044904 -0.05281862]
mean reward
0.07	0.07	0.07	-0.19	0.07	
-0.19	0.07	0.07	0.07	-0.19	
0.07	0.07	0.37	0.07	0.07	
-0.19	-0.05	-0.05	0.07	0.07	
0.07	-0.19	0.07	0.07	0.07	
mean policy
>	v	v	v	v	
v	v	v	v	<	
>	>	^	<	<	
^	>	^	^	^	
^	^	^	^	^	
features
0 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	>	
v	v	v	v	<	
>	>	<	<	<	
^	^	^	^	^	
<	>	>	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.82	-8.82	-8.82	-9.17	-8.82	
-9.17	-8.82	-8.82	-8.82	-9.17	
-8.82	-8.82	-8.55	-8.82	-8.82	
-9.17	-9.20	-9.20	-8.82	-8.82	
-8.82	-9.17	-8.82	-8.82	-8.82	
ird policy
>	v	v	v	v	
v	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	v	v	^	^	
MAP policy loss 15.746446627693665
mean policy loss 2.0058389102331833e-06
robust policy loss 2.551496570016559
regret policy loss 2.9240346354778044
ird policy loss 6.140598758641591e-10
MAP lava occupancy 0.15429761676053252
Mean lava occupancy 0.15429761676053252
Robust lava occupancy 9.72670406354648e-10
Regret lava occupancy 0.029535703402006272
IRD lava occupancy 1.6997187646762017e-11
##############
Trial  23
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.69526807  0.10309207  0.16016201  0.04147785] loglik -8.317766159157827
accepted/total = 1846/2000 = 0.923
MAP Policy on Train MDP
map_weights [-0.69526807  0.10309207  0.16016201  0.04147785]
map reward
-0.70	-0.70	-0.70	-0.70	-0.70	
-0.70	-0.70	-0.70	-0.70	-0.70	
-0.70	-0.70	0.16	-0.70	-0.70	
-0.70	-0.70	-0.70	-0.70	-0.70	
-0.70	-0.70	-0.70	-0.70	-0.70	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.20336916  0.03272587  0.19489397  0.06612732]
mean reward
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	0.19	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
mean policy
v	v	v	v	<	
>	v	v	v	<	
>	>	^	<	<	
>	^	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.509923560633549e-10
Mean policy loss 4.1925789528746583e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
^	^	v	>	^	
^	>	^	>	>	
^	^	^	v	^	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-5.00	
-1.00	-5.00	1.00	-100.00	-5.00	
-1.00	-1.00	-5.00	-100.00	-100.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	2 	3 	1 	
0 	0 	1 	3 	3 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [-0.69526807  0.10309207  0.16016201  0.04147785]
map reward
-0.70	-0.70	-0.70	-0.70	-0.70	
-0.70	0.04	-0.70	0.04	0.10	
-0.70	0.10	0.16	0.04	0.10	
-0.70	-0.70	0.10	0.04	0.04	
-0.70	0.10	-0.70	-0.70	-0.70	
Map policy
v	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	^	^	<	^	
>	v	^	^	^	
MEAN policy on test env
mean_weights [-0.20336916  0.03272587  0.19489397  0.06612732]
mean reward
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	0.07	-0.20	0.07	0.03	
-0.20	0.03	0.19	0.07	0.03	
-0.20	-0.20	0.03	0.07	0.07	
-0.20	0.03	-0.20	-0.20	-0.20	
mean policy
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
>	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	2 	3 	1 	
0 	0 	1 	3 	3 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
^	>	^	<	<	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	^	<	
^	>	^	<	<	
-------- IRD Solution -------
ird reward
-8.86	-8.86	-8.86	-8.86	-8.86	
-8.86	-9.31	-8.86	-9.31	-9.25	
-8.86	-9.25	-8.81	-9.31	-9.25	
-8.86	-8.86	-9.25	-9.31	-9.31	
-8.86	-9.25	-8.86	-8.86	-8.86	
ird policy
>	>	v	<	<	
^	>	v	<	^	
^	>	^	<	<	
^	<	^	^	^	
^	>	^	<	<	
MAP policy loss 285.35326295050845
mean policy loss 920.6310229691863
robust policy loss 554.7762814361907
regret policy loss 18.727523598474534
ird policy loss -3.022334207418246e-08
MAP lava occupancy 2.707742945317135
Mean lava occupancy 2.707742945317135
Robust lava occupancy 5.426956814969873
Regret lava occupancy 0.19257298460420183
IRD lava occupancy 5.4739440396070785e-11
##############
Trial  24
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	^	v	
>	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [ 1.69160062e-04 -7.72261041e-02  8.16380111e-01 -1.06224625e-01] loglik -9.010913342213598
accepted/total = 1470/2000 = 0.735
MAP Policy on Train MDP
map_weights [ 1.69160062e-04 -7.72261041e-02  8.16380111e-01 -1.06224625e-01]
map reward
0.00	0.00	0.00	-0.08	0.00	
0.00	0.00	0.00	0.00	0.00	
0.00	0.00	0.82	0.00	0.00	
0.00	0.00	0.00	0.00	0.00	
0.00	0.00	0.00	0.00	0.00	
Map policy
>	>	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.05327534 -0.24555352  0.37933741 -0.07424426]
mean reward
0.05	0.05	0.05	-0.25	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.38	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
>	>	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	^	v	
>	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	<	
MAP policy loss 9.455176485512196e-10
Mean policy loss 3.62298524514415e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	v	v	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-100.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	3 	1 	0 	
1 	0 	0 	0 	0 	
3 	0 	2 	0 	1 	
0 	0 	0 	3 	1 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 1.69160062e-04 -7.72261041e-02  8.16380111e-01 -1.06224625e-01]
map reward
0.00	-0.08	-0.11	-0.08	0.00	
-0.08	0.00	0.00	0.00	0.00	
-0.11	0.00	0.82	0.00	-0.08	
0.00	0.00	0.00	-0.11	-0.08	
0.00	0.00	0.00	-0.08	0.00	
Map policy
>	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.05327534 -0.24555352  0.37933741 -0.07424426]
mean reward
0.05	-0.25	-0.07	-0.25	0.05	
-0.25	0.05	0.05	0.05	0.05	
-0.07	0.05	0.38	0.05	-0.25	
0.05	0.05	0.05	-0.07	-0.25	
0.05	0.05	0.05	-0.25	0.05	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	<	<	
features
0 	1 	3 	1 	0 	
1 	0 	0 	0 	0 	
3 	0 	2 	0 	1 	
0 	0 	0 	3 	1 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-8.96	-9.43	-9.39	-9.43	-8.96	
-9.43	-8.96	-8.96	-8.96	-8.96	
-9.39	-8.96	-8.86	-8.96	-9.43	
-8.96	-8.96	-8.96	-9.39	-9.43	
-8.96	-8.96	-8.96	-9.43	-8.96	
ird policy
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
MAP policy loss 28.80788067204589
mean policy loss 5.080614633862224e-08
robust policy loss 25.198382874852207
regret policy loss 3.129021645259522e-08
ird policy loss 1.9664695097981744e-09
MAP lava occupancy 0.2863536594516248
Mean lava occupancy 0.2863536594516248
Robust lava occupancy 0.2545291184654706
Regret lava occupancy 2.981107777770183e-10
IRD lava occupancy 1.7548528079775993e-11
##############
Trial  25
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(9, 0), (8, 3), (12, 1), (12, 0), (6, 3), (17, 2), (13, 0), (11, 1), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [-0.07902694 -0.10639873  0.73775609 -0.07681824] loglik -6.067938712816044
accepted/total = 1447/2000 = 0.7235
MAP Policy on Train MDP
map_weights [-0.07902694 -0.10639873  0.73775609 -0.07681824]
map reward
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.08	-0.11	-0.08	-0.08	
-0.08	-0.08	0.74	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.03674213 -0.25200307  0.35737218  0.09205852]
mean reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	-0.25	0.04	0.04	
0.04	0.04	0.36	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
mean policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
MAP policy loss 1.4481356384113409e-06
Mean policy loss 1.905285304526494e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	<	
>	>	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	>	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-100.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-100.00	-100.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	1 	0 	3 	
0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	1 	3 	3 	0 	
MAP on testing env
map_weights [-0.07902694 -0.10639873  0.73775609 -0.07681824]
map reward
-0.08	-0.08	-0.08	-0.11	-0.08	
-0.08	-0.08	-0.11	-0.08	-0.08	
-0.08	-0.11	0.74	-0.08	-0.08	
-0.08	-0.08	-0.11	-0.08	-0.08	
-0.08	-0.11	-0.08	-0.08	-0.08	
Map policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	>	^	^	<	
MEAN policy on test env
mean_weights [ 0.03674213 -0.25200307  0.35737218  0.09205852]
mean reward
0.04	0.04	0.04	-0.25	0.04	
0.04	0.04	-0.25	0.04	0.09	
0.04	-0.25	0.36	0.04	0.04	
0.04	0.04	-0.25	0.04	0.04	
0.04	-0.25	0.09	0.09	0.04	
mean policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	>	>	^	<	
features
0 	0 	0 	1 	0 	
0 	0 	1 	0 	3 	
0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	1 	3 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
^	>	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	>	v	^	^	
-------- IRD Solution -------
ird reward
-8.75	-8.75	-8.75	-9.08	-8.75	
-8.75	-8.75	-9.08	-8.75	-9.07	
-8.75	-9.08	-8.40	-8.75	-8.75	
-8.75	-8.75	-9.08	-8.75	-8.75	
-8.75	-9.08	-9.07	-9.07	-8.75	
ird policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	^	^	^	^	
MAP policy loss 56.93464077934547
mean policy loss 46.07499998975767
robust policy loss 40.33654295993165
regret policy loss 22.56250009063097
ird policy loss 22.562500558135437
MAP lava occupancy 0.35670926429613603
Mean lava occupancy 0.35670926429613603
Robust lava occupancy 0.4170357875825394
Regret lava occupancy 0.2375000009535237
IRD lava occupancy 0.2375000049112611
##############
Trial  26
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (6, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.35994668 -0.35924458 -0.22001397  0.06079477] loglik -7.685118075582295
accepted/total = 35/2000 = 0.0175
MAP Policy on Train MDP
map_weights [-0.35994668 -0.35924458 -0.22001397  0.06079477]
map reward
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.22	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
Map policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.13285623 -0.35035176 -0.32376803  0.15920485]
mean reward
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.32	-0.13	-0.13	
-0.13	-0.13	-0.35	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.13	
mean policy
>	v	v	v	<	
>	>	>	<	<	
>	^	<	^	<	
^	^	v	^	^	
^	^	v	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 33.459608879633336
Mean policy loss 16.707820513007327
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	>	v	<	
>	>	v	<	<	
v	>	v	<	>	
<	<	^	^	^	
>	>	^	<	>	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-5.00	1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	2 	0 	0 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	0 	
MAP on testing env
map_weights [-0.35994668 -0.35924458 -0.22001397  0.06079477]
map reward
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
0.06	-0.36	-0.22	-0.36	-0.36	
-0.36	0.06	-0.36	-0.36	0.06	
-0.36	-0.36	-0.36	0.06	-0.36	
Map policy
v	v	v	v	v	
v	v	v	v	v	
<	<	<	>	v	
^	^	>	>	>	
^	^	>	v	<	
MEAN policy on test env
mean_weights [-0.13285623 -0.35035176 -0.32376803  0.15920485]
mean reward
-0.13	-0.13	-0.35	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.35	
0.16	-0.35	-0.32	-0.13	-0.13	
-0.13	0.16	-0.13	-0.13	0.16	
-0.13	-0.13	-0.13	0.16	-0.13	
mean policy
v	v	v	v	v	
v	<	<	v	v	
<	<	v	v	v	
^	>	>	>	>	
^	^	v	v	<	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	2 	0 	0 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	<	>	
^	^	^	v	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	<	
>	>	v	v	v	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
2.16	2.16	1.65	2.16	2.16	
2.16	2.16	2.16	2.16	1.65	
2.39	1.65	1.90	2.16	2.16	
2.16	2.39	2.16	2.16	2.39	
2.16	2.16	2.16	2.39	2.16	
ird policy
v	v	<	v	<	
v	<	<	v	<	
<	<	v	v	v	
^	>	v	v	>	
^	^	>	v	<	
MAP policy loss 42.9652758469758
mean policy loss 1801.6064903103627
robust policy loss 595.0280495351398
regret policy loss 19.54584841977526
ird policy loss 1759.2774278510053
MAP lava occupancy 0.4628168686702959
Mean lava occupancy 0.4628168686702959
Robust lava occupancy 5.747817708513033
Regret lava occupancy 0.23750000000330346
IRD lava occupancy 17.64390467075963
##############
Trial  27
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [-0.00222552  0.17883626  0.77609371  0.04284451] loglik -9.70406052783926
accepted/total = 1802/2000 = 0.901
MAP Policy on Train MDP
map_weights [-0.00222552  0.17883626  0.77609371  0.04284451]
map reward
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	0.78	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.20240666 -0.01562524  0.16361555 -0.00800144]
mean reward
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	0.16	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 6.227892673820639e-10
Mean policy loss 3.341380883325985e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	v	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-100.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-100.00	
-100.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.00222552  0.17883626  0.77609371  0.04284451]
map reward
-0.00	0.04	0.18	-0.00	-0.00	
-0.00	0.18	-0.00	-0.00	0.04	
0.04	-0.00	0.78	-0.00	0.18	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
Map policy
>	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	<	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.20240666 -0.01562524  0.16361555 -0.00800144]
mean reward
-0.20	-0.01	-0.02	-0.20	-0.20	
-0.20	-0.02	-0.20	-0.20	-0.01	
-0.01	-0.20	0.16	-0.20	-0.02	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
mean policy
>	^	<	<	v	
v	^	v	>	>	
<	>	^	<	^	
^	^	^	^	^	
^	^	^	^	^	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	^	<	v	
>	^	v	>	v	
<	>	v	<	>	
^	>	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-8.64	-9.00	-9.08	-8.64	-8.64	
-8.64	-9.08	-8.64	-8.64	-9.00	
-9.00	-8.64	-8.61	-8.64	-9.08	
-8.64	-8.64	-8.64	-8.64	-8.64	
-8.64	-8.64	-8.64	-8.64	-8.64	
ird policy
v	>	v	v	<	
<	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 33.332708996832935
mean policy loss 1828.3459309949499
robust policy loss 566.383138953966
regret policy loss 11.242620170002589
ird policy loss 3.274460752170902
MAP lava occupancy 0.33284184970363534
Mean lava occupancy 0.33284184970363534
Robust lava occupancy 5.418272652476686
Regret lava occupancy 0.11834337020984706
IRD lava occupancy 5.990242771223872e-08
##############
Trial  28
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (12, 0), (6, 1), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (4, 3), (13, 0)]
w_map [ 0.050028   -0.36691235  0.57763899  0.00542066] loglik -9.01091334116046
accepted/total = 1606/2000 = 0.803
MAP Policy on Train MDP
map_weights [ 0.050028   -0.36691235  0.57763899  0.00542066]
map reward
0.05	-0.37	0.05	0.05	0.05	
-0.37	0.05	0.05	0.05	0.05	
0.05	0.05	0.58	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
Map policy
v	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.02302801 -0.17866701  0.34381187 -0.01619433]
mean reward
-0.02	-0.18	-0.02	-0.02	-0.02	
-0.18	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	0.34	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
mean policy
v	v	v	v	v	
>	>	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
>	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
MAP policy loss 6.2655556775856325e-09
Mean policy loss 1.0595534002261209e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	^	v	v	
v	v	v	>	v	
>	>	<	<	v	
>	^	^	<	<	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.050028   -0.36691235  0.57763899  0.00542066]
map reward
0.05	0.05	0.05	-0.37	0.05	
0.05	-0.37	-0.37	0.05	0.05	
0.05	0.05	0.58	-0.37	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
Map policy
v	v	v	<	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	<	<	
MEAN policy on test env
mean_weights [-0.02302801 -0.17866701  0.34381187 -0.01619433]
mean reward
-0.02	-0.02	-0.02	-0.18	-0.02	
-0.02	-0.18	-0.18	-0.02	-0.02	
-0.02	-0.02	0.34	-0.18	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
mean policy
v	v	v	<	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
>	^	^	<	<	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	v	
v	v	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-8.76	-8.76	-8.76	-9.13	-8.76	
-8.76	-9.13	-9.13	-8.76	-8.76	
-8.76	-8.76	-8.59	-9.13	-8.76	
-8.76	-8.76	-8.76	-8.76	-8.76	
-8.76	-8.76	-8.76	-8.76	-8.76	
ird policy
v	>	^	v	v	
v	v	v	<	v	
>	>	v	<	v	
>	^	^	<	<	
^	^	^	<	<	
MAP policy loss 18.16365494385277
mean policy loss 0.45012210217543086
robust policy loss 23.781871330531267
regret policy loss 1.5968859146818155
ird policy loss 9.509858595047049e-08
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  29
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (12, 0), (3, 0), (6, 3), (12, 3), (17, 2), (11, 1), (2, 3), (13, 0), (4, 0)]
w_map [-0.78935657 -0.09704082  0.03862136 -0.07498126] loglik -9.010913347278802
accepted/total = 1819/2000 = 0.9095
MAP Policy on Train MDP
map_weights [-0.78935657 -0.09704082  0.03862136 -0.07498126]
map reward
-0.79	-0.79	-0.79	-0.79	-0.79	
-0.79	-0.79	-0.79	-0.79	-0.79	
-0.79	-0.79	0.04	-0.79	-0.79	
-0.79	-0.79	-0.79	-0.79	-0.79	
-0.79	-0.79	-0.79	-0.79	-0.79	
Map policy
v	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.21769257  0.04031386  0.16423598  0.00177294]
mean reward
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	0.16	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
mean policy
>	v	v	v	v	
>	>	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.49203636175002e-10
Mean policy loss 3.775518182461265e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	v	
>	>	v	<	<	
^	<	>	<	<	
<	>	>	^	<	
>	>	>	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-100.00	1.00	-1.00	-1.00	
-100.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	2 	0 	0 	
3 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.78935657 -0.09704082  0.03862136 -0.07498126]
map reward
-0.79	-0.79	-0.79	-0.79	-0.79	
-0.79	-0.79	-0.79	-0.79	-0.79	
-0.10	-0.07	0.04	-0.79	-0.79	
-0.07	-0.79	-0.10	-0.79	-0.10	
-0.79	-0.79	-0.79	-0.79	-0.79	
Map policy
v	v	v	v	v	
v	v	v	<	v	
>	>	<	<	<	
^	^	^	<	<	
^	^	^	<	^	
MEAN policy on test env
mean_weights [-0.21769257  0.04031386  0.16423598  0.00177294]
mean reward
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
0.04	0.00	0.16	-0.22	-0.22	
0.00	-0.22	0.04	-0.22	0.04	
-0.22	-0.22	-0.22	-0.22	-0.22	
mean policy
v	v	v	v	<	
v	v	v	v	<	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	2 	0 	0 	
3 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
v	v	v	v	v	
>	>	<	<	<	
^	^	^	<	>	
^	^	^	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	v	
>	>	v	v	v	
^	>	^	<	<	
>	^	^	^	^	
>	>	>	^	<	
-------- IRD Solution -------
ird reward
-8.41	-8.41	-8.41	-8.41	-8.41	
-8.41	-8.41	-8.41	-8.41	-8.41	
-8.87	-8.78	-8.32	-8.41	-8.41	
-8.78	-8.41	-8.87	-8.41	-8.87	
-8.41	-8.41	-8.41	-8.41	-8.41	
ird policy
>	>	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	v	>	^	<	
>	>	>	^	<	
MAP policy loss 238.6405409472354
mean policy loss 100.90254332681045
robust policy loss 552.3765914792797
regret policy loss 5.326465530860805
ird policy loss -1.419317016779914e-08
MAP lava occupancy 2.3171474341714617
Mean lava occupancy 2.3171474341714617
Robust lava occupancy 5.390917959113778
Regret lava occupancy 0.05370289600398678
IRD lava occupancy 6.591053501823955e-11
##############
Trial  30
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (8, 3), (12, 1), (3, 3), (12, 0), (4, 0), (12, 3), (17, 2), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [-0.1252402   0.15744655  0.48456404  0.23274922] loglik -9.010911672773489
accepted/total = 1669/2000 = 0.8345
MAP Policy on Train MDP
map_weights [-0.1252402   0.15744655  0.48456404  0.23274922]
map reward
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.13	-0.13	0.48	-0.13	-0.13	
-0.13	-0.13	-0.13	0.16	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.13	
Map policy
v	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	<	<	
>	>	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.12491482 -0.12139702  0.30312531 -0.019714  ]
mean reward
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	0.30	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
mean policy
v	v	v	v	v	
>	>	v	v	v	
>	>	<	<	<	
>	>	^	^	<	
>	>	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	^	
MAP policy loss 0.9025003914319463
Mean policy loss 0.9024999032987905
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	<	
>	>	<	<	v	
^	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	1.00	-5.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-1.00	
features
0 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	2 	1 	1 	
3 	0 	0 	0 	0 	
0 	1 	1 	1 	0 	
MAP on testing env
map_weights [-0.1252402   0.15744655  0.48456404  0.23274922]
map reward
-0.13	0.23	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	0.16	
0.16	-0.13	0.48	0.16	0.16	
0.23	-0.13	-0.13	-0.13	-0.13	
-0.13	0.16	0.16	0.16	-0.13	
Map policy
>	v	v	v	v	
v	>	v	v	v	
>	>	>	<	<	
^	^	^	^	^	
^	>	^	<	<	
MEAN policy on test env
mean_weights [-0.12491482 -0.12139702  0.30312531 -0.019714  ]
mean reward
-0.12	-0.02	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	0.30	-0.12	-0.12	
-0.02	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
mean policy
>	v	v	v	v	
>	>	v	v	v	
>	>	>	<	<	
^	^	^	^	<	
^	^	^	<	<	
features
0 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	2 	1 	1 	
3 	0 	0 	0 	0 	
0 	1 	1 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	^	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
<	^	^	^	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	<	
>	v	v	<	<	
>	>	<	<	v	
>	^	^	<	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.89	-9.29	-8.89	-8.89	-8.89	
-8.89	-8.89	-8.89	-8.89	-9.31	
-9.31	-8.89	-8.81	-9.31	-9.31	
-9.29	-8.89	-8.89	-8.89	-8.89	
-8.89	-9.31	-9.31	-9.31	-8.89	
ird policy
v	>	v	v	<	
>	>	v	<	<	
>	>	^	<	v	
>	^	^	<	<	
^	>	^	^	^	
MAP policy loss 32.9387588972663
mean policy loss 83.28473313958493
robust policy loss 360.4975494399012
regret policy loss 10.6420642578813
ird policy loss 22.5625004657074
MAP lava occupancy 0.230450383568558
Mean lava occupancy 0.230450383568558
Robust lava occupancy 3.3547645912519966
Regret lava occupancy 0.11202172901489922
IRD lava occupancy 0.23750000258925014
##############
Trial  31
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (6, 1), (17, 2), (13, 0), (11, 1), (2, 3), (5, 1), (0, 3), (12, 3), (4, 0)]
w_map [-0.09662254 -0.09421847 -0.40103919  0.40811981] loglik -13.7138831389789
accepted/total = 53/2000 = 0.0265
MAP Policy on Train MDP
map_weights [-0.09662254 -0.09421847 -0.40103919  0.40811981]
map reward
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.09	
-0.10	-0.10	-0.40	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
Map policy
>	>	>	>	v	
>	>	>	>	>	
^	^	^	>	^	
^	>	>	^	^	
>	>	>	^	^	
MEAN policy on Train MDP
mean_weights [-0.18185336 -0.17867016 -0.41307713  0.22333011]
mean reward
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.41	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
mean policy
>	>	>	>	v	
>	>	>	>	>	
^	^	^	>	^	
^	>	>	^	^	
>	>	>	^	^	
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 82.29766701420179
Mean policy loss 82.29767665849637
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	>	
>	>	<	<	<	
^	>	^	^	^	
>	^	^	<	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.09662254 -0.09421847 -0.40103919  0.40811981]
map reward
-0.10	-0.09	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	0.41	
-0.10	-0.10	-0.40	-0.10	-0.10	
-0.09	-0.10	-0.10	0.41	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
Map policy
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
>	>	>	^	^	
^	>	>	^	^	
MEAN policy on test env
mean_weights [-0.18185336 -0.17867016 -0.41307713  0.22333011]
mean reward
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	0.22	
-0.18	-0.18	-0.41	-0.18	-0.18	
-0.18	-0.18	-0.18	0.22	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
mean policy
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
>	>	>	^	^	
^	>	>	^	^	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	>	v	v	
>	>	>	>	>	
>	^	^	>	^	
>	>	>	^	^	
^	>	>	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	>	>	v	
>	>	>	>	>	
>	>	^	>	^	
>	>	>	^	^	
^	>	>	^	^	
-------- IRD Solution -------
ird reward
4.00	4.01	4.00	4.00	4.00	
4.00	4.00	4.00	4.00	4.58	
4.00	4.00	3.98	4.00	4.00	
4.01	4.00	4.00	4.58	4.00	
4.00	4.00	4.00	4.00	4.00	
ird policy
>	>	>	v	v	
>	>	>	>	>	
>	>	^	^	^	
>	>	>	^	^	
^	>	>	^	^	
MAP policy loss 1652.2405837144588
mean policy loss 1662.1160008044765
robust policy loss 1662.1160006997306
regret policy loss 1662.115971949496
ird policy loss 1662.1160008110123
MAP lava occupancy 16.508394408105413
Mean lava occupancy 16.508394408105413
Robust lava occupancy 16.601092729461335
Regret lava occupancy 16.601092452745778
IRD lava occupancy 16.601092730426533
##############
Trial  32
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [ 0.00700689  0.25032036  0.49799197 -0.24468078] loglik -9.704047627027705
accepted/total = 1593/2000 = 0.7965
MAP Policy on Train MDP
map_weights [ 0.00700689  0.25032036  0.49799197 -0.24468078]
map reward
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.50	0.01	0.01	
0.01	0.25	0.01	0.01	0.01	
0.01	0.01	0.25	0.01	0.01	
Map policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	>	v	<	<	
MEAN policy on Train MDP
mean_weights [ 0.01381472 -0.14301961  0.41022884 -0.066722  ]
mean reward
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.41	0.01	0.01	
0.01	-0.14	0.01	0.01	0.01	
0.01	0.01	-0.14	0.01	0.01	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
MAP policy loss 44.45390733406456
Mean policy loss 1.9285005536939792e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	>	v	
^	>	v	>	v	
<	<	^	>	v	
^	<	^	<	<	
>	>	^	^	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	2 	3 	0 	
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.00700689  0.25032036  0.49799197 -0.24468078]
map reward
0.01	0.01	0.01	-0.24	0.01	
-0.24	0.01	0.01	0.25	0.01	
0.01	-0.24	0.50	-0.24	0.01	
0.01	-0.24	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
Map policy
>	>	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
>	>	^	^	<	
MEAN policy on test env
mean_weights [ 0.01381472 -0.14301961  0.41022884 -0.066722  ]
mean reward
0.01	0.01	0.01	-0.07	0.01	
-0.07	0.01	0.01	-0.14	0.01	
0.01	-0.07	0.41	-0.07	0.01	
0.01	-0.07	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
mean policy
>	>	v	<	v	
>	>	v	<	v	
>	>	^	<	<	
^	>	^	<	<	
>	>	^	^	<	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	2 	3 	0 	
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
>	>	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	v	
>	>	v	<	v	
>	>	^	<	v	
^	>	^	<	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-8.77	-8.77	-8.77	-9.22	-8.77	
-9.22	-8.77	-8.77	-9.23	-8.77	
-8.77	-9.22	-8.70	-9.22	-8.77	
-8.77	-9.22	-8.77	-8.77	-8.77	
-8.77	-8.77	-8.77	-8.77	-8.77	
ird policy
>	v	v	<	v	
>	>	v	<	v	
<	>	^	<	v	
^	>	^	<	<	
>	>	^	<	<	
MAP policy loss 249.287920785035
mean policy loss 20.81280096683324
robust policy loss 81.71824854824095
regret policy loss 5.194222564576532
ird policy loss -8.965420646329936e-09
MAP lava occupancy 2.432925202185375
Mean lava occupancy 2.432925202185375
Robust lava occupancy 0.8204343604320051
Regret lava occupancy 0.052304065603468654
IRD lava occupancy 2.2117408295213132e-11
##############
Trial  33
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	v	
>	>	^	<	>	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (9, 0), (12, 0), (8, 0), (6, 3), (17, 2), (11, 1), (4, 3), (12, 3)]
w_map [-0.27341446 -0.2445027  -0.39189704  0.0901858 ] loglik -8.078232322209868
accepted/total = 903/2000 = 0.4515
MAP Policy on Train MDP
map_weights [-0.27341446 -0.2445027  -0.39189704  0.0901858 ]
map reward
-0.27	-0.27	-0.27	-0.24	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.39	-0.24	-0.27	
-0.24	-0.27	-0.27	-0.24	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
Map policy
v	>	>	^	<	
v	>	>	^	<	
v	v	>	v	<	
<	<	<	^	<	
^	^	<	^	<	
MEAN policy on Train MDP
mean_weights [-0.30843112 -0.09957178 -0.35009402 -0.10876916]
mean reward
-0.31	-0.31	-0.31	-0.10	-0.31	
-0.31	-0.31	-0.31	-0.31	-0.31	
-0.31	-0.31	-0.35	-0.10	-0.31	
-0.10	-0.31	-0.31	-0.10	-0.31	
-0.31	-0.31	-0.31	-0.31	-0.31	
mean policy
>	>	>	^	<	
v	>	v	v	v	
v	>	>	v	<	
<	>	>	^	<	
^	>	^	^	^	
Optimal Policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	v	
>	>	^	<	>	
>	^	^	<	<	
MAP policy loss 89.90532006018171
Mean policy loss 89.90532051059961
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	v	
>	>	v	<	<	
>	>	v	<	^	
>	^	^	<	<	
^	^	^	<	<	
reward
-1.00	-5.00	-100.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	1 	0 	
0 	1 	0 	0 	0 	
1 	0 	2 	3 	0 	
1 	0 	0 	0 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.27341446 -0.2445027  -0.39189704  0.0901858 ]
map reward
-0.27	-0.24	0.09	-0.24	-0.27	
-0.27	-0.24	-0.27	-0.27	-0.27	
-0.24	-0.27	-0.39	0.09	-0.27	
-0.24	-0.27	-0.27	-0.27	-0.27	
-0.27	0.09	-0.27	-0.27	-0.27	
Map policy
>	>	^	<	<	
v	>	^	v	<	
v	v	>	<	<	
>	v	<	<	<	
>	v	<	<	<	
MEAN policy on test env
mean_weights [-0.30843112 -0.09957178 -0.35009402 -0.10876916]
mean reward
-0.31	-0.10	-0.11	-0.10	-0.31	
-0.31	-0.10	-0.31	-0.31	-0.31	
-0.10	-0.31	-0.35	-0.11	-0.31	
-0.10	-0.31	-0.31	-0.31	-0.31	
-0.31	-0.11	-0.31	-0.31	-0.31	
mean policy
>	v	<	^	<	
v	^	<	^	^	
<	<	<	^	<	
^	<	<	^	^	
^	v	<	<	<	
features
0 	1 	3 	1 	0 	
0 	1 	0 	0 	0 	
1 	0 	2 	3 	0 	
1 	0 	0 	0 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	<	^	<	
v	^	v	^	<	
<	>	>	<	<	
^	v	^	^	^	
^	v	<	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	^	<	^	<	
v	^	v	^	<	
v	>	^	<	^	
^	<	<	<	^	
^	<	^	<	^	
-------- IRD Solution -------
ird reward
1.02	1.04	0.21	1.04	1.02	
1.02	1.04	1.02	1.02	1.02	
1.04	1.02	1.01	0.21	1.02	
1.04	1.02	1.02	1.02	1.02	
1.02	0.21	1.02	1.02	1.02	
ird policy
>	^	^	^	<	
>	^	<	^	<	
v	^	<	<	^	
^	<	<	<	^	
^	v	^	<	^	
MAP policy loss 299.5727734033867
mean policy loss 496.25594440679373
robust policy loss 559.2996664340753
regret policy loss 87.33093815520354
ird policy loss 87.33093914795646
MAP lava occupancy 2.30953591101788
Mean lava occupancy 2.30953591101788
Robust lava occupancy 4.950493120720279
Regret lava occupancy 1.1886495083089119e-09
IRD lava occupancy 7.591071287237375e-09
##############
Trial  34
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
^	>	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	1 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (8, 3), (12, 1), (3, 3), (12, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.24408543 -0.30098166  0.37212109  0.08281182] loglik -6.931471805634864
accepted/total = 1419/2000 = 0.7095
MAP Policy on Train MDP
map_weights [-0.24408543 -0.30098166  0.37212109  0.08281182]
map reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.30	-0.30	-0.24	-0.24	-0.30	
-0.24	-0.24	0.37	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
Map policy
>	>	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	<	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.03549642 -0.18799228  0.33494035 -0.06837022]
mean reward
0.04	0.04	0.04	0.04	0.04	
-0.19	-0.19	0.04	0.04	-0.19	
0.04	0.04	0.33	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
mean policy
>	>	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
^	^	^	^	^	
Optimal Policy
>	>	v	v	<	
^	>	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss -1.923820908134033e-09
Mean policy loss 7.538731305878343e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	^	<	
^	<	^	^	^	
reward
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-100.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-1.00	-1.00	
features
0 	1 	1 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	1 	
0 	3 	0 	1 	0 	
0 	1 	3 	0 	0 	
MAP on testing env
map_weights [-0.24408543 -0.30098166  0.37212109  0.08281182]
map reward
-0.24	-0.30	-0.30	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.30	-0.24	
-0.24	-0.24	0.37	-0.24	-0.30	
-0.24	0.08	-0.24	-0.30	-0.24	
-0.24	-0.30	0.08	-0.24	-0.24	
Map policy
v	v	v	v	<	
>	>	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
MEAN policy on test env
mean_weights [ 0.03549642 -0.18799228  0.33494035 -0.06837022]
mean reward
0.04	-0.19	-0.19	0.04	0.04	
0.04	0.04	0.04	-0.19	0.04	
0.04	0.04	0.33	0.04	-0.19	
0.04	-0.07	0.04	-0.19	0.04	
0.04	-0.19	-0.07	0.04	0.04	
mean policy
v	v	v	<	<	
>	>	v	<	<	
>	>	>	<	<	
^	>	^	<	<	
^	^	^	<	<	
features
0 	1 	1 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	1 	
0 	3 	0 	1 	0 	
0 	1 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	^	v	v	
v	^	v	v	v	
>	>	<	<	>	
>	v	^	^	<	
>	>	v	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
^	^	^	^	>	
^	<	^	<	<	
-------- IRD Solution -------
ird reward
-9.17	-9.56	-9.56	-9.17	-9.17	
-9.17	-9.17	-9.17	-9.56	-9.17	
-9.17	-9.17	-8.96	-9.17	-9.56	
-9.17	-9.56	-9.17	-9.56	-9.17	
-9.17	-9.56	-9.56	-9.17	-9.17	
ird policy
v	v	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	^	^	
MAP policy loss 23.347769705564296
mean policy loss 21.434375000067654
robust policy loss 501.6382455629173
regret policy loss 21.43672591282118
ird policy loss 1.683735944765785e-08
MAP lava occupancy 0.21427148540157334
Mean lava occupancy 0.21427148540157334
Robust lava occupancy 4.72652782458114
Regret lava occupancy 0.22562500000271038
IRD lava occupancy 1.5008907405100832e-10
##############
Trial  35
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 0), (6, 1), (12, 3), (17, 2), (2, 3), (13, 0), (4, 0)]
w_map [ 0.01646451 -0.02784589  0.83786176 -0.11782784] loglik -6.068066204080765
accepted/total = 1320/2000 = 0.66
MAP Policy on Train MDP
map_weights [ 0.01646451 -0.02784589  0.83786176 -0.11782784]
map reward
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
0.02	-0.03	0.84	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
Map policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.07095621 -0.21119904  0.36534503 -0.05138326]
mean reward
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	-0.21	0.37	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
mean policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
MAP policy loss 1.927630948264647e-06
Mean policy loss 1.931138929323062e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	^	v	
>	v	v	<	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-100.00	-5.00	
-5.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	3 	0 	0 	
0 	0 	1 	3 	1 	
1 	0 	2 	1 	1 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.01646451 -0.02784589  0.83786176 -0.11782784]
map reward
0.02	-0.03	-0.12	0.02	0.02	
0.02	0.02	-0.03	-0.12	-0.03	
-0.03	0.02	0.84	-0.03	-0.03	
0.02	0.02	0.02	0.02	-0.03	
0.02	0.02	0.02	-0.03	0.02	
Map policy
v	v	v	<	v	
>	v	v	<	v	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.07095621 -0.21119904  0.36534503 -0.05138326]
mean reward
0.07	-0.21	-0.05	0.07	0.07	
0.07	0.07	-0.21	-0.05	-0.21	
-0.21	0.07	0.37	-0.21	-0.21	
0.07	0.07	0.07	0.07	-0.21	
0.07	0.07	0.07	-0.21	0.07	
mean policy
v	v	v	v	<	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	<	
features
0 	1 	3 	0 	0 	
0 	0 	1 	3 	1 	
1 	0 	2 	1 	1 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	<	
>	v	v	v	<	
>	>	<	<	v	
>	^	^	<	<	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-8.85	-9.23	-9.25	-8.85	-8.85	
-8.85	-8.85	-9.23	-9.25	-9.23	
-9.23	-8.85	-8.62	-9.23	-9.23	
-8.85	-8.85	-8.85	-8.85	-9.23	
-8.85	-8.85	-8.85	-9.23	-8.85	
ird policy
v	^	v	v	<	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
MAP policy loss 46.04476981283305
mean policy loss 20.484374989959875
robust policy loss 20.48437524423
regret policy loss 20.484374952851894
ird policy loss 20.48437601145371
MAP lava occupancy 0.24151130950687189
Mean lava occupancy 0.24151130950687189
Robust lava occupancy 0.22562500267430308
Regret lava occupancy 0.22562500001698416
IRD lava occupancy 0.22562500674267078
##############
Trial  36
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	^	v	
v	v	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (1, 1), (13, 0)]
w_map [-0.19736557 -0.43311254  0.3424374  -0.02708449] loglik -6.931471806958633
accepted/total = 1416/2000 = 0.708
MAP Policy on Train MDP
map_weights [-0.19736557 -0.43311254  0.3424374  -0.02708449]
map reward
-0.20	-0.20	-0.20	-0.43	-0.20	
-0.20	-0.43	-0.20	-0.20	-0.20	
-0.20	-0.20	0.34	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
Map policy
>	>	v	<	v	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.06694963 -0.20855451  0.41336219  0.06632902]
mean reward
0.07	0.07	0.07	-0.21	0.07	
0.07	-0.21	0.07	0.07	0.07	
0.07	0.07	0.41	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
mean policy
>	>	v	<	v	
v	>	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	<	
Optimal Policy
>	>	v	^	v	
v	v	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 2.069449646369126e-11
Mean policy loss 2.740265291970023e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	<	v	v	
v	v	v	v	<	
>	>	^	<	<	
>	^	^	>	^	
>	^	v	>	^	
reward
-1.00	-1.00	-100.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
features
0 	0 	3 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	0 	
MAP on testing env
map_weights [-0.19736557 -0.43311254  0.3424374  -0.02708449]
map reward
-0.20	-0.20	-0.03	-0.43	-0.20	
-0.43	-0.43	-0.20	-0.20	-0.20	
-0.43	-0.20	0.34	-0.20	-0.20	
-0.20	-0.20	-0.43	-0.03	-0.20	
-0.20	-0.20	-0.20	-0.03	-0.20	
Map policy
>	>	v	<	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.06694963 -0.20855451  0.41336219  0.06632902]
mean reward
0.07	0.07	0.07	-0.21	0.07	
-0.21	-0.21	0.07	0.07	0.07	
-0.21	0.07	0.41	0.07	0.07	
0.07	0.07	-0.21	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
mean policy
>	>	v	<	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	^	^	
>	^	^	^	^	
features
0 	0 	3 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	v	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	^	^	>	^	
^	^	v	<	^	
-------- IRD Solution -------
ird reward
-8.85	-8.85	-9.26	-9.22	-8.85	
-9.22	-9.22	-8.85	-8.85	-8.85	
-9.22	-8.85	-8.65	-8.85	-8.85	
-8.85	-8.85	-9.22	-9.26	-8.85	
-8.85	-8.85	-8.85	-9.26	-8.85	
ird policy
>	v	>	v	v	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	v	<	^	
MAP policy loss 45.75492083826193
mean policy loss 21.43443566688266
robust policy loss 52.150103915753846
regret policy loss 13.361008287995885
ird policy loss 1.3581988311806015e-10
MAP lava occupancy 0.3720938527395812
Mean lava occupancy 0.3720938527395812
Robust lava occupancy 0.5358848880384248
Regret lava occupancy 0.14064219243488388
IRD lava occupancy 1.8439880674373593e-12
##############
Trial  37
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 1), (8, 3), (12, 0), (12, 3), (17, 2), (11, 1), (2, 3), (4, 3), (1, 1), (13, 0)]
w_map [-0.68600275 -0.10427169  0.09138035 -0.11834521] loglik -9.010913347278688
accepted/total = 1821/2000 = 0.9105
MAP Policy on Train MDP
map_weights [-0.68600275 -0.10427169  0.09138035 -0.11834521]
map reward
-0.69	-0.69	-0.69	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.69	-0.69	
-0.69	-0.69	0.09	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.69	-0.69	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.18761555 -0.08523501  0.18818718 -0.03969976]
mean reward
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
mean policy
v	v	v	v	<	
>	v	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.4137848477304124e-10
Mean policy loss 3.614013590209344e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	v	
v	v	v	v	<	
>	>	<	<	<	
^	>	^	<	<	
>	^	^	<	^	
reward
-1.00	-5.00	-1.00	-100.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
features
0 	1 	0 	3 	0 	
1 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
MAP on testing env
map_weights [-0.68600275 -0.10427169  0.09138035 -0.11834521]
map reward
-0.69	-0.10	-0.69	-0.12	-0.69	
-0.10	-0.10	-0.69	-0.69	-0.69	
-0.69	-0.69	0.09	-0.69	-0.69	
-0.10	-0.69	-0.69	-0.69	-0.69	
-0.69	-0.69	-0.69	-0.12	-0.69	
Map policy
v	^	>	^	<	
>	<	<	^	<	
^	^	^	<	<	
<	<	^	v	<	
^	>	>	v	<	
MEAN policy on test env
mean_weights [-0.18761555 -0.08523501  0.18818718 -0.03969976]
mean reward
-0.19	-0.09	-0.19	-0.04	-0.19	
-0.09	-0.09	-0.19	-0.19	-0.19	
-0.19	-0.19	0.19	-0.19	-0.19	
-0.09	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.04	-0.19	
mean policy
v	v	v	v	<	
>	>	v	v	v	
>	>	>	<	<	
>	^	^	^	^	
^	^	^	^	<	
features
0 	1 	0 	3 	0 	
1 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	v	v	^	<	
>	v	v	v	v	
>	>	>	<	<	
<	^	^	<	^	
^	^	^	v	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.68	-9.14	-8.68	-9.07	-8.68	
-9.14	-9.14	-8.68	-8.68	-8.68	
-8.68	-8.68	-8.54	-8.68	-8.68	
-9.14	-8.68	-8.68	-8.68	-8.68	
-8.68	-8.68	-8.68	-9.07	-8.68	
ird policy
v	>	v	<	v	
v	<	v	<	<	
>	>	>	<	<	
^	^	^	<	<	
>	^	^	^	^	
MAP policy loss 24.508639153754245
mean policy loss 48.87749998073066
robust policy loss 557.1113503229544
regret policy loss 4.469648608915962
ird policy loss 2.0924473732655824e-06
MAP lava occupancy 0.23750000023058712
Mean lava occupancy 0.23750000023058712
Robust lava occupancy 5.338685540396355
Regret lava occupancy 0.045147965601233346
IRD lava occupancy 1.744554633362188e-08
##############
Trial  38
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (12, 0), (3, 0), (6, 3), (17, 2), (11, 1), (2, 3), (13, 0), (12, 3), (4, 0)]
w_map [-0.29094322 -0.2560813   0.32193802  0.13103746] loglik -8.307067289840862
accepted/total = 1279/2000 = 0.6395
MAP Policy on Train MDP
map_weights [-0.29094322 -0.2560813   0.32193802  0.13103746]
map reward
-0.29	-0.29	-0.29	-0.29	-0.29	
-0.29	-0.29	-0.29	-0.29	-0.29	
-0.26	-0.29	0.32	-0.29	-0.29	
-0.29	-0.29	-0.29	-0.29	-0.29	
-0.29	-0.26	-0.29	-0.29	-0.29	
Map policy
v	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.01785561 -0.3023209   0.17867118  0.00477923]
mean reward
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.30	-0.02	0.18	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.30	-0.02	-0.02	-0.02	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
^	^	^	<	<	
MAP policy loss 1.8524999964808466
Mean policy loss 1.1986228676263977e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	^	
<	<	^	<	^	
^	v	^	^	^	
>	<	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	1.00	-5.00	-1.00	
-100.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	2 	1 	0 	
3 	3 	3 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.29094322 -0.2560813   0.32193802  0.13103746]
map reward
-0.29	-0.29	-0.29	-0.29	-0.29	
-0.29	-0.29	-0.29	-0.26	-0.29	
-0.29	0.13	0.32	-0.26	-0.29	
0.13	0.13	0.13	-0.29	-0.29	
-0.29	-0.29	0.13	-0.29	-0.29	
Map policy
v	v	v	v	v	
v	v	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
^	^	^	<	<	
MEAN policy on test env
mean_weights [-0.01785561 -0.3023209   0.17867118  0.00477923]
mean reward
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.30	-0.02	
-0.02	0.00	0.18	-0.30	-0.02	
0.00	0.00	0.00	-0.02	-0.02	
-0.02	-0.02	0.00	-0.02	-0.02	
mean policy
v	v	v	<	<	
v	v	v	<	v	
>	>	<	<	v	
>	^	^	<	<	
^	>	^	<	<	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	2 	1 	0 	
3 	3 	3 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	<	
>	>	v	v	v	
<	>	^	^	v	
^	^	^	<	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	<	
>	>	v	<	^	
^	>	^	<	<	
^	^	^	^	^	
^	v	>	^	^	
-------- IRD Solution -------
ird reward
-8.22	-8.22	-8.22	-8.22	-8.22	
-8.22	-8.22	-8.22	-8.62	-8.22	
-8.22	-8.54	-7.95	-8.62	-8.22	
-8.54	-8.54	-8.54	-8.22	-8.22	
-8.22	-8.22	-8.54	-8.22	-8.22	
ird policy
>	v	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	<	<	
^	^	^	^	^	
MAP policy loss 468.04312335187416
mean policy loss 912.5798491326823
robust policy loss 380.1906978127402
regret policy loss 19.82112410404436
ird policy loss 40.18803014436542
MAP lava occupancy 4.683715068339032
Mean lava occupancy 4.683715068339032
Robust lava occupancy 3.7821653850377333
Regret lava occupancy 0.23750000034705113
IRD lava occupancy 0.4518437500032477
##############
Trial  39
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	<	v	
v	v	v	v	v	
>	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (12, 3), (17, 2), (11, 1), (2, 3), (1, 1), (13, 0), (4, 0)]
w_map [-0.09570856 -0.09610753 -0.31983566 -0.48834824] loglik -12.457962473699496
accepted/total = 5/2000 = 0.0025
MAP Policy on Train MDP
map_weights [-0.09570856 -0.09610753 -0.31983566 -0.48834824]
map reward
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.32	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
Map policy
v	>	^	<	v	
v	^	^	^	v	
>	v	^	v	<	
>	>	<	<	<	
>	>	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.11721311 -0.11699813 -0.33845166 -0.4273371 ]
mean reward
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.34	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
mean policy
v	v	v	v	v	
>	>	<	<	<	
>	^	<	^	<	
^	^	<	^	^	
^	^	^	^	^	
Optimal Policy
v	>	v	<	v	
v	v	v	v	v	
>	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
MAP policy loss 16.707877404032264
Mean policy loss 51.9284610188533
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	<	<	<	
v	^	v	^	>	
>	>	v	<	v	
v	>	^	>	>	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-100.00	-5.00	-1.00	
-1.00	-5.00	1.00	-100.00	-100.00	
-100.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	3 	3 	1 	0 	
0 	1 	2 	3 	3 	
3 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.09570856 -0.09610753 -0.31983566 -0.48834824]
map reward
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.49	-0.49	-0.10	-0.10	
-0.10	-0.10	-0.32	-0.49	-0.49	
-0.49	-0.10	-0.10	-0.49	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
Map policy
v	^	^	<	<	
v	^	>	^	>	
<	v	v	^	v	
v	v	v	>	>	
>	^	^	<	<	
MEAN policy on test env
mean_weights [-0.11721311 -0.11699813 -0.33845166 -0.4273371 ]
mean reward
-0.12	-0.12	-0.12	-0.12	-0.12	
-0.12	-0.43	-0.43	-0.12	-0.12	
-0.12	-0.12	-0.34	-0.43	-0.43	
-0.43	-0.12	-0.12	-0.43	-0.12	
-0.12	-0.12	-0.12	-0.12	-0.12	
mean policy
v	>	>	v	<	
v	^	>	^	<	
>	<	v	^	v	
v	^	<	>	>	
>	^	^	<	<	
features
0 	0 	0 	0 	0 	
0 	3 	3 	1 	0 	
0 	1 	2 	3 	3 	
3 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	^	^	v	v	
v	>	>	^	<	
>	<	<	>	>	
<	^	<	^	>	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	^	^	v	<	
v	^	v	^	<	
>	<	v	<	^	
^	^	<	>	>	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
3.67	3.67	3.67	3.67	3.67	
3.67	3.27	3.27	3.67	3.67	
3.67	3.67	3.44	3.27	3.27	
3.27	3.67	3.67	3.27	3.67	
3.67	3.67	3.67	3.67	3.67	
ird policy
v	^	^	<	<	
v	^	>	^	>	
<	v	v	^	v	
v	v	v	>	>	
>	^	^	<	<	
MAP policy loss 391.8647887116259
mean policy loss 49.1571094029964
robust policy loss 49.157123909031185
regret policy loss 49.15711361462907
ird policy loss 14.377309451253957
MAP lava occupancy 3.7924856099326
Mean lava occupancy 3.7924856099326
Robust lava occupancy 1.547372748762175e-07
Regret lava occupancy 1.6056471956868285e-11
IRD lava occupancy 4.763881929644797e-10
##############
Trial  40
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (12, 0), (3, 0), (6, 3), (11, 1), (2, 3), (13, 0), (4, 0)]
w_map [-0.22290222 -0.35558357  0.41037315  0.01114106] loglik -6.068425588269307
accepted/total = 1415/2000 = 0.7075
MAP Policy on Train MDP
map_weights [-0.22290222 -0.35558357  0.41037315  0.01114106]
map reward
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.36	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	0.41	-0.22	-0.22	
-0.22	-0.22	-0.36	-0.22	-0.22	
-0.22	-0.36	-0.22	-0.22	-0.22	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.05798525 -0.20290147  0.36465641 -0.02834813]
mean reward
0.06	0.06	0.06	0.06	0.06	
-0.20	0.06	0.06	0.06	0.06	
0.06	0.06	0.36	0.06	0.06	
0.06	0.06	-0.20	0.06	0.06	
0.06	-0.20	0.06	0.06	0.06	
mean policy
>	v	v	v	v	
v	>	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
MAP policy loss -1.1524907937709994e-10
Mean policy loss 7.168550475444846e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	<	
v	<	^	v	>	
>	>	^	<	v	
^	<	^	^	<	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	0 	0 	1 	
0 	3 	2 	3 	0 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.22290222 -0.35558357  0.41037315  0.01114106]
map reward
-0.22	-0.22	0.01	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.36	
-0.22	0.01	0.41	0.01	-0.22	
-0.22	-0.22	-0.22	-0.22	0.01	
-0.22	0.01	-0.22	-0.22	-0.22	
Map policy
>	>	v	<	<	
v	v	v	v	<	
>	>	>	<	<	
>	^	^	^	^	
>	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.05798525 -0.20290147  0.36465641 -0.02834813]
mean reward
0.06	0.06	-0.03	0.06	0.06	
0.06	0.06	0.06	0.06	-0.20	
0.06	-0.03	0.36	-0.03	0.06	
0.06	0.06	0.06	0.06	-0.03	
0.06	-0.03	0.06	0.06	0.06	
mean policy
>	v	v	v	<	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	<	<	
features
0 	0 	3 	0 	0 	
0 	0 	0 	0 	1 	
0 	3 	2 	3 	0 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	^	
^	>	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	<	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-8.65	-8.65	-8.96	-8.65	-8.65	
-8.65	-8.65	-8.65	-8.65	-9.03	
-8.65	-8.96	-8.42	-8.96	-8.65	
-8.65	-8.65	-8.65	-8.65	-8.96	
-8.65	-8.96	-8.65	-8.65	-8.65	
ird policy
>	v	v	v	<	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	<	<	
MAP policy loss 607.4048734005396
mean policy loss 2.2654816221899304e-07
robust policy loss 32.01019784218213
regret policy loss 0.14232231557984973
ird policy loss 7.598811299809605e-08
MAP lava occupancy 6.130590214438918
Mean lava occupancy 6.130590214438918
Robust lava occupancy 0.3233353317383956
Regret lava occupancy 0.0014375991496521287
IRD lava occupancy 7.391357703660378e-10
##############
Trial  41
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.23665722 -0.42811245  0.32501248 -0.01021785] loglik -9.010913348709387
accepted/total = 1470/2000 = 0.735
MAP Policy on Train MDP
map_weights [-0.23665722 -0.42811245  0.32501248 -0.01021785]
map reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.43	-0.24	0.33	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.43	
-0.24	-0.24	-0.24	-0.24	-0.24	
Map policy
>	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.09377913 -0.17266411  0.43582155 -0.01349832]
mean reward
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
-0.17	0.09	0.44	0.09	0.09	
0.09	0.09	0.09	0.09	-0.17	
0.09	0.09	0.09	0.09	0.09	
mean policy
>	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
MAP policy loss 2.902167915708187e-10
Mean policy loss 7.668171653207878e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	^	v	
>	v	<	^	v	
^	>	v	<	<	
>	>	^	>	^	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-100.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.23665722 -0.42811245  0.32501248 -0.01021785]
map reward
-0.24	-0.43	-0.01	-0.24	-0.24	
-0.24	-0.24	-0.01	-0.01	-0.24	
-0.01	-0.24	0.33	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.43	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
Map policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	<	^	
^	^	^	<	^	
MEAN policy on test env
mean_weights [ 0.09377913 -0.17266411  0.43582155 -0.01349832]
mean reward
0.09	-0.17	-0.01	0.09	0.09	
0.09	0.09	-0.01	-0.01	0.09	
-0.01	0.09	0.44	0.09	0.09	
0.09	0.09	0.09	-0.17	0.09	
0.09	0.09	0.09	0.09	0.09	
mean policy
v	v	v	<	v	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	<	^	
>	^	^	<	<	
features
0 	1 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	<	^	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-9.35	-9.81	-9.81	-9.35	-9.35	
-9.35	-9.35	-9.81	-9.81	-9.35	
-9.81	-9.35	-9.28	-9.35	-9.35	
-9.35	-9.35	-9.35	-9.81	-9.35	
-9.35	-9.35	-9.35	-9.35	-9.35	
ird policy
v	v	>	^	v	
>	v	v	v	v	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	<	<	
MAP policy loss 240.4511937264823
mean policy loss 1.2656407052242385e-05
robust policy loss 84.87630295627157
regret policy loss 1.5401084471015208e-07
ird policy loss 1.7505788485472351e-09
MAP lava occupancy 2.4170940197296713
Mean lava occupancy 2.4170940197296713
Robust lava occupancy 0.8573363934580681
Regret lava occupancy 1.5537577805711869e-09
IRD lava occupancy 1.5820409942507604e-11
##############
Trial  42
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (3, 3), (12, 0), (8, 0), (6, 3), (17, 2), (13, 0), (11, 1), (5, 1), (0, 3), (12, 3), (4, 0)]
w_map [ 0.03047078 -0.05954821  0.89770555  0.01227546] loglik -8.317766166718911
accepted/total = 1325/2000 = 0.6625
MAP Policy on Train MDP
map_weights [ 0.03047078 -0.05954821  0.89770555  0.01227546]
map reward
0.03	0.03	-0.06	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
-0.06	0.03	0.90	0.03	-0.06	
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
Map policy
v	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.03702975 -0.21390682  0.36610233 -0.07742559]
mean reward
0.04	0.04	-0.21	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
-0.21	0.04	0.37	0.04	-0.21	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
mean policy
v	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	^	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss -5.958254566812426e-10
Mean policy loss 2.351568321665165e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	^	
>	>	v	<	>	
>	^	^	<	^	
>	>	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-100.00	-1.00	1.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	2 	3 	0 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
MAP on testing env
map_weights [ 0.03047078 -0.05954821  0.89770555  0.01227546]
map reward
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	-0.06	0.01	
0.01	0.03	0.90	0.01	0.03	
-0.06	0.03	0.03	0.01	0.03	
0.03	-0.06	0.03	0.01	0.03	
Map policy
>	v	v	<	<	
>	v	v	<	v	
>	>	v	<	<	
>	^	^	<	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.03702975 -0.21390682  0.36610233 -0.07742559]
mean reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	-0.21	-0.08	
-0.08	0.04	0.37	-0.08	0.04	
-0.21	0.04	0.04	-0.08	0.04	
0.04	-0.21	0.04	-0.08	0.04	
mean policy
>	v	v	<	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	^	
>	>	^	^	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	2 	3 	0 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	<	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	^	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	<	<	
>	v	v	<	^	
>	>	v	<	<	
>	>	^	<	^	
<	>	^	<	^	
-------- IRD Solution -------
ird reward
-8.82	-8.82	-8.82	-8.82	-8.82	
-8.82	-8.82	-8.82	-9.21	-9.18	
-9.18	-8.82	-8.60	-9.18	-8.82	
-9.21	-8.82	-8.82	-9.18	-8.82	
-8.82	-9.21	-8.82	-9.18	-8.82	
ird policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	^	
>	^	^	^	^	
MAP policy loss 255.40287117859697
mean policy loss 17.043093779728693
robust policy loss 103.56057098172809
regret policy loss 17.30602302901918
ird policy loss 17.043076146250616
MAP lava occupancy 2.6129022859384774
Mean lava occupancy 2.6129022859384774
Robust lava occupancy 1.0791416778891194
Regret lava occupancy 0.214343749999667
IRD lava occupancy 0.21434375024394958
##############
Trial  43
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	^	v	
>	>	v	<	<	
>	>	<	<	<	
^	>	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 1), (17, 2), (11, 1), (4, 3), (13, 0), (12, 3)]
w_map [ 0.03650913 -0.50454786  0.41696374 -0.04197926] loglik -8.317765640308153
accepted/total = 1208/2000 = 0.604
MAP Policy on Train MDP
map_weights [ 0.03650913 -0.50454786  0.41696374 -0.04197926]
map reward
0.04	0.04	0.04	-0.50	0.04	
-0.50	0.04	0.04	0.04	0.04	
0.04	0.04	0.42	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
Map policy
>	v	v	v	v	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.06000824 -0.20016134  0.32863679  0.0279338 ]
mean reward
0.06	0.06	0.06	-0.20	0.06	
-0.20	0.06	0.06	0.06	0.06	
0.06	0.06	0.33	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
^	^	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	^	v	
>	>	v	<	<	
>	>	<	<	<	
^	>	^	^	<	
^	^	^	^	<	
MAP policy loss 5.163788638698485e-07
Mean policy loss 5.187604512624233e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	<	<	
>	>	<	<	>	
^	^	^	<	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-5.00	-1.00	1.00	-100.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	3 	
1 	0 	2 	3 	0 	
0 	1 	1 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [ 0.03650913 -0.50454786  0.41696374 -0.04197926]
map reward
0.04	0.04	0.04	-0.50	0.04	
0.04	0.04	0.04	0.04	-0.04	
-0.50	0.04	0.42	-0.04	0.04	
0.04	-0.50	-0.50	0.04	0.04	
0.04	0.04	-0.04	0.04	0.04	
Map policy
>	>	v	<	v	
>	v	v	<	<	
<	>	<	<	<	
<	^	^	^	^	
>	>	>	^	^	
MEAN policy on test env
mean_weights [ 0.06000824 -0.20016134  0.32863679  0.0279338 ]
mean reward
0.06	0.06	0.06	-0.20	0.06	
0.06	0.06	0.06	0.06	0.03	
-0.20	0.06	0.33	0.03	0.06	
0.06	-0.20	-0.20	0.06	0.06	
0.06	0.06	0.03	0.06	0.06	
mean policy
>	>	v	<	v	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	3 	
1 	0 	2 	3 	0 	
0 	1 	1 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	<	>	
>	>	v	<	<	
^	>	<	<	<	
<	^	^	^	^	
^	v	>	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	<	v	
>	>	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.79	-8.79	-8.79	-9.17	-8.79	
-8.79	-8.79	-8.79	-8.79	-9.21	
-9.17	-8.79	-8.60	-9.21	-8.79	
-8.79	-9.17	-9.17	-8.79	-8.79	
-8.79	-8.79	-9.21	-8.79	-8.79	
ird policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	<	<	
^	^	^	^	^	
MAP policy loss 257.4775742211175
mean policy loss 42.92517732585757
robust policy loss 41.52238352673501
regret policy loss 30.52650493374545
ird policy loss 3.141707277665162e-08
MAP lava occupancy 2.5311975482875457
Mean lava occupancy 2.5311975482875457
Robust lava occupancy 0.39439299686932494
Regret lava occupancy 0.3213316308894304
IRD lava occupancy 3.2032393009357245e-10
##############
Trial  44
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.23697078 -0.24285202 -0.44550592 -0.07467128] loglik -12.355664814565728
accepted/total = 39/2000 = 0.0195
MAP Policy on Train MDP
map_weights [-0.23697078 -0.24285202 -0.44550592 -0.07467128]
map reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.45	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
Map policy
v	v	v	v	<	
v	v	<	v	^	
>	v	>	>	<	
>	>	>	<	^	
>	>	^	<	^	
MEAN policy on Train MDP
mean_weights [-0.22499463 -0.22298903 -0.39754031  0.03395799]
mean reward
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.40	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
mean policy
>	>	>	>	v	
>	>	>	>	>	
^	^	^	>	^	
^	>	>	^	^	
>	>	>	^	^	
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 16.70782470112501
Mean policy loss 82.2976851796519
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	>	
v	v	v	<	v	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.23697078 -0.24285202 -0.44550592 -0.07467128]
map reward
-0.24	-0.24	-0.24	-0.07	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.07	
-0.24	-0.24	-0.45	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
Map policy
>	>	>	^	<	
^	>	>	^	>	
^	^	>	^	^	
^	^	>	^	^	
>	^	>	^	^	
MEAN policy on test env
mean_weights [-0.22499463 -0.22298903 -0.39754031  0.03395799]
mean reward
-0.22	-0.22	-0.22	0.03	-0.22	
-0.22	-0.22	-0.22	-0.22	0.03	
-0.22	-0.22	-0.40	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
mean policy
>	>	>	^	v	
>	>	>	>	>	
^	^	^	^	^	
>	>	>	^	^	
^	>	>	>	^	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	>	^	<	
v	v	<	^	>	
v	<	<	^	^	
<	<	<	<	^	
^	^	<	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	v	<	^	<	
v	v	<	<	>	
v	<	<	v	<	
<	<	<	<	<	
^	^	<	<	<	
-------- IRD Solution -------
ird reward
4.24	4.24	4.24	4.01	4.24	
4.24	4.24	4.24	4.24	4.01	
4.24	4.24	4.08	4.24	4.24	
4.25	4.24	4.24	4.24	4.24	
4.24	4.24	4.24	4.24	4.24	
ird policy
v	v	v	v	>	
v	v	<	v	<	
v	v	v	v	<	
<	<	<	<	<	
^	<	<	<	<	
MAP policy loss 115.4905879883248
mean policy loss 1678.209076861266
robust policy loss 760.3267127267644
regret policy loss 424.63998996611457
ird policy loss 64.15398408767012
MAP lava occupancy 1.0033266365417528
Mean lava occupancy 1.0033266365417528
Robust lava occupancy 7.118411037524894
Regret lava occupancy 3.6054910007935743
IRD lava occupancy 6.020574467213615e-12
##############
Trial  45
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 3), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (13, 0), (4, 0)]
w_map [-0.3631025  -0.04857505  0.18086337 -0.40745908] loglik -9.704060530926142
accepted/total = 1816/2000 = 0.908
MAP Policy on Train MDP
map_weights [-0.3631025  -0.04857505  0.18086337 -0.40745908]
map reward
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	0.18	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
Map policy
v	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.18619427  0.03289249  0.19184632  0.01630415]
mean reward
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 5.540144590239797e-10
Mean policy loss 3.6744566882186604e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	0 	
MAP on testing env
map_weights [-0.3631025  -0.04857505  0.18086337 -0.40745908]
map reward
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.36	0.18	-0.36	-0.05	
-0.36	-0.36	-0.36	-0.36	-0.36	
-0.36	-0.05	-0.36	-0.05	-0.36	
Map policy
v	v	v	v	v	
>	v	v	v	v	
>	>	>	>	>	
>	v	^	v	^	
>	v	>	v	<	
MEAN policy on test env
mean_weights [-0.18619427  0.03289249  0.19184632  0.01630415]
mean reward
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	-0.19	0.19	-0.19	0.03	
-0.19	-0.19	-0.19	-0.19	-0.19	
-0.19	0.03	-0.19	0.03	-0.19	
mean policy
v	>	v	v	v	
v	>	v	>	v	
>	>	>	>	>	
>	v	^	v	^	
>	v	>	v	<	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	v	
>	v	v	v	v	
>	>	^	<	<	
^	>	^	<	^	
>	v	^	v	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	<	^	
-------- IRD Solution -------
ird reward
-8.98	-8.98	-8.98	-8.98	-8.98	
-8.98	-8.98	-8.98	-8.98	-8.98	
-8.98	-8.98	-8.81	-8.98	-9.40	
-8.98	-8.98	-8.98	-8.98	-8.98	
-8.98	-9.40	-8.98	-9.40	-8.98	
ird policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
>	>	^	^	<	
^	>	^	<	^	
MAP policy loss 1.3312643883903932
mean policy loss 87.05240519585851
robust policy loss 30.258732289233244
regret policy loss 0.14186231874575148
ird policy loss 3.5245165769248743e-07
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  46
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.81987205  0.12003045  0.00674394  0.05335356] loglik -8.317766133949249
accepted/total = 1794/2000 = 0.897
MAP Policy on Train MDP
map_weights [-0.81987205  0.12003045  0.00674394  0.05335356]
map reward
-0.82	-0.82	-0.82	-0.82	-0.82	
-0.82	-0.82	-0.82	-0.82	-0.82	
-0.82	-0.82	0.01	-0.82	-0.82	
-0.82	-0.82	-0.82	-0.82	-0.82	
-0.82	-0.82	-0.82	-0.82	-0.82	
Map policy
>	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.20466591 -0.02479002  0.15849369  0.07455031]
mean reward
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	0.16	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
mean policy
>	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 1.9747728632456968e-09
Mean policy loss 3.2586827190101385e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
<	>	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.81987205  0.12003045  0.00674394  0.05335356]
map reward
-0.82	-0.82	-0.82	-0.82	-0.82	
0.05	-0.82	-0.82	-0.82	-0.82	
-0.82	0.12	0.01	-0.82	-0.82	
-0.82	-0.82	-0.82	-0.82	-0.82	
-0.82	-0.82	-0.82	0.12	-0.82	
Map policy
v	v	v	v	v	
<	v	v	v	v	
>	>	<	v	v	
>	^	v	v	<	
>	>	>	v	<	
MEAN policy on test env
mean_weights [-0.20466591 -0.02479002  0.15849369  0.07455031]
mean reward
-0.20	-0.20	-0.20	-0.20	-0.20	
0.07	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.02	0.16	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.20	-0.20	
-0.20	-0.20	-0.20	-0.02	-0.20	
mean policy
v	v	v	<	<	
<	<	v	<	<	
^	>	<	<	<	
^	^	^	<	<	
^	^	^	^	<	
features
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	<	<	
<	<	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	<	^	
-------- IRD Solution -------
ird reward
-8.09	-8.09	-8.09	-8.09	-8.09	
-8.30	-8.09	-8.09	-8.09	-8.09	
-8.09	-8.40	-7.75	-8.09	-8.09	
-8.09	-8.09	-8.09	-8.09	-8.09	
-8.09	-8.09	-8.09	-8.40	-8.09	
ird policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
MAP policy loss 21.024839721386993
mean policy loss 919.8269647495725
robust policy loss 530.7383579810822
regret policy loss 3.39275116056767
ird policy loss 2.762932903299653e-10
MAP lava occupancy 0.11875000009573103
Mean lava occupancy 0.11875000009573103
Robust lava occupancy 5.121287214910228
Regret lava occupancy 0.03330957038671448
IRD lava occupancy 5.000997964414474e-12
##############
Trial  47
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (8, 0), (17, 2), (11, 1), (2, 3), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [0.01652152 0.01166726 0.63957327 0.33223794] loglik -7.6244502003034995
accepted/total = 1344/2000 = 0.672
MAP Policy on Train MDP
map_weights [0.01652152 0.01166726 0.63957327 0.33223794]
map reward
0.02	0.02	0.02	0.01	0.02	
0.02	0.02	0.02	0.02	0.02	
0.01	0.02	0.64	0.02	0.01	
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
Map policy
>	>	v	v	v	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.0443893  -0.17210115  0.3785949   0.05406674]
mean reward
0.04	0.04	0.04	-0.17	0.04	
0.04	0.04	0.04	0.04	0.04	
-0.17	0.04	0.38	0.04	-0.17	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 1.1597846129619838e-06
Mean policy loss 7.772824744955997e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	>	^	
^	^	v	v	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [0.01652152 0.01166726 0.63957327 0.33223794]
map reward
0.02	0.02	0.02	0.01	0.02	
0.02	0.02	0.02	0.01	0.02	
0.02	0.02	0.64	0.02	0.02	
0.02	0.02	0.33	0.33	0.02	
0.02	0.01	0.02	0.02	0.02	
Map policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	^	<	
MEAN policy on test env
mean_weights [ 0.0443893  -0.17210115  0.3785949   0.05406674]
mean reward
0.04	0.04	0.04	-0.17	0.04	
0.04	0.04	0.04	-0.17	0.04	
0.04	0.04	0.38	0.04	0.04	
0.04	0.04	0.05	0.05	0.04	
0.04	-0.17	0.04	0.04	0.04	
mean policy
v	v	v	<	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	^	<	
features
0 	0 	0 	1 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
>	>	v	v	<	
v	v	v	v	<	
>	>	v	<	<	
>	v	^	<	<	
>	v	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
>	v	v	v	v	
>	>	v	v	v	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
-8.92	-8.92	-8.92	-9.37	-8.92	
-8.92	-8.92	-8.92	-9.37	-8.92	
-8.92	-8.92	-8.83	-8.92	-8.92	
-8.92	-8.92	-9.38	-9.38	-8.92	
-8.92	-9.37	-8.92	-8.92	-8.92	
ird policy
>	v	v	>	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
^	^	v	<	^	
MAP policy loss 237.02732495827888
mean policy loss 850.4621968646622
robust policy loss 618.8431457540827
regret policy loss 3.684336675796158
ird policy loss 2.141856151202659e-06
MAP lava occupancy 2.379867723451005
Mean lava occupancy 2.379867723451005
Robust lava occupancy 5.973635560321345
Regret lava occupancy 0.03571398851172247
IRD lava occupancy 1.4889699783913185e-08
##############
Trial  48
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.24628641 -0.25241611  0.35901882 -0.14227867] loglik -7.624618671128118
accepted/total = 1453/2000 = 0.7265
MAP Policy on Train MDP
map_weights [-0.24628641 -0.25241611  0.35901882 -0.14227867]
map reward
-0.25	-0.25	-0.25	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
-0.25	-0.25	0.36	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
Map policy
>	>	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.07239658 -0.18346123  0.4097597  -0.07622978]
mean reward
0.07	0.07	0.07	0.07	0.07	
0.07	-0.18	0.07	0.07	0.07	
0.07	0.07	0.41	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	-0.18	0.07	0.07	
mean policy
>	>	v	v	v	
v	v	v	<	<	
>	>	>	<	<	
>	>	^	<	<	
^	^	^	^	^	
Optimal Policy
>	>	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	v	^	^	
MAP policy loss 1.6588915416684835e-08
Mean policy loss 2.2711880987014155e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	>	
>	v	v	<	<	
>	>	>	<	<	
>	>	^	<	^	
^	^	^	^	<	
reward
-1.00	-5.00	-5.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	1 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	2 	0 	1 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.24628641 -0.25241611  0.35901882 -0.14227867]
map reward
-0.25	-0.25	-0.25	-0.14	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.14	
-0.25	-0.25	0.36	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
-0.25	-0.25	-0.25	-0.25	-0.25	
Map policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.07239658 -0.18346123  0.4097597  -0.07622978]
mean reward
0.07	-0.18	-0.18	-0.08	0.07	
-0.18	0.07	0.07	0.07	-0.08	
0.07	0.07	0.41	0.07	-0.18	
0.07	0.07	0.07	-0.18	-0.18	
0.07	0.07	-0.18	0.07	0.07	
mean policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	^	
^	^	^	^	<	
features
0 	1 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	2 	0 	1 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	^	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
<	v	>	v	v	
>	>	v	v	<	
>	>	>	<	^	
>	>	^	^	^	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-9.06	-9.50	-9.50	-9.47	-9.06	
-9.50	-9.06	-9.06	-9.06	-9.47	
-9.06	-9.06	-8.93	-9.06	-9.50	
-9.06	-9.06	-9.06	-9.50	-9.50	
-9.06	-9.06	-9.50	-9.06	-9.06	
ird policy
v	v	v	v	v	
>	v	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
^	^	^	^	<	
MAP policy loss 20.674425256476397
mean policy loss 19.335544853739286
robust policy loss 19.33554486389549
regret policy loss 19.584272573704983
ird policy loss 19.335544856040364
MAP lava occupancy 0.2375000001551031
Mean lava occupancy 0.2375000001551031
Robust lava occupancy 0.23750000009493571
Regret lava occupancy 0.2374999999997161
IRD lava occupancy 0.23750000001772723
##############
Trial  49
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	>	^	^	^	
^	v	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (8, 3), (12, 1), (3, 3), (12, 0), (4, 0), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [-0.01102027 -0.18294449  0.75861296  0.04742227] loglik -6.761571119808423
accepted/total = 1236/2000 = 0.618
MAP Policy on Train MDP
map_weights [-0.01102027 -0.18294449  0.75861296  0.04742227]
map reward
-0.01	-0.01	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.01	-0.01	-0.01	
-0.01	-0.01	0.76	-0.01	-0.01	
-0.01	-0.18	-0.18	-0.01	-0.01	
-0.01	-0.01	-0.01	-0.01	-0.01	
Map policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.04539528 -0.23134607  0.33729913  0.06769456]
mean reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.34	0.05	0.05	
0.05	-0.23	-0.23	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	>	^	^	^	
^	v	v	^	^	
MAP policy loss 6.296433976669957e-07
Mean policy loss 9.695416691712921e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
^	>	^	<	<	
>	^	^	^	^	
reward
-1.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.01102027 -0.18294449  0.75861296  0.04742227]
map reward
-0.01	0.05	0.05	-0.01	-0.01	
-0.01	-0.18	-0.01	-0.01	-0.01	
-0.01	-0.01	0.76	-0.18	-0.01	
-0.18	-0.18	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.18	-0.01	-0.01	
Map policy
>	>	v	<	<	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.04539528 -0.23134607  0.33729913  0.06769456]
mean reward
0.05	0.07	0.07	0.05	0.05	
0.05	-0.23	0.05	0.05	0.05	
0.05	0.05	0.34	-0.23	0.05	
-0.23	-0.23	0.05	0.05	0.05	
0.05	0.05	-0.23	0.05	0.05	
mean policy
>	>	v	<	<	
v	>	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	^	^	<	
features
0 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.99
v	<	>	v	v	
v	^	v	<	<	
>	>	^	<	>	
^	>	^	<	<	
>	<	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.99
v	>	v	v	v	
v	v	v	<	<	
>	>	^	<	>	
^	^	^	<	<	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-8.48	-8.79	-8.79	-8.48	-8.48	
-8.48	-8.74	-8.48	-8.48	-8.48	
-8.48	-8.48	-8.17	-8.74	-8.48	
-8.74	-8.74	-8.48	-8.48	-8.48	
-8.48	-8.48	-8.74	-8.48	-8.48	
ird policy
v	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
^	>	^	<	<	
>	^	^	^	<	
MAP policy loss 34.059581250344976
mean policy loss 68.18624950152987
robust policy loss 6.523012461843092
regret policy loss 1.2207762231652985e-09
ird policy loss 7.2509644087614955e-09
MAP lava occupancy 0.21932873343880954
Mean lava occupancy 0.21932873343880954
Robust lava occupancy 2.1805371109407808e-09
Regret lava occupancy 1.507805585073165e-11
IRD lava occupancy 6.111985229983458e-11
