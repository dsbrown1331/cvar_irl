##############
Trial  0
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
<	>	v	<	<	
<	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (3, 3), (8, 0), (17, 2), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [ 0.3251941   0.31202322  0.87499636 -0.176838  ] loglik -5.375278387888102
accepted/total = 1697/2000 = 0.8485
MAP Policy on Train MDP
map_weights [ 0.3251941   0.31202322  0.87499636 -0.176838  ]
map reward
0.33	0.33	0.33	0.33	0.33	
0.33	0.31	0.33	0.33	0.33	
0.33	0.31	0.87	0.33	0.33	
0.33	0.33	0.33	0.31	0.33	
0.33	0.33	0.33	0.33	0.33	
Map policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	^	
>	^	^	<	^	
MEAN policy on Train MDP
mean_weights [ 0.12542306 -0.4022518   0.64934601  0.06188946]
mean reward
0.13	0.13	0.13	0.13	0.13	
0.13	-0.40	0.13	0.13	0.13	
0.13	-0.40	0.65	0.13	0.13	
0.13	0.13	0.13	-0.40	0.13	
0.13	0.13	0.13	0.13	0.13	
mean policy
>	>	v	v	v	
<	>	v	<	<	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
Optimal Policy
>	>	v	v	v	
<	>	v	<	<	
<	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
MAP policy loss 1.0036892535252029e-06
Mean policy loss -1.0775013936648037e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	^	v	v	
v	v	v	>	v	
>	>	<	<	v	
^	^	^	<	<	
>	^	^	<	^	
reward
-1.00	-100.00	-5.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	1.00	-100.00	-5.00	
-5.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	3 	1 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	2 	3 	1 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.3251941   0.31202322  0.87499636 -0.176838  ]
map reward
0.33	-0.18	0.31	0.31	0.33	
0.33	0.31	-0.18	0.33	0.33	
0.33	0.33	0.87	-0.18	0.31	
0.31	0.33	0.31	0.33	0.33	
0.33	0.33	0.33	0.31	0.33	
Map policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	^	<	^	
MEAN policy on test env
mean_weights [ 0.12542306 -0.4022518   0.64934601  0.06188946]
mean reward
0.13	0.06	-0.40	-0.40	0.13	
0.13	-0.40	0.06	0.13	0.13	
0.13	0.13	0.65	0.06	-0.40	
-0.40	0.13	-0.40	0.13	0.13	
0.13	0.13	0.13	-0.40	0.13	
mean policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
>	^	^	^	^	
features
0 	3 	1 	1 	0 	
0 	1 	3 	0 	0 	
0 	0 	2 	3 	1 	
1 	0 	1 	0 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	^	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	^	<	
>	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	<	<	>	v	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	<	<	^	
-------- IRD Solution -------
ird reward
-13.08	-13.89	-13.80	-13.80	-13.08	
-13.08	-13.80	-13.89	-13.08	-13.08	
-13.08	-13.08	-13.00	-13.89	-13.80	
-13.80	-13.08	-13.80	-13.08	-13.08	
-13.08	-13.08	-13.08	-13.80	-13.08	
ird policy
v	<	<	<	v	
v	v	<	>	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	<	<	^	
MAP policy loss 615.0199424701967
mean policy loss 39.499153410275966
robust policy loss 101.72285591561942
regret policy loss 19.136497157692418
ird policy loss 2.0934223180214895
MAP lava occupancy 6.092864033855073
Mean lava occupancy 6.092864033855073
Robust lava occupancy 1.057209747555049
Regret lava occupancy 0.2143437500030903
IRD lava occupancy 7.529980801437558e-09
##############
Trial  1
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (12, 3), (17, 2), (11, 1), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [0.00612609 0.14386335 0.96475359 0.22026418] loglik -9.01091334727846
accepted/total = 1886/2000 = 0.943
MAP Policy on Train MDP
map_weights [0.00612609 0.14386335 0.96475359 0.22026418]
map reward
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.96	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
Map policy
v	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.22632045  0.08474904  0.35370346  0.0589719 ]
mean reward
-0.23	-0.23	-0.23	-0.23	-0.23	
-0.23	-0.23	-0.23	-0.23	-0.23	
-0.23	-0.23	0.35	-0.23	-0.23	
-0.23	-0.23	-0.23	-0.23	-0.23	
-0.23	-0.23	-0.23	-0.23	-0.23	
mean policy
>	v	v	v	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.442633360257804e-10
Mean policy loss 5.521841165464624e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	>	v	v	
>	^	v	v	v	
v	v	>	<	<	
>	>	^	<	<	
^	<	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-100.00	-100.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-100.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	2 	0 	0 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	0 	
MAP on testing env
map_weights [0.00612609 0.14386335 0.96475359 0.22026418]
map reward
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.22	0.01	0.01	
0.22	0.22	0.96	0.01	0.01	
0.14	0.01	0.01	0.14	0.14	
0.01	0.22	0.22	0.01	0.01	
Map policy
v	v	v	<	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.22632045  0.08474904  0.35370346  0.0589719 ]
mean reward
-0.23	-0.23	-0.23	-0.23	-0.23	
-0.23	-0.23	0.06	-0.23	-0.23	
0.06	0.06	0.35	-0.23	-0.23	
0.08	-0.23	-0.23	0.08	0.08	
-0.23	0.06	0.06	-0.23	-0.23	
mean policy
v	v	v	<	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
3 	3 	2 	0 	0 	
1 	0 	0 	1 	1 	
0 	3 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	v	v	<	<	
>	>	^	<	<	
<	^	^	<	<	
^	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	>	v	v	
>	>	v	v	v	
>	>	v	<	<	
>	>	^	^	^	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-12.84	-12.84	-12.84	-12.84	-12.84	
-12.84	-12.84	-13.47	-12.84	-12.84	
-13.47	-13.47	-12.66	-12.84	-12.84	
-13.28	-12.84	-12.84	-13.28	-13.28	
-12.84	-13.47	-13.47	-12.84	-12.84	
ird policy
>	>	>	v	v	
>	^	v	v	v	
>	>	v	<	<	
>	>	^	^	<	
^	^	^	^	<	
MAP policy loss 465.1431466579818
mean policy loss 894.5618530210296
robust policy loss 668.7888408927838
regret policy loss 20.00899711438533
ird policy loss 1.3055089231883699e-09
MAP lava occupancy 4.706384639267385
Mean lava occupancy 4.706384639267385
Robust lava occupancy 6.66260546571329
Regret lava occupancy 0.210621022229481
IRD lava occupancy 1.3003574464526476e-11
##############
Trial  2
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
<	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
demonstration
[(7, 3), (12, 2), (9, 0), (12, 1), (8, 3), (6, 1), (17, 2), (13, 0), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [-7.11730708e-04 -8.34451397e-01 -4.59781231e-01  3.03795291e-01] loglik -16.287436888590495
accepted/total = 18/2000 = 0.009
MAP Policy on Train MDP
map_weights [-7.11730708e-04 -8.34451397e-01 -4.59781231e-01  3.03795291e-01]
map reward
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.83	-0.46	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.83	
-0.00	-0.00	-0.00	-0.83	-0.00	
Map policy
>	>	v	v	v	
>	>	>	v	v	
<	<	^	v	<	
>	>	>	^	v	
>	>	^	>	>	
MEAN policy on Train MDP
mean_weights [-9.49856042e-05 -8.14922133e-01 -4.18559708e-01  3.81425930e-01]
mean reward
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.81	-0.42	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.81	
-0.00	-0.00	-0.00	-0.81	-0.00	
mean policy
>	>	v	v	v	
>	>	>	v	v	
<	<	^	v	<	
>	>	>	^	v	
>	>	^	>	>	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
<	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
MAP policy loss 15.757820534975732
Mean policy loss 15.757820535388039
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	v	
v	v	v	<	v	
>	>	<	<	<	
^	<	^	^	<	
>	>	^	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-100.00	-1.00	-1.00	-100.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
3 	0 	0 	3 	1 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-7.11730708e-04 -8.34451397e-01 -4.59781231e-01  3.03795291e-01]
map reward
-0.00	-0.83	-0.00	-0.83	-0.00	
0.30	-0.00	-0.00	0.30	-0.83	
-0.00	-0.00	-0.46	-0.00	-0.00	
-0.00	-0.83	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
Map policy
v	v	v	v	v	
<	<	<	<	<	
^	^	<	^	<	
^	^	^	^	^	
^	v	^	^	^	
MEAN policy on test env
mean_weights [-9.49856042e-05 -8.14922133e-01 -4.18559708e-01  3.81425930e-01]
mean reward
-0.00	-0.81	-0.00	-0.81	-0.00	
0.38	-0.00	-0.00	0.38	-0.81	
-0.00	-0.00	-0.42	-0.00	-0.00	
-0.00	-0.81	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
mean policy
v	v	v	v	v	
<	<	<	<	<	
^	^	<	^	<	
^	^	^	^	^	
^	v	^	^	^	
features
0 	1 	0 	1 	0 	
3 	0 	0 	3 	1 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	^	^	v	v	
<	<	<	<	<	
^	^	<	^	<	
^	^	v	^	^	
^	<	<	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	^	^	v	v	
<	<	<	<	<	
^	>	>	^	<	
^	<	^	^	^	
^	>	^	^	^	
-------- IRD Solution -------
ird reward
2.01	1.21	2.01	1.21	2.01	
2.56	2.01	2.01	2.56	1.21	
2.01	2.01	1.77	2.01	2.01	
2.01	1.21	2.01	2.01	2.01	
2.01	2.01	2.01	2.01	2.01	
ird policy
v	v	v	v	v	
<	<	<	<	<	
^	^	<	^	<	
^	^	^	^	^	
^	^	v	^	^	
MAP policy loss 191.7902408779893
mean policy loss 1681.6028703299928
robust policy loss 1681.6026323788035
regret policy loss 1681.6028748908311
ird policy loss 1681.6028758288478
MAP lava occupancy 1.660286730896291
Mean lava occupancy 1.660286730896291
Robust lava occupancy 16.826715271334184
Regret lava occupancy 16.826717720958825
IRD lava occupancy 16.826717730465074
##############
Trial  3
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	^	v	<	<	
>	v	v	v	v	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [-0.23423399 -0.24162     0.89968605 -0.27806336] loglik -6.9312787008417445
accepted/total = 1350/2000 = 0.675
MAP Policy on Train MDP
map_weights [-0.23423399 -0.24162     0.89968605 -0.27806336]
map reward
-0.23	-0.24	-0.23	-0.23	-0.23	
-0.23	-0.23	-0.23	-0.24	-0.23	
-0.23	-0.23	0.90	-0.23	-0.23	
-0.23	-0.23	-0.23	-0.23	-0.23	
-0.23	-0.23	-0.23	-0.23	-0.23	
Map policy
v	>	v	<	<	
>	>	v	v	v	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.14718143 -0.26313996  0.54444215 -0.05729696]
mean reward
0.15	-0.26	0.15	0.15	0.15	
0.15	0.15	0.15	-0.26	0.15	
0.15	0.15	0.54	0.15	0.15	
0.15	0.15	0.15	0.15	0.15	
0.15	0.15	0.15	0.15	0.15	
mean policy
v	>	v	<	<	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
Optimal Policy
v	^	v	<	<	
>	v	v	v	v	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 9.264695919920796e-07
Mean policy loss 3.109198699682847e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	^	v	>	v	
v	>	v	<	<	
>	>	v	<	>	
>	>	^	<	^	
^	^	^	<	>	
reward
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	1 	3 	
0 	0 	1 	3 	0 	
MAP on testing env
map_weights [-0.23423399 -0.24162     0.89968605 -0.27806336]
map reward
-0.23	-0.23	-0.28	-0.28	-0.23	
-0.23	-0.24	-0.23	-0.23	-0.23	
-0.23	-0.23	0.90	-0.24	-0.23	
-0.23	-0.23	-0.23	-0.24	-0.28	
-0.23	-0.23	-0.24	-0.28	-0.23	
Map policy
v	v	v	v	v	
v	>	v	<	<	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	<	^	
MEAN policy on test env
mean_weights [ 0.14718143 -0.26313996  0.54444215 -0.05729696]
mean reward
0.15	0.15	-0.06	-0.06	0.15	
0.15	-0.26	0.15	0.15	0.15	
0.15	0.15	0.54	-0.26	0.15	
0.15	0.15	0.15	-0.26	-0.06	
0.15	0.15	-0.26	-0.06	0.15	
mean policy
v	>	v	v	v	
v	v	v	<	<	
>	>	v	<	^	
>	>	^	<	^	
^	^	^	<	^	
features
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	1 	3 	
0 	0 	1 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	^	<	>	v	
v	^	v	<	<	
>	>	v	<	>	
>	^	^	<	v	
^	^	<	>	>	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	>	v	<	<	
>	>	v	<	^	
>	>	^	<	^	
^	^	^	<	^	
-------- IRD Solution -------
ird reward
-13.10	-13.10	-13.73	-13.73	-13.10	
-13.10	-13.86	-13.10	-13.10	-13.10	
-13.10	-13.10	-13.03	-13.86	-13.10	
-13.10	-13.10	-13.10	-13.86	-13.73	
-13.10	-13.10	-13.86	-13.73	-13.10	
ird policy
v	^	v	v	v	
v	>	v	<	<	
>	>	<	<	>	
>	>	^	<	<	
^	^	^	<	>	
MAP policy loss 53.809477381495824
mean policy loss 19.742798268491686
robust policy loss 0.8104232951501646
regret policy loss 19.83890633060453
ird policy loss 5.304319414256575e-10
MAP lava occupancy 0.48128570787793057
Mean lava occupancy 0.48128570787793057
Robust lava occupancy 1.8520413897970336e-11
Regret lava occupancy 0.2375000000004517
IRD lava occupancy 6.244892255911036e-12
##############
Trial  4
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	^	v	<	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (3, 0), (4, 0), (12, 3), (17, 2), (11, 1), (2, 3), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [-0.07665077 -0.0817279   0.49420693  0.86209322] loglik -7.624618432814543
accepted/total = 1594/2000 = 0.797
MAP Policy on Train MDP
map_weights [-0.07665077 -0.0817279   0.49420693  0.86209322]
map reward
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.08	0.49	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
Map policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
^	^	^	<	^	
MEAN policy on Train MDP
mean_weights [ 0.15509865 -0.41843071  0.55007998 -0.35408492]
mean reward
0.16	-0.42	0.16	0.16	0.16	
0.16	0.16	0.16	0.16	0.16	
0.16	0.16	0.55	0.16	0.16	
0.16	0.16	0.16	-0.42	0.16	
0.16	0.16	0.16	0.16	0.16	
mean policy
v	v	v	<	<	
>	>	v	<	<	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
Optimal Policy
v	^	v	<	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	<	
MAP policy loss 7.578030465325847e-07
Mean policy loss 1.3858691971790904e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	>	v	^	^	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.07665077 -0.0817279   0.49420693  0.86209322]
map reward
-0.08	-0.08	-0.08	0.86	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	-0.08	0.49	-0.08	-0.08	
-0.08	-0.08	-0.08	-0.08	-0.08	
-0.08	0.86	-0.08	-0.08	-0.08	
Map policy
>	>	>	^	<	
<	>	v	^	<	
>	>	v	<	<	
>	v	^	<	>	
>	v	<	<	<	
MEAN policy on test env
mean_weights [ 0.15509865 -0.41843071  0.55007998 -0.35408492]
mean reward
0.16	0.16	0.16	-0.35	0.16	
0.16	0.16	-0.42	0.16	-0.42	
-0.42	0.16	0.55	0.16	0.16	
0.16	-0.42	-0.42	0.16	0.16	
0.16	-0.35	0.16	0.16	0.16	
mean policy
>	v	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	^	
^	>	^	^	^	
features
0 	0 	0 	3 	0 	
0 	0 	1 	0 	1 	
1 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	<	v	<	
>	v	v	v	v	
>	>	<	<	<	
^	^	^	^	^	
^	>	>	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	<	v	<	
>	v	>	v	<	
>	>	<	<	<	
^	^	>	^	^	
^	>	>	^	^	
-------- IRD Solution -------
ird reward
-13.03	-13.03	-13.03	-13.75	-13.03	
-13.03	-13.03	-13.67	-13.03	-13.67	
-13.67	-13.03	-12.87	-13.03	-13.03	
-13.03	-13.67	-13.67	-13.03	-13.03	
-13.03	-13.75	-13.03	-13.03	-13.03	
ird policy
>	v	<	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	^	^	
^	>	v	^	^	
MAP policy loss 41.2173443326408
mean policy loss 22.562499992315114
robust policy loss 22.56249999624342
regret policy loss 21.552035791002925
ird policy loss 5.295313925991751e-07
MAP lava occupancy 0.2429584869155678
Mean lava occupancy 0.2429584869155678
Robust lava occupancy 0.23750000000735916
Regret lava occupancy 0.22639098382411765
IRD lava occupancy 4.4104347891977656e-09
##############
Trial  5
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 3), (8, 3), (12, 0), (6, 1), (17, 2), (11, 1), (13, 0), (12, 3), (4, 0)]
w_map [-0.60914732 -0.01143361  0.68088955 -0.40644587] loglik -9.009881796445267
accepted/total = 1367/2000 = 0.6835
MAP Policy on Train MDP
map_weights [-0.60914732 -0.01143361  0.68088955 -0.40644587]
map reward
-0.61	-0.61	-0.61	-0.61	-0.61	
-0.61	-0.61	-0.61	-0.61	-0.01	
-0.61	-0.61	0.68	-0.61	-0.61	
-0.61	-0.61	-0.61	-0.61	-0.61	
-0.61	-0.61	-0.61	-0.61	-0.61	
Map policy
>	>	v	v	v	
>	>	v	v	>	
>	>	^	<	<	
>	^	^	<	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.07166724 -0.22727829  0.67856032 -0.18951044]
mean reward
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	-0.23	
0.07	0.07	0.68	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
mean policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	^	^	
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 23.17695511720697
Mean policy loss 6.97416707717835e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	v	
^	^	^	v	<	
^	^	<	<	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.60914732 -0.01143361  0.68088955 -0.40644587]
map reward
-0.61	-0.01	-0.61	-0.61	-0.61	
-0.61	-0.61	-0.61	-0.61	-0.61	
-0.61	-0.61	0.68	-0.01	-0.01	
-0.61	-0.61	-0.01	-0.61	-0.61	
-0.61	-0.61	-0.61	-0.61	-0.61	
Map policy
>	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.07166724 -0.22727829  0.67856032 -0.18951044]
mean reward
0.07	-0.23	0.07	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
0.07	0.07	0.68	-0.23	-0.23	
0.07	0.07	-0.23	0.07	0.07	
0.07	0.07	0.07	0.07	0.07	
mean policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	<	<	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	<	<	
^	^	<	<	<	
-------- IRD Solution -------
ird reward
-13.13	-13.84	-13.13	-13.13	-13.13	
-13.13	-13.13	-13.13	-13.13	-13.13	
-13.13	-13.13	-13.06	-13.84	-13.84	
-13.13	-13.13	-13.84	-13.13	-13.13	
-13.13	-13.13	-13.13	-13.13	-13.13	
ird policy
v	^	v	<	<	
>	v	v	<	<	
>	>	^	>	>	
^	^	^	v	<	
^	^	<	<	<	
MAP policy loss 17.67086331659564
mean policy loss 0.45012188546273507
robust policy loss 7.25231488073376
regret policy loss 0.21601260027302127
ird policy loss 1.1102317253736116e-07
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  6
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (1, 1), (13, 0)]
w_map [-0.72237378 -0.1170971   0.15125349 -0.66451996] loglik -8.317766127634513
accepted/total = 1862/2000 = 0.931
MAP Policy on Train MDP
map_weights [-0.72237378 -0.1170971   0.15125349 -0.66451996]
map reward
-0.72	-0.72	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	-0.72	-0.72	
-0.72	-0.72	0.15	-0.72	-0.72	
-0.72	-0.72	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	-0.72	-0.72	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.27109561  0.22203941  0.30294732 -0.24606381]
mean reward
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	0.30	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
mean policy
v	v	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 2.3908170613076186e-09
Mean policy loss 5.530537664389257e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-5.00	-5.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	0 	0 	
1 	1 	1 	0 	1 	
0 	0 	2 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.72237378 -0.1170971   0.15125349 -0.66451996]
map reward
-0.72	-0.12	-0.66	-0.72	-0.72	
-0.12	-0.12	-0.12	-0.72	-0.12	
-0.72	-0.72	0.15	-0.72	-0.72	
-0.12	-0.12	-0.72	-0.72	-0.72	
-0.72	-0.72	-0.72	-0.72	-0.72	
Map policy
v	v	v	<	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.27109561  0.22203941  0.30294732 -0.24606381]
mean reward
-0.27	0.22	-0.25	-0.27	-0.27	
0.22	0.22	0.22	-0.27	0.22	
-0.27	-0.27	0.30	-0.27	-0.27	
0.22	0.22	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
mean policy
>	v	v	<	v	
>	>	v	<	<	
^	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
features
0 	1 	3 	0 	0 	
1 	1 	1 	0 	1 	
0 	0 	2 	0 	0 	
1 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	^	<	v	
>	>	v	<	>	
^	>	^	<	<	
>	^	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	^	v	<	
v	v	v	v	v	
>	>	<	<	<	
^	>	^	^	^	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-12.37	-12.71	-12.87	-12.37	-12.37	
-12.71	-12.71	-12.71	-12.37	-12.71	
-12.37	-12.37	-12.21	-12.37	-12.37	
-12.71	-12.71	-12.37	-12.37	-12.37	
-12.37	-12.37	-12.37	-12.37	-12.37	
ird policy
v	<	^	v	<	
v	>	v	v	<	
>	>	v	<	<	
^	>	^	^	^	
>	>	^	^	^	
MAP policy loss 20.23344597613447
mean policy loss 37.16460913081616
robust policy loss 488.6879921617591
regret policy loss -6.186437820354662e-11
ird policy loss 9.400618269927996e-10
MAP lava occupancy 0.09543798075344119
Mean lava occupancy 0.09543798075344119
Robust lava occupancy 4.606221219333892
Regret lava occupancy 1.8939158933462635e-12
IRD lava occupancy 7.533251695272514e-12
##############
Trial  7
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (10, 1), (4, 3), (0, 3), (13, 0), (5, 3)]
w_map [-0.73949236 -0.24664145  0.20770788  0.59091156] loglik -8.317766152243621
accepted/total = 1893/2000 = 0.9465
MAP Policy on Train MDP
map_weights [-0.73949236 -0.24664145  0.20770788  0.59091156]
map reward
-0.74	-0.74	-0.74	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.74	-0.74	
-0.74	-0.74	0.21	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.74	-0.74	
Map policy
v	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.42303563 -0.00349407  0.13513271 -0.20789819]
mean reward
-0.42	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.42	0.14	-0.42	-0.42	
-0.42	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.42	-0.42	-0.42	-0.42	
mean policy
v	v	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 8.940825567697788e-10
Mean policy loss 5.54336981428271e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	<	<	
>	>	v	v	v	
>	>	v	<	<	
^	^	^	^	^	
<	^	v	<	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	0 	
MAP on testing env
map_weights [-0.73949236 -0.24664145  0.20770788  0.59091156]
map reward
-0.74	-0.25	-0.74	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.25	-0.74	
-0.74	-0.74	0.21	-0.74	-0.74	
0.59	-0.74	-0.74	-0.25	-0.74	
-0.74	0.59	-0.74	0.59	-0.74	
Map policy
v	v	v	v	v	
v	v	v	v	<	
v	v	v	v	<	
<	<	>	v	v	
^	v	>	v	<	
MEAN policy on test env
mean_weights [-0.42303563 -0.00349407  0.13513271 -0.20789819]
mean reward
-0.42	-0.00	-0.42	-0.42	-0.42	
-0.42	-0.42	-0.42	-0.00	-0.42	
-0.42	-0.42	0.14	-0.42	-0.42	
-0.21	-0.42	-0.42	-0.00	-0.42	
-0.42	-0.21	-0.42	-0.21	-0.42	
mean policy
>	^	<	<	<	
^	^	^	^	<	
^	^	^	<	^	
^	^	^	<	<	
^	^	>	^	<	
features
0 	1 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
3 	0 	0 	1 	0 	
0 	3 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	v	v	v	
v	v	v	<	<	
>	>	<	<	<	
<	>	^	<	<	
^	v	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	>	v	v	v	
>	>	v	<	<	
>	>	^	^	^	
>	^	^	v	^	
-------- IRD Solution -------
ird reward
-12.09	-12.43	-12.09	-12.09	-12.09	
-12.09	-12.09	-12.09	-12.43	-12.09	
-12.09	-12.09	-11.90	-12.09	-12.09	
-12.51	-12.09	-12.09	-12.43	-12.09	
-12.09	-12.51	-12.09	-12.51	-12.09	
ird policy
v	>	v	<	<	
>	>	v	v	v	
>	>	<	<	<	
>	^	^	^	^	
>	^	^	^	^	
MAP policy loss 32.60834699773697
mean policy loss 125.64097693175401
robust policy loss 491.48567702625587
regret policy loss 19.33554487232247
ird policy loss 19.335544872259284
MAP lava occupancy 0.3562500001290789
Mean lava occupancy 0.3562500001290789
Robust lava occupancy 4.740102721702936
Regret lava occupancy 0.23750000000632393
IRD lava occupancy 0.23750000000475854
##############
Trial  8
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [-0.87038647 -0.08156313  0.45763747  0.16230462] loglik -9.704060515753213
accepted/total = 1893/2000 = 0.9465
MAP Policy on Train MDP
map_weights [-0.87038647 -0.08156313  0.45763747  0.16230462]
map reward
-0.87	-0.87	-0.87	-0.87	-0.87	
-0.87	-0.87	-0.87	-0.87	-0.87	
-0.87	-0.87	0.46	-0.87	-0.87	
-0.87	-0.87	-0.87	-0.87	-0.87	
-0.87	-0.87	-0.87	-0.87	-0.87	
Map policy
v	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.41275107  0.15153522  0.22907001 -0.0145494 ]
mean reward
-0.41	-0.41	-0.41	-0.41	-0.41	
-0.41	-0.41	-0.41	-0.41	-0.41	
-0.41	-0.41	0.23	-0.41	-0.41	
-0.41	-0.41	-0.41	-0.41	-0.41	
-0.41	-0.41	-0.41	-0.41	-0.41	
mean policy
>	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.1131032434138515e-10
Mean policy loss 5.305476720386844e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	^	<	v	
v	<	^	>	<	
<	<	>	<	v	
^	v	v	>	<	
>	>	>	^	^	
reward
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	1.00	-5.00	-100.00	
-5.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	1 	3 	
1 	1 	3 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.87038647 -0.08156313  0.45763747  0.16230462]
map reward
-0.87	-0.08	-0.87	-0.08	-0.87	
-0.87	-0.87	0.16	-0.87	-0.87	
-0.87	0.16	0.46	-0.08	0.16	
-0.08	-0.08	0.16	-0.87	-0.87	
-0.87	-0.87	-0.87	-0.87	-0.87	
Map policy
>	v	v	v	<	
v	v	v	<	v	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.41275107  0.15153522  0.22907001 -0.0145494 ]
mean reward
-0.41	0.15	-0.41	0.15	-0.41	
-0.41	-0.41	-0.01	-0.41	-0.41	
-0.41	-0.01	0.23	0.15	-0.01	
0.15	0.15	-0.01	-0.41	-0.41	
-0.41	-0.41	-0.41	-0.41	-0.41	
mean policy
>	^	v	v	<	
v	>	v	v	v	
>	>	>	<	<	
>	^	^	^	^	
^	^	^	^	^	
features
0 	1 	0 	1 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	1 	3 	
1 	1 	3 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
v	v	v	<	v	
>	>	>	<	<	
>	>	^	<	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	^	<	v	
>	<	v	v	<	
<	>	>	<	v	
^	^	^	^	<	
>	>	>	^	^	
-------- IRD Solution -------
ird reward
-11.06	-11.23	-11.06	-11.23	-11.06	
-11.06	-11.06	-11.60	-11.06	-11.06	
-11.06	-11.60	-10.72	-11.23	-11.60	
-11.23	-11.23	-11.60	-11.06	-11.06	
-11.06	-11.06	-11.06	-11.06	-11.06	
ird policy
v	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	>	^	^	
MAP policy loss 656.262626876267
mean policy loss 78.35193521841431
robust policy loss 341.4108781475754
regret policy loss 298.84013407473475
ird policy loss 38.37907289973618
MAP lava occupancy 6.689143036058339
Mean lava occupancy 6.689143036058339
Robust lava occupancy 3.350414102033186
Regret lava occupancy 3.004907892611888
IRD lava occupancy 0.21434375069946046
##############
Trial  9
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
v	>	v	<	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (8, 0), (17, 2), (11, 1), (2, 3), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.01760774 -0.01803436 -0.7724191  -0.63461285] loglik -13.841472565505775
accepted/total = 19/2000 = 0.0095
MAP Policy on Train MDP
map_weights [-0.01760774 -0.01803436 -0.7724191  -0.63461285]
map reward
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.77	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
Map policy
>	>	v	v	<	
v	<	>	v	<	
<	<	<	^	v	
^	<	>	^	<	
>	>	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.00631611 -0.13056526 -0.6521414  -0.73205224]
mean reward
-0.01	-0.01	-0.01	-0.01	-0.01	
-0.01	-0.13	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.65	-0.01	-0.13	
-0.01	-0.13	-0.01	-0.01	-0.01	
-0.01	-0.01	-0.01	-0.01	-0.01	
mean policy
>	>	v	v	<	
v	>	>	v	<	
>	<	^	^	<	
^	>	>	^	<	
>	>	^	^	<	
Optimal Policy
>	>	v	v	<	
v	>	v	<	<	
>	>	>	<	<	
^	>	^	<	<	
>	>	^	^	<	
MAP policy loss 16.708010543842654
Mean policy loss 16.70782054899166
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	^	>	v	
v	v	v	v	v	
>	>	<	<	<	
>	^	v	^	v	
^	>	v	^	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-100.00	-5.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.01760774 -0.01803436 -0.7724191  -0.63461285]
map reward
-0.02	-0.02	-0.02	-0.63	-0.02	
-0.02	-0.63	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.77	-0.02	-0.02	
-0.02	-0.02	-0.63	-0.02	-0.63	
-0.02	-0.63	-0.02	-0.02	-0.02	
Map policy
>	>	^	>	v	
v	<	^	<	v	
>	v	v	v	<	
^	^	v	v	v	
^	<	v	^	<	
MEAN policy on test env
mean_weights [-0.00631611 -0.13056526 -0.6521414  -0.73205224]
mean reward
-0.01	-0.01	-0.01	-0.73	-0.01	
-0.01	-0.73	-0.13	-0.13	-0.01	
-0.01	-0.01	-0.65	-0.01	-0.01	
-0.01	-0.01	-0.73	-0.01	-0.73	
-0.01	-0.73	-0.01	-0.01	-0.01	
mean policy
>	>	^	^	v	
v	v	^	>	v	
>	<	v	v	<	
^	^	^	^	>	
^	v	v	^	<	
features
0 	0 	0 	3 	0 	
0 	3 	1 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	^	v	v	
v	>	^	v	v	
>	<	^	v	<	
>	^	<	^	^	
^	^	v	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	^	<	v	
v	v	^	>	v	
>	<	<	v	<	
>	^	<	^	v	
^	<	v	^	<	
-------- IRD Solution -------
ird reward
4.40	4.40	4.40	3.58	4.40	
4.40	3.58	4.19	4.19	4.40	
4.40	4.40	3.87	4.40	4.40	
4.40	4.40	3.58	4.40	3.58	
4.40	3.58	4.40	4.40	4.40	
ird policy
>	>	^	^	v	
v	v	^	>	v	
>	<	>	v	<	
^	^	^	^	>	
^	v	v	^	<	
MAP policy loss 359.79064284248096
mean policy loss 16.707820518293858
robust policy loss 16.707820521108566
regret policy loss 16.70782051351619
ird policy loss 16.707820513200943
MAP lava occupancy 3.397565499582194
Mean lava occupancy 3.397565499582194
Robust lava occupancy 4.3659110712451454e-12
Regret lava occupancy 8.77135467132753e-12
IRD lava occupancy 4.3705112893692804e-12
##############
Trial  10
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	>	v	<	
v	v	v	v	>	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (8, 3), (1, 3), (12, 1), (3, 3), (12, 0), (6, 3), (17, 2), (11, 1), (13, 0), (12, 3), (4, 0)]
w_map [-0.43770027 -0.6164345   0.61835531  0.21462454] loglik -3.9889840467589552
accepted/total = 1705/2000 = 0.8525
MAP Policy on Train MDP
map_weights [-0.43770027 -0.6164345   0.61835531  0.21462454]
map reward
-0.44	-0.44	-0.44	-0.44	-0.44	
-0.44	-0.44	-0.62	-0.44	-0.62	
-0.44	-0.44	0.62	-0.44	-0.44	
-0.44	-0.44	-0.44	-0.44	-0.44	
-0.44	-0.44	-0.44	-0.44	-0.44	
Map policy
v	v	v	v	<	
v	v	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.17258036 -0.3531595   0.65689275  0.05010233]
mean reward
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	-0.35	0.17	-0.35	
0.17	0.17	0.66	0.17	0.17	
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	0.17	0.17	0.17	
mean policy
v	v	v	v	<	
v	v	v	v	<	
>	>	v	<	<	
>	>	^	^	^	
>	^	^	^	^	
Optimal Policy
v	v	>	v	<	
v	v	v	v	>	
>	>	v	<	<	
>	^	^	^	^	
>	^	^	^	^	
MAP policy loss -9.118655570211631e-11
Mean policy loss 1.5460454791393907e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	<	<	
v	<	v	^	<	
<	^	v	<	^	
^	^	>	>	^	
<	v	>	>	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-100.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-100.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	3 	0 	
3 	0 	3 	0 	0 	
0 	1 	3 	1 	0 	
MAP on testing env
map_weights [-0.43770027 -0.6164345   0.61835531  0.21462454]
map reward
-0.44	-0.44	-0.44	-0.44	-0.44	
-0.44	-0.44	0.21	-0.44	-0.44	
-0.44	0.21	0.62	0.21	-0.44	
0.21	-0.44	0.21	-0.44	-0.44	
-0.44	-0.62	0.21	-0.62	-0.44	
Map policy
>	v	v	v	v	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	<	<	
^	>	^	<	<	
MEAN policy on test env
mean_weights [ 0.17258036 -0.3531595   0.65689275  0.05010233]
mean reward
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	0.05	0.17	0.17	
0.17	0.05	0.66	0.05	0.17	
0.05	0.17	0.05	0.17	0.17	
0.17	-0.35	0.05	-0.35	0.17	
mean policy
>	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
0 	3 	2 	3 	0 	
3 	0 	3 	0 	0 	
0 	1 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
v	>	v	<	<	
<	>	v	<	<	
>	^	^	^	<	
<	^	^	>	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	<	<	<	
v	<	v	^	<	
<	>	v	<	^	
>	v	^	>	<	
<	^	^	^	^	
-------- IRD Solution -------
ird reward
-13.11	-13.11	-13.11	-13.11	-13.11	
-13.11	-13.11	-13.86	-13.11	-13.11	
-13.11	-13.86	-13.04	-13.86	-13.11	
-13.86	-13.11	-13.86	-13.11	-13.11	
-13.11	-13.79	-13.86	-13.79	-13.11	
ird policy
v	v	<	<	<	
v	<	v	^	<	
<	v	v	<	^	
v	v	<	>	^	
<	<	<	<	^	
MAP policy loss 875.5948128852831
mean policy loss 877.3700670827972
robust policy loss 541.0999429453659
regret policy loss 405.6558106685648
ird policy loss 1.2403192988855172e-06
MAP lava occupancy 9.003697655516898
Mean lava occupancy 9.003697655516898
Robust lava occupancy 5.572604973687624
Regret lava occupancy 4.177711738697352
IRD lava occupancy 1.2130223734937706e-08
##############
Trial  11
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	^	v	
>	>	v	<	<	
>	>	<	<	^	
>	>	^	<	<	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 0), (8, 0), (17, 2), (11, 1), (2, 3), (4, 3), (1, 1), (12, 3)]
w_map [-0.43348498 -0.58701008  0.63721014  0.24793785] loglik -4.682131227159207
accepted/total = 1611/2000 = 0.8055
MAP Policy on Train MDP
map_weights [-0.43348498 -0.58701008  0.63721014  0.24793785]
map reward
-0.43	-0.43	-0.43	-0.59	-0.43	
-0.43	-0.43	-0.43	-0.43	-0.43	
-0.43	-0.43	0.64	-0.59	-0.43	
-0.43	-0.43	-0.43	-0.43	-0.43	
-0.43	-0.43	-0.43	-0.43	-0.43	
Map policy
>	v	v	v	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
MEAN policy on Train MDP
mean_weights [ 0.17059588 -0.316781    0.63014533  0.12418221]
mean reward
0.17	0.17	0.17	-0.32	0.17	
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	0.63	-0.32	0.17	
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	0.17	0.17	0.17	
mean policy
>	>	v	v	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
Optimal Policy
>	>	v	^	v	
>	>	v	<	<	
>	>	<	<	^	
>	>	^	<	<	
^	^	^	<	<	
MAP policy loss -9.147902227391036e-11
Mean policy loss 1.849068224923922e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	>	
>	v	v	v	>	
>	>	^	<	<	
>	>	^	<	>	
^	<	^	<	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	1.00	-5.00	-100.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	2 	1 	3 	
0 	0 	0 	1 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.43348498 -0.58701008  0.63721014  0.24793785]
map reward
-0.43	-0.43	-0.43	0.25	-0.43	
-0.43	-0.43	-0.43	0.25	-0.43	
0.25	-0.43	0.64	-0.59	0.25	
-0.43	-0.43	-0.43	-0.59	-0.43	
-0.43	0.25	-0.43	-0.43	-0.43	
Map policy
v	v	>	v	<	
v	v	>	^	<	
<	<	<	<	>	
^	v	^	^	^	
>	v	^	^	^	
MEAN policy on test env
mean_weights [ 0.17059588 -0.316781    0.63014533  0.12418221]
mean reward
0.17	0.17	0.17	0.12	0.17	
0.17	0.17	0.17	0.12	0.17	
0.12	0.17	0.63	-0.32	0.12	
0.17	0.17	0.17	-0.32	0.17	
0.17	0.12	0.17	0.17	0.17	
mean policy
>	>	v	<	v	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	<	^	
^	^	^	<	<	
features
0 	0 	0 	3 	0 	
0 	0 	0 	3 	0 	
3 	0 	2 	1 	3 	
0 	0 	0 	1 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
v	v	v	<	<	
>	>	^	<	>	
^	^	^	<	^	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	>	v	<	<	
>	>	<	<	<	
>	>	^	<	>	
^	^	^	<	<	
-------- IRD Solution -------
ird reward
-13.17	-13.17	-13.17	-13.82	-13.17	
-13.17	-13.17	-13.17	-13.82	-13.17	
-13.82	-13.17	-13.10	-13.86	-13.82	
-13.17	-13.17	-13.17	-13.86	-13.17	
-13.17	-13.82	-13.17	-13.17	-13.17	
ird policy
>	>	v	v	v	
>	v	v	v	^	
>	>	^	<	v	
>	>	^	<	v	
^	>	^	<	<	
MAP policy loss 39.61904789031261
mean policy loss 18.159919874734413
robust policy loss 80.8014768370778
regret policy loss 18.159919879568065
ird policy loss 7.736292256810273e-08
MAP lava occupancy 0.43608540260951745
Mean lava occupancy 0.43608540260951745
Robust lava occupancy 0.858367999635371
Regret lava occupancy 0.22562500006693373
IRD lava occupancy 7.264908661088022e-10
##############
Trial  12
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (8, 3), (12, 1), (3, 3), (12, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.81329356  0.25547036  0.04296151 -0.52100172] loglik -9.01091334727846
accepted/total = 1885/2000 = 0.9425
MAP Policy on Train MDP
map_weights [-0.81329356  0.25547036  0.04296151 -0.52100172]
map reward
-0.81	-0.81	-0.81	-0.81	-0.81	
-0.81	-0.81	-0.81	-0.81	-0.81	
-0.81	-0.81	0.04	-0.81	-0.81	
-0.81	-0.81	-0.81	-0.81	-0.81	
-0.81	-0.81	-0.81	-0.81	-0.81	
Map policy
>	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.27555369  0.23857276  0.34665754  0.0381134 ]
mean reward
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	0.35	-0.28	-0.28	
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	-0.28	-0.28	-0.28	
mean policy
v	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
>	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.510482761756456e-10
Mean policy loss 5.396912508364403e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
<	>	v	>	>	
^	>	<	<	^	
<	<	^	^	>	
>	>	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
-100.00	-1.00	1.00	-1.00	-100.00	
-1.00	-100.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	2 	0 	3 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.81329356  0.25547036  0.04296151 -0.52100172]
map reward
-0.81	-0.81	-0.81	-0.81	-0.81	
-0.81	0.26	-0.81	-0.52	-0.81	
-0.52	-0.81	0.04	-0.81	-0.52	
-0.81	-0.52	0.26	-0.52	-0.81	
-0.81	-0.81	-0.81	0.26	-0.81	
Map policy
>	v	v	v	v	
>	v	v	v	v	
>	>	v	v	v	
>	>	>	v	<	
>	>	>	v	<	
MEAN policy on test env
mean_weights [-0.27555369  0.23857276  0.34665754  0.0381134 ]
mean reward
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	0.24	-0.28	0.04	-0.28	
0.04	-0.28	0.35	-0.28	0.04	
-0.28	0.04	0.24	0.04	-0.28	
-0.28	-0.28	-0.28	0.24	-0.28	
mean policy
v	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	>	
>	^	^	^	<	
features
0 	0 	0 	0 	0 	
0 	1 	0 	3 	0 	
3 	0 	2 	0 	3 	
0 	3 	1 	3 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	>	v	<	v	
<	>	v	<	>	
>	>	^	<	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
<	>	v	<	>	
>	>	<	<	<	
^	^	^	^	<	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-12.90	-12.90	-12.90	-12.90	-12.90	
-12.90	-13.54	-12.90	-13.49	-12.90	
-13.49	-12.90	-12.68	-12.90	-13.49	
-12.90	-13.49	-13.54	-13.49	-12.90	
-12.90	-12.90	-12.90	-13.54	-12.90	
ird policy
>	>	v	<	<	
<	>	v	<	>	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	^	^	
MAP policy loss 64.6879730229396
mean policy loss 99.65798748930852
robust policy loss 779.573330103503
regret policy loss 27.111395368716977
ird policy loss 42.008999822849084
MAP lava occupancy 0.5814478957974174
Mean lava occupancy 0.5814478957974174
Robust lava occupancy 7.6132655604319
Regret lava occupancy 0.29476255589769024
IRD lava occupancy 0.45124999797188253
##############
Trial  13
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [-0.28100759 -0.24995839  0.91060307 -0.17134056] loglik -9.70405825689221
accepted/total = 1760/2000 = 0.88
MAP Policy on Train MDP
map_weights [-0.28100759 -0.24995839  0.91060307 -0.17134056]
map reward
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	0.91	-0.28	-0.28	
-0.28	-0.25	-0.28	-0.28	-0.28	
-0.28	-0.28	-0.25	-0.28	-0.28	
Map policy
v	v	v	v	v	
v	>	v	<	v	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.01980079 -0.27068388  0.63720956 -0.11924136]
mean reward
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	0.64	-0.02	-0.02	
-0.02	-0.27	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.27	-0.02	-0.02	
mean policy
>	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
MAP policy loss 1.804999769673593
Mean policy loss 1.8674148893893516e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	<	<	<	
>	v	v	<	>	
>	>	v	<	v	
^	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-5.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	2 	0 	1 	
3 	0 	0 	0 	0 	
0 	0 	3 	1 	0 	
MAP on testing env
map_weights [-0.28100759 -0.24995839  0.91060307 -0.17134056]
map reward
-0.28	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	-0.25	-0.25	-0.28	
-0.25	-0.28	0.91	-0.28	-0.25	
-0.17	-0.28	-0.28	-0.28	-0.28	
-0.28	-0.28	-0.17	-0.25	-0.28	
Map policy
v	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	>	^	<	<	
MEAN policy on test env
mean_weights [-0.01980079 -0.27068388  0.63720956 -0.11924136]
mean reward
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.27	-0.27	-0.02	
-0.27	-0.02	0.64	-0.02	-0.27	
-0.12	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.12	-0.27	-0.02	
mean policy
>	v	v	<	<	
>	v	v	v	v	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	0 	1 	1 	0 	
1 	0 	2 	0 	1 	
3 	0 	0 	0 	0 	
0 	0 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	<	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	<	<	
^	>	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	<	<	<	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-13.27	-13.27	-13.27	-13.27	-13.27	
-13.27	-13.27	-13.90	-13.90	-13.27	
-13.90	-13.27	-13.23	-13.27	-13.90	
-13.99	-13.27	-13.27	-13.27	-13.27	
-13.27	-13.27	-13.99	-13.90	-13.27	
ird policy
v	v	<	<	<	
>	v	v	v	>	
>	>	>	<	<	
<	>	^	^	<	
>	^	v	^	^	
MAP policy loss 47.53373721534384
mean policy loss 0.45014019397037486
robust policy loss 50.9488442641793
regret policy loss 2.6232917811682666
ird policy loss 1.6609160449765348e-08
MAP lava occupancy 0.3759172086589775
Mean lava occupancy 0.3759172086589775
Robust lava occupancy 0.4631250000022814
Regret lava occupancy 0.024350376447706337
IRD lava occupancy 1.1132961235517457e-10
##############
Trial  14
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	<	<	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 0), (3, 0), (6, 3), (17, 2), (11, 1), (2, 3), (12, 3), (4, 0)]
w_map [ 0.05453682 -0.03012728  0.99803825 -0.00614204] loglik -6.067980371387648
accepted/total = 1677/2000 = 0.8385
MAP Policy on Train MDP
map_weights [ 0.05453682 -0.03012728  0.99803825 -0.00614204]
map reward
0.05	0.05	0.05	0.05	0.05	
-0.03	0.05	0.05	0.05	0.05	
0.05	0.05	1.00	-0.03	-0.03	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
Map policy
>	v	v	v	<	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	<	<	
MEAN policy on Train MDP
mean_weights [ 0.19431265 -0.27848173  0.64358958 -0.16481725]
mean reward
0.19	0.19	0.19	0.19	0.19	
-0.28	0.19	0.19	0.19	0.19	
0.19	0.19	0.64	-0.28	-0.28	
0.19	0.19	0.19	0.19	0.19	
0.19	0.19	0.19	0.19	0.19	
mean policy
>	v	v	<	<	
>	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
^	^	^	<	<	
Optimal Policy
>	v	v	<	<	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	<	<	
^	^	^	<	<	
MAP policy loss 2.001890305596346e-06
Mean policy loss -6.168418734131542e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	<	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-5.00	-1.00	-100.00	-1.00	-5.00	
-1.00	-5.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
1 	0 	3 	0 	1 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.05453682 -0.03012728  0.99803825 -0.00614204]
map reward
0.05	0.05	-0.01	0.05	0.05	
-0.03	0.05	-0.01	0.05	-0.03	
0.05	-0.03	1.00	-0.03	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
Map policy
>	v	^	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.19431265 -0.27848173  0.64358958 -0.16481725]
mean reward
0.19	0.19	-0.16	0.19	0.19	
-0.28	0.19	-0.16	0.19	-0.28	
0.19	-0.28	0.64	-0.28	0.19	
0.19	0.19	0.19	0.19	0.19	
0.19	0.19	0.19	0.19	0.19	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
features
0 	0 	3 	0 	0 	
1 	0 	3 	0 	1 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
v	>	v	<	v	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
v	v	v	v	v	
v	>	v	<	v	
>	>	^	<	<	
>	>	^	<	<	
-------- IRD Solution -------
ird reward
-13.22	-13.22	-13.96	-13.22	-13.22	
-13.97	-13.22	-13.96	-13.22	-13.97	
-13.22	-13.97	-13.17	-13.97	-13.22	
-13.22	-13.22	-13.22	-13.22	-13.22	
-13.22	-13.22	-13.22	-13.22	-13.22	
ird policy
>	v	^	v	<	
<	^	^	^	>	
<	v	v	v	>	
>	>	^	<	<	
>	>	^	<	<	
MAP policy loss 315.2845007562131
mean policy loss 40.725312499886066
robust policy loss 40.72531250075066
regret policy loss 18.469837925112127
ird policy loss 6.6391602616301455
MAP lava occupancy 3.0803642026170626
Mean lava occupancy 3.0803642026170626
Robust lava occupancy 0.42868750000717587
Regret lava occupancy 0.19441934658316556
IRD lava occupancy 3.9757307091754256e-11
##############
Trial  15
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 1), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [0.10130212 0.04951178 0.90278354 0.41505222] loglik -7.454715675435523
accepted/total = 1607/2000 = 0.8035
MAP Policy on Train MDP
map_weights [0.10130212 0.04951178 0.90278354 0.41505222]
map reward
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.90	0.10	0.10	
0.10	0.10	0.05	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.05879027 -0.36968645  0.5596135   0.2615751 ]
mean reward
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.56	0.06	0.06	
0.06	0.06	-0.37	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 1.752025249480288e-06
Mean policy loss 8.429088768635573e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
v	^	^	<	<	
>	>	^	^	<	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-5.00	1.00	-1.00	-5.00	
-5.00	-100.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	2 	0 	1 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [0.10130212 0.04951178 0.90278354 0.41505222]
map reward
0.10	0.10	0.05	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
0.42	0.05	0.90	0.10	0.05	
0.05	0.42	0.10	0.10	0.05	
0.10	0.05	0.10	0.10	0.10	
Map policy
v	v	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.05879027 -0.36968645  0.5596135   0.2615751 ]
mean reward
0.06	0.06	-0.37	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.26	-0.37	0.56	0.06	-0.37	
-0.37	0.26	0.06	0.06	-0.37	
0.06	-0.37	0.06	0.06	0.06	
mean policy
v	v	v	v	<	
>	>	v	v	<	
<	>	>	<	<	
>	>	^	^	<	
^	^	^	^	<	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
3 	1 	2 	0 	1 	
1 	3 	0 	0 	1 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	>	v	<	<	
<	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	>	v	v	<	
<	^	>	<	<	
>	>	^	<	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-12.45	-12.45	-12.96	-12.45	-12.45	
-12.45	-12.45	-12.45	-12.45	-12.45	
-12.87	-12.96	-12.14	-12.45	-12.96	
-12.96	-12.87	-12.45	-12.45	-12.96	
-12.45	-12.96	-12.45	-12.45	-12.45	
ird policy
>	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
^	^	^	^	^	
>	>	^	^	<	
MAP policy loss 39.90755156696158
mean policy loss 22.33687575468863
robust policy loss 79.5974253821985
regret policy loss 1.5996921138625046
ird policy loss 9.962622254944264e-09
MAP lava occupancy 0.27052638042422944
Mean lava occupancy 0.27052638042422944
Robust lava occupancy 0.7926834178986744
Regret lava occupancy 0.01615850620011769
IRD lava occupancy 8.229319154605577e-11
##############
Trial  16
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	<	v	
>	v	v	<	v	
>	>	>	<	<	
>	>	^	^	^	
^	>	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	1 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (12, 0), (3, 0), (6, 1), (17, 2), (11, 1), (2, 3), (13, 0), (12, 3), (4, 0)]
w_map [-0.51156411 -0.49551829 -0.68182259  0.16697827] loglik -11.814846093178723
accepted/total = 907/2000 = 0.4535
MAP Policy on Train MDP
map_weights [-0.51156411 -0.49551829 -0.68182259  0.16697827]
map reward
-0.51	-0.51	-0.51	-0.51	-0.51	
-0.50	-0.51	-0.51	-0.50	-0.51	
-0.51	-0.51	-0.68	-0.51	-0.51	
-0.51	-0.51	-0.51	-0.51	-0.50	
-0.51	-0.50	-0.51	-0.51	-0.51	
Map policy
v	v	v	v	v	
<	v	>	v	v	
^	v	>	v	v	
>	v	<	>	>	
>	v	<	<	^	
MEAN policy on Train MDP
mean_weights [-0.59553334 -0.2378649  -0.46347006  0.12027226]
mean reward
-0.60	-0.60	-0.60	-0.60	-0.60	
-0.24	-0.60	-0.60	-0.24	-0.60	
-0.60	-0.60	-0.46	-0.60	-0.60	
-0.60	-0.60	-0.60	-0.60	-0.24	
-0.60	-0.24	-0.60	-0.60	-0.60	
mean policy
v	v	v	v	v	
<	v	v	v	v	
^	v	v	v	v	
>	v	v	>	>	
>	v	<	<	^	
Optimal Policy
>	v	v	<	v	
>	v	v	<	v	
>	>	>	<	<	
>	>	^	^	^	
^	>	^	^	<	
MAP policy loss 90.85531992975362
Mean policy loss 90.8553205112928
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	^	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	^	^	^	<	
>	>	^	^	^	
reward
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	3 	0 	0 	0 	
0 	1 	1 	0 	0 	
1 	0 	2 	0 	0 	
0 	3 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.51156411 -0.49551829 -0.68182259  0.16697827]
map reward
-0.51	0.17	-0.51	-0.51	-0.51	
-0.51	-0.50	-0.50	-0.51	-0.51	
-0.50	-0.51	-0.68	-0.51	-0.51	
-0.51	0.17	-0.51	-0.51	-0.51	
-0.51	-0.51	-0.50	-0.51	-0.51	
Map policy
>	^	<	<	<	
>	^	<	<	<	
^	^	^	^	<	
>	^	<	<	<	
^	^	<	<	<	
MEAN policy on test env
mean_weights [-0.59553334 -0.2378649  -0.46347006  0.12027226]
mean reward
-0.60	0.12	-0.60	-0.60	-0.60	
-0.60	-0.24	-0.24	-0.60	-0.60	
-0.24	-0.60	-0.46	-0.60	-0.60	
-0.60	0.12	-0.60	-0.60	-0.60	
-0.60	-0.60	-0.24	-0.60	-0.60	
mean policy
>	^	<	<	<	
>	^	<	<	<	
^	^	^	^	>	
>	^	<	^	>	
^	^	^	<	<	
features
0 	3 	0 	0 	0 	
0 	1 	1 	0 	0 	
1 	0 	2 	0 	0 	
0 	3 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	v	<	<	
v	>	<	<	<	
<	^	^	<	<	
^	^	v	<	<	
^	>	v	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	<	
v	>	<	<	<	
<	>	^	<	<	
^	>	^	^	<	
^	>	v	<	<	
-------- IRD Solution -------
ird reward
2.74	3.56	2.74	2.74	2.74	
2.74	3.47	3.47	2.74	2.74	
3.47	2.74	4.15	2.74	2.74	
2.74	3.56	2.74	2.74	2.74	
2.74	2.74	3.47	2.74	2.74	
ird policy
>	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	<	<	
MAP policy loss 14.90282248877019
mean policy loss 1683.199973968141
robust policy loss 488.9546894001502
regret policy loss 139.93555567940453
ird policy loss 79.30898295248505
MAP lava occupancy 1.838055495100946e-08
Mean lava occupancy 1.838055495100946e-08
Robust lava occupancy 4.229519672643841
Regret lava occupancy 0.6029423123866728
IRD lava occupancy 0.46312498788129397
##############
Trial  17
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 3), (17, 2), (13, 0), (11, 1), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [-5.12098209e-04 -5.97542221e-01 -4.18403646e-01 -6.84018583e-01] loglik -18.186346344186994
accepted/total = 25/2000 = 0.0125
MAP Policy on Train MDP
map_weights [-5.12098209e-04 -5.97542221e-01 -4.18403646e-01 -6.84018583e-01]
map reward
-0.00	-0.00	-0.00	-0.60	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.42	-0.00	-0.60	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
Map policy
v	v	^	^	v	
v	v	<	<	<	
>	<	>	v	>	
^	^	<	<	<	
^	^	^	<	<	
MEAN policy on Train MDP
mean_weights [-0.00065845 -0.44890624 -0.61643384 -0.5993795 ]
mean reward
-0.00	-0.00	-0.00	-0.45	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.62	-0.00	-0.45	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
mean policy
v	v	^	v	v	
v	v	<	<	<	
>	<	>	v	>	
^	^	<	<	<	
^	^	^	<	<	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	<	
MAP policy loss 16.707820514935143
Mean policy loss 16.707820517142782
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	v	<	v	
>	>	v	<	<	
>	>	<	<	^	
^	^	^	^	v	
^	v	<	^	<	
reward
-1.00	-100.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-5.00	-100.00	
-1.00	-1.00	-100.00	-5.00	-1.00	
features
0 	3 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	1 	3 	1 	3 	
0 	0 	3 	1 	0 	
MAP on testing env
map_weights [-5.12098209e-04 -5.97542221e-01 -4.18403646e-01 -6.84018583e-01]
map reward
-0.00	-0.68	-0.00	-0.60	-0.00	
-0.60	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.42	-0.60	-0.00	
-0.00	-0.60	-0.68	-0.60	-0.68	
-0.00	-0.00	-0.68	-0.60	-0.00	
Map policy
<	v	^	>	v	
^	>	^	<	<	
>	^	<	^	>	
^	v	v	v	^	
^	v	<	>	>	
MEAN policy on test env
mean_weights [-0.00065845 -0.44890624 -0.61643384 -0.5993795 ]
mean reward
-0.00	-0.60	-0.00	-0.45	-0.00	
-0.45	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.62	-0.45	-0.00	
-0.00	-0.45	-0.60	-0.45	-0.60	
-0.00	-0.00	-0.60	-0.45	-0.00	
mean policy
<	v	^	>	v	
^	>	^	<	<	
>	^	<	^	>	
^	v	v	v	^	
^	v	<	>	>	
features
0 	3 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
0 	1 	3 	1 	3 	
0 	0 	3 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
<	>	^	<	v	
>	>	^	<	<	
>	^	<	^	>	
^	^	v	>	^	
^	v	<	<	>	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
<	<	^	<	v	
^	>	<	<	<	
>	^	^	v	>	
^	v	v	v	^	
^	v	>	>	>	
-------- IRD Solution -------
ird reward
2.46	1.83	2.46	1.74	2.46	
1.74	2.46	2.46	2.46	2.46	
2.46	2.46	2.16	1.74	2.46	
2.46	1.74	1.83	1.74	1.83	
2.46	2.46	1.83	1.74	2.46	
ird policy
<	<	^	v	v	
v	>	^	<	<	
>	^	<	v	>	
^	v	>	v	^	
^	v	<	>	>	
MAP policy loss 371.8825855691058
mean policy loss 13.047945522687193
robust policy loss 13.047945512409003
regret policy loss 13.047945543453118
ird policy loss 13.047945532435802
MAP lava occupancy 3.445305096528078
Mean lava occupancy 3.445305096528078
Robust lava occupancy 1.4433755631104642e-13
Regret lava occupancy 3.685407498853008e-10
IRD lava occupancy 1.8839919907339597e-10
##############
Trial  18
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	^	v	<	<	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 3), (12, 2), (12, 1), (12, 0), (6, 1), (9, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [ 0.36008475  0.22426586  0.86623328 -0.2639767 ] loglik -6.068422941903918
accepted/total = 1278/2000 = 0.639
MAP Policy on Train MDP
map_weights [ 0.36008475  0.22426586  0.86623328 -0.2639767 ]
map reward
0.36	0.22	0.36	0.36	0.36	
0.36	0.36	0.36	0.36	0.36	
0.36	0.36	0.87	0.36	0.36	
0.36	0.36	0.22	0.36	0.36	
0.36	0.36	0.36	0.36	0.36	
Map policy
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.08365406 -0.37026192  0.44153289 -0.17975387]
mean reward
0.08	-0.37	0.08	0.08	0.08	
0.08	0.08	0.08	0.08	0.08	
0.08	0.08	0.44	0.08	0.08	
0.08	0.08	-0.37	0.08	0.08	
0.08	0.08	0.08	0.08	0.08	
mean policy
v	v	v	<	<	
>	>	v	<	<	
>	>	>	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
v	^	v	<	<	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	^	^	
MAP policy loss 5.751204159576417e-07
Mean policy loss 1.7846786548592064e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	^	^	v	v	
v	>	v	<	<	
>	>	^	<	<	
>	>	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [ 0.36008475  0.22426586  0.86623328 -0.2639767 ]
map reward
0.36	0.36	-0.26	0.36	0.36	
0.36	0.22	0.36	0.36	0.36	
0.36	0.36	0.87	0.36	0.22	
0.36	0.36	0.36	0.36	0.36	
0.36	0.36	0.22	0.36	0.36	
Map policy
v	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.08365406 -0.37026192  0.44153289 -0.17975387]
mean reward
0.08	0.08	-0.18	0.08	0.08	
0.08	-0.37	0.08	0.08	0.08	
0.08	0.08	0.44	0.08	-0.37	
0.08	0.08	0.08	0.08	0.08	
0.08	0.08	-0.37	0.08	0.08	
mean policy
v	^	v	v	<	
v	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	^	
features
0 	0 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	^	>	v	v	
v	^	v	v	<	
>	>	^	<	>	
>	>	^	<	<	
^	^	v	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	^	>	v	<	
v	<	v	<	<	
>	>	^	<	^	
>	>	^	<	<	
^	^	>	^	^	
-------- IRD Solution -------
ird reward
-13.05	-13.05	-13.75	-13.05	-13.05	
-13.05	-13.69	-13.05	-13.05	-13.05	
-13.05	-13.05	-12.90	-13.05	-13.69	
-13.05	-13.05	-13.05	-13.05	-13.05	
-13.05	-13.05	-13.69	-13.05	-13.05	
ird policy
v	^	v	v	<	
v	v	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
^	^	^	^	^	
MAP policy loss 7.222516438664769
mean policy loss 8.666145787095125e-10
robust policy loss 5.605282379975863
regret policy loss 0.05571391345999884
ird policy loss 7.344973685027845e-07
MAP lava occupancy 0.062157123153097724
Mean lava occupancy 0.062157123153097724
Robust lava occupancy 6.961385019715606e-11
Regret lava occupancy 2.662018951321785e-10
IRD lava occupancy 4.080952878043066e-09
##############
Trial  19
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (6, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [ 0.0075775  -0.07936357  0.72595722  0.6831033 ] loglik -6.761572555721273
accepted/total = 1609/2000 = 0.8045
MAP Policy on Train MDP
map_weights [ 0.0075775  -0.07936357  0.72595722  0.6831033 ]
map reward
0.01	-0.08	0.01	0.01	0.01	
-0.08	0.01	0.01	0.01	0.01	
0.01	0.01	0.73	0.01	0.01	
0.01	0.01	-0.08	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.0221271  -0.46791811  0.57858679 -0.08463433]
mean reward
0.02	-0.47	0.02	0.02	0.02	
-0.47	0.02	0.02	0.02	0.02	
0.02	0.02	0.58	0.02	0.02	
0.02	0.02	-0.47	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
MAP policy loss 1.0082917624495286e-06
Mean policy loss 5.336584302309544e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	^	
>	>	^	<	<	
^	^	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [ 0.0075775  -0.07936357  0.72595722  0.6831033 ]
map reward
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	-0.08	0.73	-0.08	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.68	0.01	0.01	
Map policy
>	v	v	v	<	
>	>	v	<	<	
v	>	v	<	v	
>	>	v	<	<	
>	>	v	<	<	
MEAN policy on test env
mean_weights [ 0.0221271  -0.46791811  0.57858679 -0.08463433]
mean reward
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	0.02	0.02	0.02	
0.02	-0.47	0.58	-0.47	0.02	
0.02	0.02	0.02	0.02	0.02	
0.02	0.02	-0.08	0.02	0.02	
mean policy
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
-12.86	-12.86	-12.86	-12.86	-12.86	
-12.86	-12.86	-12.86	-12.86	-12.86	
-12.86	-13.41	-12.75	-13.41	-12.86	
-12.86	-12.86	-12.86	-12.86	-12.86	
-12.86	-12.86	-13.51	-12.86	-12.86	
ird policy
>	>	v	<	<	
>	>	v	<	<	
v	>	^	<	v	
>	>	^	<	<	
^	^	v	^	^	
MAP policy loss 23.478594998261016
mean policy loss 4.250728486429947e-08
robust policy loss 44.67374999444185
regret policy loss 1.7073583201352216
ird policy loss 2.1372017003362664e-11
MAP lava occupancy 1.0667416092503386e-07
Mean lava occupancy 1.0667416092503386e-07
Robust lava occupancy 0.45124999994168363
Regret lava occupancy 0.017246043577721543
IRD lava occupancy 1.0233959773186022e-12
##############
Trial  20
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [-0.89301141 -0.03747838  0.28974174  0.34230939] loglik -9.704060466598548
accepted/total = 1913/2000 = 0.9565
MAP Policy on Train MDP
map_weights [-0.89301141 -0.03747838  0.28974174  0.34230939]
map reward
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	-0.89	0.29	-0.89	-0.89	
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	-0.89	-0.89	-0.89	-0.89	
Map policy
v	v	v	v	<	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.40206575 -0.06949852  0.31540676 -0.0177786 ]
mean reward
-0.40	-0.40	-0.40	-0.40	-0.40	
-0.40	-0.40	-0.40	-0.40	-0.40	
-0.40	-0.40	0.32	-0.40	-0.40	
-0.40	-0.40	-0.40	-0.40	-0.40	
-0.40	-0.40	-0.40	-0.40	-0.40	
mean policy
v	v	v	v	<	
>	v	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.096434649310124e-09
Mean policy loss 4.769543216187047e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	v	
>	v	v	<	v	
>	>	<	<	<	
>	^	^	^	<	
>	^	<	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
1 	0 	2 	0 	0 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.89301141 -0.03747838  0.28974174  0.34230939]
map reward
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	-0.89	-0.89	0.34	-0.89	
-0.04	-0.89	0.29	-0.89	-0.89	
-0.89	-0.89	0.34	-0.04	-0.04	
-0.89	-0.89	-0.04	-0.89	-0.89	
Map policy
v	v	v	v	<	
v	v	v	<	<	
>	>	v	<	v	
>	>	^	<	<	
>	>	^	<	^	
MEAN policy on test env
mean_weights [-0.40206575 -0.06949852  0.31540676 -0.0177786 ]
mean reward
-0.40	-0.40	-0.40	-0.40	-0.40	
-0.40	-0.40	-0.40	-0.02	-0.40	
-0.07	-0.40	0.32	-0.40	-0.40	
-0.40	-0.40	-0.02	-0.07	-0.07	
-0.40	-0.40	-0.07	-0.40	-0.40	
mean policy
v	v	v	v	<	
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
1 	0 	2 	0 	0 	
0 	0 	3 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	>	v	v	v	
<	>	v	<	v	
^	>	^	<	<	
>	>	v	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-13.15	-13.15	-13.15	-13.15	-13.15	
-13.15	-13.15	-13.15	-13.75	-13.15	
-13.84	-13.15	-13.04	-13.15	-13.15	
-13.15	-13.15	-13.75	-13.84	-13.84	
-13.15	-13.15	-13.84	-13.15	-13.15	
ird policy
>	v	v	<	v	
>	v	v	<	v	
<	>	^	<	<	
>	^	^	^	^	
^	^	^	^	<	
MAP policy loss 231.450844474573
mean policy loss 853.2171971143432
robust policy loss 673.6193935576694
regret policy loss 1.858783671900487
ird policy loss 2.044367229513816e-07
MAP lava occupancy 2.329260576324892
Mean lava occupancy 2.329260576324892
Robust lava occupancy 6.568162909316614
Regret lava occupancy 0.01877559264597729
IRD lava occupancy 1.6918601537188277e-09
##############
Trial  21
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	v	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (6, 1), (8, 0), (17, 2), (13, 0), (11, 1), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [-0.15765504  0.31671768  0.76097135  0.54383582] loglik -9.70404953143543
accepted/total = 1739/2000 = 0.8695
MAP Policy on Train MDP
map_weights [-0.15765504  0.31671768  0.76097135  0.54383582]
map reward
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.16	-0.16	0.76	-0.16	-0.16	
-0.16	-0.16	-0.16	-0.16	-0.16	
-0.16	0.32	-0.16	-0.16	-0.16	
Map policy
v	v	v	v	v	
v	v	v	<	v	
>	>	v	<	<	
>	v	^	^	<	
>	v	<	<	<	
MEAN policy on Train MDP
mean_weights [ 0.01103903 -0.19150259  0.57741739  0.20864375]
mean reward
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	0.01	0.58	0.01	0.01	
0.01	0.01	0.01	0.01	0.01	
0.01	-0.19	0.01	0.01	0.01	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	<	<	
Optimal Policy
>	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	v	^	^	<	
MAP policy loss 44.501410526894965
Mean policy loss 1.2268183517682996e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	v	v	<	v	
>	>	^	<	<	
^	^	^	^	<	
^	^	>	^	^	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.15765504  0.31671768  0.76097135  0.54383582]
map reward
-0.16	-0.16	0.54	-0.16	-0.16	
-0.16	-0.16	-0.16	0.32	0.54	
-0.16	-0.16	0.76	-0.16	0.32	
-0.16	-0.16	0.32	-0.16	-0.16	
-0.16	-0.16	0.32	-0.16	-0.16	
Map policy
>	>	^	<	v	
>	>	v	>	>	
>	>	v	<	^	
>	>	^	<	^	
>	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.01103903 -0.19150259  0.57741739  0.20864375]
mean reward
0.01	0.01	0.21	0.01	0.01	
0.01	0.01	0.01	-0.19	0.21	
0.01	0.01	0.58	0.01	-0.19	
0.01	0.01	-0.19	0.01	0.01	
0.01	0.01	-0.19	0.01	0.01	
mean policy
>	>	v	<	<	
>	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
^	^	^	^	<	
features
0 	0 	3 	0 	0 	
0 	0 	0 	1 	3 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	v	
>	v	v	v	>	
>	>	v	<	<	
^	^	^	^	<	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	>	v	v	v	
>	>	>	<	<	
^	^	^	^	<	
^	^	<	^	<	
-------- IRD Solution -------
ird reward
-13.17	-13.17	-13.78	-13.17	-13.17	
-13.17	-13.17	-13.17	-13.88	-13.78	
-13.17	-13.17	-13.07	-13.17	-13.88	
-13.17	-13.17	-13.88	-13.17	-13.17	
-13.17	-13.17	-13.88	-13.17	-13.17	
ird policy
v	v	v	<	<	
>	>	v	>	v	
>	>	<	<	<	
^	^	^	^	<	
^	^	^	^	^	
MAP policy loss 34.64386314830934
mean policy loss 43.77125116962384
robust policy loss 133.65243108799632
regret policy loss 12.294708495380169
ird policy loss 21.434375017338873
MAP lava occupancy 0.21494017329932272
Mean lava occupancy 0.21494017329932272
Robust lava occupancy 1.223168447031246
Regret lava occupancy 0.12941798389812578
IRD lava occupancy 0.22562500007638456
##############
Trial  22
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	>	
^	^	^	^	<	
^	^	>	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (11, 1), (2, 3), (1, 1), (13, 0), (4, 0)]
w_map [0.34900838 0.34386006 0.85656791 0.16200255] loglik -6.067970708036228
accepted/total = 1679/2000 = 0.8395
MAP Policy on Train MDP
map_weights [0.34900838 0.34386006 0.85656791 0.16200255]
map reward
0.35	0.35	0.35	0.35	0.35	
0.35	0.35	0.35	0.35	0.35	
0.35	0.35	0.86	0.35	0.34	
0.35	0.35	0.34	0.35	0.35	
0.35	0.35	0.35	0.35	0.35	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.2278378  -0.27110609  0.69270689 -0.11383686]
mean reward
0.23	0.23	0.23	0.23	0.23	
0.23	0.23	0.23	0.23	0.23	
0.23	0.23	0.69	0.23	-0.27	
0.23	0.23	-0.27	0.23	0.23	
0.23	0.23	0.23	0.23	0.23	
mean policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	^	^	^	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	>	
^	^	^	^	<	
^	^	>	^	<	
MAP policy loss 9.659837812078798e-07
Mean policy loss 2.5705940848164133e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	<	^	^	^	
^	>	v	^	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [0.34900838 0.34386006 0.85656791 0.16200255]
map reward
0.35	0.35	0.35	0.34	0.35	
0.34	0.35	0.35	0.35	0.34	
0.35	0.35	0.86	0.35	0.35	
0.34	0.16	0.16	0.35	0.35	
0.35	0.34	0.35	0.35	0.35	
Map policy
>	v	v	v	<	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.2278378  -0.27110609  0.69270689 -0.11383686]
mean reward
0.23	0.23	0.23	-0.27	0.23	
-0.27	0.23	0.23	0.23	-0.27	
0.23	0.23	0.69	0.23	0.23	
-0.27	-0.11	-0.11	0.23	0.23	
0.23	-0.27	0.23	0.23	0.23	
mean policy
>	v	v	v	v	
v	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
features
0 	0 	0 	1 	0 	
1 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
1 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
v	>	v	v	<	
>	>	<	<	<	
^	^	^	^	<	
^	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	>	>	^	^	
-------- IRD Solution -------
ird reward
-13.28	-13.28	-13.28	-14.01	-13.28	
-14.01	-13.28	-13.28	-13.28	-14.01	
-13.28	-13.28	-13.24	-13.28	-13.28	
-14.01	-13.94	-13.94	-13.28	-13.28	
-13.28	-14.01	-13.28	-13.28	-13.28	
ird policy
>	v	v	v	>	
v	v	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
<	^	v	^	^	
MAP policy loss 15.485777812658696
mean policy loss 1.845447456481908e-08
robust policy loss 124.29954068115968
regret policy loss 2.1915622233636967e-08
ird policy loss 6.453910296476759
MAP lava occupancy 0.15169879611356177
Mean lava occupancy 0.15169879611356177
Robust lava occupancy 1.2555509149142219
Regret lava occupancy 1.9238253171609055e-10
IRD lava occupancy 2.742935027204808e-10
##############
Trial  23
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.88876043 -0.2608459   0.08880693  0.36630266] loglik -8.317766111083188
accepted/total = 1912/2000 = 0.956
MAP Policy on Train MDP
map_weights [-0.88876043 -0.2608459   0.08880693  0.36630266]
map reward
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	-0.89	0.09	-0.89	-0.89	
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	-0.89	-0.89	-0.89	-0.89	
Map policy
>	v	v	v	<	
>	v	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.31334488  0.18556917  0.31357438  0.04936837]
mean reward
-0.31	-0.31	-0.31	-0.31	-0.31	
-0.31	-0.31	-0.31	-0.31	-0.31	
-0.31	-0.31	0.31	-0.31	-0.31	
-0.31	-0.31	-0.31	-0.31	-0.31	
-0.31	-0.31	-0.31	-0.31	-0.31	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 3.8526485426519175e-09
Mean policy loss 5.376180143022756e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	<	
^	^	v	>	^	
^	>	^	>	>	
^	^	^	v	^	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-100.00	-5.00	
-1.00	-5.00	1.00	-100.00	-5.00	
-1.00	-1.00	-5.00	-100.00	-100.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	2 	3 	1 	
0 	0 	1 	3 	3 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [-0.88876043 -0.2608459   0.08880693  0.36630266]
map reward
-0.89	-0.89	-0.89	-0.89	-0.89	
-0.89	0.37	-0.89	0.37	-0.26	
-0.89	-0.26	0.09	0.37	-0.26	
-0.89	-0.89	-0.26	0.37	0.37	
-0.89	-0.26	-0.89	-0.89	-0.89	
Map policy
v	v	>	v	v	
>	v	>	v	<	
>	>	>	v	<	
>	>	>	^	>	
>	^	^	^	^	
MEAN policy on test env
mean_weights [-0.31334488  0.18556917  0.31357438  0.04936837]
mean reward
-0.31	-0.31	-0.31	-0.31	-0.31	
-0.31	0.05	-0.31	0.05	0.19	
-0.31	0.19	0.31	0.05	0.19	
-0.31	-0.31	0.19	0.05	0.05	
-0.31	0.19	-0.31	-0.31	-0.31	
mean policy
v	v	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
^	>	^	<	^	
>	^	^	^	^	
features
0 	0 	0 	0 	0 	
0 	3 	0 	3 	1 	
0 	1 	2 	3 	1 	
0 	0 	1 	3 	3 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	v	<	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	v	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	^	<	
^	<	^	<	<	
-------- IRD Solution -------
ird reward
-13.21	-13.21	-13.21	-13.21	-13.21	
-13.21	-13.91	-13.21	-13.91	-13.84	
-13.21	-13.84	-13.13	-13.91	-13.84	
-13.21	-13.21	-13.84	-13.91	-13.91	
-13.21	-13.84	-13.21	-13.21	-13.21	
ird policy
>	>	v	<	<	
^	>	v	<	^	
^	>	^	<	<	
^	<	^	^	^	
^	^	^	<	<	
MAP policy loss 285.36282580269415
mean policy loss 123.82187237159204
robust policy loss 733.873064987509
regret policy loss 12.180948342418896
ird policy loss -2.454425177408361e-08
MAP lava occupancy 2.707840858534123
Mean lava occupancy 2.707840858534123
Robust lava occupancy 7.052933290850061
Regret lava occupancy 0.12467725065251817
IRD lava occupancy 1.0455259252489153e-10
##############
Trial  24
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	^	v	
>	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [ 0.16524322 -0.83888767  0.49000175 -0.16988361] loglik -9.010908978287375
accepted/total = 1706/2000 = 0.853
MAP Policy on Train MDP
map_weights [ 0.16524322 -0.83888767  0.49000175 -0.16988361]
map reward
0.17	0.17	0.17	-0.84	0.17	
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	0.49	0.17	0.17	
0.17	0.17	0.17	0.17	0.17	
0.17	0.17	0.17	0.17	0.17	
Map policy
>	>	v	^	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.10182563 -0.42978125  0.62860212 -0.25457616]
mean reward
0.10	0.10	0.10	-0.43	0.10	
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.63	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
mean policy
>	>	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	^	v	
>	v	v	v	<	
>	>	^	<	<	
>	>	^	<	<	
^	^	^	^	<	
MAP policy loss 5.239376927174133e-07
Mean policy loss 2.026814480138306e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	v	v	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-100.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	3 	1 	0 	
1 	0 	0 	0 	0 	
3 	0 	2 	0 	1 	
0 	0 	0 	3 	1 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [ 0.16524322 -0.83888767  0.49000175 -0.16988361]
map reward
0.17	-0.84	-0.17	-0.84	0.17	
-0.84	0.17	0.17	0.17	0.17	
-0.17	0.17	0.49	0.17	-0.84	
0.17	0.17	0.17	-0.17	-0.84	
0.17	0.17	0.17	-0.84	0.17	
Map policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.10182563 -0.42978125  0.62860212 -0.25457616]
mean reward
0.10	-0.43	-0.25	-0.43	0.10	
-0.43	0.10	0.10	0.10	0.10	
-0.25	0.10	0.63	0.10	-0.43	
0.10	0.10	0.10	-0.25	-0.43	
0.10	0.10	0.10	-0.43	0.10	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	>	^	^	>	
>	^	^	<	<	
features
0 	1 	3 	1 	0 	
1 	0 	0 	0 	0 	
3 	0 	2 	0 	1 	
0 	0 	0 	3 	1 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-12.82	-13.47	-13.56	-13.47	-12.82	
-13.47	-12.82	-12.82	-12.82	-12.82	
-13.56	-12.82	-12.63	-12.82	-13.47	
-12.82	-12.82	-12.82	-13.56	-13.47	
-12.82	-12.82	-12.82	-13.47	-12.82	
ird policy
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
MAP policy loss 29.059300968044493
mean policy loss 1.3185924629954826e-08
robust policy loss 3.015354492347244e-07
regret policy loss 1.9849680232830735e-10
ird policy loss 1.2940511960601864e-09
MAP lava occupancy 0.2889623728374173
Mean lava occupancy 0.2889623728374173
Robust lava occupancy 2.8769363584734317e-09
Regret lava occupancy 2.3797387110146536e-12
IRD lava occupancy 1.1925631703886878e-11
##############
Trial  25
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(9, 0), (8, 3), (12, 1), (12, 0), (6, 3), (17, 2), (13, 0), (11, 1), (4, 3), (5, 1), (0, 3), (12, 3)]
w_map [0.1001242  0.03511382 0.99355296 0.03993339] loglik -6.06792972725998
accepted/total = 1654/2000 = 0.827
MAP Policy on Train MDP
map_weights [0.1001242  0.03511382 0.99355296 0.03993339]
map reward
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.04	0.10	0.10	
0.10	0.10	0.99	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.05466539 -0.45784635  0.5629315   0.27093043]
mean reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	-0.46	0.05	0.05	
0.05	0.05	0.56	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
v	v	^	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
MAP policy loss 9.215764546609967e-07
Mean policy loss 8.706827804398631e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	<	
>	>	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
^	^	^	>	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-100.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-100.00	-100.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	1 	0 	3 	
0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	1 	3 	3 	0 	
MAP on testing env
map_weights [0.1001242  0.03511382 0.99355296 0.03993339]
map reward
0.10	0.10	0.10	0.04	0.10	
0.10	0.10	0.04	0.10	0.04	
0.10	0.04	0.99	0.10	0.10	
0.10	0.10	0.04	0.10	0.10	
0.10	0.04	0.04	0.04	0.10	
Map policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.05466539 -0.45784635  0.5629315   0.27093043]
mean reward
0.05	0.05	0.05	-0.46	0.05	
0.05	0.05	-0.46	0.05	0.27	
0.05	-0.46	0.56	0.05	0.05	
0.05	0.05	-0.46	0.05	0.05	
0.05	-0.46	0.27	0.27	0.05	
mean policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
^	>	v	^	<	
features
0 	0 	0 	1 	0 	
0 	0 	1 	0 	3 	
0 	1 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	1 	3 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
^	>	v	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	>	>	^	^	
-------- IRD Solution -------
ird reward
-12.98	-12.98	-12.98	-13.76	-12.98	
-12.98	-12.98	-13.76	-12.98	-13.65	
-12.98	-13.76	-12.86	-12.98	-12.98	
-12.98	-12.98	-13.76	-12.98	-12.98	
-12.98	-13.76	-13.65	-13.65	-12.98	
ird policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	>	>	^	^	
MAP policy loss 56.94720357455583
mean policy loss 46.07500000067407
robust policy loss 95.23668146357096
regret policy loss 22.562500321140778
ird policy loss 22.562499990800188
MAP lava occupancy 0.35681604164089087
Mean lava occupancy 0.35681604164089087
Robust lava occupancy 0.9669826479929025
Regret lava occupancy 0.23750000285262732
IRD lava occupancy 0.23750000000768634
##############
Trial  26
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (12, 0), (9, 3), (6, 3), (11, 1), (14, 0), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.1448118  -0.56722209  0.80309398  0.11103471] loglik -6.761572395278165
accepted/total = 1643/2000 = 0.8215
MAP Policy on Train MDP
map_weights [-0.1448118  -0.56722209  0.80309398  0.11103471]
map reward
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.14	0.80	-0.14	-0.14	
-0.14	-0.14	-0.57	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.04692397 -0.44130372  0.54817747  0.23071585]
mean reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.55	0.05	0.05	
0.05	0.05	-0.44	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 4.7365263167523097e-07
Mean policy loss 9.067688043137845e-08
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	>	v	<	
>	>	v	<	<	
v	>	v	<	>	
<	<	^	^	^	
>	>	^	<	>	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-100.00	-5.00	1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-100.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	2 	0 	0 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	0 	
MAP on testing env
map_weights [-0.1448118  -0.56722209  0.80309398  0.11103471]
map reward
-0.14	-0.14	-0.57	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.57	
0.11	-0.57	0.80	-0.14	-0.14	
-0.14	0.11	-0.14	-0.14	0.11	
-0.14	-0.14	-0.14	0.11	-0.14	
Map policy
v	v	v	v	<	
>	>	v	v	<	
<	>	>	<	<	
>	>	^	^	<	
>	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.04692397 -0.44130372  0.54817747  0.23071585]
mean reward
0.05	0.05	-0.44	0.05	0.05	
0.05	0.05	0.05	0.05	-0.44	
0.23	-0.44	0.55	0.05	0.05	
0.05	0.23	0.05	0.05	0.23	
0.05	0.05	0.05	0.23	0.05	
mean policy
v	v	v	v	<	
>	>	v	v	<	
<	>	v	<	<	
>	>	^	^	^	
^	^	^	^	^	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	1 	
3 	1 	2 	0 	0 	
0 	3 	0 	0 	3 	
0 	0 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	<	v	
<	>	>	<	<	
>	>	^	^	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	<	v	
<	v	>	<	<	
>	v	^	^	<	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-13.08	-13.08	-13.78	-13.08	-13.08	
-13.08	-13.08	-13.08	-13.08	-13.78	
-13.75	-13.78	-12.94	-13.08	-13.08	
-13.08	-13.75	-13.08	-13.08	-13.75	
-13.08	-13.08	-13.08	-13.75	-13.08	
ird policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
<	>	^	^	^	
>	>	^	^	^	
MAP policy loss 58.67110513073182
mean policy loss 41.672421508462925
robust policy loss 48.70738005448451
regret policy loss 19.335544876719005
ird policy loss 19.33554487208365
MAP lava occupancy 0.507444316993764
Mean lava occupancy 0.507444316993764
Robust lava occupancy 0.533526951994981
Regret lava occupancy 0.23750000004460922
IRD lava occupancy 0.23750000000234728
##############
Trial  27
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (4, 3), (13, 0)]
w_map [-0.74247138  0.32071554  0.58506785  0.05977793] loglik -9.704060515753994
accepted/total = 1862/2000 = 0.931
MAP Policy on Train MDP
map_weights [-0.74247138  0.32071554  0.58506785  0.05977793]
map reward
-0.74	-0.74	-0.74	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.74	-0.74	
-0.74	-0.74	0.59	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.74	-0.74	
Map policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.26673419 -0.09892531  0.2199713   0.00219523]
mean reward
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	0.22	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	>	^	<	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.1173477908357237e-10
Mean policy loss 5.355002301099055e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	v	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-100.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-100.00	
-100.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.74247138  0.32071554  0.58506785  0.05977793]
map reward
-0.74	0.06	0.32	-0.74	-0.74	
-0.74	0.32	-0.74	-0.74	0.06	
0.06	-0.74	0.59	-0.74	0.32	
-0.74	-0.74	-0.74	-0.74	-0.74	
-0.74	-0.74	-0.74	-0.74	-0.74	
Map policy
>	>	^	<	v	
>	^	<	>	v	
>	^	>	>	>	
^	^	^	^	^	
^	v	v	^	^	
MEAN policy on test env
mean_weights [-0.26673419 -0.09892531  0.2199713   0.00219523]
mean reward
-0.27	0.00	-0.10	-0.27	-0.27	
-0.27	-0.10	-0.27	-0.27	0.00	
0.00	-0.27	0.22	-0.27	-0.10	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
mean policy
>	^	<	<	v	
v	^	v	>	>	
<	<	<	<	^	
^	^	^	^	^	
^	^	^	^	^	
features
0 	3 	1 	0 	0 	
0 	1 	0 	0 	3 	
3 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	^	<	v	
v	^	v	<	>	
<	>	v	<	^	
^	^	^	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
-------- IRD Solution -------
ird reward
-13.01	-13.65	-13.86	-13.01	-13.01	
-13.01	-13.86	-13.01	-13.01	-13.65	
-13.65	-13.01	-12.95	-13.01	-13.86	
-13.01	-13.01	-13.01	-13.01	-13.01	
-13.01	-13.01	-13.01	-13.01	-13.01	
ird policy
<	>	^	v	<	
<	>	v	<	<	
v	>	^	<	>	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 33.189756584942224
mean policy loss 1828.345945494288
robust policy loss 660.3669327978167
regret policy loss 9.337360278443963
ird policy loss 3.2744576657261053
MAP lava occupancy 0.3314539474223536
Mean lava occupancy 0.3314539474223536
Robust lava occupancy 6.468315206093057
Regret lava occupancy 0.09828800287965973
IRD lava occupancy 2.0889799681214772e-08
##############
Trial  28
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (12, 0), (6, 1), (9, 3), (12, 3), (17, 2), (11, 1), (14, 0), (4, 3), (13, 0)]
w_map [-0.28586529 -0.76211635  0.270607   -0.51403459] loglik -9.010913333462419
accepted/total = 1779/2000 = 0.8895
MAP Policy on Train MDP
map_weights [-0.28586529 -0.76211635  0.270607   -0.51403459]
map reward
-0.29	-0.76	-0.29	-0.29	-0.29	
-0.76	-0.29	-0.29	-0.29	-0.29	
-0.29	-0.29	0.27	-0.29	-0.29	
-0.29	-0.29	-0.29	-0.29	-0.29	
-0.29	-0.29	-0.29	-0.29	-0.29	
Map policy
>	v	v	v	<	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.06488583 -0.2690748   0.62283551  0.01219746]
mean reward
0.06	-0.27	0.06	0.06	0.06	
-0.27	0.06	0.06	0.06	0.06	
0.06	0.06	0.62	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
>	v	v	v	<	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	<	
MAP policy loss 1.168918410156261e-08
Mean policy loss 2.9944307971792664e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	>	^	v	v	
v	v	v	>	v	
>	>	<	<	v	
>	^	^	<	<	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.28586529 -0.76211635  0.270607   -0.51403459]
map reward
-0.29	-0.29	-0.29	-0.76	-0.29	
-0.29	-0.76	-0.76	-0.29	-0.29	
-0.29	-0.29	0.27	-0.76	-0.29	
-0.29	-0.29	-0.29	-0.29	-0.29	
-0.29	-0.29	-0.29	-0.29	-0.29	
Map policy
v	v	v	<	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.06488583 -0.2690748   0.62283551  0.01219746]
mean reward
0.06	0.06	0.06	-0.27	0.06	
0.06	-0.27	-0.27	0.06	0.06	
0.06	0.06	0.62	-0.27	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
v	v	^	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	<	<	
features
0 	0 	0 	1 	0 	
0 	1 	1 	0 	0 	
0 	0 	2 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	v	<	v	
v	v	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.11340254  0.475       8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	<	v	
v	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-12.76	-12.76	-12.76	-13.50	-12.76	
-12.76	-13.50	-13.50	-12.76	-12.76	
-12.76	-12.76	-12.65	-13.50	-12.76	
-12.76	-12.76	-12.76	-12.76	-12.76	
-12.76	-12.76	-12.76	-12.76	-12.76	
ird policy
v	>	^	^	v	
v	>	<	>	v	
>	>	<	<	v	
>	>	^	<	<	
>	^	^	<	<	
MAP policy loss 18.16579911100385
mean policy loss 0.4501218847365367
robust policy loss 0.45012210647167217
regret policy loss 1.3161144309459398
ird policy loss 5.327034091617833e-10
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  29
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (12, 0), (3, 0), (6, 3), (12, 3), (17, 2), (11, 1), (2, 3), (13, 0), (4, 0)]
w_map [-0.9565059   0.17021131 -0.18945524  0.14223675] loglik -9.010913347278347
accepted/total = 1887/2000 = 0.9435
MAP Policy on Train MDP
map_weights [-0.9565059   0.17021131 -0.18945524  0.14223675]
map reward
-0.96	-0.96	-0.96	-0.96	-0.96	
-0.96	-0.96	-0.96	-0.96	-0.96	
-0.96	-0.96	-0.19	-0.96	-0.96	
-0.96	-0.96	-0.96	-0.96	-0.96	
-0.96	-0.96	-0.96	-0.96	-0.96	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	^	<	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.27290624  0.10575377  0.34322724  0.09797549]
mean reward
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	0.34	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
mean policy
>	v	v	v	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.3926012215881206e-10
Mean policy loss 5.421665122602205e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	v	
>	>	v	<	<	
^	<	>	<	<	
<	>	>	^	<	
>	>	>	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-100.00	1.00	-1.00	-1.00	
-100.00	-1.00	-5.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	2 	0 	0 	
3 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.9565059   0.17021131 -0.18945524  0.14223675]
map reward
-0.96	-0.96	-0.96	-0.96	-0.96	
-0.96	-0.96	-0.96	-0.96	-0.96	
0.17	0.14	-0.19	-0.96	-0.96	
0.14	-0.96	0.17	-0.96	0.17	
-0.96	-0.96	-0.96	-0.96	-0.96	
Map policy
v	v	v	<	v	
v	v	v	v	v	
<	<	<	<	v	
^	^	^	>	>	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.27290624  0.10575377  0.34322724  0.09797549]
mean reward
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
0.11	0.10	0.34	-0.27	-0.27	
0.10	-0.27	0.11	-0.27	0.11	
-0.27	-0.27	-0.27	-0.27	-0.27	
mean policy
v	v	v	v	<	
v	v	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	<	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	3 	2 	0 	0 	
3 	0 	1 	0 	1 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
^	>	^	<	<	
^	^	^	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	v	v	
>	>	v	v	<	
^	>	^	<	<	
>	^	^	^	^	
>	>	>	^	<	
-------- IRD Solution -------
ird reward
-12.64	-12.64	-12.64	-12.64	-12.64	
-12.64	-12.64	-12.64	-12.64	-12.64	
-13.26	-13.22	-12.35	-12.64	-12.64	
-13.22	-12.64	-13.26	-12.64	-13.26	
-12.64	-12.64	-12.64	-12.64	-12.64	
ird policy
>	>	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	v	^	^	<	
>	>	>	^	<	
MAP policy loss 238.64023468946323
mean policy loss 100.90254519024924
robust policy loss 473.32618287108556
regret policy loss 6.713560473050455
ird policy loss -2.251656514972583e-08
MAP lava occupancy 2.317147433310021
Mean lava occupancy 2.317147433310021
Robust lava occupancy 4.586436337314685
Regret lava occupancy 0.06780337028851592
IRD lava occupancy 1.148258553012488e-12
##############
Trial  30
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (8, 3), (12, 1), (3, 3), (12, 0), (4, 0), (12, 3), (17, 2), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [-0.0969213   0.36388262  0.7409369   0.55606494] loglik -9.010911223295182
accepted/total = 1763/2000 = 0.8815
MAP Policy on Train MDP
map_weights [-0.0969213   0.36388262  0.7409369   0.55606494]
map reward
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
-0.10	-0.10	0.74	-0.10	-0.10	
-0.10	-0.10	-0.10	0.36	-0.10	
-0.10	-0.10	-0.10	-0.10	-0.10	
Map policy
v	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	^	<	
>	>	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.15433812 -0.23242625  0.51758762 -0.07136476]
mean reward
-0.15	-0.15	-0.15	-0.15	-0.15	
-0.15	-0.15	-0.15	-0.15	-0.15	
-0.15	-0.15	0.52	-0.15	-0.15	
-0.15	-0.15	-0.15	-0.23	-0.15	
-0.15	-0.15	-0.15	-0.15	-0.15	
mean policy
>	v	v	v	<	
>	>	v	v	<	
>	>	v	<	<	
>	>	^	>	^	
^	^	^	<	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	^	
MAP policy loss 0.9025004185704348
Mean policy loss 2.1251272443179037e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	<	
>	>	<	<	v	
^	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-5.00	-1.00	1.00	-5.00	-5.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-5.00	-1.00	
features
0 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	2 	1 	1 	
3 	0 	0 	0 	0 	
0 	1 	1 	1 	0 	
MAP on testing env
map_weights [-0.0969213   0.36388262  0.7409369   0.55606494]
map reward
-0.10	0.56	-0.10	-0.10	-0.10	
-0.10	-0.10	-0.10	-0.10	0.36	
0.36	-0.10	0.74	0.36	0.36	
0.56	-0.10	-0.10	-0.10	-0.10	
-0.10	0.36	0.36	0.36	-0.10	
Map policy
>	^	<	<	v	
v	^	v	v	v	
v	>	>	<	<	
<	^	^	^	^	
^	v	^	<	<	
MEAN policy on test env
mean_weights [-0.15433812 -0.23242625  0.51758762 -0.07136476]
mean reward
-0.15	-0.07	-0.15	-0.15	-0.15	
-0.15	-0.15	-0.15	-0.15	-0.23	
-0.23	-0.15	0.52	-0.23	-0.23	
-0.07	-0.15	-0.15	-0.15	-0.15	
-0.15	-0.23	-0.23	-0.23	-0.15	
mean policy
>	v	v	v	<	
v	v	v	<	<	
>	>	<	<	<	
>	>	^	<	<	
^	^	^	<	^	
features
0 	3 	0 	0 	0 	
0 	0 	0 	0 	1 	
1 	0 	2 	1 	1 	
3 	0 	0 	0 	0 	
0 	1 	1 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	v	v	v	
v	>	v	v	v	
>	>	>	<	<	
^	>	^	^	^	
^	>	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	v	v	<	<	
>	>	<	^	<	
>	^	^	<	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-13.04	-13.55	-13.04	-13.04	-13.04	
-13.04	-13.04	-13.04	-13.04	-13.81	
-13.81	-13.04	-12.99	-13.81	-13.81	
-13.55	-13.04	-13.04	-13.04	-13.04	
-13.04	-13.81	-13.81	-13.81	-13.04	
ird policy
v	>	v	v	<	
>	v	v	<	<	
>	>	v	<	v	
>	^	^	<	<	
<	^	^	^	^	
MAP policy loss 32.98825784248041
mean policy loss 46.07499999719859
robust policy loss 240.8656743070394
regret policy loss 11.249901072439805
ird policy loss 3.226955129319432
MAP lava occupancy 0.2313446032953541
Mean lava occupancy 0.2313446032953541
Robust lava occupancy 2.080234588486083
Regret lava occupancy 0.11842001129302039
IRD lava occupancy 1.736327282789544e-11
##############
Trial  31
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (6, 1), (17, 2), (13, 0), (11, 1), (2, 3), (5, 1), (0, 3), (12, 3), (4, 0)]
w_map [ 0.04434018  0.0397591   0.99060676 -0.1230911 ] loglik -8.317761094597927
accepted/total = 1639/2000 = 0.8195
MAP Policy on Train MDP
map_weights [ 0.04434018  0.0397591   0.99060676 -0.1230911 ]
map reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.99	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
Map policy
v	v	v	v	<	
>	v	v	v	v	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.10222198 -0.37433227  0.58139702 -0.26477973]
mean reward
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.10	0.10	-0.37	
0.10	0.10	0.58	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
0.10	0.10	0.10	0.10	0.10	
mean policy
v	v	v	v	<	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	^	
>	^	^	^	^	
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 2.115719186276954e-06
Mean policy loss 2.1600450594849718e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	>	
>	>	<	<	<	
^	>	^	^	^	
>	^	^	<	^	
reward
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.04434018  0.0397591   0.99060676 -0.1230911 ]
map reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	-0.12	
0.04	0.04	0.99	0.04	0.04	
0.04	0.04	0.04	-0.12	0.04	
0.04	0.04	0.04	0.04	0.04	
Map policy
v	v	v	v	<	
>	>	v	v	v	
>	>	>	<	<	
^	>	^	^	^	
>	^	^	<	^	
MEAN policy on test env
mean_weights [ 0.10222198 -0.37433227  0.58139702 -0.26477973]
mean reward
0.10	-0.37	0.10	0.10	0.10	
0.10	0.10	0.10	0.10	-0.26	
0.10	0.10	0.58	0.10	0.10	
-0.37	0.10	0.10	-0.26	0.10	
0.10	0.10	0.10	0.10	0.10	
mean policy
v	>	v	v	<	
>	>	v	<	v	
>	>	>	<	<	
>	^	^	<	^	
>	^	^	<	^	
features
0 	1 	0 	0 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	v	v	<	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	^	^	
>	^	^	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
^	^	^	<	^	
>	^	^	<	^	
-------- IRD Solution -------
ird reward
-13.24	-13.87	-13.24	-13.24	-13.24	
-13.24	-13.24	-13.24	-13.24	-13.87	
-13.24	-13.24	-13.17	-13.24	-13.24	
-13.87	-13.24	-13.24	-13.87	-13.24	
-13.24	-13.24	-13.24	-13.24	-13.24	
ird policy
v	^	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
<	>	^	<	^	
>	^	^	<	^	
MAP policy loss 12.446901659809988
mean policy loss 3.0163365270508002e-06
robust policy loss 1.4153782835414639
regret policy loss 9.498851595568514e-08
ird policy loss 1.1524357509951066e-09
MAP lava occupancy 0.11618801105670812
Mean lava occupancy 0.11618801105670812
Robust lava occupancy 0.014296749771308171
Regret lava occupancy 8.841532512854918e-10
IRD lava occupancy 9.336685312503637e-12
##############
Trial  32
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.12805261  0.38433414  0.89186283 -0.2011728 ] loglik -9.704053473354406
accepted/total = 1805/2000 = 0.9025
MAP Policy on Train MDP
map_weights [-0.12805261  0.38433414  0.89186283 -0.2011728 ]
map reward
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.13	
-0.13	-0.13	0.89	-0.13	-0.13	
-0.13	0.38	-0.13	-0.13	-0.13	
-0.13	-0.13	0.38	-0.13	-0.13	
Map policy
v	v	v	v	v	
v	v	v	v	v	
>	>	v	<	<	
>	^	^	v	<	
>	>	v	<	<	
MEAN policy on Train MDP
mean_weights [ 0.04181196 -0.28562207  0.68476307 -0.14611694]
mean reward
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
0.04	0.04	0.68	0.04	0.04	
0.04	-0.29	0.04	0.04	0.04	
0.04	0.04	-0.29	0.04	0.04	
mean policy
>	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	^	<	
^	v	v	^	^	
MAP policy loss 44.45391036235329
Mean policy loss 1.5654154916777685e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	>	v	
^	>	v	>	v	
<	<	^	>	v	
^	<	^	<	<	
>	>	^	^	<	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-100.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	2 	3 	0 	
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.12805261  0.38433414  0.89186283 -0.2011728 ]
map reward
-0.13	-0.13	-0.13	-0.20	-0.13	
-0.20	-0.13	-0.13	0.38	-0.13	
-0.13	-0.20	0.89	-0.20	-0.13	
-0.13	-0.20	-0.13	-0.13	-0.13	
-0.13	-0.13	-0.13	-0.13	-0.13	
Map policy
>	v	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
^	^	^	<	<	
>	>	^	^	<	
MEAN policy on test env
mean_weights [ 0.04181196 -0.28562207  0.68476307 -0.14611694]
mean reward
0.04	0.04	0.04	-0.15	0.04	
-0.15	0.04	0.04	-0.29	0.04	
0.04	-0.15	0.68	-0.15	0.04	
0.04	-0.15	0.04	0.04	0.04	
0.04	0.04	0.04	0.04	0.04	
mean policy
>	>	v	v	v	
>	>	v	<	v	
>	>	^	<	<	
^	>	^	<	<	
>	>	^	^	<	
features
0 	0 	0 	3 	0 	
3 	0 	0 	1 	0 	
0 	3 	2 	3 	0 	
0 	3 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	v	v	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	<	<	
>	>	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	v	
>	>	v	<	v	
>	>	^	<	v	
^	>	^	<	<	
>	>	^	^	<	
-------- IRD Solution -------
ird reward
-13.10	-13.10	-13.10	-13.84	-13.10	
-13.84	-13.10	-13.10	-13.66	-13.10	
-13.10	-13.84	-13.01	-13.84	-13.10	
-13.10	-13.84	-13.10	-13.10	-13.10	
-13.10	-13.10	-13.10	-13.10	-13.10	
ird policy
>	>	v	^	v	
<	>	v	<	v	
<	v	^	<	v	
^	^	^	<	<	
>	>	^	<	<	
MAP policy loss 249.5792392936813
mean policy loss 20.812778114732993
robust policy loss 100.20209122170905
regret policy loss 6.751146809710965
ird policy loss -2.5534958609380043e-09
MAP lava occupancy 2.4358479562296234
Mean lava occupancy 2.4358479562296234
Robust lava occupancy 1.0071398420571749
Regret lava occupancy 0.06826532019634743
IRD lava occupancy 8.120856676313738e-11
##############
Trial  33
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	v	
>	>	^	<	>	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (9, 0), (12, 0), (8, 0), (6, 3), (17, 2), (11, 1), (4, 3), (12, 3)]
w_map [-0.30282377 -0.27475491 -0.68032459  0.60824827] loglik -7.586746461890382
accepted/total = 1140/2000 = 0.57
MAP Policy on Train MDP
map_weights [-0.30282377 -0.27475491 -0.68032459  0.60824827]
map reward
-0.30	-0.30	-0.30	-0.27	-0.30	
-0.30	-0.30	-0.30	-0.30	-0.30	
-0.30	-0.30	-0.68	-0.27	-0.30	
-0.27	-0.30	-0.30	-0.27	-0.30	
-0.30	-0.30	-0.30	-0.30	-0.30	
Map policy
>	>	>	^	<	
v	>	>	^	<	
v	v	>	v	<	
<	<	>	^	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.50127393 -0.17604504 -0.55758081 -0.13432415]
mean reward
-0.50	-0.50	-0.50	-0.18	-0.50	
-0.50	-0.50	-0.50	-0.50	-0.50	
-0.50	-0.50	-0.56	-0.18	-0.50	
-0.18	-0.50	-0.50	-0.18	-0.50	
-0.50	-0.50	-0.50	-0.50	-0.50	
mean policy
>	>	>	^	<	
v	v	>	^	<	
v	v	>	v	<	
<	<	>	^	<	
^	^	^	^	^	
Optimal Policy
v	v	v	v	v	
v	v	v	<	<	
>	>	<	<	v	
>	>	^	<	>	
>	^	^	<	<	
MAP policy loss 89.90531992391969
Mean policy loss 89.90532051281114
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	v	
>	>	v	<	<	
>	>	v	<	^	
>	^	^	<	<	
^	^	^	<	<	
reward
-1.00	-5.00	-100.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	1 	0 	
0 	1 	0 	0 	0 	
1 	0 	2 	3 	0 	
1 	0 	0 	0 	0 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.30282377 -0.27475491 -0.68032459  0.60824827]
map reward
-0.30	-0.27	0.61	-0.27	-0.30	
-0.30	-0.27	-0.30	-0.30	-0.30	
-0.27	-0.30	-0.68	0.61	-0.30	
-0.27	-0.30	-0.30	-0.30	-0.30	
-0.30	0.61	-0.30	-0.30	-0.30	
Map policy
>	>	^	<	<	
>	>	^	<	<	
>	v	^	^	<	
>	v	<	^	>	
>	v	<	<	<	
MEAN policy on test env
mean_weights [-0.50127393 -0.17604504 -0.55758081 -0.13432415]
mean reward
-0.50	-0.18	-0.13	-0.18	-0.50	
-0.50	-0.18	-0.50	-0.50	-0.50	
-0.18	-0.50	-0.56	-0.13	-0.50	
-0.18	-0.50	-0.50	-0.50	-0.50	
-0.50	-0.13	-0.50	-0.50	-0.50	
mean policy
>	>	^	<	<	
>	^	^	<	<	
v	<	<	<	^	
<	v	<	^	>	
>	v	<	<	<	
features
0 	1 	3 	1 	0 	
0 	1 	0 	0 	0 	
1 	0 	2 	3 	0 	
1 	0 	0 	0 	0 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	<	^	<	
v	^	<	^	<	
<	<	>	^	<	
^	<	^	^	^	
^	v	<	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	<	^	<	
v	^	v	^	<	
<	>	^	^	^	
<	<	<	<	^	
^	^	^	<	^	
-------- IRD Solution -------
ird reward
3.04	3.21	2.26	3.21	3.04	
3.04	3.21	3.04	3.04	3.04	
3.21	3.04	3.11	2.26	3.04	
3.21	3.04	3.04	3.04	3.04	
3.04	2.26	3.04	3.04	3.04	
ird policy
>	v	^	^	<	
v	^	<	^	<	
<	<	<	^	^	
^	<	<	<	^	
^	v	^	<	<	
MAP policy loss 299.86763189449755
mean policy loss 1804.8809454989353
robust policy loss 406.490933465598
regret policy loss 87.33093655761566
ird policy loss 87.3309393378338
MAP lava occupancy 2.3125079609590107
Mean lava occupancy 2.3125079609590107
Robust lava occupancy 3.343965217115581
Regret lava occupancy 1.3921897891160883e-08
IRD lava occupancy 8.925933451724703e-10
##############
Trial  34
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	<	
^	>	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	1 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (8, 3), (12, 1), (3, 3), (12, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.58051204 -0.06979389  0.40802404  0.70117827] loglik -11.307025234690137
accepted/total = 1180/2000 = 0.59
MAP Policy on Train MDP
map_weights [-0.58051204 -0.06979389  0.40802404  0.70117827]
map reward
-0.58	-0.58	-0.58	-0.58	-0.58	
-0.07	-0.07	-0.58	-0.58	-0.07	
-0.58	-0.58	0.41	-0.58	-0.58	
-0.58	-0.58	-0.58	-0.58	-0.58	
-0.58	-0.58	-0.58	-0.58	-0.58	
Map policy
v	v	>	v	v	
>	<	v	>	>	
^	>	>	<	^	
^	>	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.62110857 -0.16160116 -0.08449689  0.02522137]
mean reward
-0.62	-0.62	-0.62	-0.62	-0.62	
-0.16	-0.16	-0.62	-0.62	-0.16	
-0.62	-0.62	-0.08	-0.62	-0.62	
-0.62	-0.62	-0.62	-0.62	-0.62	
-0.62	-0.62	-0.62	-0.62	-0.62	
mean policy
v	v	>	v	v	
>	<	>	>	>	
^	^	^	^	^	
^	^	^	^	^	
^	^	^	v	^	
Optimal Policy
>	>	v	v	<	
^	>	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 89.00282002942369
Mean policy loss 89.00282051075108
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	<	
>	>	v	<	<	
>	>	v	<	<	
^	>	^	^	<	
^	<	^	^	^	
reward
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-100.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-100.00	-1.00	-1.00	
features
0 	1 	1 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	1 	
0 	3 	0 	1 	0 	
0 	1 	3 	0 	0 	
MAP on testing env
map_weights [-0.58051204 -0.06979389  0.40802404  0.70117827]
map reward
-0.58	-0.07	-0.07	-0.58	-0.58	
-0.58	-0.58	-0.58	-0.07	-0.58	
-0.58	-0.58	0.41	-0.58	-0.07	
-0.58	0.70	-0.58	-0.07	-0.58	
-0.58	-0.07	0.70	-0.58	-0.58	
Map policy
>	v	v	v	<	
v	v	v	v	v	
v	v	v	<	<	
>	v	v	v	<	
>	>	v	<	<	
MEAN policy on test env
mean_weights [-0.62110857 -0.16160116 -0.08449689  0.02522137]
mean reward
-0.62	-0.16	-0.16	-0.62	-0.62	
-0.62	-0.62	-0.62	-0.16	-0.62	
-0.62	-0.62	-0.08	-0.62	-0.16	
-0.62	0.03	-0.62	-0.16	-0.62	
-0.62	-0.16	0.03	-0.62	-0.62	
mean policy
>	v	v	v	<	
v	v	v	v	v	
v	v	v	<	<	
>	v	v	<	<	
>	>	v	<	<	
features
0 	1 	1 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	1 	
0 	3 	0 	1 	0 	
0 	1 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	^	^	<	<	
>	^	v	<	v	
>	>	<	<	>	
>	v	^	<	^	
>	v	v	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	>	<	<	<	
v	>	v	<	v	
>	>	<	<	>	
>	v	^	^	^	
>	v	v	<	^	
-------- IRD Solution -------
ird reward
1.03	1.49	1.49	1.03	1.03	
1.03	1.03	1.03	1.49	1.03	
1.03	1.03	1.64	1.03	1.49	
1.03	0.68	1.03	1.49	1.03	
1.03	1.49	0.68	1.03	1.03	
ird policy
>	^	^	<	<	
>	^	v	v	v	
>	>	>	>	>	
^	v	^	^	^	
>	v	<	v	^	
MAP policy loss 15.805321341244355
mean policy loss 1659.4468497237585
robust policy loss 540.4371024464906
regret policy loss 124.88868862785435
ird policy loss 89.00282051407761
MAP lava occupancy 8.161170387341664e-09
Mean lava occupancy 8.161170387341664e-09
Robust lava occupancy 4.751939983154729
Regret lava occupancy 0.3777459803154211
IRD lava occupancy 1.395275423072367e-11
##############
Trial  35
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 0), (6, 1), (12, 3), (17, 2), (2, 3), (13, 0), (4, 0)]
w_map [-0.07192596 -0.07729881 -0.97829999 -0.17827136] loglik -12.112996922248016
accepted/total = 23/2000 = 0.0115
MAP Policy on Train MDP
map_weights [-0.07192596 -0.07729881 -0.97829999 -0.17827136]
map reward
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.08	-0.98	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
Map policy
>	>	v	v	v	
>	>	>	v	v	
<	<	<	v	<	
>	>	>	^	^	
>	>	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.07045062 -0.08099486 -0.9347741  -0.27548257]
mean reward
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.08	-0.93	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
-0.07	-0.07	-0.07	-0.07	-0.07	
mean policy
>	>	v	v	v	
>	>	>	v	v	
<	<	<	v	<	
>	>	>	^	^	
>	>	^	^	^	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	^	<	
>	>	^	^	^	
MAP policy loss 16.707824790595282
Mean policy loss 16.70782149425424
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	^	v	
>	v	v	<	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-100.00	-5.00	
-5.00	-1.00	1.00	-5.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	1 	3 	0 	0 	
0 	0 	1 	3 	1 	
1 	0 	2 	1 	1 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.07192596 -0.07729881 -0.97829999 -0.17827136]
map reward
-0.07	-0.08	-0.18	-0.07	-0.07	
-0.07	-0.07	-0.08	-0.18	-0.08	
-0.08	-0.07	-0.98	-0.08	-0.08	
-0.07	-0.07	-0.07	-0.07	-0.08	
-0.07	-0.07	-0.07	-0.08	-0.07	
Map policy
v	<	>	^	<	
>	v	<	^	^	
^	v	^	v	v	
>	>	>	<	v	
>	>	^	>	>	
MEAN policy on test env
mean_weights [-0.07045062 -0.08099486 -0.9347741  -0.27548257]
mean reward
-0.07	-0.08	-0.28	-0.07	-0.07	
-0.07	-0.07	-0.08	-0.28	-0.08	
-0.08	-0.07	-0.93	-0.08	-0.08	
-0.07	-0.07	-0.07	-0.07	-0.08	
-0.07	-0.07	-0.07	-0.08	-0.07	
mean policy
v	<	>	^	<	
>	v	<	^	^	
^	v	^	v	^	
>	>	>	<	<	
>	>	^	^	>	
features
0 	1 	3 	0 	0 	
0 	0 	1 	3 	1 	
1 	0 	2 	1 	1 	
0 	0 	0 	0 	1 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	^	<	<	
>	v	^	^	^	
^	v	^	^	^	
>	>	>	<	<	
>	>	^	>	>	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	>	^	<	
>	v	<	^	<	
^	v	^	^	^	
>	>	>	<	v	
>	>	v	>	>	
-------- IRD Solution -------
ird reward
6.48	6.46	5.85	6.48	6.48	
6.48	6.48	6.46	5.85	6.46	
6.46	6.48	5.70	6.46	6.46	
6.48	6.48	6.48	6.48	6.46	
6.48	6.48	6.48	6.46	6.48	
ird policy
v	<	^	^	<	
>	v	<	^	^	
^	v	>	>	^	
>	>	>	<	<	
>	>	^	<	>	
MAP policy loss 216.13984801459253
mean policy loss 13.047945856980778
robust policy loss 595.1419997807761
regret policy loss 53.52862717816701
ird policy loss 13.047945704285507
MAP lava occupancy 1.7868789177299909
Mean lava occupancy 1.7868789177299909
Robust lava occupancy 5.876836620854549
Regret lava occupancy 0.4088957491387156
IRD lava occupancy 5.610398398176107e-10
##############
Trial  36
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	^	v	
v	v	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (12, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (1, 1), (13, 0)]
w_map [-0.37030651 -0.37744122  0.84865182 -0.01418779] loglik -6.9314717843809035
accepted/total = 1676/2000 = 0.838
MAP Policy on Train MDP
map_weights [-0.37030651 -0.37744122  0.84865182 -0.01418779]
map reward
-0.37	-0.37	-0.37	-0.38	-0.37	
-0.37	-0.38	-0.37	-0.37	-0.37	
-0.37	-0.37	0.85	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	-0.37	
Map policy
>	>	v	<	v	
v	>	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.0903929  -0.38694983  0.65742902 -0.03983784]
mean reward
0.09	0.09	0.09	-0.39	0.09	
0.09	-0.39	0.09	0.09	0.09	
0.09	0.09	0.66	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
mean policy
>	>	v	<	v	
v	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	>	v	^	v	
v	v	v	v	<	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 2.961847555410513e-10
Mean policy loss 2.4958175604342103e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	<	v	v	
v	v	v	v	<	
>	>	^	<	<	
>	^	^	>	^	
>	^	v	>	^	
reward
-1.00	-1.00	-100.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
features
0 	0 	3 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	0 	
MAP on testing env
map_weights [-0.37030651 -0.37744122  0.84865182 -0.01418779]
map reward
-0.37	-0.37	-0.01	-0.38	-0.37	
-0.38	-0.38	-0.37	-0.37	-0.37	
-0.38	-0.37	0.85	-0.37	-0.37	
-0.37	-0.37	-0.38	-0.01	-0.37	
-0.37	-0.37	-0.37	-0.01	-0.37	
Map policy
>	>	v	<	<	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.0903929  -0.38694983  0.65742902 -0.03983784]
mean reward
0.09	0.09	-0.04	-0.39	0.09	
-0.39	-0.39	0.09	0.09	0.09	
-0.39	0.09	0.66	0.09	0.09	
0.09	0.09	-0.39	-0.04	0.09	
0.09	0.09	0.09	-0.04	0.09	
mean policy
>	>	v	<	v	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	^	^	
>	^	^	^	^	
features
0 	0 	3 	1 	0 	
1 	1 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	1 	3 	0 	
0 	0 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	v	
^	>	v	<	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	>	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
>	^	<	^	^	
>	^	<	<	^	
-------- IRD Solution -------
ird reward
-13.17	-13.17	-13.74	-13.92	-13.17	
-13.92	-13.92	-13.17	-13.17	-13.17	
-13.92	-13.17	-13.12	-13.17	-13.17	
-13.17	-13.17	-13.92	-13.74	-13.17	
-13.17	-13.17	-13.17	-13.74	-13.17	
ird policy
<	^	v	v	v	
v	>	v	v	<	
>	>	^	<	<	
>	^	^	^	^	
^	^	v	<	^	
MAP policy loss 46.494899444404076
mean policy loss 21.434375161711493
robust policy loss 67.28375002441145
regret policy loss 8.952106657536696
ird policy loss 3.2744602544873205
MAP lava occupancy 0.37971440745295904
Mean lava occupancy 0.37971440745295904
Robust lava occupancy 0.6887500002356723
Regret lava occupancy 0.0942327014573661
IRD lava occupancy 5.479148143326634e-08
##############
Trial  37
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 1), (8, 3), (12, 0), (12, 3), (17, 2), (11, 1), (2, 3), (4, 3), (1, 1), (13, 0)]
w_map [-0.90772504  0.33774254 -0.09339335 -0.23074427] loglik -9.010913347278347
accepted/total = 1911/2000 = 0.9555
MAP Policy on Train MDP
map_weights [-0.90772504  0.33774254 -0.09339335 -0.23074427]
map reward
-0.91	-0.91	-0.91	-0.91	-0.91	
-0.91	-0.91	-0.91	-0.91	-0.91	
-0.91	-0.91	-0.09	-0.91	-0.91	
-0.91	-0.91	-0.91	-0.91	-0.91	
-0.91	-0.91	-0.91	-0.91	-0.91	
Map policy
v	v	v	v	<	
>	>	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [-0.36673896 -0.1914406   0.23741067  0.02286624]
mean reward
-0.37	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	0.24	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	-0.37	-0.37	-0.37	
mean policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.473665575900447e-10
Mean policy loss 5.463474200232288e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	<	v	
v	v	v	v	<	
>	>	<	<	<	
^	>	^	<	<	
>	^	^	<	^	
reward
-1.00	-5.00	-1.00	-100.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-100.00	-1.00	
features
0 	1 	0 	3 	0 	
1 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
MAP on testing env
map_weights [-0.90772504  0.33774254 -0.09339335 -0.23074427]
map reward
-0.91	0.34	-0.91	-0.23	-0.91	
0.34	0.34	-0.91	-0.91	-0.91	
-0.91	-0.91	-0.09	-0.91	-0.91	
0.34	-0.91	-0.91	-0.91	-0.91	
-0.91	-0.91	-0.91	-0.23	-0.91	
Map policy
v	v	<	<	<	
<	<	<	<	<	
v	^	<	<	<	
<	<	<	<	^	
^	^	<	<	<	
MEAN policy on test env
mean_weights [-0.36673896 -0.1914406   0.23741067  0.02286624]
mean reward
-0.37	-0.19	-0.37	0.02	-0.37	
-0.19	-0.19	-0.37	-0.37	-0.37	
-0.37	-0.37	0.24	-0.37	-0.37	
-0.19	-0.37	-0.37	-0.37	-0.37	
-0.37	-0.37	-0.37	0.02	-0.37	
mean policy
>	>	>	^	<	
>	>	v	^	<	
^	>	>	<	^	
<	^	^	v	^	
>	>	>	v	<	
features
0 	1 	0 	3 	0 	
1 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	^	<	
>	<	v	<	v	
^	>	v	<	<	
<	^	^	^	^	
^	^	^	v	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	<	v	
v	>	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	^	<	^	
-------- IRD Solution -------
ird reward
-11.89	-12.77	-11.89	-12.43	-11.89	
-12.77	-12.77	-11.89	-11.89	-11.89	
-11.89	-11.89	-11.66	-11.89	-11.89	
-12.77	-11.89	-11.89	-11.89	-11.89	
-11.89	-11.89	-11.89	-12.43	-11.89	
ird policy
>	>	v	<	v	
v	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
>	^	^	^	^	
MAP policy loss 24.508450885410134
mean policy loss 1806.0090703682683
robust policy loss 686.3741512681031
regret policy loss 2.3445276568182836
ird policy loss 1.0450152494179132e-06
MAP lava occupancy 0.23750000057415985
Mean lava occupancy 0.23750000057415985
Robust lava occupancy 6.594444783846594
Regret lava occupancy 0.02368209751483259
IRD lava occupancy 8.797576389220852e-09
##############
Trial  38
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
^	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (12, 0), (3, 0), (6, 3), (17, 2), (11, 1), (2, 3), (13, 0), (12, 3), (4, 0)]
w_map [-0.24192714 -0.24188153 -0.81049851  0.47545426] loglik -13.854966063331347
accepted/total = 1308/2000 = 0.654
MAP Policy on Train MDP
map_weights [-0.24192714 -0.24188153 -0.81049851  0.47545426]
map reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.81	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
Map policy
v	<	<	<	<	
v	v	<	<	<	
<	<	<	>	>	
^	v	<	<	>	
>	v	<	<	<	
MEAN policy on Train MDP
mean_weights [-0.17796521 -0.52178719 -0.57548034 -0.13502989]
mean reward
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.52	-0.18	-0.58	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.52	-0.18	-0.18	-0.18	
mean policy
>	>	v	v	v	
>	>	>	v	v	
^	^	<	>	<	
>	>	>	^	^	
^	^	v	^	^	
Optimal Policy
>	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	^	^	<	<	
^	^	^	<	<	
MAP policy loss 85.60618299883984
Mean policy loss 16.707820521600823
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	^	
<	<	^	<	^	
^	v	^	^	^	
>	<	>	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-100.00	1.00	-5.00	-1.00	
-100.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	2 	1 	0 	
3 	3 	3 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [-0.24192714 -0.24188153 -0.81049851  0.47545426]
map reward
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	-0.24	-0.24	-0.24	-0.24	
-0.24	0.48	-0.81	-0.24	-0.24	
0.48	0.48	0.48	-0.24	-0.24	
-0.24	-0.24	0.48	-0.24	-0.24	
Map policy
v	v	v	v	v	
v	v	<	v	<	
>	v	v	v	<	
>	>	<	<	<	
^	>	^	<	<	
MEAN policy on test env
mean_weights [-0.17796521 -0.52178719 -0.57548034 -0.13502989]
mean reward
-0.18	-0.18	-0.18	-0.18	-0.18	
-0.18	-0.18	-0.18	-0.52	-0.18	
-0.18	-0.14	-0.58	-0.52	-0.18	
-0.14	-0.14	-0.14	-0.18	-0.18	
-0.18	-0.18	-0.14	-0.18	-0.18	
mean policy
v	v	v	<	<	
v	v	<	<	v	
v	v	v	v	v	
>	>	<	<	<	
^	^	^	<	<	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
0 	3 	2 	1 	0 	
3 	3 	3 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
>	>	<	v	v	
<	^	>	^	>	
^	v	>	>	<	
>	v	>	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	>	<	<	
>	<	^	>	>	
<	v	^	<	>	
^	v	<	>	<	
<	v	<	^	^	
-------- IRD Solution -------
ird reward
1.43	1.43	1.43	1.43	1.43	
1.43	1.43	1.43	0.85	1.43	
1.43	0.50	1.36	0.85	1.43	
0.50	0.50	0.50	1.43	1.43	
1.43	1.43	0.50	1.43	1.43	
ird policy
v	v	v	<	<	
v	>	<	<	>	
<	<	^	v	^	
>	>	v	>	^	
>	<	v	^	^	
MAP policy loss 42.534069333260156
mean policy loss 1736.16575533311
robust policy loss 294.45680777171077
regret policy loss 18.371488756539925
ird policy loss 11.756066995316193
MAP lava occupancy 0.30713842376199063
Mean lava occupancy 0.30713842376199063
Robust lava occupancy 2.855563014629238
Regret lava occupancy 0.06682244920860514
IRD lava occupancy 7.237264168304346e-09
##############
Trial  39
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	>	v	<	v	
v	v	v	v	v	
>	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (12, 3), (17, 2), (11, 1), (2, 3), (1, 1), (13, 0), (4, 0)]
w_map [ 0.06381496  0.05940253  0.32265551 -0.94249266] loglik -6.931132710764302
accepted/total = 1619/2000 = 0.8095
MAP Policy on Train MDP
map_weights [ 0.06381496  0.05940253  0.32265551 -0.94249266]
map reward
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.32	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
Map policy
v	>	v	<	v	
v	>	v	<	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.08627065 -0.34732383  0.59421615 -0.36845077]
mean reward
0.09	0.09	0.09	0.09	0.09	
0.09	-0.35	0.09	-0.35	0.09	
0.09	0.09	0.59	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
0.09	0.09	0.09	0.09	0.09	
mean policy
v	>	v	<	v	
v	<	v	>	v	
>	>	^	<	<	
>	^	^	^	<	
>	^	^	^	<	
Optimal Policy
v	>	v	<	v	
v	v	v	v	v	
>	>	^	<	<	
>	>	^	<	<	
>	^	^	^	<	
MAP policy loss 1.519089405432883e-06
Mean policy loss 1.267201912048821e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	<	<	<	<	
v	^	v	^	>	
>	>	v	<	v	
v	>	^	>	>	
>	^	^	<	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-100.00	-100.00	-5.00	-1.00	
-1.00	-5.00	1.00	-100.00	-100.00	
-100.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	3 	3 	1 	0 	
0 	1 	2 	3 	3 	
3 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [ 0.06381496  0.05940253  0.32265551 -0.94249266]
map reward
0.06	0.06	0.06	0.06	0.06	
0.06	-0.94	-0.94	0.06	0.06	
0.06	0.06	0.32	-0.94	-0.94	
-0.94	0.06	0.06	-0.94	0.06	
0.06	0.06	0.06	0.06	0.06	
Map policy
v	<	<	<	<	
v	v	v	^	<	
>	>	v	<	v	
>	>	^	<	>	
>	^	^	<	<	
MEAN policy on test env
mean_weights [ 0.08627065 -0.34732383  0.59421615 -0.36845077]
mean reward
0.09	0.09	0.09	0.09	0.09	
0.09	-0.37	-0.37	-0.35	0.09	
0.09	-0.35	0.59	-0.37	-0.37	
-0.37	0.09	0.09	-0.37	0.09	
0.09	0.09	0.09	0.09	0.09	
mean policy
v	v	v	<	<	
v	>	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
features
0 	0 	0 	0 	0 	
0 	3 	3 	1 	0 	
0 	1 	2 	3 	3 	
3 	0 	0 	3 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
v	>	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	v	<	<	
v	>	v	<	>	
>	>	v	v	v	
>	>	^	v	v	
>	^	^	<	<	
-------- IRD Solution -------
ird reward
-13.19	-13.19	-13.19	-13.19	-13.19	
-13.19	-13.97	-13.97	-13.79	-13.19	
-13.19	-13.79	-13.15	-13.97	-13.97	
-13.97	-13.19	-13.19	-13.97	-13.19	
-13.19	-13.19	-13.19	-13.19	-13.19	
ird policy
v	^	<	<	<	
v	>	v	^	>	
^	v	v	<	v	
>	>	^	<	>	
>	^	^	<	<	
MAP policy loss 481.7473350016587
mean policy loss 19.74689948179707
robust policy loss 40.10956824939339
regret policy loss 20.510609644300548
ird policy loss 6.02339887224502
MAP lava occupancy 4.8009123156023
Mean lava occupancy 4.8009123156023
Robust lava occupancy 0.42868765384258184
Regret lava occupancy 0.222382852842613
IRD lava occupancy 5.916955414096288e-12
##############
Trial  40
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	1 	0 	0 	
0 	1 	0 	0 	0 	
demonstration
[(0, 1), (12, 2), (7, 3), (1, 3), (12, 1), (12, 0), (3, 0), (6, 3), (11, 1), (2, 3), (13, 0), (4, 0)]
w_map [-0.32706096 -0.81789438  0.47027343  0.05406304] loglik -6.068425588238872
accepted/total = 1658/2000 = 0.829
MAP Policy on Train MDP
map_weights [-0.32706096 -0.81789438  0.47027343  0.05406304]
map reward
-0.33	-0.33	-0.33	-0.33	-0.33	
-0.82	-0.33	-0.33	-0.33	-0.33	
-0.33	-0.33	0.47	-0.33	-0.33	
-0.33	-0.33	-0.82	-0.33	-0.33	
-0.33	-0.82	-0.33	-0.33	-0.33	
Map policy
>	v	v	v	v	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.11682683 -0.37574748  0.60730198  0.1109076 ]
mean reward
0.12	0.12	0.12	0.12	0.12	
-0.38	0.12	0.12	0.12	0.12	
0.12	0.12	0.61	0.12	0.12	
0.12	0.12	-0.38	0.12	0.12	
0.12	-0.38	0.12	0.12	0.12	
mean policy
>	v	v	v	v	
>	v	v	v	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
Optimal Policy
>	v	v	v	<	
v	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	v	^	^	
MAP policy loss -9.467761991066581e-11
Mean policy loss 5.8637983004102e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	<	
>	>	v	<	<	
v	<	^	v	>	
>	>	^	<	v	
^	<	^	^	<	
reward
-1.00	-1.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-100.00	1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-100.00	-1.00	-1.00	-1.00	
features
0 	0 	3 	0 	0 	
0 	0 	0 	0 	1 	
0 	3 	2 	3 	0 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
MAP on testing env
map_weights [-0.32706096 -0.81789438  0.47027343  0.05406304]
map reward
-0.33	-0.33	0.05	-0.33	-0.33	
-0.33	-0.33	-0.33	-0.33	-0.82	
-0.33	0.05	0.47	0.05	-0.33	
-0.33	-0.33	-0.33	-0.33	0.05	
-0.33	0.05	-0.33	-0.33	-0.33	
Map policy
>	>	v	<	<	
v	v	v	v	<	
>	>	>	<	<	
^	^	^	^	<	
>	^	^	^	^	
MEAN policy on test env
mean_weights [ 0.11682683 -0.37574748  0.60730198  0.1109076 ]
mean reward
0.12	0.12	0.11	0.12	0.12	
0.12	0.12	0.12	0.12	-0.38	
0.12	0.11	0.61	0.11	0.12	
0.12	0.12	0.12	0.12	0.11	
0.12	0.11	0.12	0.12	0.12	
mean policy
v	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
^	>	^	^	<	
features
0 	0 	3 	0 	0 	
0 	0 	0 	0 	1 	
0 	3 	2 	3 	0 	
0 	0 	0 	0 	3 	
0 	3 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	<	v	
>	>	v	<	<	
>	>	^	<	^	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	^	<	
-------- IRD Solution -------
ird reward
-12.96	-12.96	-13.41	-12.96	-12.96	
-12.96	-12.96	-12.96	-12.96	-13.66	
-12.96	-13.41	-12.89	-13.41	-12.96	
-12.96	-12.96	-12.96	-12.96	-13.41	
-12.96	-13.41	-12.96	-12.96	-12.96	
ird policy
>	v	v	v	<	
>	>	v	<	>	
v	>	^	<	>	
>	>	^	<	^	
^	^	^	^	<	
MAP policy loss 609.4279633667301
mean policy loss 7.744171743669266e-06
robust policy loss 121.48530245476636
regret policy loss 0.6858037200011698
ird policy loss 2.1045936981567773e-07
MAP lava occupancy 6.151022328568677
Mean lava occupancy 6.151022328568677
Robust lava occupancy 1.2271242672115146
Regret lava occupancy 0.00692731025829562
IRD lava occupancy 1.977782720338637e-09
##############
Trial  41
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (9, 0), (12, 1), (8, 3), (12, 0), (6, 3), (12, 3), (17, 2), (11, 1), (4, 3), (5, 1), (0, 3), (13, 0)]
w_map [-0.00168717 -0.00835852 -0.65777861  0.75316306] loglik -15.178337493046662
accepted/total = 16/2000 = 0.008
MAP Policy on Train MDP
map_weights [-0.00168717 -0.00835852 -0.65777861  0.75316306]
map reward
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.01	-0.00	-0.66	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.01	
-0.00	-0.00	-0.00	-0.00	-0.00	
Map policy
>	>	v	v	v	
>	>	>	<	v	
v	^	>	>	<	
>	^	<	^	v	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.00160256 -0.02587832 -0.66583125  0.74374548]
mean reward
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
-0.03	-0.00	-0.67	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.03	
-0.00	-0.00	-0.00	-0.00	-0.00	
mean policy
>	>	v	v	v	
>	>	>	<	v	
^	^	>	>	<	
>	^	>	^	<	
>	^	^	<	<	
Optimal Policy
>	v	v	v	v	
>	v	v	v	v	
>	>	<	<	<	
>	>	^	<	<	
>	^	^	^	<	
MAP policy loss 16.70782520847594
Mean policy loss 16.707820517309102
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	>	^	v	
>	v	<	^	v	
^	>	v	<	<	
>	>	^	>	^	
>	^	^	<	<	
reward
-1.00	-5.00	-100.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-100.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	1 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.00168717 -0.00835852 -0.65777861  0.75316306]
map reward
-0.00	-0.01	0.75	-0.00	-0.00	
-0.00	-0.00	0.75	0.75	-0.00	
0.75	-0.00	-0.66	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.01	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
Map policy
v	>	^	<	<	
v	>	^	<	<	
<	<	^	^	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on test env
mean_weights [-0.00160256 -0.02587832 -0.66583125  0.74374548]
mean reward
-0.00	-0.03	0.74	-0.00	-0.00	
-0.00	-0.00	0.74	0.74	-0.00	
0.74	-0.00	-0.67	-0.00	-0.00	
-0.00	-0.00	-0.00	-0.03	-0.00	
-0.00	-0.00	-0.00	-0.00	-0.00	
mean policy
v	>	^	<	<	
v	>	^	<	<	
<	<	^	^	<	
^	^	^	^	^	
^	^	^	<	^	
features
0 	1 	3 	0 	0 	
0 	0 	3 	3 	0 	
3 	0 	2 	0 	0 	
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	^	<	<	
v	>	^	<	<	
<	<	^	^	<	
^	^	<	^	^	
^	^	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	>	^	<	<	
v	>	^	<	<	
<	<	^	^	<	
^	^	<	^	^	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
5.47	5.47	6.23	5.47	5.47	
5.47	5.47	6.23	6.23	5.47	
6.23	5.47	4.82	5.47	5.47	
5.47	5.47	5.47	5.47	5.47	
5.47	5.47	5.47	5.47	5.47	
ird policy
v	>	^	<	<	
v	>	^	<	<	
<	<	^	^	<	
^	^	^	^	^	
^	^	^	^	^	
MAP policy loss 288.82738876842217
mean policy loss 1760.100914275822
robust policy loss 1760.1009322069453
regret policy loss 1760.1009140738474
ird policy loss 1760.100914640841
MAP lava occupancy 2.6771248816957027
Mean lava occupancy 2.6771248816957027
Robust lava occupancy 17.610031410661456
Regret lava occupancy 17.610031248034524
IRD lava occupancy 17.610031250381816
##############
Trial  42
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-1.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	1 	0 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (12, 1), (3, 3), (12, 0), (8, 0), (6, 3), (17, 2), (13, 0), (11, 1), (5, 1), (0, 3), (12, 3), (4, 0)]
w_map [ 0.04725729 -0.01394792  0.89595047  0.44141246] loglik -8.317766166718684
accepted/total = 1591/2000 = 0.7955
MAP Policy on Train MDP
map_weights [ 0.04725729 -0.01394792  0.89595047  0.44141246]
map reward
0.05	0.05	-0.01	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
-0.01	0.05	0.90	0.05	-0.01	
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	0.05	0.05	
Map policy
v	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.11289941 -0.32237508  0.57996839 -0.00354716]
mean reward
0.11	0.11	-0.32	0.11	0.11	
0.11	0.11	0.11	0.11	0.11	
-0.32	0.11	0.58	0.11	-0.32	
0.11	0.11	0.11	0.11	0.11	
0.11	0.11	0.11	0.11	0.11	
mean policy
v	v	v	v	v	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	^	<	
>	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss -5.092364169234642e-10
Mean policy loss 3.093790778632699e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	<	<	
>	>	v	<	^	
>	>	v	<	>	
>	^	^	<	^	
>	>	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-100.00	
-100.00	-1.00	1.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-100.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	2 	3 	0 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
MAP on testing env
map_weights [ 0.04725729 -0.01394792  0.89595047  0.44141246]
map reward
0.05	0.05	0.05	0.05	0.05	
0.05	0.05	0.05	-0.01	0.44	
0.44	0.05	0.90	0.44	0.05	
-0.01	0.05	0.05	0.44	0.05	
0.05	-0.01	0.05	0.44	0.05	
Map policy
v	v	v	v	v	
v	>	v	<	v	
>	>	>	<	<	
^	>	^	^	<	
^	>	^	^	<	
MEAN policy on test env
mean_weights [ 0.11289941 -0.32237508  0.57996839 -0.00354716]
mean reward
0.11	0.11	0.11	0.11	0.11	
0.11	0.11	0.11	-0.32	-0.00	
-0.00	0.11	0.58	-0.00	0.11	
-0.32	0.11	0.11	-0.00	0.11	
0.11	-0.32	0.11	-0.00	0.11	
mean policy
>	v	v	<	<	
>	v	v	<	v	
>	>	v	<	<	
>	^	^	<	^	
>	>	^	^	^	
features
0 	0 	0 	0 	0 	
0 	0 	0 	1 	3 	
3 	0 	2 	3 	0 	
1 	0 	0 	3 	0 	
0 	1 	0 	3 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	<	
>	v	v	<	v	
>	>	<	<	<	
>	^	^	<	^	
>	>	^	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	<	
>	v	v	<	^	
^	>	v	<	<	
>	>	^	<	^	
>	>	^	<	^	
-------- IRD Solution -------
ird reward
-13.22	-13.22	-13.22	-13.22	-13.22	
-13.22	-13.22	-13.22	-13.90	-13.91	
-13.91	-13.22	-13.14	-13.91	-13.22	
-13.90	-13.22	-13.22	-13.91	-13.22	
-13.22	-13.90	-13.22	-13.91	-13.22	
ird policy
>	v	v	<	<	
>	v	v	<	^	
>	>	v	<	<	
>	>	^	^	^	
>	>	^	<	^	
MAP policy loss 255.28190613046968
mean policy loss 17.043076125544946
robust policy loss 17.043076121797878
regret policy loss 17.04307612091111
ird policy loss 17.043076143037382
MAP lava occupancy 2.611680416765473
Mean lava occupancy 2.611680416765473
Robust lava occupancy 0.21434375001115988
Regret lava occupancy 0.21434374999736364
IRD lava occupancy 0.21434374997166772
##############
Trial  43
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	^	v	
>	>	v	<	<	
>	>	<	<	<	
^	>	^	^	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
1 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (9, 0), (8, 3), (12, 0), (6, 1), (17, 2), (11, 1), (4, 3), (13, 0), (12, 3)]
w_map [ 0.44278132 -0.35046548  0.82306042 -0.06074704] loglik -8.317764605957336
accepted/total = 1689/2000 = 0.8445
MAP Policy on Train MDP
map_weights [ 0.44278132 -0.35046548  0.82306042 -0.06074704]
map reward
0.44	0.44	0.44	-0.35	0.44	
-0.35	0.44	0.44	0.44	0.44	
0.44	0.44	0.82	0.44	0.44	
0.44	0.44	0.44	0.44	0.44	
0.44	0.44	0.44	0.44	0.44	
Map policy
>	v	v	^	v	
<	v	v	<	<	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.12825835 -0.28610182  0.62609859  0.33178909]
mean reward
0.13	0.13	0.13	-0.29	0.13	
-0.29	0.13	0.13	0.13	0.13	
0.13	0.13	0.63	0.13	0.13	
0.13	0.13	0.13	0.13	0.13	
0.13	0.13	0.13	0.13	0.13	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	v	<	<	
^	^	^	<	<	
^	^	^	^	<	
Optimal Policy
>	v	v	^	v	
>	>	v	<	<	
>	>	<	<	<	
^	>	^	^	<	
^	^	^	^	<	
MAP policy loss 3.597133809707387e-07
Mean policy loss 2.037926771780718e-11
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	>	v	<	<	
>	>	<	<	>	
^	^	^	<	<	
^	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-5.00	-1.00	1.00	-100.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	3 	
1 	0 	2 	3 	0 	
0 	1 	1 	0 	0 	
0 	0 	3 	0 	0 	
MAP on testing env
map_weights [ 0.44278132 -0.35046548  0.82306042 -0.06074704]
map reward
0.44	0.44	0.44	-0.35	0.44	
0.44	0.44	0.44	0.44	-0.06	
-0.35	0.44	0.82	-0.06	0.44	
0.44	-0.35	-0.35	0.44	0.44	
0.44	0.44	-0.06	0.44	0.44	
Map policy
>	v	v	v	v	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	>	^	^	
MEAN policy on test env
mean_weights [ 0.12825835 -0.28610182  0.62609859  0.33178909]
mean reward
0.13	0.13	0.13	-0.29	0.13	
0.13	0.13	0.13	0.13	0.33	
-0.29	0.13	0.63	0.33	0.13	
0.13	-0.29	-0.29	0.13	0.13	
0.13	0.13	0.33	0.13	0.13	
mean policy
>	>	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	>	^	^	^	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	3 	
1 	0 	2 	3 	0 	
0 	1 	1 	0 	0 	
0 	0 	3 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
^	^	^	^	^	
>	>	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	>	v	<	<	
>	>	<	<	<	
^	^	^	^	^	
^	^	^	^	^	
-------- IRD Solution -------
ird reward
-13.28	-13.28	-13.28	-14.01	-13.28	
-13.28	-13.28	-13.28	-13.28	-13.97	
-14.01	-13.28	-13.23	-13.97	-13.28	
-13.28	-14.01	-14.01	-13.28	-13.28	
-13.28	-13.28	-13.97	-13.28	-13.28	
ird policy
>	>	v	<	>	
>	>	v	<	<	
<	>	<	<	>	
<	>	<	>	^	
^	v	^	^	^	
MAP policy loss 257.35289559443163
mean policy loss 872.1221971142513
robust policy loss 356.8871964921209
regret policy loss 36.994334212517764
ird policy loss 9.820991097965415
MAP lava occupancy 2.529815380166175
Mean lava occupancy 2.529815380166175
Robust lava occupancy 3.6236332976505077
Regret lava occupancy 0.38941404434375154
IRD lava occupancy 6.350759318957639e-09
##############
Trial  44
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-5.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	1 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.21956332 -0.22546003 -0.7111148  -0.62870936] loglik -12.355463776075737
accepted/total = 38/2000 = 0.019
MAP Policy on Train MDP
map_weights [-0.21956332 -0.22546003 -0.7111148  -0.62870936]
map reward
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.23	
-0.22	-0.22	-0.71	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
Map policy
v	v	v	v	<	
v	v	<	v	^	
>	v	>	>	<	
>	>	>	<	^	
>	>	^	<	^	
MEAN policy on Train MDP
mean_weights [-0.3249749  -0.3257187  -0.67765552 -0.49026095]
mean reward
-0.32	-0.32	-0.32	-0.32	-0.32	
-0.32	-0.32	-0.32	-0.32	-0.33	
-0.32	-0.32	-0.68	-0.32	-0.32	
-0.32	-0.32	-0.32	-0.32	-0.32	
-0.32	-0.32	-0.32	-0.32	-0.32	
mean policy
v	v	v	<	<	
v	v	<	v	v	
>	v	^	>	>	
>	>	>	<	<	
>	>	^	^	<	
Optimal Policy
v	v	v	v	<	
>	v	v	<	>	
>	>	>	<	<	
>	^	^	<	<	
>	^	^	^	^	
MAP policy loss 16.707825223509452
Mean policy loss 16.70782103707669
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	<	>	
v	v	v	<	v	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	^	
reward
-1.00	-1.00	-1.00	-100.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
MAP on testing env
map_weights [-0.21956332 -0.22546003 -0.7111148  -0.62870936]
map reward
-0.22	-0.22	-0.22	-0.63	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.63	
-0.22	-0.22	-0.71	-0.22	-0.22	
-0.23	-0.22	-0.22	-0.22	-0.22	
-0.22	-0.22	-0.22	-0.22	-0.22	
Map policy
>	>	v	<	>	
v	>	>	<	v	
<	^	>	^	<	
>	^	>	^	^	
>	^	^	^	^	
MEAN policy on test env
mean_weights [-0.3249749  -0.3257187  -0.67765552 -0.49026095]
mean reward
-0.32	-0.32	-0.32	-0.49	-0.32	
-0.32	-0.32	-0.32	-0.32	-0.49	
-0.32	-0.32	-0.68	-0.32	-0.32	
-0.33	-0.32	-0.32	-0.32	-0.32	
-0.32	-0.32	-0.32	-0.32	-0.32	
mean policy
>	>	v	<	>	
>	>	>	<	v	
>	<	v	^	<	
v	^	>	^	^	
>	^	^	^	^	
features
0 	0 	0 	3 	0 	
0 	0 	0 	0 	3 	
0 	0 	2 	0 	0 	
1 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	>	>	^	<	
v	>	>	>	>	
v	v	^	^	^	
<	<	>	^	^	
^	v	^	^	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	>	^	<	
v	v	<	>	>	
v	v	^	^	^	
<	<	<	<	^	
^	<	<	<	^	
-------- IRD Solution -------
ird reward
6.23	6.23	6.23	5.58	6.23	
6.23	6.23	6.23	6.23	5.58	
6.23	6.23	5.48	6.23	6.23	
6.10	6.23	6.23	6.23	6.23	
6.23	6.23	6.23	6.23	6.23	
ird policy
>	>	v	>	>	
>	>	>	<	^	
<	^	v	^	<	
v	^	>	^	^	
>	^	^	^	^	
MAP policy loss 115.45634856259767
mean policy loss 12.530873653919784
robust policy loss 1051.4362296608451
regret policy loss 680.3476001624817
ird policy loss 12.530866240298264
MAP lava occupancy 1.0029847680772395
Mean lava occupancy 1.0029847680772395
Robust lava occupancy 10.174847825888932
Regret lava occupancy 6.330993342066535
IRD lava occupancy 8.086392957542614e-09
##############
Trial  45
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (1, 3), (12, 1), (3, 3), (8, 3), (12, 0), (6, 1), (12, 3), (17, 2), (11, 1), (13, 0), (4, 0)]
w_map [-0.54156653  0.29203735  0.78676717 -0.04916607] loglik -9.70406051575329
accepted/total = 1891/2000 = 0.9455
MAP Policy on Train MDP
map_weights [-0.54156653  0.29203735  0.78676717 -0.04916607]
map reward
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	0.79	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.54	-0.54	
Map policy
>	v	v	v	v	
>	v	v	<	<	
>	>	<	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.26766562  0.02898035  0.3241367   0.02161084]
mean reward
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	0.32	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
mean policy
>	v	v	v	v	
>	v	v	<	<	
>	>	^	<	<	
>	^	^	^	<	
^	^	^	^	<	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.1111828947681256e-10
Mean policy loss 5.49740866530638e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	<	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	0 	
MAP on testing env
map_weights [-0.54156653  0.29203735  0.78676717 -0.04916607]
map reward
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	-0.54	0.79	-0.54	0.29	
-0.54	-0.54	-0.54	-0.54	-0.54	
-0.54	0.29	-0.54	0.29	-0.54	
Map policy
>	v	v	v	v	
>	>	v	v	v	
>	>	>	>	>	
<	v	^	v	^	
>	v	<	v	<	
MEAN policy on test env
mean_weights [-0.26766562  0.02898035  0.3241367   0.02161084]
mean reward
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	-0.27	0.32	-0.27	0.03	
-0.27	-0.27	-0.27	-0.27	-0.27	
-0.27	0.03	-0.27	0.03	-0.27	
mean policy
>	v	v	v	v	
v	v	v	v	v	
>	>	v	<	>	
>	>	^	v	^	
>	v	>	v	<	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	1 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
v	v	v	v	v	
>	>	^	<	<	
^	>	^	<	^	
>	v	^	v	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	<	^	
-------- IRD Solution -------
ird reward
-13.07	-13.07	-13.07	-13.07	-13.07	
-13.07	-13.07	-13.07	-13.07	-13.07	
-13.07	-13.07	-13.02	-13.07	-13.74	
-13.07	-13.07	-13.07	-13.07	-13.07	
-13.07	-13.74	-13.07	-13.74	-13.07	
ird policy
>	v	v	v	<	
>	v	v	v	<	
>	>	v	<	<	
>	^	^	^	<	
^	>	v	<	^	
MAP policy loss 1.3257947118525146
mean policy loss 68.58086531947171
robust policy loss 36.13777970015974
regret policy loss 0.192402146286676
ird policy loss 4.129136533387312e-08
MAP lava occupancy 0.0
Mean lava occupancy 0.0
Robust lava occupancy 0.0
Regret lava occupancy 0.0
IRD lava occupancy 0.0
##############
Trial  46
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (3, 0), (17, 2), (11, 1), (2, 3), (13, 0), (1, 1), (12, 3), (4, 0)]
w_map [-0.14462457 -0.64107537  0.70959334 -0.25413264] loglik -8.31776615915743
accepted/total = 1861/2000 = 0.9305
MAP Policy on Train MDP
map_weights [-0.14462457 -0.64107537  0.70959334 -0.25413264]
map reward
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.14	0.71	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
Map policy
>	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	^	^	<	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [-0.2982387  -0.10343393  0.26633095  0.18709188]
mean reward
-0.30	-0.30	-0.30	-0.30	-0.30	
-0.30	-0.30	-0.30	-0.30	-0.30	
-0.30	-0.30	0.27	-0.30	-0.30	
-0.30	-0.30	-0.30	-0.30	-0.30	
-0.30	-0.30	-0.30	-0.30	-0.30	
mean policy
>	v	v	v	v	
>	>	v	v	<	
>	>	<	<	<	
>	>	^	^	<	
>	^	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	<	<	
>	>	v	<	<	
>	^	^	<	<	
^	^	^	^	^	
MAP policy loss 4.5084221707465677e-10
Mean policy loss 5.540618439308503e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	v	v	v	v	
<	>	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-100.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
features
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
MAP on testing env
map_weights [-0.14462457 -0.64107537  0.70959334 -0.25413264]
map reward
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.25	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.64	0.71	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.14	-0.14	
-0.14	-0.14	-0.14	-0.64	-0.14	
Map policy
>	v	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
MEAN policy on test env
mean_weights [-0.2982387  -0.10343393  0.26633095  0.18709188]
mean reward
-0.30	-0.30	-0.30	-0.30	-0.30	
0.19	-0.30	-0.30	-0.30	-0.30	
-0.30	-0.10	0.27	-0.30	-0.30	
-0.30	-0.30	-0.30	-0.30	-0.30	
-0.30	-0.30	-0.30	-0.10	-0.30	
mean policy
v	v	<	<	<	
<	<	<	<	<	
^	^	<	<	<	
^	^	^	^	^	
^	^	^	^	<	
features
0 	0 	0 	0 	0 	
3 	0 	0 	0 	0 	
0 	1 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	0 	1 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	<	<	
<	<	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	v	v	
>	>	v	<	<	
>	>	>	<	<	
>	>	^	<	<	
>	>	^	^	^	
-------- IRD Solution -------
ird reward
-12.49	-12.49	-12.49	-12.49	-12.49	
-13.43	-12.49	-12.49	-12.49	-12.49	
-12.49	-13.20	-12.40	-12.49	-12.49	
-12.49	-12.49	-12.49	-12.49	-12.49	
-12.49	-12.49	-12.49	-13.20	-12.49	
ird policy
>	v	v	v	<	
<	>	v	<	<	
<	>	v	<	<	
>	>	^	^	<	
>	>	^	<	^	
MAP policy loss 21.02483982916164
mean policy loss 1641.3734989503953
robust policy loss 529.2241152599495
regret policy loss 4.337291393571115
ird policy loss 4.523370294995388e-08
MAP lava occupancy 0.118750000021279
Mean lava occupancy 0.118750000021279
Robust lava occupancy 5.101436911374164
Regret lava occupancy 0.04285778691923586
IRD lava occupancy 2.532869376297982e-10
##############
Trial  47
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-5.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	0 	0 	
1 	0 	2 	0 	1 	
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (9, 0), (12, 1), (12, 0), (8, 0), (17, 2), (11, 1), (2, 3), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.02092516 -0.02911615  0.84600259  0.5319718 ] loglik -7.624290421708565
accepted/total = 1631/2000 = 0.8155
MAP Policy on Train MDP
map_weights [-0.02092516 -0.02911615  0.84600259  0.5319718 ]
map reward
-0.02	-0.02	-0.02	-0.03	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.03	-0.02	0.85	-0.02	-0.03	
-0.02	-0.02	-0.02	-0.02	-0.02	
-0.02	-0.02	-0.02	-0.02	-0.02	
Map policy
>	>	v	v	v	
>	v	v	v	<	
>	>	<	<	<	
>	^	^	^	<	
>	^	^	^	<	
MEAN policy on Train MDP
mean_weights [ 0.06291594 -0.35533508  0.61413821  0.13406824]
mean reward
0.06	0.06	0.06	-0.36	0.06	
0.06	0.06	0.06	0.06	0.06	
-0.36	0.06	0.61	0.06	-0.36	
0.06	0.06	0.06	0.06	0.06	
0.06	0.06	0.06	0.06	0.06	
mean policy
>	>	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
Optimal Policy
>	>	v	v	v	
>	>	v	v	<	
>	>	>	<	<	
>	^	^	^	<	
>	^	^	^	<	
MAP policy loss 2.1631370873478883e-06
Mean policy loss -6.092800187984526e-10
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
>	>	v	v	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	>	^	
^	^	v	v	^	
reward
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	-1.00	-5.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-100.00	-100.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	1 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
MAP on testing env
map_weights [-0.02092516 -0.02911615  0.84600259  0.5319718 ]
map reward
-0.02	-0.02	-0.02	-0.03	-0.02	
-0.02	-0.02	-0.02	-0.03	-0.02	
-0.02	-0.02	0.85	-0.02	-0.02	
-0.02	-0.02	0.53	0.53	-0.02	
-0.02	-0.03	-0.02	-0.02	-0.02	
Map policy
v	v	v	v	v	
v	>	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
^	>	^	^	^	
MEAN policy on test env
mean_weights [ 0.06291594 -0.35533508  0.61413821  0.13406824]
mean reward
0.06	0.06	0.06	-0.36	0.06	
0.06	0.06	0.06	-0.36	0.06	
0.06	0.06	0.61	0.06	0.06	
0.06	0.06	0.13	0.13	0.06	
0.06	-0.36	0.06	0.06	0.06	
mean policy
>	>	v	v	v	
>	v	v	v	v	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	<	
features
0 	0 	0 	1 	0 	
0 	0 	0 	1 	0 	
0 	0 	2 	0 	0 	
0 	0 	3 	3 	0 	
0 	1 	0 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	v	
>	>	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	^	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
>	v	v	<	v	
>	v	v	<	v	
>	>	^	<	<	
>	^	^	^	^	
^	^	^	<	^	
-------- IRD Solution -------
ird reward
-12.45	-12.45	-12.45	-13.14	-12.45	
-12.45	-12.45	-12.45	-13.14	-12.45	
-12.45	-12.45	-12.28	-12.45	-12.45	
-12.45	-12.45	-13.01	-13.01	-12.45	
-12.45	-13.14	-12.45	-12.45	-12.45	
ird policy
>	v	v	v	v	
>	v	v	^	v	
>	>	^	<	<	
>	^	^	<	^	
^	v	v	v	^	
MAP policy loss 236.92316017329995
mean policy loss 850.4621853983556
robust policy loss 63.874197961038135
regret policy loss 0.36444825900509836
ird policy loss -2.264135570001491e-09
MAP lava occupancy 2.3788635983002075
Mean lava occupancy 2.3788635983002075
Robust lava occupancy 0.6451938887355313
Regret lava occupancy 0.0036812955572200017
IRD lava occupancy 2.4422564257052935e-12
##############
Trial  48
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	>	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	1 	0 	0 	
demonstration
[(0, 1), (7, 3), (12, 2), (12, 1), (12, 0), (9, 3), (17, 2), (11, 1), (2, 3), (14, 0), (4, 3), (13, 0), (1, 1), (12, 3)]
w_map [-0.42233639 -0.42805607  0.75000586 -0.27548354] loglik -7.624603672353601
accepted/total = 1671/2000 = 0.8355
MAP Policy on Train MDP
map_weights [-0.42233639 -0.42805607  0.75000586 -0.27548354]
map reward
-0.42	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.43	-0.42	-0.42	-0.42	
-0.42	-0.42	0.75	-0.42	-0.42	
-0.42	-0.42	-0.42	-0.42	-0.42	
-0.42	-0.42	-0.43	-0.42	-0.42	
Map policy
v	>	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.15168661 -0.27873806  0.67784712 -0.11427966]
mean reward
0.15	0.15	0.15	0.15	0.15	
0.15	-0.28	0.15	0.15	0.15	
0.15	0.15	0.68	0.15	0.15	
0.15	0.15	0.15	0.15	0.15	
0.15	0.15	-0.28	0.15	0.15	
mean policy
>	>	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
Optimal Policy
>	>	v	v	v	
v	v	v	<	<	
>	>	v	<	<	
>	^	^	^	<	
^	^	v	^	^	
MAP policy loss 2.67922196633269e-07
Mean policy loss 5.509524993868586e-07
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	>	
>	v	v	<	<	
>	>	>	<	<	
>	>	^	<	^	
^	^	^	^	<	
reward
-1.00	-5.00	-5.00	-100.00	-1.00	
-5.00	-1.00	-1.00	-1.00	-100.00	
-1.00	-1.00	1.00	-1.00	-5.00	
-1.00	-1.00	-1.00	-5.00	-5.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	1 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	2 	0 	1 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.42233639 -0.42805607  0.75000586 -0.27548354]
map reward
-0.42	-0.43	-0.43	-0.28	-0.42	
-0.43	-0.42	-0.42	-0.42	-0.28	
-0.42	-0.42	0.75	-0.42	-0.43	
-0.42	-0.42	-0.42	-0.43	-0.43	
-0.42	-0.42	-0.43	-0.42	-0.42	
Map policy
v	v	v	v	v	
>	v	v	v	<	
>	>	>	<	<	
>	>	^	<	^	
^	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.15168661 -0.27873806  0.67784712 -0.11427966]
mean reward
0.15	-0.28	-0.28	-0.11	0.15	
-0.28	0.15	0.15	0.15	-0.11	
0.15	0.15	0.68	0.15	-0.28	
0.15	0.15	0.15	-0.28	-0.28	
0.15	0.15	-0.28	0.15	0.15	
mean policy
v	v	v	v	<	
>	>	v	v	<	
>	>	>	<	<	
>	>	^	<	^	
^	^	^	^	<	
features
0 	1 	1 	3 	0 	
1 	0 	0 	0 	3 	
0 	0 	2 	0 	1 	
0 	0 	0 	1 	1 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
v	v	v	v	v	
>	v	v	<	<	
>	>	>	<	<	
>	>	^	<	^	
^	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	v	>	v	v	
>	>	v	v	<	
>	>	>	<	^	
>	>	^	<	^	
^	^	^	^	<	
-------- IRD Solution -------
ird reward
-13.10	-13.69	-13.69	-13.66	-13.10	
-13.69	-13.10	-13.10	-13.10	-13.66	
-13.10	-13.10	-13.00	-13.10	-13.69	
-13.10	-13.10	-13.10	-13.69	-13.69	
-13.10	-13.10	-13.69	-13.10	-13.10	
ird policy
v	v	v	v	<	
>	>	v	<	<	
>	>	^	<	<	
>	>	^	<	^	
^	^	^	^	<	
MAP policy loss 20.62679059225909
mean policy loss 19.335549365339382
robust policy loss 19.33554488421956
regret policy loss 19.33554485347198
ird policy loss 19.335544854211506
MAP lava occupancy 0.23750000101542895
Mean lava occupancy 0.23750000101542895
Robust lava occupancy 0.2375000002496653
Regret lava occupancy 0.23750000000036756
IRD lava occupancy 0.23750000000189023
##############
Trial  49
##############

[  -1   -5    1 -100]
[  -1   -5    1 -100]
===========================
Training MDP with No Lava
===========================
Optimal Policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	>	^	^	^	
^	v	v	^	^	
reward
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-1.00	-1.00	
-1.00	-5.00	-5.00	-1.00	-1.00	
-1.00	-1.00	-1.00	-1.00	-1.00	
features
0 	0 	0 	0 	0 	
0 	0 	0 	0 	0 	
0 	0 	2 	0 	0 	
0 	1 	1 	0 	0 	
0 	0 	0 	0 	0 	
demonstration
[(12, 2), (7, 3), (8, 3), (12, 1), (3, 3), (12, 0), (4, 0), (11, 1), (10, 1), (0, 3), (13, 0), (5, 3)]
w_map [-0.04238106 -0.16417831  0.97857965  0.11675269] loglik -6.761570043329698
accepted/total = 1574/2000 = 0.787
MAP Policy on Train MDP
map_weights [-0.04238106 -0.16417831  0.97857965  0.11675269]
map reward
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
-0.04	-0.04	0.98	-0.04	-0.04	
-0.04	-0.16	-0.16	-0.04	-0.04	
-0.04	-0.04	-0.04	-0.04	-0.04	
Map policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	^	^	^	^	
MEAN policy on Train MDP
mean_weights [ 0.03327952 -0.46824908  0.57749706  0.2784341 ]
mean reward
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
0.03	0.03	0.58	0.03	0.03	
0.03	-0.47	-0.47	0.03	0.03	
0.03	0.03	0.03	0.03	0.03	
mean policy
>	v	v	v	<	
>	v	v	<	<	
>	>	^	<	<	
^	^	^	^	^	
^	v	^	^	^	
Optimal Policy
>	v	v	v	<	
>	>	v	v	<	
>	>	^	<	<	
^	>	^	^	^	
^	v	v	^	^	
MAP policy loss 1.1917689982827995e-06
Mean policy loss -1.1290227919236173e-09
===========================
Testing MDP with Lava
===========================
Opt Policy under true reward
v	v	v	v	v	
v	>	v	<	<	
>	>	^	<	<	
^	>	^	<	<	
>	^	^	^	^	
reward
-1.00	-100.00	-100.00	-1.00	-1.00	
-1.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	1.00	-5.00	-1.00	
-5.00	-5.00	-1.00	-1.00	-1.00	
-1.00	-1.00	-5.00	-1.00	-1.00	
features
0 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
MAP on testing env
map_weights [-0.04238106 -0.16417831  0.97857965  0.11675269]
map reward
-0.04	0.12	0.12	-0.04	-0.04	
-0.04	-0.16	-0.04	-0.04	-0.04	
-0.04	-0.04	0.98	-0.16	-0.04	
-0.16	-0.16	-0.04	-0.04	-0.04	
-0.04	-0.04	-0.16	-0.04	-0.04	
Map policy
>	>	v	<	<	
>	v	v	<	<	
>	>	<	<	<	
^	^	^	<	<	
>	^	^	^	<	
MEAN policy on test env
mean_weights [ 0.03327952 -0.46824908  0.57749706  0.2784341 ]
mean reward
0.03	0.28	0.28	0.03	0.03	
0.03	-0.47	0.03	0.03	0.03	
0.03	0.03	0.58	-0.47	0.03	
-0.47	-0.47	0.03	0.03	0.03	
0.03	0.03	-0.47	0.03	0.03	
mean policy
>	>	v	<	<	
<	>	v	<	<	
>	>	<	<	^	
^	^	^	<	<	
>	^	^	^	^	
features
0 	3 	3 	0 	0 	
0 	1 	0 	0 	0 	
0 	0 	2 	1 	0 	
1 	1 	0 	0 	0 	
0 	0 	1 	0 	0 	
------ Robust Solution ---------
Policy for lambda=0.0 and alpha=0.95
>	>	v	<	<	
<	v	v	<	<	
>	>	<	<	<	
^	>	^	<	<	
>	^	^	^	<	
------ Regret Solution ---------
expert u_sa [11.58840254  0.          8.29318688  0.        ]
Policy for lambda=0.0 and alpha=0.95
v	<	v	v	v	
v	>	v	<	<	
>	>	^	<	>	
^	^	^	<	<	
>	^	^	^	^	
-------- IRD Solution -------
ird reward
-12.97	-13.51	-13.51	-12.97	-12.97	
-12.97	-13.48	-12.97	-12.97	-12.97	
-12.97	-12.97	-12.81	-13.48	-12.97	
-13.48	-13.48	-12.97	-12.97	-12.97	
-12.97	-12.97	-13.48	-12.97	-12.97	
ird policy
v	>	v	v	v	
v	>	v	<	<	
>	>	^	<	>	
^	>	^	<	<	
>	^	^	^	^	
MAP policy loss 33.8633375565787
mean policy loss 68.18625034051836
robust policy loss 129.6290853010825
regret policy loss 0.05868991202779347
ird policy loss 2.1946229772709658e-10
MAP lava occupancy 0.21745276159784827
Mean lava occupancy 0.21745276159784827
Robust lava occupancy 1.3033359391491317
Regret lava occupancy 2.1257397872683556e-12
IRD lava occupancy 2.3134825846849293e-12
